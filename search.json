[{"title":"03 条形图","url":"/2021/06/21/MLearning/matplotlib/03_barGraph/","content":"\n## 竖着画图\n![](01.png)\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmean_values = [1, 2, 3]\nvariance = [0.2, 0.4, 0.5]\nbar_label = [\"bar1\", \"bar2\", \"bar3\"]\n\n# range(3)等价于 range(0, 3), 是 [0, 1, 2] 没有3.\nx_pos = list(range(len(bar_label)))\n\n# plt.bar: 表示竖着画图.\n# yerr: 梯形图上方的误差棒\n# alpha: 条形图透明程度\n# color: 条形图颜色\nplt.bar(x_pos, mean_values, yerr=variance, color='g', alpha=0.3)\n\n# zip(): 将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表\n# list(zip(mean_values, variance)) 返回： [(1, 0.2), (2, 0.4), (3, 0.5)]\n# max(zip(mean_values, variance)) 返回: (3, 0.5)\nmax_y = max(zip(mean_values, variance))\n\nplt.ylim([0, (max_y[0] + max_y[1]) * 1.2])\nplt.ylabel(\"variable y\")\nplt.xticks(x_pos, bar_label)\nplt.show()\n```\n\n\n\n## 横着画图\n![](02.png)\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# 生成 [1 2 3] 数组\nx1 = np.array([1, 2, 3])\nx2 = np.array([2, 2, 3])\nbar_labels = [\"bar1\", \"bar2\", \"bar3\"]\nfig = plt.figure(figsize=(8, 6))\n\n# 返回 [0 1 2] 数组\ny_pos = np.arange(len(x1))\n\n# 返回 [0, 1, 2] 数组\ny_pos = [x for x in y_pos]\n\n# 横着画图\nplt.barh(y_pos, x1, color='y', alpha=0.5)\nplt.barh(y_pos, -x2, color='b', alpha=0.5)\n\nplt.ylim(-1, len(x1) + 1)\nplt.xlim(-max(x2) - 1, max(x1) + 1)\nplt.show()\n```\n\n\n\n## 斜向上图\n![](03.png)\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.linspace(0, 10, 200)\ny1 = x * 2 + 1\ny2 = x * 3 + 1.2\n\ny_mean = x * 0.5 * np.cos(2 * x) + 2.5 * x + 1.1\nfig, ax = plt.subplots()\nax.fill_between(x, y1, y2, color='red')\nax.plot(x, y_mean, color='black')\nplt.show()\n```\n\n\n## 误差棒\n![](04.png)\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nmean_values = [1, 2, 3]\nvariance = [0.2, 0.4, 0.5]\nbar_label = ['bar1', 'bar2', 'bar3']\n\nx_pos = list(range(len(bar_label)))\n# yerr: 误差范围\n# alpha: 透明程度\nplt.bar(x_pos, mean_values, yerr=variance, alpha=0.3)\nmax_y = max(zip(mean_values, variance))\nplt.ylim([0, (max_y[0] + max_y[1]) * 1.2])\nplt.ylabel('variable y')\nplt.xticks(x_pos, bar_label)\nplt.show()\n```\n\n## 分组条形图\n![](05.png)\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ngreen_data = [1, 2, 3]\nblue_data = [6, 5, 4]\nred_data = [9, 7, 8]\nlabels = ['group 1', 'group 2', 'group 3']\n\npos = list(range(len(green_data)))\nwidth = 0.2\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nbar0 = plt.bar(pos, green_data, width, alpha=0.5, color='g', label=labels[0])\n# 画最小值所在位置的虚线\nplt.hlines(min(green_data), -1, len(labels) + width * 2, linestyles='dashed')\n\nplt.bar([p + width for p in pos],\n        blue_data,\n        width,\n        alpha=0.5,\n        color='b',\n        label=labels[1])\nplt.hlines(min(blue_data), -1, len(labels) + width * 2, linestyles='dashed')\n\nplt.bar([p + width * 2 for p in pos],\n        red_data,\n        width,\n        alpha=0.5,\n        color='r',\n        label=labels[2])\nplt.hlines(min(red_data), -1, len(labels) + width * 2, linestyles='dashed')\n\nplt.title(\"Test Multi Group\")\nplt.xlabel(\"Color Group\")\nplt.ylabel(\"Color Data\")\n\nplt.savefig('05.png')\nplt.show()\n\n```\n\n\n## color map\n![](06.png)\n``` py\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as col\nimport matplotlib.cm as cm\n\nmean_values = range(10, 18)\nx_pos = range(len(mean_values))\n\ncmap1 = cm.ScalarMappable(\n    col.Normalize(min(mean_values), max(mean_values), cm.hot))\ncmap2 = cm.ScalarMappable(col.Normalize(0, 20, cm.hot))\n\nplt.subplot(121)\nplt.bar(x_pos, mean_values, color=cmap1.to_rgba(mean_values))\n\nplt.subplot(122)\nplt.bar(x_pos, mean_values, color=cmap2.to_rgba(mean_values))\n\nplt.savefig(\"matplotlib/06.png\")\nplt.show()\n\n```\n\n\n## 条形图填充\n![](07.png)\n``` py\nimport matplotlib.pyplot as plt\n\npatterns = ('_', '+', 'x', '\\\\', '*', 'o', 'O', '.')\nfig = plt.gca()\n\nmean_value = range(1, len(patterns) + 1)\nx_pos = list(range(len(mean_value)))\n\nbars = plt.bar(x_pos, mean_value, color='white')\n\nfor bar, pattern in zip(bars, patterns):\n    bar.set_hatch(pattern)\n\nplt.savefig(\"matplotlib/07.png\")\nplt.show()\n```\n\n\n\n\n\n\n","tags":["MLearning"],"categories":["MLearning","matplotlib"]},{"title":"c union","url":"/2021/06/21/language/c/union/","content":"\n## 定义\n\n* 和结构体不同的是, 结构体的每个成员都是占用一块独立的存储空间, 而共用体所有的成员都占用同一块存储空间\n* 和结构体一样, 共用体在使用之前必须先定义共用体类型, 再定义共用体变量\n定义共用体类型格式:\n``` c\nunion 共用体名{\n    数据类型 属性名称;\n    数据类型 属性名称;\n    ...   ....\n};\n```\n定义共用体类型变量格式:\n``` c\nunion 共用体名 共用体变量名称;\n```\n\n**(特点: 由于所有属性共享同一块内存空间, 所以只要其中一个属性发生了改变, 其它的属性都会受到影响){.red}**\n\n示例:\n\n``` c\n    union Test{\n        int age;\n        char ch;\n    };\n    union Test t;\n    printf(\"sizeof(p) = %i\\n\", sizeof(t));\n\n    t.age = 33;\n    printf(\"t.age = %i\\n\", t.age); // 33\n    t.ch = 'a';\n    printf(\"t.ch = %c\\n\", t.ch); // a\n    printf(\"t.age = %i\\n\", t.age); // 97\n```\n\n## 应用场景\n\n1. 通信中的数据包会用到共用体，因为不知道对方会发送什么样的数据包过来，用共用体的话就简单了，定义几种格式的包，收到包之后就可以根据包的格式取出数据。\n2. 节约内存。如果有2个很长的数据结构，但不会同时使用，比如一个表示老师，一个表示学生，要统计老师和学生的情况，用结构体就比较浪费内存，这时就可以考虑用共用体来设计。\n3. 某些应用需要大量的临时变量，这些变量类型不同，而且会随时更换。而你的堆栈空间有限，不能同时分配那么多临时变量。这时可以使用共用体让这些变量共享同一个内存空间，这些临时变量不用长期保存，用完即丢，和寄存器差不多，不用维护。\n\n\n\n","categories":["language","c"]},{"title":"c typedef","url":"/2021/06/21/language/c/typedef/","content":"\n## typedef关键字\nC语言不仅􏰀供了丰富的数据类型,而且还允许由用户自己定义类型说明符,也就是说允许由用户为数据类型取“别名”。\n`格式: typedef 原类型名 新类型名;`\n* 其中原类型名中含有定义部分,新类型名一般用大写表示,以便于区别.\n* 有时也可用宏定义来代替typedef的功能,但是宏定义是由预处理完成的,而typedef则是在编译 时完成的,后者更为灵活方便.\n\n\n## 基本数据类型\n``` c\ntypedef int Integer;\ntypedef Integer MyInteger;\nMyInteger a; // 等价于 int a;\n\ntypedef char* String;\nint main(int argc, const char * argv[])\n{\n     String str = \"This is a string!\";\n     return 0;\n}\n```\n\n``` c\n#define String char *\nint main(int argc, const char * argv[])\n{\n    String str = \"This is a string!\";\n     return 0;\n}\n```\n\n## 数组类型\n``` c\ntypedef char NAME[20]; // 表示NAME是字符数组类型,数组长度为20。然后可用NAME 说明变量,\nNAME a; // 等价于 char a[20];\n```\n\n\n## 结构体类型\n;;;id01 第一种形式\n``` c\n struct Person{\n    int age;\n    char *name;\n};\n\ntypedef struct Person PersonType;\n```\n;;;\n\n;;; id01 第二种形式\n``` c\ntypedef struct Person{\n    int age;\n    char *name;\n} PersonType;\n```\n;;;\n\n;;; id01 第三种形式\n``` c\ntypedef struct {\n    int age;\n    char *name;\n} PersonType;\n```\n;;;\n\n\n## 枚举\n;;; id02 第一种形式\n``` c\nenum Sex{\n    SexMan,\n    SexWoman,\n    SexOther\n};\ntypedef enum Sex SexType;\n```\n;;;\n\n;;; id02 第二种形式\n``` c\ntypedef enum Sex{\n    SexMan,\n    SexWoman,\n    SexOther\n} SexType;\n```\n;;;\n\n;;; id02 第三种形式\n``` c\ntypedef enum{\n    SexMan,\n    SexWoman,\n    SexOther\n} SexType;\n```\n;;;\n\n\n## 指针\n* typedef与指向结构体的指针\n``` c\n// 定义一个结构体并起别名\ntypedef struct {\n    float x;\n    float y;\n} Point;\n\n// 起别名\ntypedef Point *PP;\n```\n\n* typedef与指向函数的指针\n``` c\n// 定义一个sum函数，计算a跟b的和\nint sum(int a, int b) {\n    int c = a + b;\n    printf(\"%d + %d = %d\", a, b, c);\n    return c;\n}\ntypedef int (*MySum)(int, int);\n\n// 定义一个指向sum函数的指针变量p\nMySum p = sum;\n```\n\n\n\n\n\n","categories":["language","c"]},{"title":"c static","url":"/2021/06/21/language/c/static/","content":"\n## static对局部变量作用\n\n* 延长局部变量的生命周期,从程序启动到程序退出,但是它并没有改变变量的作用域\n* 定义变量的代码在整个程序运行期间仅仅会执行一次\n``` c\n#include <stdio.h>\nvoid test();\nint main()\n{\n    test();\n    test();\n    test();\n\n    return 0;\n}\nvoid test(){\n    static int num = 0; // 局部变量\n    num++; \n    // 如果不加static输出 1 1 1\n    // 如果添加static输出 1 2 3\n    printf(\"num = %i\\n\", num); \n}\n```\n\n## static对全局变量作用\n\n* 外部变量:可以在其他文件中访问的变量,默认所有全局变量都是外部变量\n* 默认情况下多个同名的全局变量共享一块空间, 这样会导致全局变量污染问题\n* 如果想让某个全局变量只在某个文件中使用, 并且不和其他文件中同名全局变量共享同一块存储空间, 那么就可以使用static\n1. 不用static关键字\n``` c\n/***ds.c***/\n// A文件中的代码\nint num; // 和B文件中的num共享\nvoid test(){\n    printf(\"ds.c中的 num = %i\\n\", num);\n}\n\n/***main.c***/\n// B文件中的代码\n#include <stdio.h>\n#include \"ds.h\"\n\nint num; // 和A文件中的num共享\nint main()\n{\n    num = 666;\n    test(); // test中输出666\n    return 0;\n}\n\n2. 加上static关键字\n``` c\n/***ds.c***/\n// A文件中的代码\nstatic int num; // 不和B文件中的num共享\nvoid test(){\n    printf(\"ds.c中的 num = %i\\n\", num);\n}\n\n/***main.c***/\n// B文件中的代码\n#include <stdio.h>\n#include \"ds.h\"\n\nint num; // 不和A文件中的num共享\nint main()\n{\n    num = 666;\n    test(); // test中输出0\n    return 0;\n}\n```\n\n## static对函数作用\n\n默认情况下所有的函数都是外部函数.  \n内部函数:只能在本文件中访问的函数.  \n\n* 声明一个内部函数\n``` c\nstatic int sum(int num1,int num2);\n```\n\n* 定义一个内部函数\n``` c\nstatic int sum(int num1,int num2)\n{\n  return num1 + num2;\n}\n```\n\n::: info\n如果只有函数声明添加了static与extern, 而定义中没有添加static与extern, 那么无效\n:::\n\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"c Memory Management","url":"/2021/06/21/language/c/memoryManagement/","content":"\n## 进程空间图示\n![](01.png)\n\n\n## 栈内存(Stack)\n* 栈的最大尺寸固定，超出则引起栈溢出.\n* 局部变量过多，过大 或 递归层数太多等就会导致栈溢出\n\n``` c\nint ages[1024000 * 1024000]; // 程序会崩溃, 栈溢出\n```\n\n``` c\n#include <stdio.h>\n\nint main()\n{\n    // 存储在栈中, 内存地址从大到小\n    int a = 10;\n    int b = 20;\n    printf(\"&a = %p\\n\", &a); // &a = 000000000061FE1C\n    printf(\"&b = %p\\n\", &b); // &b = 000000000061FE18\n\n    return 0;\n}\n```\n\n\n## 堆内存(Heap)\n* 堆内存可以存放任意类型的数据，但需要自己申请与释放\n* 堆大小，想像中的无穷大，但实际使用中，受限于实际内存的大小和内存是否连续性\n``` c\nint *p = (int *)malloc(10240 * 1024); // 不一定会崩溃\n```\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n\nint main()\n{\n    // 存储在栈中, 内存地址从小到大, 申请4个字节存储空间\n    int *p1 = malloc(4);\n    *p1 = 10;\n    int *p2 = malloc(4);\n    *p2 = 20;\n   \n    printf(\"p1 = %x\\n\", p1);        // p1 = 9b4a10\n    printf(\"p1 = %x\\n\", p1 + 1);    // p1 + 1 = 9b4a14\n    printf(\"p2 = %x\\n\", p2);        // p2 = 9b4a50\n\n    free(p1);\n    free(p2);\n    return 0;\n}\n```\n\n\n## malloc函数\n\n| 函数声明 | void * malloc(size_t _Size); |\n| :----- | :----- |\n| 所在文件 | stdlib.h |\n| 函数功能 | 申请堆内存空间并返回,所申请的空间并未初始化。 |\n| 常见的初始化方法是 | memset 字节初始化。 |\n| 参数及返回解析 |\n| 参数 | size_t _size 表示要申请的字符数 |\n| 返回值 | void * 成功返回非空指针指向申请的空间, 失败返回 NULL |\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main()\n{\n    /*\n     * malloc\n     * 第一个参数: 需要申请多少个字节空间\n     * 返回值类型: void *\n     */ \n    // 申请4个字节存储空间\n    int *p = (int *)malloc(sizeof(int));\n    printf(\"p = %i\\n\", *p); // 保存垃圾数据, 如: -1163005939\n    /*\n     * 第一个参数: 需要初始化的内存地址\n     * 第二个初始: 需要初始化的值\n     * 第三个参数: 需要初始化对少个字节\n     */ \n    memset(p, 0, sizeof(int)); // 对申请的内存空间进行初始化\n    printf(\"p = %i\\n\", *p); // 初始化为0\n\n    free(p);\n    return 0;\n}\n```\n\n\n## free函数\n::: info\n注意: 通过malloc申请的存储空间一定要释放, 所以malloc和free函数总是成对出现\n:::\n\n| 函数声明 | void free(void *p); |\n| :----- | :----- |\n| 所在文件 | stdlib.h |\n| 函数功能 | 释放申请的堆内存 |\n| 参数及返回解析 |\n| 参数 | void* p 指向手动申请的空间 |\n| 返回值 | void 无返回 |\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main()\n{\n    // 1.申请4个字节存储空间\n    int *p = (int *)malloc(sizeof(int));\n    // 2.初始化4个字节存储空间为0\n    memset(p, 0, sizeof(int));\n    // 3.释放申请的存储空间\n    free(p);\n    return 0;\n}\n```\n\n\n## calloc函数\n\n| 函数声明 | void *calloc(size_t nmemb, size_t size); |\n| :----- | :----- |\n| 所在文件 | stdlib.h |\n| 函数功能 | 申请堆内存空间并返回，所申请的空间，自动清零 |\n| 参数及返回解析 |\n| 参数 | size_t nmemb 所需内存单元数量 |\n| 参数 | size_t size 内存单元字节数量 |\n| 返回值 | void * 成功返回非空指针指向申请的空间, 失败返回 NULL |\n\n::: info\n`malloc` 与 `calloc` 的简单区别为 `malloc` 申请空间后不会自动清零, 而 `calloc` 会自动清零.\n:::\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main()\n{\n    /*\n    // 1.申请3块4个字节存储空间\n    int *p = (int *)malloc(sizeof(int) * 3);\n    // 2.使用申请好的3块存储空间\n    p[0] = 1;\n    p[1] = 3;\n    p[2] = 5;\n    printf(\"p[0] = %i\\n\", p[0]);\n    printf(\"p[1] = %i\\n\", p[1]);\n    printf(\"p[2] = %i\\n\", p[2]);\n    // 3.释放空间\n    free(p);\n    */\n\n    // 1.申请3块4个字节存储空间\n    int *p = calloc(3, sizeof(int));\n    // 2.使用申请好的3块存储空间\n    p[0] = 1;\n    p[1] = 3;\n    p[2] = 5;\n\n    printf(\"p[0] = %i, address = %x\\n\", p[0], &p[0]); // p[0] = 1, address = 994a10\n    printf(\"p[1] = %i, address = %x\\n\", p[1], &p[1]); // p[1] = 3, address = 994a14\n    printf(\"p[2] = %i, address = %x\\n\", p[2], &p[2]); // p[2] = 5, address = 994a18\n    // 3.释放空间\n    free(p);\n\n    return 0;\n}\n```\n\n\n## realloc函数\n\n| 函数声明 | void *realloc(void *ptr, size_t size); |\n| :----- | :----- |\n| 所在文件 | stdlib.h |\n| 函数功能 | 扩容(缩小)原有内存的大小。通常用于扩容，缩小会会导致内存缩去的部分数据丢失. |\n| 参数及返回解析 |\n| 参数 | void * ptr 表示待扩容(缩小)的指针， ptr 为之前用 malloc 或者 calloc 分配的内存地址. |\n| 参数 | size_t size 表示扩容(缩小)后内存的大小. |\n| 返回值 | void* 成功返回非空指针指向申请的空间, 失败返回 NULL. |\n\n* 若参数ptr==NULL，则该函数等同于 malloc\n* 返回的指针，可能与 ptr 的值相同，也有可能不同。若相同，则说明在原空间后面申请，否则，则可能后续空间不足，重新申请的新的连续空间，原数据拷贝到新空间， 原有空间自动释放\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main()\n{\n    // 1.申请4个字节存储空间\n    int *p = NULL;\n    p = realloc(p, sizeof(int)); // 此时等同于malloc\n    // 2.使用申请好的空间\n    *p = 666;\n    printf(\"*p = %i\\n\",  *p);\n    // 3.释放空间\n    free(p);\n\n    return 0;\n}\n```\n\n``` c\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\nint main()\n{\n    // 1.申请4个字节存储空间\n    int *p = malloc(sizeof(int));\n    printf(\"p = %p\\n\", p);\t\t// 0000000000764A10\n    // 如果能在传入存储空间地址后面扩容, 返回传入存储空间地址\n    // 如果不能在传入存储空间地址后面扩容, 返回一个新的存储空间地址\n    p = realloc(p, sizeof(int) * 2);\n    printf(\"p = %p\\n\", p);\t\t// 0000000000764A10\n    // 2.使用申请好的空间\n    *p = 666;\n    printf(\"*p = %i\\n\",  *p);\t// 666\n    // 3.释放空间\n    free(p);\n\n    return 0;\n}\n```\n\n\n\n\n\n","categories":["language","c"]},{"title":"c enum","url":"/2021/06/21/language/c/enum/","content":"\n## 枚举类型\n\n* 在实际问题中,有些变量的取值被限定在一个有限的范围内。例如,一个星期内只有七天,一年只有十二个月,一个班每周有六门课程等等。如果把这些量说明为整型,字符型或其它类型 显然是不妥当的。\n* C语言提供了一种称为“枚举”的类型。在“枚举”类型的定义中列举出所有可能的取值, 被说明为该“枚举”类型的变量取值不能超过定义的范围。\n* 该说明的是,枚举类型是一种基本数据类型,而不是一种构造类型,因为它不能再分解为任何基本类型。\n\n## 枚举类型定义\n* 格式:\n``` c\nenum　枚举名　{\n    枚举元素1,\n    枚举元素2,\n    ……\n};\n```\n* 示例:\n``` c\n// 表示一年四季\nenum Season {\n    Spring,\n    Summer,\n    Autumn,\n    Winter\n};\n```\n\n## 枚举变量\n1. 先定义枚举类型，再定义枚举变量\n``` c\nenum Season {\n    Spring,\n    Summer,\n    Autumn,\n    Winter\n};\nenum Season s;\n```\n\n2. 定义枚举类型的同时定义枚举变量\n``` c\nenum Season {\n    Spring,\n    Summer,\n    Autumn,\n    Winter\n} s;\n```\n\n3. 省略枚举名称，直接定义枚举变量\n``` c\nenum {\n    Spring,\n    Summer,\n    Autumn,\n    Winter\n} s;\n```\n\n## 赋值和使用\n``` c\nenum Season {\nSpring,\nSummer,\nAutumn,\nWinter\n} s;\ns = Spring; // 等价于 s = 0;\ns = 3; // 等价于 s = winter;\nprintf(\"%d\\n\", s);\nint day = 2;\nswitch (day)\n{\ncase Autumn:\n    printf(\"今天是Autumn.\");\n    break;\n\ndefault:\n    break;\n}\n```\n\n``` c\nenum Season {\n    Spring,\n    Summer,\n    Autumn,\n    Winter\n};\n// 也就是说spring的值为0，summer的值为1，autumn的值为2，winter的值为3\n```\n\n``` c\nenum Season {\n    Spring = 9,\n    Summer,\n    Autumn,\n    Winter\n};\n// 也就是说spring的值为9，summer的值为10，autumn的值为11，winter的值为12\n```\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"c define","url":"/2021/06/21/language/c/define/","content":"\n## 预处理指令概念\n* 为了区分预处理指令和一般的C语句，所有预处理指令都以符号“#”开头，并且结尾不用分号.\n* 预处理指令可以出现在程序的任何位置，它的作用范围是从它出现的位置到文件尾。习惯上我们尽可能将预处理指令写在源程序开头，这种情况下，它的作用范围就是整个源程序文件.\n\n## 宏定义\n\n* 被定义为 `宏` 的标识符称为 `宏名`. 在编译预处理时,对程序中所有出现的 `宏名`, 都用宏定义中的字符串去代换,这称为 `宏代换` 或 `宏展开`.\n* `宏定义` 是由源程序中的宏定义命令完成的. `宏代换` 是由预处理程序自动完成的. 在C语言中, `宏` 分为有参数和无参数两种.\n* `宏名` 一般用大写字母，以便与变量名区别开来，但用小写也没有语法错误\n\n## 不带参数的宏定义\n格式: `#define 标识符 字符串`\n* 其中的 `#` 表示这是一条预处理命令。凡是以 `#` 开头的均为预处理命令。\n* `define` 为宏定义命令. `标识符` 为所定义的宏名。\n* `字符串` 可以是常数、表达式、格式串等。\n``` c\n#include <stdio.h>\n\n// 源程序中所有的宏名PI在编译预处理的时候都会被3.14所代替\n#define PI 3.14\n\n// 根据圆的半径计radius算周长\nfloat girth(float radius) {\n    return 2 * PI *radius;\n}\n\nint main ()\n{\n    float g = girth(2);\n\n    printf(\"周长为：%f\", g);\n    return 0;\n}\n```\n\n* `宏名` 的有效范围是从定义位置到文件结束. 如果需要终止宏定义的作用域，可以用 `#undef` 命令\n``` c\n#define PI 3.14\nint main ()\n {\n    printf(\"%f\", PI);\n    return 0;\n}\n#undef PI\nvoid test()\n{\n    printf(\"%f\", PI); // 不能使用\n}\n```\n\n* 定义一个宏时可以引用已经定义的宏名\n``` c\n#define R  3.0\n#define PI 3.14\n#define L  2*PI*R\n#define S  PI*R*R\n```\n\n* 可用宏定义表示数据类型,使书写方便\n``` c\n#define String char *\nint main(int argc, const char * argv[])\n{\n     String str = \"This is a string!\";\n     return 0;\n}\n```\n\n\n## 带参数宏定义\nC语言允许宏带有参数。在宏定义中的参数称为形式参数,在宏调用中的参数称为实际参数。对带参数的宏,在调用中,不仅要宏展开,而且要用实参去代换形参\n* `格式: #define 宏名(形参表) 字符串`\n``` c\n// 第1行中定义了一个带有2个参数的宏average，\n #define average(a, b) (a+b)/2\n\nint main ()\n  {\n  // 第4行其实会被替换成：int a = (10 + 4)/2;，\n      int a = average(10, 4);\n  // 输出结果为：7是不是感觉这个宏有点像函数呢？\n      printf(\"平均值：%d\", a);\n     return 0;\n }\n```\n\n1. 宏名和参数列表之间不能有空格，否则空格后面的所有字符串都作为替换的字符串.\n2. 带参数的宏在展开时，只作简单的字符和参数的替换，不进行任何计算操作。所以在定义宏时，一般用一个小括号括住字符串的参数.\n3. 计算结果最好也用括号括起来\n``` c\n#include <stdio.h>\n// 计算结果用括号括起来\n#define Pow(a) ( (a) * (a) )\n\nint main(int argc, const char * argv[])      {\n// 代码被替换为:int b = ( (10) * (10) ) / ( (2) * (2) );\n// 简化之后：int b = (10 * 10) / (2 *2);，最后输出结果：25\n      int b = Pow(10) / Pow(2);\n\n      printf(\"%d\", b);\n      return 0;\n}\n```\n\n\n## 条件编译\n``` c\n#define SCORE 67\n#if SCORE > 90\n    printf(\"优秀\\n\");\n#elif SCORE > 60\n    printf(\"良好\\n\");\n#else\n    printf(\"不及格\\n\");\n#endif\n```\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"c const","url":"/2021/06/21/language/c/const/","content":"\n## const关键字\n1. 可以定义const常量,具有不可变性\n``` c\nconst int Max=100;\nint Array[Max];\n```\n\n2. 便于进行类型检查,使编译器对处理内容有更多了解,消除了一些隐患\n``` c\n// 编译器就会知道i是一个常量,不允许修改;\n// 可以保护被修饰的东西,防止意外的修改,增强程序的健壮性。 还是上面的例子,如果在 函数体内修改了i,编译器就会报错;\nvoid f(const int i) { .........}\n```\n\n\n## 使用const\n1. 修饰一般常量一般常量是指简单类型的常量。这种常量在定义时,修饰符const可以用在类型说明符前,也可以用在类型说明符后\n``` c\nint const x=2; 或 const int x=2;\n```\n\n当然,我们可以偷梁换柱进行更新: 通过强制类型转换,将地址赋给变量,再作修改即可以改变const常量值.\n``` c\n// const对于基本数据类型, 无论写在左边还是右边, 变量中的值不能改变\nconst int a = 5;\n// a = 666; // 直接修改会报错\n// 偷梁换柱, 利用指针指向变量\nint *p;\np = &a;\n// 利用指针间接修改变量中的值\n*p = 10;\nprintf(\"%d\\n\", a); \nprintf(\"%d\\n\", *p);\n```\n\n2. 修饰常数组(值不能够再改变了)定义或说明一个常数组可采用如下格式\n``` c\nint const a[5]={1, 2, 3, 4, 5};\nconst int a[5]={1, 2, 3, 4, 5};\na[1] = 55; // 错误\n```\n\n3. 修饰函数的常参数const修饰符也可以修饰函数的传递参数,格式如下:void Fun(const int Var); 告诉编译器Var在函数体中的无法改变,从而防止了使用者的一些无 意的或错误的修改\n\n4. 修饰函数的返回值: const修饰符也可以修饰函数的返回值,是返回值不可被改变,格式如下\n``` c\nconst int Fun1();\nconst MyClass Fun2();\n```\n\n5. 修饰常指针\n* const int *A; //const修饰指针,A可变,A指向的值不能被修改\n* int const *A; //const修饰指向的对象,A可变,A指向的对象不可变\n* int *const A; //const修饰指针A, A不可变,A指向的对象可变\n* const int *const A;//指针A和A指向的对象都不可变\n\n\n\n\n","categories":["language","c"]},{"title":"c struct","url":"/2021/06/16/language/c/struct/","content":"\n## 定义与初始化\n\n格式: `struct 结构体名 结构体变量名;`\n\n1. 先定义结构体类型，再定义变量\n``` c\nstruct Student {\n     char *name;\n     int age;\n};\n\nstruct Student stu;\n```\n\n2. 定义结构体类型的同时定义变量\n``` c\nstruct Student {\n    char *name;\n    int age;\n} stu;\n```\n\n3. 匿名结构体定义结构体变量\n``` c\nstruct {\n    char *name;\n    int age;\n} stu;\n```\n::: info\n第三种方法与第二种方法的区别在于,第三种方法中省去了结构体类型名称,而直接给出结构变量,这种结构体最大的问题是结构体类型不能复用.\n:::\n\n\n## 结构体变量初始化\n1. 定义的同时按顺序初始化\n``` c\nstruct Student {\n     char *name;\n     int age;\n};\nstruct Student stu = {\"lnj\", 27};\n\n```\n2. 定义的同时不按顺序初始化\n``` c\nstruct Student {\n     char *name;\n     int age;\n};\nstruct Student stu = {.age = 35, .name = \"lnj\"};\n\n```\n3. 先定义后逐个初始化\n``` c\nstruct Student {\n     char *name;\n     int age;\n};\nstruct Student stu;\nstu.name = \"lnj\";\nstu.age = 35;\n\n```\n4. 先定义后一次性初始化\n``` c\nstruct Student {\n     char *name;\n     int age;\n};\nstruct Student stu;\nstu2 = (struct Student){\"lnj\", 35};\n\n```\n\n\n## 成员访问\n\n一般对结构体变量的操作是以成员为单位进行的，引用的一般形式为： `结构体变量名.成员名`\n``` c\nstruct Student {\n    char *name;\n    int age;\n};\nstruct Student stu;\n// 访问stu的age成员\nstu.age = 27;\nprintf(\"age = %d\", stu.age);\n```\n\n## 作用域\n\n定义局部结构体会 屏蔽 全局同名的结构体.\n\n\n## 结构体数组\n结构体数组和普通数组并无太大差异, 只不过是数组中的元素都是结构体而已\n格式: `struct 结构体类型名称 数组名称[元素个数]`\n``` c\nstruct Student {\n    char *name;\n    int age;\n};\nstruct Student stu[2]; \n```\n1. 定义同时初始化\n``` c\nstruct Student {\n    char *name;\n    int age;\n};\nstruct Student stu[2] = {{\"lnj\", 35},{\"zs\", 18}}; \n```\n\n2. 先定义后初始化\n``` c\nstruct Student {\n    char *name;\n    int age;\n};\nstruct Student stu[2]; \nstu[0] = {\"lnj\", 35};\nstu[1] = {\"zs\", 18};\n```\n\n\n## 结构体指针\n一个指针变量当用来指向一个结构体变量时,称之为结构体指针变量\n格式: `struct 结构名 *结构指针变量名`\n\n通过结构体指针访问结构体成员, 可以通过以下两种方式\n* (*结构指针变量).成员名\n* 结构指针变量->成员名(用熟)\n\n``` c\n// 定义一个结构体变量\nstruct Student stu = {“lnj\", 18};\n\n// 定义一个指向结构体的指针变量\nstruct Student *p;\n\n// 指向结构体变量stu\np = &stu;\n\n// 方式1：结构体变量名.成员名\nprintf(\"name=%s, age = %d \\n\", stu.name, stu.age);\n\n// 方式2：(*指针变量名).成员名\nprintf(\"name=%s, age = %d \\n\", (*p).name, (*p).age);\n\n// 方式3：指针变量名->成员名\nprintf(\"name=%s, age = %d \\n\", p->name, p->age);\n```\n\n::: info\n(*结构指针变量)两侧的括号不可少,因为成员符`.` 的优先级高于`*`.\n如去掉括号写作 `*pstu.num` 则等效于 `*(pstu.num)`,这样,意义就完全不对了.\n:::\n\n\n## 结构体内存分析\n\n* `给结构体变量开辟存储空间和给普通开辟存储空间一样, 会从内存地址大的位置开始开辟`\n* `给结构体成员开辟存储空间和给数组元素开辟存储空间一样, 会从所占用内存地址小的位置开始开辟`\n* `结构体变量占用的内存空间永远是所有成员中占用内存最大成员的倍数(对齐问题)`\n\n::: info\n多实际的计算机系统对基本类型数据在内存中存放的位置有限制,它们会要求这些数据的起始地址的值是 某个数k的倍数,这就是所谓的内存对齐,而这个k则被称为该数据类型的对齐模数(alignment modulus).  \n\n这种强制的要求一来简化了处理器与内存之间传输系统的设计,二来可以提升读取数据的速度。比如这么一种处理器,它每次读写内存的时候都从某个8倍数的地址开始,一次读出或写入8个字节的数据,假如软件能 保证double类型的数据都从8倍数地址开始,那么读或写一个double类型数据就只需要一次内存操作。否则,我们就可能需要两次内存操作才能完成这个动作,因为数据或许恰好横跨在两个符合对齐要求的8字节 内存块上.  \n:::\n\n### 存储空间大小分析\n``` c\n    struct Person{\n        int age; // 4\n        char ch; // 1\n        double score; // 8\n    };\n    struct Person p;\n    printf(\"sizeof = %i\\n\", sizeof(p)); // 16\n```\n* 占用内存最大属性是score, 占8个字节, 所以第一次计算机会先给结构体分配8个字节.\n* 将第一次分配的8个字节分配给age4个,分配给ch1个, 还剩下3个字节\n* 当需要分配给score时, 发现只剩下3个字节, 所以会再次开辟8个字节存储空间\n* 一共开辟了两次8个字节空间, 所以最终p占用16个字节\n\n``` c\n    struct Person{\n        int age; // 4\n        double score; // 8\n        char ch; // 1\n    };\n    struct Person p;\n    printf(\"sizeof = %i\\n\", sizeof(p)); // 24\n```\n* 占用内存最大属性是score, 占8个字节, 所以第一次计算机会先给结构体分配8个字节.\n* 将第一次分配的8个字节分配给age4个,还剩下4个字节\n* 当需要分配给score时, 发现只剩下4个字节, 所以会再次开辟8个字节存储空间\n* 将新分配的8个字节分配给score, 还剩下0个字节\n* 当需要分配给ch时, 发现上一次分配的已经没有了, 所以会再次开辟8个字节存储空间\n* 一共开辟了3次8个字节空间, 所以最终p占用24个字节\n\n## 结构体嵌套\n\n![](01.png)\n\n``` c\nstruct Date{\n     int month;\n     int day;\n     int year;\n}\nstruct  stu{\n     int num;\n    char *name;\n    char sex;\n    struct Date birthday;\n    Float score;\n}\n```\n::: info 注意\n结构体不可以嵌套自己变量,可以嵌套指向自己这种类型的指针\n``` c\nstruct Student {\n    int age;\n    struct Student stu;\n};\n```\n:::\n\n### 对嵌套结构体成员的访问\n``` c\nstruct Date {\n       int year;\n       int month;\n       int day;\n  };\n\n  struct Student {\n      char *name;\n      struct Date birthday;\n };\n\n struct Student stu;\n stu.birthday.year = 1986;\n stu.birthday.month = 9;\n stu.birthday.day = 10;\n```\n\n## 结构体值拷贝\n\n1. 结构体虽然是构造类型, 但是结构体之间赋值是值拷贝, 而不是地址传递\n``` c\n    struct Person{\n        char *name;\n        int age;\n    };\n    struct Person p1 = {\"lnj\", 35};\n    struct Person p2;\n    p2 = p1;\n    p2.name = \"zs\"; // 修改p2不会影响p1\n    printf(\"p1.name = %s\\n\", p1.name); // lnj\n    printf(\"p2.name = %s\\n\", p2.name); //  zs\n```\n\n2. 所以结构体变量作为函数形参时也是值传递, 在函数内修改形参, 不会影响外界实参\n``` c\n#include <stdio.h>\n\nstruct Person{\n    char *name;\n    int age;\n};\n\nvoid test(struct Person per);\n\nint main()\n{\n    struct Person p1 = {\"lnj\", 35};\n    printf(\"p1.name = %s\\n\", p1.name); // lnj\n    test(p1);\n    printf(\"p1.name = %s\\n\", p1.name); // lnj\n    return 0;\n}\nvoid test(struct Person per){\n    per.name = \"zs\";\n}\n```\n\n\n","categories":["language","c"]},{"title":"c sort","url":"/2021/06/16/language/c/sorting_algorithm/","content":"\n## 总览\n\n插, 冒, 选, 希, 快, 堆, 并, 基;\n\n插冒(稳定)选希快堆(不稳定)并基(稳定)\n\n![](01.png)\n\n1. 稳定性\n\n* 归并排序、冒泡排序、插入排序、基数排序是稳定的\n\n* 选择排序、快速排序、希尔排序、堆排序是不稳定的\n\n2. 时间复杂度\n\n最基础的四个算法：冒泡、选择、插入、快排中，快排的时间复杂度最小O(n*log2n)，其他都是O（n2）\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 | 大部分已排序时较好 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 | n小时较好 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 | n小时较好 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 | n大时较好 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 | n大时较好 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 | n大时较好 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n## 插入排序\n插入排序（Insertion-Sort）的算法描述是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入.  \n\n![](insert-sort.gif)\n\n### 排序思路\n\n假设按照升序排序\n1. 从索引为1的元素开始向前比较, 一旦前面一个元素大于自己就与前面的元素交换位置\n2. 直到没有可比较元素或者前面的元素小于自己的时候, 就已经将自己交换到了当前空出来的位置\n\n### 代码实现\n``` c\n#include<stdio.h>\n\nvoid swapElement(int* m, int* n)\n{\n    int tmp = *m;\n    *m = *n;\n    *n = tmp;\n}\n\nint main()\n{\n    // 待排序数组\n    int num[7] = {3, 1, 16, 9, 1, 0, 9};\n\n    // 0.计算待排序数组长度\n    int len = sizeof(num) / sizeof(num[0]);\n\n    // 1.从第一个元素而不是第0个开始依次取出所有用于比较元素\n    for (int i = 1; i < len; i++)\n    {\n        // 2.遍历取出前面元素进行比较\n        for (int j = i; j > 0; j-- )\n        {\n            // 3.如果前面一个元素大于当前元素,就交换位置\n            if (num[j] < num[j - 1])\n            {\n                swapElement(&num[j], &num[j - 1]);\n            }\n        }\n    }\n\n    for (int i = 0; i < len; i++)\n    {\n        printf(\"%i\\n\", num[i]);\n    }\n\n    return 0;\n}\n```\n\n\n## 冒泡排序\n\n冒泡排序(Bubble Sort)是一种简单的排序算法。它重复 地走访过要排序的数列,一次比较两个元素,如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换,也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端.  \n\n![](002.gif)\n\n### 排序思路:\n* 假设按照升序排序\n* 1.从第0个元素开始, 每次都用相邻两个元素进行比较\n* 2.一旦发现后面一个元素小于前面一个元素就交换位置\n* 3.经过一轮比较之后最后一个元素就是最大值\n* 4.排除最后一个元素, 以此类推, 每次比较完成之后最大值都会出现再被比较所有元素的最后\n* 直到当前元素没有可比较的元素, 排序完成\n\n### 代码实现\n\n``` c\n#include<stdio.h>\n\nvoid swapElement(int* m, int* n)\n{\n    int tmp = *m;\n    *m = *n;\n    *n = tmp;\n}\n\nint main()\n{\n    // 待排序数组\n    int num[7] = {3, 1, 16, 9, 1, 0, 9};\n\n    int len = sizeof(num) / sizeof(num[0]);\n\n    for (int i = 0; i < len; i++)\n    {\n        for (int j = 0; j < len - i - 1; j++)\n        {\n            if (num[j] > num[j + 1])\n            {\n                swapElement(&num[j], &num[j + 1]);\n            }\n        }\n    }\n\n    for (int i = 0; i < len; i++)\n    {\n        printf(\"%i\\n\", num[i]);\n    }\n\n    return 0;\n}\n```\n\n\n## 选择排序\n\n选择排序(Selection sort)是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小元素,存放到排序序列的起始位置,然后,再从剩余未排序元素中继续寻找最小元素,然后放到排序序列末尾。以此类推,直到所有元素均排序完毕.  \n\n![](001.gif)\n\n### 排序思路\n\n* 假设按照升序排序\n* 1.用第0个元素和后面所有元素依次比较\n* 2.判断第0个元素是否大于当前被比较元素, 一旦小于就交换位置\n* 3.第0个元素和后续所有元素比较完成后, 第0个元素就是最小值\n* 4.排除第0个元素, 用第1个元素重复1~3操作, 比较完成后第1个元素就是倒数第二小的值\n* 以此类推, 直到当前元素没有可比较的元素, 排序完成\n\n### 代码实现\n\n``` c\n#include<stdio.h>\n\n// 指针(地址)传递\nvoid swapElement(int* m, int* n)\n{\n    int tmp = *m;\n    *m = *n;\n    *n = tmp;\n}\n\nint main()\n{\n    // 待排序数组\n    int num[7] = {3, 1, 16, 9, 1, 0, 9};\n    int len = sizeof(num) / sizeof(num[0]);\n\n    for (int i = 0; i < len - 1; i++)\n    {\n        for (int j = i + 1; j < len; j++)\n        {\n            if (num[i] > num[j])\n            {\n                swapElement(&num[i], &num[j]);\n            }\n\n        }\n    }\n\n    for (int i = 0; i < len; i++)\n    {\n        printf(\"%i\\n\", num[i]);\n    }\n    \n    return 0;\n}\n```\n\n\n## 希尔排序\n\n1959年Shell发明，第一个突破O(n2)的排序算法，是简单插入排序的改进版。它与插入排序的不同之处在于，它会优先比较距离较远的元素。希尔排序又叫缩小增量排序.  \n\n![](shell-sort.gif)\n\n![](shell-01.JPG)\n\n![](shell-02.JPG)\n\n![](shell-03.JPG)\n\n### 排序思路\n1. 希尔排序可以理解为插入排序的升级版, 先将待排序数组按照指定步长划分为几个小数组\n2. 利用插入排序对小数组进行排序, 然后将几个排序的小数组重新合并为原始数组\n3. 重复上述操作, 直到步长为1时,再利用插入排序排序即可\n\n### 代码实现\n``` c\n#include<stdio.h>\n\nvoid swapElement(int* m, int* n)\n{\n    int tmp = *m;\n    *m = *n;\n    *n = tmp;\n}\n\nint main()\n{\n    // 待排序数组\n    int num[10] = {84, 83, 88, 87, 61, 50, 70, 60, 80, 99};\n\n    // 0.计算待排序数组长度\n    int len = sizeof(num) / sizeof(num[0]);\n\n    // 1.计算步长\n    int gap = len / 2;\n\n    do {\n        // 1.从第一个(而非第0个)元素(gap处元素)开始往后依次取出待排序数组的所有元素, 与各自所在当前组前面的元素进行简单插入排序.\n        // 从gap处按顺序取待排序数组元素排序，而不是排完一组元素后再排另外一组元素, 跟上图演示有些不一样.\n        for (int i = gap; i < len; i++)\n        {\n            // 2.遍历取出前面元素进行比较\n            int j = i;\n            while ((j - gap) >= 0)\n            {\n                // 3.如果前面一个元素大于当前元素,就交换位置\n                if (num[j] < num[j - gap])\n                {\n                    swapElement(&num[j], &num[j - gap]);\n                }\n                // 4.继续往当前组的前一元素移动\n                j-= gap;\n            }            \n        }\n        // 每个小数组排序完成, 重新计算步长\n        gap = gap / 2;\n    } while (gap >= 1);\n\n    for (int i = 0; i < len; i++)\n    {\n        printf(\"%i\\n\", num[i]);\n    }\n\n    return 0;\n}\n```\n\n\n\n## 计数排序（Counting Sort）\n\n计数排序是一个非基于比较的排序算法，该算法于1954年由 Harold H. Seward 提出。它的优势在于在 `对一定范围内的整数排序` 时，快于任何比较排序算法\n\n### 排序思路:\n * 1.找出待排序数组最大值\n * 2.定义一个索引最大值为待排序数组最大值的数组\n * 3.遍历待排序数组, 将待排序数组遍历到的值作新数组索引\n * 4.在新数组对应索引存储值原有基础上+1\n\n![](001.png)\n\n### 简单代码实现\n\n``` c\n#include<stdio.h>\n\nint main()\n{\n    // 待排序数组\n    int num[7] = {3, 1, 16, 9, 1, 0, 9};\n    // 用于排序数组\n    int newNums[17] = {0};\n    // 计算待排序数组长度\n    int len = sizeof(num) / sizeof(num[0]);\n    // 编列待排序数组\n    for (int i = 0; i < len; i++)\n    {\n        // 去除待排序数组当前值\n        int index = num[i];\n        // 将待排序数组当前值作为排序数组索引\n        // 将用于排序数组对应索引原有值+1\n        newNums[index] = newNums[index] + 1;\n    }\n\n    // 计算排序数组长度\n    int len2 = sizeof(newNums) / sizeof(newNums[0]);\n    // 输出排序数组索引， 就是排序之后的结果\n    for (int i = 0; i < len2; i++)\n    {\n        // 只有所在索引值大于0，说明此索引值是原待排序数组中的值, 可以输出.\n        for (int j = 0; j < newNums[i]; j++)\n        {\n            printf(\"%i\\n\", i);\n        }\n    }\n\n    return 0;\n}\n```\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"Two dimensional array of c language","url":"/2021/06/11/language/c/two-dimensional-array/","content":"\nReference Link: https://blog.csdn.net/weixin_44617968/article/details/117656810?utm_medium=distribute.pc_feed_v2.none-task-blog-hot_rank_bottoming-8.pc_personrecdepth_1-utm_source=distribute.pc_feed_v2.none-task-blog-hot_rank_bottoming-8.pc_personrec\n\n## 字符串的基本概念\n字符串是位于双引号中的字符序列, 在内存中以 `\\0` 结束,所占字节比实际多一个.  \n![](001.png)\n在C语言中没有专门的字符串变量,通常用一个字符数组来存放一个字符串.  \n* 当把一个字符串存入一个数组时,会把结束符 `\\0` 存入数组,并以此作为该字符串是否结束的标志.\n* 有了 `\\0` 标志后,就不必再用字符数组的长度来判断字符串的长度了.\n``` c\n// \"中间不能包含\\0\", 因为 `\\0` 是字符串的结束标志\n// \\0的作用：字符串结束的标志\nchar name[] = \"c\\0ool\";\nprintf(\"name = %s\\n\",name);\n输出结果: c\n```\n\n## 字符串输出\n\n* `%s` 的本质就是根据传入的name的地址逐个去取数组中的元素然后输出，直到遇到 `\\0` 位置\n``` c\nchar chs[] = \"lnj\";\nprintf(\"%s\\n\", chs);\n```\n\n### 注意点\n1. `\\0` 引发的脏读问题\n``` c\nchar name[] = {'c', 'o', 'o', 'l' , '\\0'};\nchar name2[] = {'l', 'n', 'j'};\nprintf(\"name2 = %s\\n\", name2); // 输出结果: lnjcool\n\n// 实际开发中这样定义\nchar name2[] = {'l', 'n', 'j', '\\0'};\n```\n\n2. 初始化\n``` c\nchar ch[10];\nscanf(\"%s\",ch);\n```\n* 对一个字符串数组, 如果不做初始化赋值, 必须指定数组长度\n* ch最多存放由9个字符构成的字符串，其中最后一个字符的位置要留给字符串的结尾标示‘\\0’\n* 当用scanf函数输入字符串时,字符串中不能含有空格,否则将以空格作为串的结束符\n\n\n## 二维数组的初始化\n\n二维数的初始化可分为两种:\n* 定义的同时初始化\n* 先定义后初始化\n### 定义的同时初始化\n``` c\n// 按行分段赋值\nint a[2][3]={{80,75,92}, {61,65,71}};\n\n// 按行连续赋值\nint a[2][3]={80,75,92,61,65,71};\n\n// 完全初始化,省略第一维的长度\nint a[][3]={{1,2,3},{4,5,6}};\nint a[][3]={1,2,3,4,5,6};\n\n// 部分初始化,可以省略第一维的长度\nint a[][3]={{1},{4,5}};\nint a[][3]={1,2,3,4};\n\n// 指定元素的初始化\nint a[2][3]={[1][2]=10};\nint a[2][3]={[1]={1,2,3}};\n```\n::: info\n注意: 有些人可能想不明白，为什么可以省略行数，但不可以省略列数。也有人可能会问，可不可以只指定行数，但是省略列数？其实这个问题很简单，如果我们这样写：\nint a[2][] = {1, 2, 3, 4, 5, 6}; // 错误写法\n大家都知道，二维数组会先存放第1行的元素，由于不确定列数，也就是不确定第1行要存放多少个元素，所以这里会产生很多种情况，可能1、2是属于第1行的，也可能1、2、3、4是第一行的，甚至1、2、3、4、5、6全部都是属于第1行的\n:::\n\n### 先定义后初始化\n``` c\nint a[2][3];\na[0][0] = 80;\na[0][1] = 75;\na[0][2] = 92;\na[1][0] = 61;\na[1][1] = 65;\na[1][2] = 71;\n```\n\n## 作为函数参数注意点\n\n* 形参写法\n``` c\nvoid test(char cs[2][]) // 错误写法\n{\n    printf(\"我被执行了\\n\");\n}\n\nvoid test(char cs[2][3]) // 正确写法\n{\n    printf(\"我被执行了\\n\");\n}\n\nvoid test(char cs[][3]) // 正确写法\n{\n    printf(\"我被执行了\\n\");\n}\n```\n\n* 二维数组作为函数参数，在被调函数中不能获得其有多少行，需要通过参数传入\n```c\nvoid test(char cs[2][3])\n{\n    int row = sizeof(cs); // 二维数组长度, 输出8, 一维数组(字符串)在内存中以“\\0”结束,所占字节比实际多一个\n    printf(\"row = %zu\\n\", row);\n}\n```\n\n* 二维数组作为函数参数，在被调函数中可以计算出二维数组有多少列\n``` c\nvoid test(char cs[2][3])\n{\n    size_t col = sizeof(cs[0]); // 输出3\n    printf(\"col = %zd\\n\", col);\n}\n```\n\n\n## 值传递\n``` c\n#include <stdio.h>\n\n// 和一位数组一样, 只看形参是基本类型还是数组类型\n// 如果是基本类型在函数中修改形参不会影响实参\nvoid change(char ch){\n    ch = 'n';\n}\nint main()\n{\n    char cs[2][3] = {\n        {'a', 'b', 'c'},\n        {'d', 'e', 'f'}\n    };\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // a\n    change(cs[0][0]);\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // a\n    return 0;\n}\n\n```\n\n\n## 地址传递\n\n``` c\n#include <stdio.h>\n\n// 和一位数组一样, 只看形参是基本类型还是数组类型\n// 如果是数组类型在函数中修改形参会影响实参\nvoid change(char ch[]){\n    ch[0] = 'n';\n}\nint main()\n{\n    char cs[2][3] = {\n        {'a', 'b', 'c'},\n        {'d', 'e', 'f'}\n    };\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // a\n    change(cs[0]);\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // n\n    return 0;\n}\n\n```\n\n``` c\n#include <stdio.h>\n\n// 和一位数组一样, 只看形参是基本类型还是数组类型\n// 如果是数组类型在函数中修改形参会影响实参\nvoid change(char ch[][3]){\n    ch[0][0] = 'n';\n}\nint main()\n{\n    char cs[2][3] = {\n        {'a', 'b', 'c'},\n        {'d', 'e', 'f'}\n    };\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // a\n    change(cs);\n    printf(\"cs[0][0] = %c\\n\", cs[0][0]); // n\n    return 0;\n}\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"c pointer","url":"/2021/06/11/language/c/pointer/","content":"\nReference Link: https://blog.csdn.net/weixin_44617968/article/details/117656810?utm_medium=distribute.pc_feed_v2.none-task-blog-hot_rank_bottoming-8.pc_personrecdepth_1-utm_source=distribute.pc_feed_v2.none-task-blog-hot_rank_bottoming-8.pc_personrec\n\n## 定义初始化\n\nC语言中提供了*来定义指针变量和访问指针变量指向的内存存储空间.  \n  * 在定义变量的时候 * 是一个类型说明符,说明定义的这个变量是一个指针变量\n  * 在不是定义变量的时候 *是一个操作符,代表访问指针所指向存储空间\nC语言中提供了地址运算符&来表示变量的地址。其一般形式为: &变量名.  \n\n``` c\nint *p=NULL; // 定义指针变量\nint a = 5;\nint *p1 = &a;\nprintf(\"a = %d\", *p1); // 访问指针变量\n```\n\n## 野指针\n\n指针没有初始化里面是一个垃圾值,这时候的指针是一个野指针.\n\n![](001.png)\n\n* 野指针可能会导致程序崩溃\n* 野指针访问你不该访问数据\n* 所以指针必须初始化才可以访问其所指向存储区域\n\n## 二级指针\n如果一个指针变量存放的又是另一个指针变量的地址,则称这个指针变量为指向指针的指针变量。也称为“二级指针”.  \n\n![](Secondary-pointer.png)\n\n## 数组指针\n只要一个指针变量保存了数组元素的地址, 我们就称之为数组元素指针.\n![](array-pointer.png)\n\n``` c\nprintf(“%p %p”, &(a[0]), a); //输出结果:0x1100, 0x1100\n```\n\n::: info\n注意: 数组名a不代表整个数组,只代表数组首元素的地址。\n“p=a;”的作用是“把a数组的首元素的地址赋给指针变量p”,而不是“把数组a各元素的值赋给 p”\n:::\n\n### 访问数组元素\n访问数组元素,可用下面两种方法:\n* 下标法, 如a[i]形式\n* 指针法, *(p+i)形式\n\n## 字符串指针\n``` c\n// 数组名保存的是数组第0个元素的地址, 指针也可以保存第0个元素的地址\nchar *str = \"abc\"\n```\n### 注意事项\n1. 可以查看字符串的每一个字符\n``` c\nhar *str = \"lnj\";\nfor(int i = 0; i < strlen(str);i++)\n{\n  printf(\"%c-\", *(str+i)); // 输出结果:l-n-j\n}\n```\n2. 不可以修改字符串内容\n``` c\n//   + 使用字符数组来保存的字符串是保存栈里的,保存栈里面东西是可读可写,所有可以修改字符串中的的字符\n//   + 使用字符指针来保存字符串,它保存的是字符串常量地址,常量区是只读的,所以我们不可以修改字符串中的字符\nchar *str = \"lnj\";\n*(str+2) = 'y'; // 错误\n```\n3. 不能够直接接收键盘输入\n``` c\n// 错误的原因是:str是一个野指针,他并没有指向某一块内存空间\n// 所以不允许这样写如果给str分配内存空间是可以这样用 的\nchar *str;\nscanf(\"%s\", str);\n```\n\n## 函数指针\n为什么指针可以指向一个函数？\n  * 函数作为一段程序，在内存中也要占据部分存储空间，它也有一个起始地址\n  * 函数有自己的地址，那就好办了，我们的指针变量就是用来存储地址的。\n  * 因此可以利用一个指针指向一个函数。其中，函数名就代表着函数的地址。\n指针函数的定义\n  * 格式: `返回值类型 (*指针变量名)(形参1, 形参2, ...);`\n``` c\nint sum(int a,int b)\n{\n    return a + b;\n}\n\nint (*p)(int,int);\np = sum;\n```\n\n### 应用场景\n* 调用函数\n* 将函数作为参数在函数间传递\n\n### 注意点\n* 由于这类指针变量存储的是一个函数的入口地址，所以对它们作加减运算(比如p++)是无意义的.\n* 函数调用中 `(*指针变量名)` 的两边的括号不可少,其中的不应该理解为求值运算,在此处它只是一种表示符号.\n\n\n\n\n\n\n","categories":["language","c"]},{"title":"缩小home并扩充增大root和根目录空间","url":"/2021/06/10/linux/shrink_home_and_expand_root_space/","content":"\nReference Link: https://serverfault.com/questions/771921/how-to-shrink-home-and-add-more-space-on-centos7\n\n## 查看磁盘空间\n\n``` shell\n$ df -h\n  Filesystem               Size  Used Avail Use% Mounted on\n  devtmpfs                  16G  4.0K   16G   1% /dev\n  tmpfs                     16G  4.0K   16G   1% /dev/shm\n  tmpfs                     16G  1.6G   14G  11% /run\n  tmpfs                     16G     0   16G   0% /sys/fs/cgroup\n  /dev/mapper/centos-root   50G   28G   23G  55% /\n  /dev/nvme0n1p2          1014M  221M  794M  22% /boot\n  /dev/nvme0n1p1           200M   12M  189M   6% /boot/efi\n  /dev/mapper/centos-home  410G   13G  398G   4% /home\n\n$ lvs -v\n  LV   VG     #Seg Attr       LSize   Maj Min KMaj KMin Pool Origin Data%  Meta%  Move Cpy%Sync Log Convert LV UUID                                LProfile\n  home centos    1 -wi-ao---- 410.05g  -1  -1  253    2                                                     8fo1p0-XK0U-N3zw-MjcU-ZBFI-JMrp-NhqJN9\n  root centos    1 -wi-ao----  50.00g  -1  -1  253    0                                                     ZXqpMd-OaaD-c3U0-VWDx-2ysu-ptZk-sE02xr\n  swap centos    1 -wi-a----- <15.69g  -1  -1  253    1                                                     7lNXQY-I3Wa-jfTX-20Ti-vbee-7RmK-v5zH14\n\n$ vgs -v\n  VG     Attr   Ext   #PV #LV #SN VSize   VFree VG UUID                                VProfile\n  centos wz--n- 4.00m   1   3   0 475.74g 4.00m G4DGmH-S9n4-BDqF-fbsV-xWNC-2BiL-xYVvar\n\n$ pvs -v\n  PV             VG     Fmt  Attr PSize   PFree DevSize PV UUID\n  /dev/nvme0n1p3 centos lvm2 a--  475.74g 4.00m 475.74g ggeHwe-NAVG-wngq-gpry-cKgt-8HoM-0ZL2J0\n\n```\n\n## 创建备份磁盘目录\n\nmount一个磁盘足够容纳/root和/home目录空间大小, 并mount到一个目录.  \n\n``` shell\n$ lsblk\n  NAME            MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n  sda               8:0    0   3.7T  0 disk\n  ├─sda1            8:1    0    10G  0 part\n  ├─sda2            8:2    0    10G  0 part /mnt/test\n  ├─sda3            8:3    0    30G  0 part\n  ├─sda4            8:4    0   200G  0 part\n  └─sda5            8:5    0   500G  0 part\n\n$ mkfs.ext4 /dev/sda5\n\n$ mount /dev/sda5 /mnt/tmp/\n\n```\n\n## 压缩备份home\n\nBackup the contents of `/home`.\n``` shell\n$ cd /mnt/tmp\n\n// 将/home目录仅打包, 不压缩\n$ tar -cvf /mnt/tmp/home.tgz -C /home .\n//(推荐)将/home目录打包后, 以gzip压缩\n$ tar -czvf /mnt/tmp/home.tgz -C /home .\n```\n\nTest the backup.\n``` shell\n// 查看压缩文件内容\n$ tar -tvf home.tgz\n```\n\n## umount home\nUnmount home\n``` shell\n$ umount /dev/mapper/centos-home\n  umount: /home: target is busy.\n          (In some cases useful info about processes that use\n           the device is found by lsof(8) or fuser(1))\n\n// mount报磁盘busy, 可以用如下命令\n$ umount -l /dev/mapper/centos-home\n```\n\n## remove home logical volume\n``` shell\n// 移除home逻辑卷时候可能会报filesystem in use信息.\n$ lvremove /dev/mapper/centos-home\n  Logical volume centos/home contains a filesystem in use.\n// 解决方法如下\n$ fuser -kuc /dev/mapper/centos-home\n$ lvremove /dev/mapper/centos-home\n  Do you really want to remove active logical volume centos/home? [y/n]: y\n  Logical volume \"home\" successfully removed\n\n// 再次查看发现已经没有/home逻辑卷\n$ lvs -v\n  LV   VG     #Seg Attr       LSize   Maj Min KMaj KMin Pool Origin Data%  Meta%  Move Cpy%Sync Log Convert LV UUID                                LProfile\n  root centos    1 -wi-ao----  50.00g  -1  -1  253    0                                                     ZXqpMd-OaaD-c3U0-VWDx-2ysu-ptZk-sE02xr       \n  swap centos    1 -wi-a----- <15.69g  -1  -1  253    1                                                     7lNXQY-I3Wa-jfTX-20Ti-vbee-7RmK-v5zH14 \n```\n\n## Recreate a logical volume for home\n\nRecreate a new 300GB logical volume for /home, format and mount it.\n\n``` shell\n$ lvcreate -L 300GB -n home centos\n\n$ lvs -v\n  LV   VG     #Seg Attr       LSize   Maj Min KMaj KMin Pool Origin Data%  Meta%  Move Cpy%Sync Log Convert LV UUID                                LProfile\n  home centos    1 -wi-a----- 300.00g  -1  -1  253    2                                                     CbNJO2-DWMb-VvES-3gNv-YufK-VpvN-IQw2ch       \n  root centos    1 -wi-ao----  50.00g  -1  -1  253    0                                                     ZXqpMd-OaaD-c3U0-VWDx-2ysu-ptZk-sE02xr       \n  swap centos    1 -wi-a----- <15.69g  -1  -1  253    1                                                     7lNXQY-I3Wa-jfTX-20Ti-vbee-7RmK-v5zH14       \n\n$ mkfs.xfs /dev/centos/home\n$ mount /dev/mapper/centos-home\n```\n\n## Extend root space\nExtend your /root volume with ALL of the remaining space and resize (-r) the file system while doing so.\n``` shell\n$ lvextend -r -l +100%FREE /dev/mapper/centos-root\n```\n\n## Restore your backup\n``` shell\ntar -xzvf /mnt/tmp/home.tgz -C /home\n```\n\n## Update /etc/fstab\n\nCheck `/etc/fstab` for any mapping of `/home` volume. IF it is using UUID you should update the UUID portion. (Since we created a new volume, UUID has changed).\n\n``` shell\n$ vim /etc/fstab\n  ......\n  /dev/mapper/centos-home /home                   xfs     defaults        0 0\n  ......\n```\n\n## Sync the changes\n``` shell\n$ dracut --regenerate-all --force\n```\n","categories":["linux"]},{"title":"xargs","url":"/2021/06/09/linux/xargs/","content":"\n\n## xargs介绍\n\nxargs 用作替换工具，读取输入数据重新格式化后输出\n\n## 实例\n\n### 读文本数据\n读多行文本数据并一行输出\n\n``` shell\n$ cat test.txt\na b c d e f g\nh i j k l m n\no p q\nr s t\nu v w x y z\n\n$ cat test.txt | xargs\na b c d e f g h i j k l m n o p q r s t u v w x y z\n```\n\n### 获取ping输出\n\n``` shell\nping hci-node01 -c 2 | xargs\n```\n\n\n\n\n","categories":["linux"]},{"title":"环境变量加载顺序","url":"/2021/06/07/linux/environment_variable/","content":"\n\n## 环境变量分类\n\n环境变量可以简单的分成 `用户自定义的环境变量` 以及 `系统级别的环境变量`.  \n**[后加载的环境变量会替换先加载的同名的环境变量]{.red}**  \n\n* 用户级别环境变量定义文件： `~/.bashrc`、`~/.profile`（部分系统为：`~/.bash_profile`）\n* 系统级别环境变量定义文件：`/etc/bashrc`、`/etc/profile`(部分系统为：`/etc/bash_profile`）、`/etc/environment`\n\n另外在用户环境变量中，系统会首先读取`~/.bash_profile`（或者`~/.profile`）文件，如果没有该文件则读取`~/.bash_login`，根据这些文件中内容再去读取`~/.bashrc`.\n\n\n## 加载顺序\n\n在每个文件中的第一行都加上下面这句代码，并相应的把冒号后的内容修改为当前文件的绝对文件名。\n\n``` shell\nexport UU_ORDER=\"$UU_ORDER:/Path/to/this/file\"\n```\n\n**[Linux加载环境变量的顺序如下：]{.red}**\n\n系统环境变量 -> 用户自定义环境变量\n`/etc/environment` -> `/etc/profile` -> `~/.profile`\n\n\n**[Linux加载环境变量的顺序如下：]{.red}**\n\n1. `/etc/environment`\n2. `/etc/profile`\n3. `/etc/bash.bashrc`\n4. `/etc/profile.d/test.sh`\n5. `~/.profile`\n6. `~/.bashrc`\n\n\n## 自定义环境变量\n\n比如在某个项目下定义 `uusama.profile` ，在这个文件中使用 `export` 定义一系列变量，然后在 `~/.profile` 文件后面加上： `sourc uusama.profile`，这样你每次登陆都可以在Shell脚本中使用自己定义的一系列变量。  \n\n\n\n","categories":["linux"]},{"title":"docker container/image 复制(copy)文件/文件夹","url":"/2021/06/07/technologies/docker/docker_copy_file/","content":"\n## container copy files\n\n从容器中 copy 文件或文件夹到 host 机器.\n``` shell\ndocker  cp  <ImageID>:/Image/path/to/file  /Host/path/to/destination\n```\n\n从 host 机器 copy 文件或文件夹到容器中.\n``` shell\ndocker  cp  /Host/path/to/destination  <ImageID>:/Image/path/to/file\n```\n\n## image copy files\n\n从image中直接 copy 文件或文件夹到 host 机器.\n\n;;;id10 第一种 copy file\n``` shell\ndocker run --rm --entrypoint cat <Image-Name>:<Tag>  /Image/path/to/file >  /Host/path/to/destination\n```\n;;;\n\n;;;id10 第二种 copy file/folder\n``` shell\n$ docker cp $(docker create --rm <Image-Name>:<Tag>):/Image/path/to/file  /Host/path/to/destination\n```\n;;;\n\n\n\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"mount/umount","url":"/2021/06/03/linux/mount_umount/","content":"\n## mount\n\n**语法**\n``` shell\nmount [-fnrsvw] [-t vfstype] [-o options] device dir\n```\n\n**实例**\n\n``` shell\n// 将 /dev/hda1 挂在 /mnt 之下。\nmount /dev/hda1 /mnt\n// 将 /dev/hda1 用唯读模式挂在 /mnt 之下。\nmount -o ro /dev/hda1 /mnt\n```\n\n## umount\n\n**语法**\n\n``` shell\numount [-ahnrvV][-t <文件系统类型>][文件系统]\n```\n**参数：**\n\n* -a 卸除/etc/mtab中记录的所有文件系统。\n* -h 显示帮助。\n* -n 卸除时不要将信息存入/etc/mtab文件中。\n* -r 若无法成功卸除，则尝试以只读的方式重新挂入文件系统。\n* -t<文件系统类型> 仅卸除选项中所指定的文件系统。\n* -v 执行时显示详细的信息。\n* -V 显示版本信息。\n[文件系统] 除了直接指定文件系统外，也可以用设备名称或挂入点来表示文件系统。\n\n**实例**\n\n下面两条命令分别通过设备名和挂载点卸载文件系统，同时输出详细信息：\n``` shell\n# umount -v /dev/sda1          通过设备名卸载  \n/dev/sda1 umounted  \n# umount -v /mnt/mymount/      通过挂载点卸载  \n/tmp/diskboot.img umounted \n```\n\n如果设备正忙，卸载即告失败。卸载失败的常见原因是，某个打开的shell当前目录为挂载点里的某个目录：\n``` shell\n# umount -v /mnt/mymount/  \numount: /mnt/mymount: device is busy  \numount: /mnt/mymount: device is busy \n```\n\n\n## 遇到的问题\n### mount挂载磁盘\n\n``` shell\nmount /dev/sda8 /mnt/minio_data/\n  mount: /dev/sda8 is write-protected, mounting read-only\n  mount: unknown filesystem type '(null)'\n```\n\n原因是磁盘分区之后直接挂载出现这样的错误， 解决方法\n``` shell\nmkfs.ext4  /dev/sda8\n```\n不是刚分区的磁盘慎用，防止格式化.\n\n\n### unmount出错\n\nLinux下挂载后的分区或者磁盘某些时候需要umount的时候出现类似“umount: /mnt: target is busy.”等字样，或者“umount: /xxx: device is busy.”。\n\n解决办法：\n\n;;; id001 使用fuser命令处理\n``` shell\n$ yum install psmisc \n\n// 查看挂载的磁盘目录使用进程\n$ fuser -mv /mnt/disk01\n                       USER        PID ACCESS COMMAND\n  /mnt:                root     kernel mount /mnt/disk01\n                       root      13830 ..c.. bash\n// 杀死占用的进程\n$ kill -9 13830\n\n// 发现 bash 退出了，新开shell并再次查看\n$ fuser -mv /mnt/\n                       USER        PID ACCESS COMMAND\n  /mnt:                root     kernel mount /mnt/disk01\n\n// 确认无进程连接后，使用卸载命令\n$ umount /mnt/disk01\n```\n;;;\n\n;;; id001 通过lsof命令处理\n``` shell\n$ lsof /mnt/disk01\n  COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n  bash    16302 root  cwd    DIR   8,17       50   64 /mnt/disk01\n```\n找到PID对应的进程或者服务，然后杀死或者停止相应服务即可.  \n;;;\n\n;;; id001 重启系统\n重启后挂载自动解除.  \n;;;\n\n\n\n","categories":["linux"]},{"title":"shell","url":"/2021/06/01/linux/shell/","content":"\n## 切换目录\n\n``` shell\ntouch a.sh\n\n# 用vi 打开a.sh文件进行编写\n#!/bin/sh  \ncd /data/usr/data/\npwd\nls -l\n```\n\n如果只执行 `./a.sh`,可以看到 `/data/usr/data/` 下的内容. 但shell退出后还是原来的工作目录。\n\n如果执行`source ./a.sh`, shell程序运行完成后不仅能看到 `/data/usr/data/` 下的内容，而且工作目录变成了`/data/usr/data/`.  \n\n","categories":["linux"]},{"title":"01 子图与标注","url":"/2021/06/01/MLearning/matplotlib/01_子图与标注/","content":"\npython中有很多画图库, 基本上都是用matplotlib进行封装\n\n<!-- more -->\n\n## matplotlib\n\n```\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n// 查看matplotlib包所在位置\nprint(mpl.__file__)\n// 输出: /usr/local/lib64/python3.6/site-packages/matplotlib/__init__.py\n\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"-.\", color=\"r\")\n#plt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], linestyle=\"-.\", color=\"r\")\n\nplt.xlabel(\"xlabel\", fontsize=16)\nplt.ylabel(\"ylabel\")\nplt.show()\n#plt.savefig(\"matplotlib/test01.png\")\n\n```\n\n## 线条\n\n### 线条形状\n更多线条参考reference: \nhttps://matplotlib.org/tutorials/index.html\nhttps://matplotlib.org/tutorials/introductory/sample_plots.html\nhttps://matplotlib.org/tutorials/introductory/pyplot.html\nhttps://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_linestyle\nhttps://matplotlib.org/examples/pylab_examples/multicolored_line.html\n\n| 字符 | 类型 |\n| :--------: | :--------: |\n| '-' or 'solid' | 实线solid hline |\n| '_' | 横线点 |\n| '|' | vline |\n| '+' | plus |\n| '--' or 'dashed' | 虚线dashed line |\n| '-.' or 'dashdot' | 虚点线dash-dotted line |\n| ':' or 'dotted' | 点线dotted line |\n| 'None' or ' ' or '' | draw nothing |\n| '.' | 点 |\n| ',' | 像素点 |\n| 'o' | 圆点 |\n| 's' | 正方点 |\n| 'p' | 五角点 |\n| '*' | star |\n| 'h' | 六边形点1 |\n| 'H' | 六边形点2 |\n| 'D' | 实心菱形点 |\n| 'd' | 瘦菱形点 |\n| 'x' | filled |\n| 'v' | 下三角点 |\n| '^' | 上三角点 |\n| '<' | 左三角点 |\n| '>' | 右三角点 |\n| '1' | 下三叉点 |\n| '2' | 上三叉点 |\n| '3' | 左三叉点 |\n| '4' | 右三叉点 |\n\n``` python\nimport matplotlib.pylab as plt\nmarkers = [\n    '.', ',', 'o', 'v', '^', '<', '>', '1', '2', '3', '4', '8', 's', 'p', 'P',\n    '*', 'h', 'H', '+', 'x', 'X', 'D', 'd', '|', '_'\n]\ndescriptions = [\n    'point', 'pixel', 'circle', 'triangle_down', 'triangle_up',\n    'triangle_left', 'triangle_right', 'tri_down', 'tri_up', 'tri_left',\n    'tri_right', 'octagon', 'square', 'pentagon', 'plus (filled)', 'star',\n    'hexagon1', 'hexagon2', 'plus', 'x', 'x (filled)', 'diamond',\n    'thin_diamond', 'vline', 'hline'\n]\nx = []\ny = []\nfor i in range(5):\n    for j in range(5):\n        x.append(i)\n        y.append(j)\nplt.figure()\nfor i, j, m, l in zip(x, y, markers, descriptions):\n    plt.scatter(i, j, marker=m)\n    plt.text(i - 0.15, j + 0.15, s=m + ' : ' + l)\nplt.axis([-0.1, 4.8, -0.1, 4.5])\nplt.tight_layout()\nplt.axis('off')\nplt.show()\n```\n![](line_style.png)\n\n### 线条颜色\nreference:\nhttps://matplotlib.org/tutorials/index.html#colors\n\n```\n// 线条与颜色分开写\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"-.\", color=\"r\")\n// 线条与颜色连在一块写\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"r-.\")\n```\n\n## 绘制多条线\n\n``` python\nplt.plot(tang_numpy, tang_numpy, \"r--\")\nplt.plot(tang_numpy, tang_numpy**2, \"bs\")\n// 上面两行等同于以下一行python代码\nplt.plot(tang_numpy, tang_numpy, \"r--\", tang_numpy, tang_numpy**2, \"bs\")\n```\n\n指定线条宽度, 关键点标志\n```\nx = np.linspace(-10, 10)\ny = np.sin(x)\n\nplt.plot(x,\n         y,\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\t\t\t# 指定x轴值所在位置标志图形\n         markerfacecolor=\"r\",\t# 标志颜色\n         markersize=10,\t\t\t# 标志大小\n         alpha=0.5)\t\t\t\t# 透明程度\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n```\n\n先画图, 再设置参数\n``` python\nx = np.linspace(-10, 10)\ny = np.sin(x)\nline = plt.plot(x, y)\nplt.setp(line,\n         color=\"r\",\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\n         markerfacecolor=\"r\",\n         markersize=10,\n         alpha=0.5)\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n```\n![](line01.png)\n\n\n## 子图\n\n![](subpic01.png)\n\n``` python\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pylab as plt\n\ntang_numpy = np.arange(0, 10, 0.5)\nplt.plot(tang_numpy, tang_numpy, \"r--\")\nplt.plot(tang_numpy, tang_numpy**2, \"bs\")\n# 上面两行等价于下面一行\n#plt.plot(tang_numpy, tang_numpy, \"r--\", tang_numpy, tang_numpy**2, \"bs\")\nplt.show()\n\n#====================================================================================\n\nx = np.linspace(-10, 10)\ny = np.sin(x)\nline = plt.plot(x, y)\nplt.setp(line,\n         color=\"r\",\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\n         markerfacecolor=\"r\",\n         markersize=10,\n         alpha=0.5)\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n\n#====================================================================================\n\nx = np.linspace(-10, 10)\ny = np.sin(x)\n# 211 表示一会要画的图是3行2列的，最后一个1表示子图当中第1个图\nplt.subplot(321)\nplt.plot(x, y, color=\"r\")\nplt.xlabel(\"01XXX\")\nplt.ylabel(\"01YYY\")\nplt.title(\"Test 01\")\n# 在图中坐标(5, 0)处添加信息\nplt.text(5, 0, \"Info\")\n# 添加注释箭头\nplt.annotate(\n    \"zhushi001\",\n    xy=(-5, 0),\n    xytext=(-1, 0.3),\n    arrowprops=dict(\n        facecolor=\"black\",  # 箭头颜色\n        shrink=0.01,  # 箭头大小\n        headlength=20,  # 箭头长度\n        headwidth=10))  # 箭头宽度\n# 添加格子\nplt.grid(True)\n\nplt.subplot(324)\nplt.plot(x, x**2, color=\"b\")\nplt.xlabel(\"01XXX\")\nplt.ylabel(\"01YYY\")\nplt.title(\"Test 03\")\n\nplt.savefig(\"matplotlib\\matplotlib.png\")\nplt.show()\n```\n\n\n","tags":["MLearning"],"categories":["MLearning","matplotlib"]},{"title":"02 风格设置","url":"/2021/06/01/MLearning/matplotlib/02_风格设置/","content":"\n## 查看多少风格\n``` py\nimport matplotlib.pyplot as plt\nimport numpy as np\nprint(plt.style.available)\n```\n\n## 单个风格\n``` py\nx = np.linspace(-10, 10)\ny = np.sin(x)\nplt.style.use('ggplot')  # 使用风格步骤一定要在画图前面, 最好在一开始就设置好.\nplt.plot(x, y)\nplt.show()\n```\n![](01.png)\n\n## 混合风格\n``` py\nx = np.linspace(-10, 10)\ny = np.sin(x)\nplt.style.use(['ggplot', 'bmh'])  # 使用风格步骤一定要在画图前面, 最好在一开始就设置好.\nplt.plot(x, y)\nplt.show()\n```\n![](02.png)\n\n## 歪歪扭扭风格\n``` py\nx = np.linspace(-10, 10)\ny = np.sin(x)\nplt.xkcd()\nplt.plot(x, y)\nplt.show()\n```\n![](03.png)\n","tags":["MLearning"],"categories":["MLearning","matplotlib"]},{"title":"Docker Content Trust(DCT) and Notary","url":"/2021/05/31/technologies/docker/DCT/","content":"\nreference: https://docs.docker.com/engine/security/trust/\n\n<!-- more -->\n\n**`Note:`** 每次重新部署DCT和Notary时候要重启shell终端, 因为设置了很多临时环境变量会导致重新部署Notary不成功!\n```\nunset DOCKER_CONTENT_TRUST_SERVER\nunset DOCKER_CONTENT_TRUST\n```\n\n## Machine configuration\n\n| 机器 | IP | 作用 |\n| :----- | :----- | :------: |\n| master | 10.239.140.65 | 部署了docker registry仓库和notary服务器 |\n| laboratory | 10.239.131.157 | 用来测试上传下载master上的registry中的images |\n\n * master机器：\n```\ncat /etc/hosts\n......\n10.239.140.65 master\n10.239.131.157 laboratory\n127.0.0.1 notary-server\n```\n * laboratory机器:\n```\n10.239.140.65 master notaryserver\n10.239.131.157 laboratory\n```\n\n## master机器\n\n### 部署registry本地仓库\n\n```\n//改变容器运行时存储位置\n$ vim /etc/docker/daemon.json\n{\n\"insecure-registries\" :[\"10.239.140.65:5000\", \"master:5000\"],\n\"registry-mirrors\": [\"https://uxk0ognt.mirror.aliyuncs.com\"],\n\"live-restore\": true,\n\"data-root\": \"/home/<user-name>/data\",\n}\n\n// 重新加载docker daemon, systemctl daemon-reload会重启doker运行时.\n$ systemctl reload docker\n\n// 部署私有仓库\n$ docker run -d -p 5000:5000 -v /home/<user-name>/data/registry:/var/lib/registry --restart=always --name registry registry:2\n```\n\n\n### 安装docker-compose\n```\n$ sudo curl -L \"https://github.com/docker/compose/releases/download/1.28.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n\n// Apply executable permissions to the binary:\n$ sudo chmod +x /usr/local/bin/docker-compose\n```\n\n### 部署notary server\n**`master机器上部署notary server, signer, mysql服务`**\n\nnotary只能在master机器上部署\n\n```\ngit clone https://github.com/theupdateframework/notary.git\ncd notary/fixtures/\n\nvim regenerateTestingCerts.sh\n# Centos添加openssl.cnf路径\n# 在\"notary-server.cnf\"部分的subjectAltName变量修改成如上, 加上master机器IP\n# subjectAltName = DNS:notary-server, DNS:notaryserver, DNS:localhost, IP:10.239.140.65, IP:127.0.0.1\n# 另外 notary-server.key, notary-signer.key, notary-escrow.key默认用已存在的key\n# 需要再在regenerateTestingCerts.sh脚本里添加重新生成这三者\n  ......\n  7 for path in /etc/pki/tls/openssl.cnf /etc/openssl/openssl.cnf /etc/ssl/openssl.cnf /usr/local/etc/openssl/openssl.cnf; do\n  ......\n  56 # Then generate notary-server\n  57 openssl genrsa -out \"notary-server.key\" 3072\n  58 # Use the existing notary-server key\n  ......\n  62 cat > \"notary-server.cnf\" <<EOL\n  63 [notary_server]\n  64 authorityKeyIdentifier=keyid,issuer\n  65 basicConstraints = critical,CA:FALSE\n  66 extendedKeyUsage=serverAuth,clientAuth\n  67 keyUsage = critical, digitalSignature, keyEncipherment\n  68 subjectAltName = DNS:notary-server, DNS:notaryserver, DNS:localhost, IP:10.239.140.65, IP:127.0.0.1\n  69 subjectKeyIdentifier=hash\n  70 EOL\n  ......\n  80 # Then generate notary-signer\n  81 openssl genrsa -out \"notary-signer.key\" 3072\n  82 # Use the existing notary-signer key\n  ......\n  104 # Then generate notary-escrow\n  105 openssl genrsa -out \"notary-escrow.key\" 3072\n  106 # Use the existing notary-escrow key\n  ......\n\n# 执行脚本，最后出错的信息不用管他，用不着.\n./regenerateTestingCerts.sh\n\n\ncd ..    //回到notary根目录\nvim  -O server.Dockerfile signer.Dockerfile 添加如下两行配置proxy\n\tENV http_proxy=http://child-prc.intel.com:913\n\tENV https_proxy=http://child-prc.intel.com:913\ndocker-compose build  // 构建image, 但不会启动容器\ndocker-compose up -d  // 启动容器, -d:后台运行\nmkdir -p ~/.notary && cp cmd/notary/config.json cmd/notary/root-ca.crt ~/.notary\n\n// 拷贝 notary-server.crt到docker可信赖目录\nmkdir -p ~/.docker/tls/127.0.0.1:4443\ncp fixtures/notary-server.crt ~/.docker/tls/127.0.0.1:4443/ca.crt\n```\n\n**`NOTE: `** 完整的regenerateTestingCerts.sh文件请参考[regenerateTestingCerts.sh](../../../../../02/02/technologies/docker/regenerateTestingCerts/)文档\n\n修改**`~/.notary/config.json`**文件内容如下:\n\n```\n{\n    \"trust_dir\" : \"~/.docker/trust\",\n    \"remote_server\": {\n        \"url\": \"https://127.0.0.1:4443\",\n        \"root_ca\": \"root-ca.crt\"\n    }\n}\n```\n所有notary server 配置文件设置信息请参考如下连接:\nhttps://docs.docker.com/notary/reference/server-config/\n\n添加**`trust_dir`**后运行如下命令可以查看已存在的key, 但是需要参考下面下载notary client才可执行\n```\n$ notary key list\n```\n\nAdd **`127.0.0.1 notary-server`** to your **`/etc/hosts`**, or if using docker-machine, add $(docker-machine ip) notary-server).\nYou can run through the examples in the getting started docs and advanced usage docs, but without the**` -s`** (server URL) argument to the **`notary`** command since the server URL is specified already in the configuration, file you copied.\n\nYou can also leave off the **`-d ~/.docker/trust`** argument if you do not care to use **`notary`** with Docker images.\n\n```\ndocker ps | grep notary\n\tbcaa8af05a11        notary_server                    \"/usr/bin/env sh -c …\"   3 hours ago         Up 3 hours          0.0.0.0:4443->4443/tcp, 0.0.0.0:32769->8080/tcp   notary_server_1\n\t8a5f51586629        notary_signer                    \"/usr/bin/env sh -c …\"   3 hours ago         Up 3 hours                                                            notary_signer_1\n\t17920752ae65        mariadb:10.4                     \"docker-entrypoint.s…\"   3 hours ago         Up 3 hours          3306/tcp                                          notary_mysql_1\n```\n\n验证证书:\n```\nopenssl verify -CAfile <(cat fixtures/intermediate-ca.crt fixtures/root-ca.crt) fixtures/notary-server.crt\n```\n\n(可选操作)添加notary根证书到主机 **`/etc/pki/ca-trust/source/anchors/`**目录, 让主机信任证书, 这里可以跳过不设置.\n```\nsudo cp fixtures/root-ca.crt /etc/pki/ca-trust/source/anchors/\nsudo cp fixtures/intermediate-ca.crt /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust\n```\n\n\n测试 connect to notary server\n\n```\nopenssl s_client -connect 10.239.140.65:4443 -CAfile fixtures/root-ca.crt -no_ssl3 -no_ssl2\nopenssl s_client -connect 10.239.140.65:4443 -CAfile ~/.notary/root-ca.crt -no_ssl3 -no_ssl2\n```\n\n### 设置环境变量\n配置notary server端证书到docker目录\n```\n//export DOCKER_CONTENT_TRUST_SERVER=https://<URL>:<PORT>\nexport DOCKER_CONTENT_TRUST_SERVER=https://127.0.0.1:4443\nexport DOCKER_CONTENT_TRUST=1\n```\n\n\n### 生成key和证书.\n\n**可以在master机器上生成key和证书可以用来**`sign image`**, 在laboratory机器上生成key和证书可以用来sign image**, **`此时master机器和laboraty机器作用一样`**\n\nA prerequisite for signing an image is a Docker Registry with a Notary server attached (Such as the Docker Hub ). Instructions for standing up a self-hosted environment can be found here [deploying_notary](https://docs.docker.com/engine/security/trust/deploying_notary/).\n\nTo sign a Docker Image you will need a delegation key pair. These keys can be generated locally using **`$ docker trust key generate`** or generated by a certificate authority.\n\n在master机器和laboratry分别执行以下操作生成key和证书, 名字可以一个叫jeff, 另一个叫tester01.\n```\nopenssl genrsa -out key.pem 3072\nopenssl req -new -sha512 -key key.pem -out key.csr\n\t填写信息直接回车, 到要填写CN处输入*然后回车\n\tCommon Name (eg, your name or your server's hostname) []:* or you-node-hostname 如 'master'\nopenssl x509 -req -sha512 -days 365 -in key.csr -signkey key.pem -out key.crt\nchmod 600 key.pem\nexport DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"12345678\" //有这一步后下面的命令就不需要再输入密码\n\ndocker trust key load key.pem --name jeff\n\t输入passphrase: 12345678\n```\n\n之后会在 **`~/.docker/trust/private/`** 生成相应的key\n\nWithin the Docker CLI we can sign and push a container image with the **`$ docker trust`** command syntax. This is built on top of the Notary feature set, more information on Notary can be found here [notary-get-start](https://docs.docker.com/notary/getting_started/).\n\n\n\n### 签名仓库和镜像\n**私有仓库(如10.239.140.65:5000/python)先前已经存在, 上传 `jeff` 的 `signer key` 到此私有仓库，于此同时初始化此私有仓库**\n```\ndocker trust signer add --key certs/key.crt jeff 10.239.140.65:5000/python\nnotary delegation list 10.239.140.65:5000/python\n```\n\n**`NOTE:`** 通过registry:2 搭建的private repository里的 **`python目录`** 就相当于一个私有仓库, 而其它目录如 **`dir02`** 就是另外的仓库.\n如果用 **`trust signer`** 上传到 **`dir02`** 就会重新生成singer key到 **`dir02`**仓库, 此过程会需要你重新设置 **dir02**仓库singer key的PASSPHRASE\n上面操作后应该是会在宿主机 **`ls ~/.docker/trust/private/`** 生成signer key, 下次操作时候可以注意下.\n\n**签名本地image并上传到private repository**\nuse the delegation private key to sign a particular tag and push it up to the registry：\n使用  **`jeff`** 的 **`signer key`**签名 image并上传到部署的registry私有仓库.\n```\n\tdocker trust sign 10.239.140.65:5000/python:3.9.1-slim-security-01\n```\nRemote trust data for a tag or a repository can be viewed by the $ docker trust inspect command:\n如果sign过的image显示信息如下:\n\n```\n\t$ docker trust inspect --pretty 10.239.140.65:5000/python:3.9.1-slim-security-01\n\tSignatures for 10.239.140.65:5000/python:3.9.1-slim-security-01\n\t\n\tSIGNED TAG               DIGEST                                                             SIGNERS\n\t3.9.1-slim-security-01   b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33   jeff\n\t\n\tList of signers and their keys for 10.239.140.65:5000/python:3.9.1-slim-security-01\n\t\n\tSIGNER              KEYS\n\tjeff                3e6bcd979c6c\n\t\n\tAdministrative keys for 10.239.140.65:5000/python:3.9.1-slim-security-01\n\t\n\t  Repository Key:       fd98d671f8da8902c3202891513c743408039cf747185ce74a3790142a0a2535\n\t  Root Key:     f6ee60c9640fb9ed3a3086c3d884a1fde655a31e5fd0eed70e07ba985c634f95\n```\n\n没有sign过的image显示信息如下, 缺少SIGNED TAG信息:\n```\n\t$ docker trust inspect --pretty 10.239.140.65:5000/python:3.9.1-slim\n\tNo signatures for 10.239.140.65:5000/python:3.9.1-slim\n\t\n\tList of signers and their keys for 10.239.140.65:5000/python:3.9.1-slim\n\t\n\tSIGNER              KEYS\n\tjeff                3e6bcd979c6c\n\t\n\tAdministrative keys for 10.239.140.65:5000/python:3.9.1-slim\n\t\n\t  Repository Key:       fd98d671f8da8902c3202891513c743408039cf747185ce74a3790142a0a2535\n\t  Root Key:     f6ee60c9640fb9ed3a3086c3d884a1fde655a31e5fd0eed70e07ba985c634f95\n```\n\n### 去掉镜像签名\n\n去除Remote Trust data for a tag:\n```\n\tdocker trust revoke 10.239.140.65:5000/python:3.9.1-slim-security-01\n```\n\n使docker 客户端强制实行DCT\nContent trust is disabled by default in the Docker Client. To enable it, set the DOCKER_CONTENT_TRUST environment variable to 1\n```\n\texport DOCKER_CONTENT_TRUST=1\n```\n\nWhen DCT is enabled in the Docker client, **`docker`** CLI commands that operate on tagged images must either have content signatures or explicit content hashes. The commands that operate with DCT are:\n • push\n • build\n • create\n • pull\n • run\n\nFor example, with DCT enabled a docker pull someimage:latest only succeeds if someimage:latest is signed. However, an operation with an explicit content hash always succeeds as long as the hash exists:\n```\n\t$ docker pull 10.239.140.65:5000/python:3.9.1-slim\n\tNo valid trust data for 3.9.1-slim\n\t\n\t$ docker pull 10.239.140.65:5000/python:3.9.1-slim-security-01\n\tPull (1 of 1): 10.239.140.65:5000/python:3.9.1-slim-security-01@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\tsha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33: Pulling from python\n\tDigest: sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\tStatus: Image is up to date for 10.239.140.65:5000/python@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\tTagging 10.239.140.65:5000/python@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33 as 10.239.140.65:5000/python:3.9.1-slim-security-01\n\t10.239.140.65:5000/python:3.9.1-slim-security-01\n\t\n\t$ docker pull 10.239.140.65:5000/python:3.9.1-slim@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\tsha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33: Pulling from python\n\tDigest: sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\tStatus: Image is up to date for 10.239.140.65:5000/python@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\t10.239.140.65:5000/python:3.9.1-slim@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\t\n\t$ docker images | grep python \n\t看不到上面的10.239.140.65:5000/python:3.9.1-slim@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33\n\t\n\t但是可以通过tag命令给此image再添加标签如:\n\t$ docker tag 10.239.140.65:5000/python:3.9.1-slim@sha256:b2013807b8af03d66f60a979f20d4e93e4e4a111df1287d9632c8cfd41ecfa33 10.239.140.65:5000/python:test\n\t$ docker images | grep python\n\t10.239.140.65:5000/python                               test                           edd87973afe0        2 weeks ago         114MB\n```\n\n## laboratory机器\n\n### 配置环境变量\n```\ncat /etc/hosts\n10.239.140.65 master notaryserver\n10.239.131.157 laboratory\n```\n\n设置DCT 可信任变量\n```\nexport DOCKER_CONTENT_TRUST_SERVER=https://10.239.140.65:4443\nexport DOCKER_CONTENT_TRUST=1\n```\n\n### 拷贝master notary证书\nscp master机器的notary-server证书和root-ca证书 到laboratory机器.\n\n```\nmkdir -p ~/.docker/tls/10.239.140.65:4443/\nscp root@10.239.140.65:/root/.docker/tls/127.0.0.1:4443/ca.crt ~/.docker/tls/10.239.140.65:4443/ca.crt\nmkdir -p ~/.notary\nscp root@10.239.140.65:/root/.notary/root-ca.crt ~/.notary/root-ca.crt\n```\n\n编写notary config 文件\n```\ncat ~/.notary/config.json\n{\n        \"trust_dir\" : \"~/.docker/trust\",\n        \"remote_server\": {\n                \"url\": \"https://10.239.140.65:4443\",\n                \"root_ca\": \"root-ca.crt\"\n        }\n}\n```\n\n### 生成key和证书\n在laboratory机器上生成signer **`bob`** 的 key 和 证书, 添加signer **`bob`** 的 key到指定 registry 仓库, 之后sign image\n```\nmkdir certs\ncd certs\nopenssl genrsa -out key.pem 3072\nopenssl req -new -sha512 -key key.pem -out key.csr\n\t填写信息直接回车, 到要填写CN处输入*然后回车\n\tCommon Name (eg, your name or your server's hostname) []:* or you-node-hostname 如 'laboratory'\nopenssl x509 -req -sha512 -days 365 -in key.csr -signkey key.pem -out key.crt\nchmod 600 key.pem\nexport DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE=\"12345678\" //有这一步后下面的命令就不需要再输入密码\n\ndocker trust key load key.pem --name bob\n\t输入passphrase: 12345678\n之后会在 **`~/.docker/trust/private/`** 生成相应的key\n```\n\n### 拷贝master机器上的Repository Key到本机器\n\n``` shell\n# 到master机器上查看 Repository Key\ndocker trust inspect --pretty 10.239.140.65:5000/python\n  Signatures for inspire-dev-nodea:5000/test/hello-world\n  \n  SIGNED TAG          DIGEST                                                             SIGNERS\n  trust-1             1b26826f602946860c279fce658f31050cff2c596583af237d971f4629b57792   yazhan\n  \n  List of signers and their keys for inspire-dev-nodea:5000/test/hello-world\n  \n  SIGNER              KEYS\n  bob                 e2023d903e94\n  yazhan              57e9a0ca4094\n  \n  Administrative keys for inspire-dev-nodea:5000/test/hello-world\n  \n    Repository Key:       8200a5fab04b51f18dec2dc4fc7bd7860aae0d83f717b524aee76b30bc19c093\n    Root Key:     574e81ebb53ec457ffdfe34afd3013e7dcee00dc80fe7a7898853c7f337786a2\n\n# 从master机器上拷贝 Repository Key 到laboratory机器\nscp -r ~/.docker/trust/private/8200a5fab04b51f18dec2dc4fc7bd7860aae0d83f717b524aee76b30bc19c093.key  root@10.239.140.157:~/.docker/trust/private/\n```\n\n### 签名仓库和镜像并上传\n```\n// 添加signer key到10.239.140.65:5000/python-test10仓库\n$ docker trust signer add --key certs/key.crt bob 10.239.140.65:5000/python-test10\n\n// 查看delegation key\n$ notary delegation list 10.239.140.65:5000/python-test10\n  ROLE                PATHS             KEY IDS                                                             THRESHOLD\n  ----                -----             -------                                                             ---------\n  targets/releases    \"\" <all paths>    7e1f01ed1d6d0698d5cd5856ec9be27debed33645d2450f20495b70a8a5db83c    1\n  targets/bob         \"\" <all paths>    7e1f01ed1d6d0698d5cd5856ec9be27debed33645d2450f20495b70a8a5db83c    1\n\n// sign image并上传到10.239.140.65:5000/python-test10仓库\n$ docker trust sign 10.239.140.65:5000/python-test10:3.9.1-slim-security-01\n\n// 再次sign同一个image会提示image的sign已存在\n$ docker trust sign 10.239.140.65:5000/python-test10:3.9.1-slim-security-01\n  Signing and pushing trust metadata for 10.239.140.65:5000/python-test10:3.9.1-slim-security-01\n  Existing signatures for tag 3.9.1-slim-security-01 digest b9c45d9576c0e70dfe19ec5f481ec4b59daf970a5036463fcb523cf8ebf7fc5c from:\n  bob\n  Successfully signed 10.239.140.65:5000/python-test10:3.9.1-slim-security-01\n```\n\n### 查看registry私有仓库\n```\n//查看所有仓库\n$ curl 10.239.140.65:5000/v2/_catalog\n\n//查看指定仓库所有tag\n$ curl 10.239.140.65:5000/v2/python-test10/tags/list\n{\"name\":\"python-test10\",\"tags\":[\"3.9.1-slim-security-01\", \"3.9.1-slim-security-02\"]}\n```\n\n\n## notary client\n**master和laboratory都需要下载notary client, notary client是一个二进制可执行文件.**\n\nIf you would like to use your own Notary server, it is important to use the same or a newer Notary version, as the client for feature compatibility (ex: client version 0.2, server/signer version >= 0.2). (reference: https://docs.docker.com/notary/getting_started/#inspect-a-docker-hub-repository)\n查看nodary server signer版本\n\n```\ndocker images | grep notary\n\nwget https://github.com/theupdateframework/notary/releases/download/v0.6.1/notary-Linux-amd64\nchmod +x notary-Linux-amd64\ncp notary-Linux-amd64 /usr/local/bin/notary\n```\n\n查看notary client版本\n```\n$ notary version\nnotary\n Version:    0.6.1\n Git commit: d6e1431f\n```\n\nThe Notary client used in isolation does not know where the trust repositories are located. So, you must provide the **`-s`** (or long form **`--server`** ) flag to tell the client which repository server it should communicate with.\n\nAdditionally, Notary stores your own signing keys, and a cache of previously downloaded trust metadata in a directory, provided with the **`-d`** flag. When interacting with Docker Hub repositories, you must instruct the client to use the associated trust directory, which by default is found at **`.docker/trust`** within the calling user’s home directory (failing to use this directory may result in errors when publishing updates to your trust data):\n\n```\n$ notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/library/alpine\n   NAME                                 DIGEST                                SIZE (BYTES)    ROLE\n------------------------------------------------------------------------------------------------------\n  2.6      e9cec9aec697d8b9d450edd32860ecd363f2f3174c8338beb5f809422d182c63   1374           targets\n  2.7      9f08005dff552038f0ad2f46b8e65ff3d25641747d3912e3ea8da6785046561a   1374           targets\n  3.1      e876b57b2444813cd474523b9c74aacacc238230b288a22bccece9caf2862197   1374           targets\n  3.2      4a8c62881c6237b4c1434125661cddf09434d37c6ef26bf26bfaef0b8c5e2f05   1374           targets\n  3.3      2d4f890b7eddb390285e3afea9be98a078c2acd2fb311da8c9048e3d1e4864d3   1374           targets\n  edge     878c1b1d668830f01c2b1622ebf1656e32ce830850775d26a387b2f11f541239   1374           targets\n  latest   24a36bbc059b1345b7e8be0df20f1b23caa3602e85d42fff7ecd9d0bd255de56   1377           targets\n```\n\n\n## Rotate keys\nReference: https://github.com/theupdateframework/notary/blob/master/docs/advanced_usage.md#manage-keys\n\n\n\n## 清理notary\n\n```\ncd notary\nrm -rf ~/.notary\nrm -rf ~/.docker\ndocker-compose down\ndocker rmi notary_server:latest\ndocker rmi notary_signer:latest\nunset DOCKER_CONTENT_TRUST_REPOSITORY_PASSPHRASE\nunset DOCKER_CONTENT_TRUST_SERVER\nunset DOCKER_CONTENT_TRUST\n```\n\n### 清理notary key \nRemove the key from the local Docker Trust store\n```\n// 查看有哪些key\nnotary key list\n\n// 清理key\nnotary key remove 1091060d7bfd938dfa5be703fa057974f9322a4faef6f580334f3d6df44c02d1\n```\n以下命令可以直接从本地删除key, 因此避免误删最好备份.\n```\nrm -rf ~/.notary/trust/private/<key-number>\n```\n\n\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"docker 02 private hub & journal","url":"/2021/05/31/technologies/docker/docker_02_private_hub/","content":"\n## Docker 引擎\n目前 Docker 支持 Docker 引 擎、 Docker Hub 、 Docker Cloud 等多种服务 。\n * Docker 引擎：包括支持在桌面系统或云平台安装 Docker，以及为企业提供简单安全弹性的容器集群编排和管理；\n * DockerHub ：官方提供的云托管服务，可以提供公有或私有的镜像仓库；\n * DockerCloud ：官方提供的容器云服务，可以完成容器的部署与管理，可以完整地支持容器化项目，还有 CI 、 CD 功能 \n用户可以通过如下命令检查自己的内核版本详细信息 ：\n\n```shell\nuname -a\ncat /proc/version\n```\n\n## 查看日志\n如果服务工作不正常，可以通过查看 Docker 服务的日志信息来确定问题，例如\n在 RedHat 系统上的系统运行日志文件为 \n\n```\n/var/log/messages\n```\n在 Ubuntu 或 CentOS 系统上可以执行命令\n\n```shell\njournalctl -u docker.service \njournalctl -xe\njournalctl -f -n 10 -u docker.service //滚屏输出10条服务docker.service的log信息\n```\n\n## 访问Docker 仓库 Repository\n分公共仓库和私有仓库\n### 公共仓库\nDocker Hub 是 Docker 官方提供的最大的公共镜像仓库, docker search 命令来查找官方仓库中的镜像\n\n```shell\ndocker search centos\ndocker pull centos\ndocker images 命令来查看下载到本地的镜像\n```\n国内不少云服务商都提供了 Docker 镜像市场包括腾讯云 、 网易云、阿里云等\n下载第三方服务商如 腾讯云 提供的镜像格式为 index.tenxcloud.com/<namespace>/<repository>:<tag>\n\n```shell\ndocker pull index.tenxcloud.com/docker_library/node:latest\n```\n下载后，可以更新镜像的标签，与官方标签保持一致，方便使用：\n\n```shell\ndocker tag index.tenxcloud.com/docker_library/node:latest node:latest\n```\n\n## 搭建本地私有仓库\n### 使用 registry 镜像创建私有仓库\n\n```shell\ndocker search registry --limit=5\t// Hub official website 查找registry镜像\ndocker run -d -p 5000:5000 registry:2\n  Unable to find image 'registry:2' locally\n  2: Pulling from library/registry\n  486039affc0a: Downloading [==========>                                        ]  475.1kB/2.207MB\n  ba51a3b098e6: Download complete\n  8bb4c43d6c8e: Downloading [==>                                                ]  363.8kB/6.824MB\n  6f5f453e5f2d: Download complete\n  42bc10b72f42: Download complete\n\ndocker run -d -p 5000:5000 registry:2\n```\n\n默认情况下，仓库会被创建在容器的 /var/lib/registry 目录下, 容器退出后, 存储的数据也会丢失.\n因此实际开发环境中私有仓库registry存储路径必须绑定到主机目录.\n可以通过 －v 参数来将镜像文件存放在本地的指定路径 。 例如下面的例子将上传的镜像放到/opt/data/registry 目录：\n\n```shell\ndocker run -d -p 5000:5000 -v /opt/data/registry:/var/lib/registry --restart=always --name registry registry:2 \n```\n\n如果创建容器时没有添加自动重启参数 --restart=always ，导致的后果是：当 Docker 服务重启时，容器未能自动启动。\n第一种添加修改该参数(实测有效): \n\n```shell\ndocker container update --restart=always 容器名字或容器ID\n```\n第二种修改该参数；\n\n```\n首先停止容器，不然无法修改配置文件(实测中不用停止容器也修改了, 但是docker服务重启后容器没有重启)\n配置文件路径为：/var/lib/docker/containers/容器ID(容器ID通过 $ docker ps | grep 容器Name 进行查看)\n在该目录下找到一个文件 hostconfig.json ，找到该文件中关键字 RestartPolicy\n修改前配置：\"RestartPolicy\":{\"Name\":\"no\",\"MaximumRetryCount\":0}\n修改后配置：\"RestartPolicy\":{\"Name\":\"always\",\"MaximumRetryCount\":0}\n最后启动容器。\n\n另外也可以查看容器绑定存储文件路径\n\"Binds\":[\"/opt/data/registry1:/var/lib/registry\"]\n没有绑定到主机目录的容器此hostconfig.json文件内容为 \"Binds\":null\n```\n\n此时， 在本地将启动一个私有仓库服务，监听端口为 5000 \n\n```shell\ndocker ps\n  CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                    NAMES\n  e0bb400f0ba6        registry:2             \"/entrypoint.sh /etc…\"   11 hours ago        Up 11 hours         0.0.0.0:5000->5000/tcp   registry\n```\n\n### 从私有仓库上传下载镜像 & 查看\n> 应为docker客户端发送的是https请求，从私有仓库下载镜像时候需要用http请求协议，因此在其它机器上要上传或下载10.239.140.186:5000(me office)机器上的image时候会出问题\n> 需要在其它要下载10.239.140.186机器上image的机器比如10.239.85.243(solu02)上添加并修改如下文件：\nmaster和worker节点都需要添加启动仓库registry:2所在的机器IP和开放的端口，内容和步骤如下:\n\n```shell\nvim /etc/docker/daemon.json\t// 没有的话需要新建这个json文件, 然后添加如下内容, 表示信任10.239.140.186机器5000端口提供的服务\n  {\n  \"insecure-registries\":[\"10.239.140.186:5000\"]\n  \"registry-mirrors\": [\"https://registry.docker-cn.com\"],\n  \"live-restore\": true\n  }\n\nsystemctl daemon-reload\t\t\t// 重新加载daemon\nsystemctl restart docker\t\t\t// 重启docker\nsystemctl enable docker.service\t// 开机默认启动\n\n# 如果是在运行registry容器的机器上重启docker service之后一定要查看registry容器是否重新启动.\ndocker run -d -p 5000:5000 registry:2\n```\n\n> 在solu02机器上从hub公共image库下载个image如ubuntu:latest\n\n```shell\ndcoker pull ubuntu\t\t\t\t// 默认下载latest\ndocker images\n  ➜  ~ docker images\n  REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n  ubuntu              latest              1d622ef86b13        5 days ago          73.9MB\n```\n\n### 上传image\n给要上传的image重新打个标签\n\n```shell\ndocker tag ubuntu:latest 10.239.140.186:5000/solu02_utunbu:latest\n```\n\n使用 docker push 上传标记的镜像：\n\n```shell\ndocker push 10.239.140.186:5000/solu02_utunbu:latest  // 要和 \"/etc/docker/daemon.json中配置的insecure-registries的值一致\"\n  或者 $ docker push master-node:5000/solu02_utunbu:latest\n  ➜  ~ docker push 10.239.140.186:5000/solu02_utunbu:latest\n  The push refers to repository [10.239.140.186:5000/solu02_utunbu]\n  8891751e0a17: Pushed\n  2a19bd70fcd4: Pushed\n  9e53fd489559: Pushed\n  7789f1a3d4e9: Pushed\n  latest: digest: sha256:5747316366b8cc9e3021cd7286f42b2d6d81e3d743e2ab571f55bcd5df788cc8 size: 1152\n  ➜  ~\n```\n\n### 查看image\n在仓库主机10.239.140.186机器上使用curl 查看仓库 10.239.140.186:5000(me office) 中的镜像：\n是用10.239.140.186访问, 还是用在/etc/hosts中将主机IP:10.239.140.186映射为master-node访问,\n主要是跟 /etc/docker/daemon-reload 设置的 insecure-registries 值一致.\n\n```shell\ncurl http://10.239.140.186:5000/v2/_catalog\n  或 $ curl http://master-node:5000/v2/_catalog\n   {\"repositories\":[\"solu02_utunbu\",\"test_ubuntu\",\"test_ubuntu1\",\"docker.io/nginx\"]}\n```\n查看镜像和相应tag:\n\n```shell\ncurl -XGET http://master-node:5000/v2/{image_name}/tags/list\ncurl 10.239.140.186:5000/v2/docker.io/nginx/tags/list\n  {\"name\":\"docker.io/nginx\",\"tags\":[\"alpine\"]}\n```\n\n浏览器输入如下地址查看\nhttp://localhost:5000/v2/_catalog \t\t\t\t\t// 查看所有images\nhttp://localhost:5000/v2/docker.io/nginx/tags/list\t// 查看所有images的tags\n\n### 下载image\n在solu02(10.239.85.243)机器上下载10.239.140.186私有仓库里的image:\n\n```shell\ndocker pull 10.239.140.186:5000/test_ubuntu\n```\n拉去指定tag的image\n\n```shell\ndocker pull 10.239.140.186:5000/docker.io/nginx:alpine\n```\n\n### DNS 解析IP\n用master-node等代替registry运行所在的主机的IP\n\n```xml\nvim /etc/hosts\n  10.239.140.186 master-node\n  10.239.141.101 node-1\n\nvim /etc/docker/daemon.json\n  {\n  \"insecure-registries\": [\"master-node:5000\"]\n  \"registry-mirrors\": [\"https://registry.docker-cn.com\"],\n  \"live-restore\": true\n  }\n```\n\n## 配置docker的HTTP_PROXY, HTTPS_PROXY, NO_PROXY\n\n第一种: 放到一个文件里\n\n```xml\nvim /etc/systemd/system/docker.service.d/http-proxy.conf\n  [Service]\n  Environment=\"HTTP_PROXY=http://proxy:913\" \"NO_PROXY=localhost,127.0.0.1,master-node\"\n```\n\n第二种: 将docker的HTTP_PROXY, HTTPS_PROXY, NO_PROXY放到三个文件\n新建三个文件, 并添加类似的如下信息\n\n```xml\ntouch /etc/systemd/system/docker.service.d/no-proxy.conf\n  [Service]\n  Environment=\"NO_PROXY=10.67.108.211,10.67.109.142,10.67.109.147,10.67.109.144,hci-node01,hci-node02,hci-node03,hci-node04\"\ntouch /etc/systemd/system/docker.service.d/http-proxy.conf\n  [Service]\n  Environment=\"HTTP_PROXY=http://proxy:913/\"\ntouch /etc/systemd/system/docker.service.d/https-proxy.conf\n  [Service]\n  Environment=\"HTTPS_PROXY=http://proxy:913/\"\n```\n\n修改完上面信息之后重启docker\n\n```shell\nsystemctl daemon-reload\nsystemctl restart docker\n\ndocker tag docker.io/nginx:alpine master-node:5000/docker.io/nginx:alpine\ndocker push master-node:5000/docker.io/nginx:alpine\ncurl 10.239.140.186:5000/v2/_catalog\n\n# 或者\ncurl master-node:5000/v2/_catalog\n\n# 遇到访问不了如下面显示信息时候, 原因是通过curl方式访问master-node主机时仍通过linux设置的公司的proxy访问\n<HTML>\n<HEAD><TITLE>Redirection</TITLE></HEAD>\n<BODY><H1>Redirect</H1></BODY>\n</HTML>\n\n# 设置linux的NO_PROXY环境变量, 使curl访问 master-node 时不通过公司的proxy\nexport NO_PROXY=master-node\ncurl master-node:5000/v2/_catalog\n  {\"repositories\":[\"docker.io/nginx\",\"hello-world\",\"hello-world1\",\"hello-world11\"]}\n```\n\n## 遇到的问题\n\n### 503错误\n```\n$ docker pull hci-node01:5000/greenplum-1.13.0/greenplum-operator:v1.13.0\n  Error response from daemon: received unexpected HTTP status: 503 Service Unavailable\n\n$ docker pull 10.67.108.211:5000/storage/kes:0.1\n  Error response from daemon: Get https://10.67.108.211:5000/v2/: net/http: TLS handshake timeout\n```\n解决方法，在docker的配置文件中加上NO_PROXY(确保添加的NO_PROXY中的机器域名正确, 否则也会导致上面问题), 如下：\n```\nvim /etc/systemd/system/docker.service.d/proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"HTTPS_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"NO_PROXY=127.0.0.1,k8s-vip,hci-node01,hci-node02,hci-node03,hci-node04\"\n```\n\n\n\n\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"docker 01 installation and control commands","url":"/2021/05/31/technologies/docker/docker_01_installation/","content":"\n## Docker 引擎\n目前 Docker 支持 Docker 引 擎、 Docker Hub 、 Docker Cloud 等多种服务 。\n * Docker 引擎：包括支持在桌面系统或云平台安装 Docker，以及为企业提供简单安全弹性的容器集群编排和管理；\n * DockerHub ：官方提供的云托管服务，可以提供公有或私有的镜像仓库；\n * DockerCloud ：官方提供的容器云服务，可以完成容器的部署与管理，可以完整地支持容器化项目，还有 CI 、 CD 功能 \n\n<!-- more -->\n用户可以通过如下命令检查自己的内核版本详细信息 ：\n\n```shell\n $ uname -a\n $ cat /proc/version\n```\n\n## Ubuntu18.04 docker环境安装：\n官网: https://www.docker.com/get-started\n查找image: https://hub.docker.com/search?type=image\n\n## 简介\n> Docker 是基于 Go 语言实现的开源容器项目 。 它诞生于 2013 年年初，最初发起者是dotCloud 公司，dotCloud 公司也随之快速发展壮大，在 2013 年年底直接改名为 Docker Inc ，并专注于Docker 相关技术和产品的开发，目前已经成为全球最大的 Docker 容器服务提供商 。 官方网站为 docker.com \n> 在 Linux 基金会最近一次关于“最受欢 迎的 云 计算开源项目”的调查中， Docker 仅次于 2010 年发起的 OpenStack 项目，并仍处于上升趋势 。2014 年， Docker 镜像下载数达到了一百万次， 2015 年直接突破十亿次， 2017 年更是突破了惊人的百亿次\n> Docker 的构想是要实现“ Build , Ship and Run Any App, Anywhere ”，即通过对应用的封装（ Packaging）、分发（ Distribution ）、部署（ Deployment）、运行（ Runtime ）生命周期进行管理，达到应用组件级别的“一次封装 ，到处运行”\n> 与大部分新兴技术的诞生一样， Docker 也并非“从石头缝里蹦出来的”，而是站在前人的肩膀上。 其中最重要的就是 Linux 容器（ Linux Containers, LXC ）技术.\n> 每个容器内运行着一个应用，不同的容器相互隔离，容器之间也可以通过网络互相通信 。 容器的创建和停止十分快速，几乎跟创建和终止原生应用－致.\n> 容器自身对系统资源的额外需求也十分有限，远远低于传统虚拟机。 很多时候，甚至直接把容器当作应用本身也没有任何问题.\n\n## 为什么用Docker\n> 在云时代，开发者创建的应用必须要能很方便地在网络上传播，也就是说应用必须脱离底层物理硬件的限制；同时必须是“任何时间任何地点”可获取的 。 因此，开发者们需要一种新型的创建分布式应用程序的方式，快速分发和部署，而这正是 Docker 所能够提供的最大优势\n> Docker 提供了一种更为聪明的方式，通过容器来打包应用、解藕应用和运行平台。这意味着迁移的时候，只需要在新的服务器上启动需要的容器就可以了，无论新旧服务器是否是同一类型的平台 。 这无疑将帮助我们节约大量的宝贵时间，并降低部署过程出现问题的风险\n> 传统虚拟机方式运行 N 个不同的应用就要启用 N 个虚拟机（每个虚拟机需要单独分配独占的内存、磁盘等资源），而 Docker 只需要启动 N 个隔离得“很薄的”容器，并将应用放进容器内即可 。 应用获得的是接近原生的运行性能\n> 传统方式是在硬件层面实现虚拟化，需要有额外的虚拟机管理应用和虚拟机操作系统层。 Docker 容器是在操作系统层面上实现虚拟化，直接复用本地主机的操作系统，因此更加轻量级 \n\n## docker ce与 docker ee区别\n * Docker Engine改为Docker CE(社区版), 它包含了CLI客户端、后台进程/服务以及API。用户像以前以同样的方式获取。\n  docker-ce是docker公司维护的开源项目，是一个基于moby项目的免费的容器产品；\n\n * Docker Data Center改为Docker EE（企业版）\n  docker-ee是docker公司维护的闭源产品，是docker公司的商业产品；\n这些ce和ee版并不影响Docker Compose以及Docker Machine\ndocker-ce project是docker公司维护，docker-ee是闭源的；\n\n要使用免费的docker，从网页docker-ce上获取；\n\n要使用收费的docker，从网页docker-ee上获取\n\n## docker UCP 介绍\n：：： info\nDocker Universal Control Plane（UCP）是Docker公司在2015年底巴塞罗那的开发者大会上发布的，这是一个跟单信用证，是一个新的Docker支付服务的组合的一部分,旨在帮助运维团队轻松地设置一个集群,使开发人员可以快速部署Dockerized应用。他们构建Docker DataCenter的其中重要的组成部分。\nUCP集群包含两种节点：\nController: 管理集群，并持久化集群配置\nNode：运行容器\n：：：\n\n## 安装curl:\n\n;;;id0 第一种方法:\n\n```shell\nhttps://curl.haxx.se/download.html\ncurl-7.69.1.tar.gz\n./configure --prefix=/usr/local/curl\nmake -j12\nmake install\nln -s /usr/local/curl/bin/curl /usr/bin\nvim ~/.bashrc 添加 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/curl/lib\nsource ~/.bashrc\ncurl --version \t\t// 查看curl版本和支持的协议如http, https\n```\n;;;\n\n;;;id0 第二种方法(推荐，方便快捷):\n\n```shell\napt-get update\napt-get upgrade\napt-get install curl\ncurl --version\n```\n;;;\n\n提前设置好系统的proxy如:\n\n```shell\nexport http_proxy=child-prc.intel.com:913\nexport https_proxy=child-prc.intel.com:913 // https的proxy与上面的http的要一样\n```\n\n## 安装docker\n\n;;;id1 Ubuntu安装docker\n### 卸载旧版本\n\nDocker 的旧版本被称为 docker，docker.io 或 docker-engine 。如果已安装，请卸载它们：\n``` shell\n$ sudo apt-get remove docker docker-engine docker.io containerd runc\n```\n当前称为 Docker Engine-Community 软件包 docker-ce.\n\n### 设置仓库\n在新主机上首次安装 Docker Engine-Community 之前，需要设置 Docker 仓库。之后，您可以从仓库安装和更新 Docker.\n``` shell\nsudo apt-get update\nsudo apt-get install \\\n    apt-transport-https \\\n    ca-certificates \\\n    curl \\\n    gnupg-agent \\\n    software-properties-common\n\n# 添加 Docker 的官方 GPG 密钥：\ncurl -fsSL https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 通过搜索指纹的后8个字符，验证您现在是否拥有带有指纹的密钥。\nsudo apt-key fingerprint 0EBFCD88\n  pub   rsa4096 2017-02-22 [SCEA]\n        9DC8 5822 9FC7 DD38 854A  E2D8 8D81 803C 0EBF CD88\n  uid           [ unknown] Docker Release (CE deb) <docker@docker.com>\n  sub   rsa4096 2017-02-22 [S]\n\n# 使用以下指令设置稳定版仓库\nsudo add-apt-repository \\\n   \"deb [arch=amd64] https://mirrors.ustc.edu.cn/docker-ce/linux/ubuntu/ \\\n  $(lsb_release -cs) \\\n  stable\"\n```\n\n### 安装指定版本docker\n``` shell\n# 在仓库中列出可用版本\napt-cache madison docker-ce\napt-cache madison docker-ce-cli\napt-cache madison containerd.io\n\n# 安装特定版本\napt-get install docker-ce=5:19.03.8~3-0~ubuntu-bionic docker-ce-cli=5:19.03.8~3-0~ubuntu-bionic containerd.io\n```\n\n### 配置docker daemon\n配置docker daemon, 没有就新建.\n\n``` shell\n# 配置docker daemon\nvim  /etc/docker/daemon.json\n{\n\"insecure-registries\": [\"<your-private-repository-node-IP>:5000\"],\n\"registry-mirrors\": [\"https://uxk0ognt.mirror.aliyuncs.com\"],\n\"live-restore\": true\n\"data-root\": \"/home/docker/data\"\n\"userland-proxy\": false,\n\"no-new-privileges\": false,\n\"icc\": false\n}\n\n# mount 磁盘\nmount /dev/sd* /home/docker/data\n```\n\n::: info\n**[\"data-root\"]{.blue}**, 最好设置下，否则docker下载的image和image运行时的容器layer挂在在系统上的目录是在 [/var/lib/docker/...]{.blue}, 而此目录数据默认是存储在根 [“/”]{.blue}目录下，我们装系统默认给根目录分区是50G大小，根目录快用完时, 会发现系统上有些docker容器被默认退出, 下载的image也被删除.\n:::\n\n### 配置docker proxy\n``` shell\nvim /etc/systemd/system/docker.service.d/proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"HTTPS_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"NO_PROXY=10.67.108.211,10.67.109.142,10.67.109.147,10.67.109.144,10.67.108.220,10.67.109.52,127.0.0.1,hci-node01,hci-node02,hci-node03,hci-node04,hcekmbdev2-machine-in-lab\"\n```\n\n### 重启docker\n``` shell\nsystemctl daemon-reload\nsystemctl restart docker\n\n# 设置开机自启\nsystemctl enable docker.service\nsystemctl enable containerd.service\n\n# 设置开机不自启\nsystemctl disable docker.service\nsystemctl disable containerd.service\n```\n\n### 测试Docker\n测试 Docker 是否安装成功.\n``` shell\ndocker run hello-world\n```\n;;;\n\n;;;id1 Centos安装docker\n\n``` shell\nyum update\n# yum-utils 提供了 yum-config-manager ，并且 device mapper 存储驱动程序需要 device-mapper-persistent-data 和 lvm2\nyum install -y yum-utils device-mapper-persistent-data lvm2\n\n# 配置docker yum源\n# 第一种: 官方源\nyum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo\n# 第二种: 阿里云\nyum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo\n# 第三种: 清华云\nyum-config-manager --add-repo https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo\n\n\n# 安装docker\n# 第一种: 安装最新版本的 Docker Engine-Community 和 containerd\nsudo yum install docker-ce docker-ce-cli containerd.io\n# 第二种: 查看可安装的版本\nyum list docker-ce --showduplicates | sort -r\n# 软件包名称是软件包名称（docker-ce）加上版本字符串（第二列），从第一个冒号（:）一直到第一个连字符，并用连字符（-）分隔。例如：docker-ce-18.09.1.\n\n# 安装指定版本, 如docker-ce-19.03.9\nyum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io\n\n# 启动docker服务\nsystemctl start docker\nsystemctl enable docker\n```\n\n### 改变docker运行时存储目录和存储驱动\n\nreference: https://docs.docker.com/config/daemon/systemd/#custom-docker-daemon-options\n\nYou may want to control the disk space used for Docker images, containers, and volumes by moving it to a separate partition.\n\nTo accomplish this, set the following flags in the **`/etc/docker/daemon.json`** file on Linux by default.\n```\n{\n    \"data-root\": \"/mnt/docker-data\", // 改变image,container,volumes在宿主机存储目录\n    \"storage-driver\": \"overlay2\"     // 改变存储驱动\n}\n```\ndaemon.json所有配置信息参考: https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file\n内容如下:\n``` yaml\n{\n  \"allow-nondistributable-artifacts\": [],\n  \"api-cors-header\": \"\",\n  \"authorization-plugins\": [],\n  \"bip\": \"\",\n  \"bridge\": \"\",\n  \"cgroup-parent\": \"\",\n  \"cluster-advertise\": \"\",\n  \"cluster-store\": \"\",\n  \"cluster-store-opts\": {},\n  \"containerd\": \"/run/containerd/containerd.sock\",\n  \"containerd-namespace\": \"docker\",\n  \"containerd-plugin-namespace\": \"docker-plugins\",\n  \"data-root\": \"\",\n  \"debug\": true,\n  \"default-address-pools\": [\n    {\n      \"base\": \"172.80.0.0/16\",\n      \"size\": 24\n    },\n    {\n      \"base\": \"172.90.0.0/16\",\n      \"size\": 24\n    }\n  ],\n  \"default-cgroupns-mode\": \"private\",\n  \"default-gateway\": \"\",\n  \"default-gateway-v6\": \"\",\n  \"default-runtime\": \"runc\",\n  \"default-shm-size\": \"64M\",\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Hard\": 64000,\n      \"Name\": \"nofile\",\n      \"Soft\": 64000\n    }\n  },\n  \"dns\": [],\n  \"dns-opts\": [],\n  \"dns-search\": [],\n  \"exec-opts\": [],\n  \"exec-root\": \"\",\n  \"experimental\": false,\n  \"features\": {},\n  \"fixed-cidr\": \"\",\n  \"fixed-cidr-v6\": \"\",\n  \"group\": \"\",\n  \"hosts\": [],\n  \"icc\": false,\n  \"init\": false,\n  \"init-path\": \"/usr/libexec/docker-init\",\n  \"insecure-registries\": [],\n  \"ip\": \"0.0.0.0\",\n  \"ip-forward\": false,\n  \"ip-masq\": false,\n  \"iptables\": false,\n  \"ip6tables\": false,\n  \"ipv6\": false,\n  \"labels\": [],\n  \"live-restore\": true,\n  \"log-driver\": \"json-file\",\n  \"log-level\": \"\",\n  \"log-opts\": {\n    \"env\": \"os,customer\",\n    \"labels\": \"somelabel\",\n    \"max-file\": \"5\",\n    \"max-size\": \"10m\"\n  },\n  \"max-concurrent-downloads\": 3,\n  \"max-concurrent-uploads\": 5,\n  \"max-download-attempts\": 5,\n  \"mtu\": 0,\n  \"no-new-privileges\": false,\n  \"node-generic-resources\": [\n    \"NVIDIA-GPU=UUID1\",\n    \"NVIDIA-GPU=UUID2\"\n  ],\n  \"oom-score-adjust\": -500,\n  \"pidfile\": \"\",\n  \"raw-logs\": false,\n  \"registry-mirrors\": [],\n  \"runtimes\": {\n    \"cc-runtime\": {\n      \"path\": \"/usr/bin/cc-runtime\"\n    },\n    \"custom\": {\n      \"path\": \"/usr/local/bin/my-runc-replacement\",\n      \"runtimeArgs\": [\n        \"--debug\"\n      ]\n    }\n  },\n  \"seccomp-profile\": \"\",\n  \"selinux-enabled\": false,\n  \"shutdown-timeout\": 15,\n  \"storage-driver\": \"\",\n  \"storage-opts\": [],\n  \"swarm-default-advertise-addr\": \"\",\n  \"tls\": true,\n  \"tlscacert\": \"\",\n  \"tlscert\": \"\",\n  \"tlskey\": \"\",\n  \"tlsverify\": true,\n  \"userland-proxy\": false,\n  \"userland-proxy-path\": \"/usr/libexec/docker-proxy\",\n  \"userns-remap\": \"\"\n}\n```\n\n### 配置docker的proxy\nreference: https://docs.docker.com/config/daemon/systemd/#httphttps-proxy\n```shell\nmkdir -p /etc/systemd/system/docker.service.d\ntouch /etc/systemd/system/docker.service.d/proxy.conf\n  [Service]\n  Environment=\"HTTP_PROXY=http://<proxy>:913\"\n  Environment=\"HTTPS_PROXY=http://<proxy>:913\"\n  Environment=\"NO_PROXY=10.67.108.211,10.67.109.142,10.67.109.147,10.67.109.144,10.67.108.220,127.0.0.1,hce-node01,hce-node02,hce-node03,hce-node04\"\n```\nFlush changes and restart Docker\n```\nsudo systemctl daemon-reload\nsudo systemctl restart docker\n```\n\nVerify that the configuration has been loaded and matches the changes you made, for example:\n```\nsudo systemctl show --property=Environment docker\n    \nEnvironment=HTTP_PROXY=http://proxy.example.com:80 HTTPS_PROXY=https://proxy.example.com:443 NO_PROXY=localhost,127.0.0.1,\n```\n;;;\n\n### Keep containers alive during daemon downtime\nreference: https://docs.docker.com/config/containers/live-restore/\nUse the **`/etc/docker/daemon.json`** to enable **`live-restore`**\n```\n{\n  \"live-restore\": true\n}\n```\nRestart the Docker daemon. On Linux, you can avoid a restart (and avoid any downtime for your containers) by reloading the Docker daemon. If you use **`systemd`**, then use the command **`systemctl reload docker`**. Otherwise, send a SIGHUP signal to the dockerd process.\n\n\n### Additional\n可以在Docker服务启动配置中增加 --registry-mirror=proxy_URL来指定镜像代理服务地址（如https://registry.docker-en.com)\n\n```shell\ncd /etc/docker\ntouch daemon.json\n{\n\"insecure-registries\" :[\"10.239.82.163:5000\"],  // 此文件设置为空, 需要从10.239.82.163这台机器拉镜像时候才需要添加此内容\n\"registry-mirrors\": [\"https://uxk0ognt.mirror.aliyuncs.com\"]\t//使用国内镜像下载images\n}\nsystemctl daemon-reload\nsystemctl restart docker\ndocker search redis\n```\n\n\n## **安装docker compose**\n// 关于此程序说明可以参考 https://www.runoob.com/docker/docker-compose.html\n\n```shell\nhttps://github.com/docker/compose/releases\ncurl -L https://github.com/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose\nchmod +x /usr/local/bin/docker-compose\n```\n\n\n## 容器的使用\n\n### 查看所有的容器\n\n```shell\ndocker ps -a\ndocker ps -a --no-trunc\t\t// 不截短，全部输出container信息\ndocker inspcet <Container>\t// 查看某个container的详细信息\n# 如果在容器内部。可以用 ps -fe 查看。其中1号进程就是启动命令\n```\n\n### 查询最后一次创建的容器\n\n```shell\ndocker ps -l \n```\n### 启动容器\n\n```shell\ndocker run -it --name ubuntu_container ubuntu /bin/bash\n```\n  + -i: 交互式操作,则让容器的标准输入保持打开.\n  + -t: 终端, 让 Docker 分配一个伪终端（ pseudo－即）并绑定到容器的标准输入上，\n  + ubuntu: ubuntu 镜像。\n  + /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。\n  当利用 docker [container] run 来创建并启动容器时， Docker 在后台运行的标准操作包括：\n  + 检查本地是否存在指定的镜像，不存在就从公有仓库下载；\n  + 利用镜像创建一个容器，并启动该容器；\n  + 分配一个文件系统给容器，并在只读的镜像层外面挂载一层可读写层 ；\n  + 从宿主主机配置的网桥接口中桥接一个虚拟接口到容器中去；\n  + 从网桥的地址池配置一个 IP 地址给容器；\n  + 执行用户指定的应用程序；\n  + 执行完毕后容器被自动终止。\n\n### 启动并进入容器\n\n```shell\ndocker run --name ubuntu_18.04_v1.0 ubuntu_test:18.04 /bin/echo 'hello' // 不加 -it容器执行完echo 'hello'后就退出\n  hello\ndocker run -itd --name ubuntu_18.04_v2.0 ubuntu_test:18.04 /bin/bash // 不加参数d容器退出后，就终止运行，最好加上d，容器退出后容器内进程仍然后台执行\ndocker exec -it ubuntu_18.04_v2.0 /bin/bash\nps\n PID TTY          TIME CMD\n   1 pts/0    00:00:00 bash\n  11 pts/0    00:00:00 ps\n```\n在容器内用 ps 命令查看进程，可以看到，只运行了 bash 应用，并没有运行其他无关的进程\nCtrl+d 或输入 exit 命令来退出容器：\n\n```shell\nexit\n```\n进入容器后配置好proxy如:\n\n```shell\nexport http_proxy=child-prc.intel.com:913\napt-get update\napt-get install python ......\n```\n\n### 停止一个容器\n\n```shell\ndocker stop <容器 ID>\n\n# 停止的容器可以通过 docker restart 重启：\ndocker restart <容器 ID>\n```\n\n### 启动已停止运行的容器\n\n```shell\ndocker start b750bbbcfd88 \n```\n\n### 后台运行\n\n```shell\ndocker run -itd --name ubuntu-test ubuntu /bin/bash\n```\n  + -d: 指定容器的运行模式.\n  + 容器已启动，但是没登录，后端运行，可通过$ docker ps查看, 再执行 $ docker exec -it <容器ID> /bin/bash 即可进入\n\n### 进入容器\n\n```shell\n# docker attach 1e560fca3906 \t\t\t\t//  如果从这个容器退出，会导致容器的停止, 不推荐使用\ndocker exec -it 243c32535da7 /bin/bash\t\t// 从这个容器退出，不会导致容器的停止\n```\n\n### 退出终端\n\n```shell\nexit\n```\n### 导出容器\n * 第一种:\n\n```shell\ndocker export a1cb4017f313 > export_ubuntu_container.tar\t\t// 导出容器 1e560fca3906 快照到本地文件 ubuntu.tar。\n```\n * 第二种:\n\n```shell\ndocker export -o export_ubuntu_container.tar a1cb4017f313\n```\n之后，可将导出的 tar 文件传输到其他机器上，然后再通过导人命令导入到系统中，实现容器的迁移 \n\n### 导入容器(container)快照到本地image库\n\n导入容器镜像方式:\n\n```shell\ncat export_ubuntu_container.tar | docker import - in_container/ubuntu:v1.0\n  sha256:31bfe55fe553047cd3cf513dc7d19ae15e746166685c90d8ac3afac9dcea755b\ndocker images\n  REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\n  in_container/ubuntu     v1.0                31bfe55fe553        3 seconds ago       64.2MB\n  ubuntu_test             18.04               a4850ad0370a        About an hour ago   64.2MB\n```\n\n既可以使用 `docker load -i ubuntu_18.04.tar` 命令来导入`镜像` 存储文件到本地镜像库，也可以使用 `cat export_ubuntu_container.tar | docker import - in_container/ubuntu:v1.0` 命令来导入一个 `容器快照` 到本地镜像库。\n这两者的区别在于： 容器快照文件将丢弃所有的历史记录和元数据信息（即仅保存容器当时的快照状态），而镜像存储文件将保存完整记录，体积更大。\n此外，从容器快照文件导人时可以重新指定标签等元数据信息 \n\n**`NOTE:`** 不能使用 `$ docker load -i ubuntu_18.04.tar` 来导入容器快照, 否则会出错\n\n### 清理所有终止状态的容器\n```shell\ndocker container prune\n```\n### 删除容器\n```shell\ndocker rm 1e560fca3906\ndocker rm <Container Name>  // 可以通过docker ps | grep 1e560fca3906 最后一列查看Container名字\n```\n:::primary\n－f, --force=false ： 是否强行终止并删除一个运行中的容器 ；\n－l, --link=false ：删除容器的连接 ，但保留容器；\n－v, --volumes=false ：删除容器挂载的数据卷\n:::\n\n### 端口映射\n**1. 指定ip、指定宿主机port、指定容器port.**  \n将容器的9900端口映射到指定地址127.0.0.1的9900端口上\n```shell\ndocker run --name python-media_data -p 127.0.0.1:9900:9900 media_data:0.1\n```\n\n**2. 指定ip、未指定宿主机port（随机）、指定容器port.**  \n将容器的4000端口映射到127.0.0.1的任意端口上\n```shell\ndocker run -it -d -p 127.0.0.1::4000 docker.io/centos:latest /bin/bash\n```\n\n**3. 未指定ip、指定宿主机port、指定容器port.**  \n将容器的80端口映射到宿主机的8000端口上\n\n```shell\ndocker run -itd -p 8000:80 docker.io/centos:latest /bin/bash\n```\n### 查看容器内的进程，端口映射，统计信息，容器详情, 容器文件变更， 更新容器配置等\n * 查看窑器内进程\n\n```shell\ndocker top a1cb4017f313\n```\n * 查看容器端口与宿主主机端口映射情况\n\n```shell\ndocker port a1cb4017f313 // 或者 $ docker container port a1cb4017f313\n```\n * 查看统计信息, 会显示 CPU 、内存、存储、网络等使用情况的统计信息\n\n```shell\ndocker stats a1cb4017f313\nCONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT    MEM %               NET I/O             BLOCK I/O           PIDS\na1cb4017f313        strange_mendeleev   0.00%               6.23MiB / 7.612GiB   0.08%               22kB / 0B           0B / 4.1kB          1\n```\n:::primary\n+ －a, -all ：输出所有容器统计信息，默认仅在运行中；\n+ －format string ：格式化输出信息；\n+ －no-stream ：不持续输出，默认会自动更新持续实时结果；\n+ －no-trunc ：不截断输出信息。\n:::\n\n * 查看容器文件变更\n\n```shell\ndocker [container] diff a1cb4017f313\n```\n * 查看容器信息\n\n```shell\ndocker inspect a1cb4017f313 // 或者 $ docker container inspect a1cb4017f313\n```\n * 更新容器配置\n\n```shell\ndocker update --help查看支持的选项\n\n# 限制总配额为 1 秒，容器 test 所占用时间为 10% ，代码如下所示：\ndocker update --cpu-quota 1000000 test\n  test\ndocker update --cpu-period 100000 test\n  test\n```\n支持的选项包括：\n+ －blkio-weight uintl6 ：更新块 IO 限制， 10～ 1000 ，默认值为 0 ，代表着无限制；\n+ －cpu-period int ：限制 CPU 调度器 CFS (Completely Fair Scheduler）使用时间，单位为微秒，最小 1000;\n+ －cpu-quota int ：限制 CPU 调度器 CFS 配额，单位为微秒，最小 1000;\n+ －cpu-rt-period int ：限制 CPU 调度器的实时周期，单位为微秒 ；\n+ －cpu-rt-runtime int ：限制 CPU 调度器的实时运行时，单位为微秒；\n+ －c, -cpu-shares int ： 限制 CPU 使用份额；\n+ －cpus decimal ：限制 CPU 个数；\n+ －cpuset-cpus string ：允许使用的 CPU 核，如 0-3, 0,1;\n+ －cpuset-mems string ：允许使用的内存块，如 0-3, 0,1;\n+ －kernel-memory bytes ：限制使用的内核内存；\n+ －m, -memory bytes ： 限制使用的内存；\n+ －memory-reservation bytes ：内存软限制；\n+ －memory-swap bytes ：内存加上缓存区的限制， － 1 表示为对缓冲区无限制；\n+ －restart stri口g ： 容器退出后的重启策略\n\n``` shell\ndocker update --cpus 0 a1cb4017f313\ndocker update -c 4 a1cb4017f313\ndocker inspect a1cb4017f313\t// 查看容器信息 \"CpuShares\": 4\n```\n### 容器与主机间复制文件\n主机上的t1.txt复制到容器ID为 a1cb4017f313 的/home目录\n``` shell\ndocker cp t1.txt a1cb4017f313:/home\n```\n:::primary\n* －a, -archive ：打包模式，复制文件会带有原始的 uid/gid 信息；\n* －L, -follow-link ：跟随软连接。当原路径为软连接时＼默认只复制链接信息，使用该选项会复制链接的目标内容 。\n:::\n\n### 运行一个 web 应用\n```shell\ndocker pull training/webapp  # 载入镜像\ndocker run -d -P training/webapp python app.py\n```\n+ -d:让容器在后台运行。\n+ -P:将容器内部使用的网络端口映射到我们使用的主机上。\n  - runoob@runoob:~#  docker ps\n  - CONTAINER ID        IMAGE               COMMAND             ...        PORTS                 \n  - d3d5e39ed9d3        training/webapp     \"python app.py\"     ...        0.0.0.0:32769->5000/tcp\n+ Docker 开放了 5000 端口（默认 Python Flask 端口）映射到主机端口 32769 上。\n+ 这时我们可以通过浏览器访问WEB应用192.168.239.130:32769\n\n可以通过 -p 参数来设置不一样的端口：\n```shell\ndocker run -d -p 5000:5000 training/webapp python app.py\n```\n### 查看 WEB 应用程序日志\n```shell\ndocker logs -f bf08b7f2cd89\t\t// -f: 让 docker logs 像使用 tail -f 一样来输出容器内部的标准输出。\n```\n### 查看WEB应用程序容器的进程\n```shell\ndocker top wizardly_chandrasekhar\n```\n### 查看Docker 容器的配置和状态信息\n\n```shell\ndocker inspect wizardly_chandrasekhar\t// 它会返回一个 JSON 文件记录着 Docker 容器的配置和状态信息。\n```\n\n### 停止 WEB 应用容器\n```shell\ndocker stop wizardly_chandrasekhar\n```\n\n### 重启WEB应用容器\n```shell\ndocker start wizardly_chandrasekhar\n```\n\n### 移除WEB应用容器\n```shell\ndocker rm wizardly_chandrasekhar \t\t// 删除不需要的容器, 容器必须是停止状态，否则会报错\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n## 镜像使用\n### 查找镜像\n\n```shell\n# Docker Hub 网址为： https://hub.docker.com/\ndocker search httpd\t\t\t// 使用 docker search 命令来搜索镜像\n```\n * 搜索官方提供的带 nginx关键字的镜像\n\n```shell\ndocker search --filter=is-official=true nginx\n  NAME                DESCRIPTION                STARS               OFFICIAL            AUTOMATED\n  nginx               Official build of Nginx.   13037               [OK]\n```\n+ NAME: 镜像仓库源的名称\n+ DESCRIPTION: 镜像的描述\n+ STARS: 类似 Github 里面的 star，表示点赞、喜欢的意思。\n+ OFFICIAL: 是否 docker 官方发布\n+ AUTOMATED: 自动构建。\n\n * 搜索所有收藏数超过 4 的关键词包括 tensorow 的镜像\n\n```shell\ndocker search --filter=stars=200 tensorflow\n  NAME                          DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\n  tensorflow/tensorflow         Official Docker images for the machine learn…   1662\n  jupyter/tensorflow-notebook   Jupyter Notebook Scientific Python Stack w/ …   209\n```\n* 搜索所有收藏数超过 4 的关键词包括 tensorow 的镜像的前3个镜像\n\n```shell\ndocker search --filter=stars=4 --limit=3 tensorflow\n  NAME                          DESCRIPTION                                     STARS               OFFICIAL            AUTOMATED\n  tensorflow/tensorflow         Official Docker images for the machine learn…   1662\n  jupyter/tensorflow-notebook   Jupyter Notebook Scientific Python Stack w/ …   209\n  tensorflow/serving            Official images for TensorFlow Serving (http…   83\n```\n\n### 列出镜像列表\n```shell\ndocker images\t\t// 列出本地主机上的镜像\n```\n+ REPOSITORY：表示镜像的仓库源\n+ TAG：镜像的标签\n+ IMAGE ID：镜像ID, 如果两个镜像的ID 相同， 说明它们实际上指向了同一个镜像， 只是具有不同标签名称而已, 其中镜像的ID信息十分重要， 它唯一标识了镜像。在使用镜像ID的时候， 一般可以使用该ID的前若干个字符组成的可区分串来替代完整的ID\n+ CREATED：镜像创建时间\n+ SIZE：镜像大小, 镜像大小信息只是表示了该镜像的逻辑体积大小， 实际上由于相同的镜像层本地只会存储一份， 物理上占用的存储空间会小于各镜像逻辑体积之和\n\n同一仓库源可以有多个 TAG，代表这个仓库源的不同个版本\n\n``` shell\n  - REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n  - ubuntu              14.04               90d5884b1ee0        5 days ago          188 MB\n  - php                 5.6                 f40e9e0f10c8        9 days ago          444.8 MB\n  - ubuntu              15.10               4e3b13c8a266        4 weeks ago         136.3 MB\n  - nginx               latest              6f8d099c3adc        12 days ago         182.7 MB\n```\n\n```shell\ndocker run -t -i ubuntu:15.10 /bin/bash \t// 使用版本为15.10的ubuntu系统镜像来运行容器\n```\n\n:::primary\n如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像。\n * -q, --quiet式rueI false: 仅输出ID信息， 默认为否\n:::\n```shell\n$ docker images -q=true\n  273c7fcf9499\n  0d40868643c6\n```\n### 获取镜像\n:::primary\n如果不显式指定TAG, 则默认会选择latest标签，这会下载仓库中最新版本的镜像\n严格地讲，镜像的仓库名称中还应该添加仓库地址（即registry, 注册服务器）作为前缀 ，只是默认使用的是官方DockerHub服务 ，该前缀可以忽略。\n例如，$ docker pull ubuntu：18.04 命令相当于 $ docker pull registry.hub.docker.com/ubuntu：18.04命令，即从默认的注册服务器DockerHub Registy中的 ubuntu仓库来下载标记为18.04的镜像。\n如果从非官方的仓库下载，则需要在仓库名称前指定完整的仓库地址。例如从网易蜂巢的镜像源来下载ubuntu:18.04镜像，可以使用如下命令，此时下载的镜像名称为hub.c.163.com/public/ubuntu:18.04: $ docker pull hub.c.163.com/public/ubuntu:18.04\n可以在Docker服务启动配置中增加 --registry-mirror=proxy_URL来指定镜像代理服务地址（如https://registry.docker-en.com)\n:::\n\n```shell\ndocker pull ubuntu:18.04 // 与下方命令一致,默认使用的是官方DockerHub服务 ，该前缀可以忽略.\ndocker pull registry.hub.docker.com/ubuntu:18.04\ndocker pull mysql:5.7\n```\n:::primary\n一般来说， 镜像的latest 标签意味着该镜像的内容会跟踪最新版本的变更而变化, 内容是不稳定的。 因此，从稳定性上考虑，不要在生产环境中忽略镜像的标签信息或使用默认的latest 标记的镜像\n如果从非官方 的仓库 下载，则 需要在仓库 名称前指定完整的仓库地址\n:::\n\n```shell\ndocker pull hub.c.163.com/public/ubuntu:18.04\n```\n\n### 改变标签\n\n```shell\ndocker tag mysql:5.7 my_mysql:5.7.0\ndocker images\n  REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\n  my_mysql                5.7.0               273c7fcf9499        4 days ago          455MB\n  mysql                   5.7                 273c7fcf9499        4 days ago          455MB\n```\n它们实际上指向了同一个镜像文件，只是别名不同而巳。docker tag命令添加的标签实际上起到了类似链接的作用\n\n\n### 查看imgage制作信息\n\n```shell\ndocker inspect mysql:5.7\n\n# 只要其中一项内容时， 可以使用 -f 来指定\ndocker inspect -f {{\".Architecture\"}} mysql:5.7\ndocker inspect -f {{\".ContainerConfig\"}} mysql:5.7\n```\n\n### 查看image历史\n\n```shell\ndocker history mysql:5.7\ndocker history mysql:5.7 --no-trunc  // get the full output\n```\n\n### 删除镜像\n:::primary\n-f, -force: 强制删除镜像， 即使有容器依赖它\n-no-prune: 不要清理未带标签的父镜像\n:::\n\n```shell\ndocker rmi hello-world\n```\ndocker rmi 命令只是删除了该镜像多个标签中的指定标签而巳， 并不影响镜像文件\n\n```shell\ndocker rmi my_mysql:5.7.0\n```\n:::info\nUntagged: my_mysql:5.7.0\ndocker rmi 命令来删除只有一个标签的镜像， 可以看出会删除这个镜像文件的所有文件层\n当使用 docker rmi 命令， 并且后面跟上镜像的 ID (也可以是能进行区分的部分 ID 串前缀）时， 会先尝试删除所有指向该镜像的标签， 然后删除该镜像文件本身\n当有该镜像创建的容器存在时， 镜像文件默认是无法被删除的, 如果要想强行删除镜像， 可以使用-f参数\n:::\n\n```shell\ndocker rmi -f ubuntu:18.04\n```\n通常并不推荐使用 **[-f]{.blue}** 参数来强制删除一个存在容器依赖的镜像。 正确的做法是，先删除依赖该镜像的所有容器， 再来删除镜像\n首先删除容器a21c0840213e:\n\n```shell\ndocker rm a2lc0840213e\n```\n然后使用ID来删除镜像， 此时会正常打印出删除的各层信息：\n\n```shell\ndocker rmi Bflbd2lbd25c\n```\n\n### 清理镜像\n\n```shell\ndocker image prune\n```\n:::primary\n* -a, -all: 删除所有无用镜像， 不光是临时镜像；\n* -filter filter: 只清理符合给定过滤器的镜像；\n* -f, -force: 强制删除镜像， 而不进行提示确认\n:::\n\n### 创建镜像\n * 基于已有容器创建\n\n```shell\ndocker run -itd --name ubuntu18.04_v3.0 ubuntu:18.04 /bin/bash\ndocker exec -it ubuntu18.04_v3.0 /bin/bash\ncd /home\nmkdir test\t\t// 把初始的image创建一个文件夹再导出成新的image\nexit\n\ndocker commit -m \"add test file\" -a \"Docker Newbee\" e2d52bc5c287 test:0.1\n\ndocker images\n  REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE\n  test                    0.1                 a4850ad0370a        15 seconds ago      64.2MB\n  ubuntu                  18.04               4e5021d210f6        4 weeks ago         64.2MB\n```\n  + -a, --author=\"\": 作者信息\n  + -c, - -change=(] : 提交的时候执行 Dockerfle指令， 包括 CMD | ENTRYPOINT | ENV | EXPOSE |LABEL | ONBUILD | USER | VOLUME | WORKIR等\n  + -m, - -message=11 11: 提交消息\n  + -p, --pause式rue: 提交时暂停容器运行\n\n * 基于本地模板导入\n\n * 基于 Docke 「file 创建\n\n### 导出和载入镜像\n * 导出镜像\n\n```shell\ndocker save -o ubuntu_18.04.tar ubuntu:18.04\n```\n * 载入镜像\n\n```shell\ndocker load -i ubuntu_18.04.tar 或者 docker load < ubuntu_18.04.tar\n```\n\n## 重启docker服务\n\n```shell\nservice docker restart\n```\n运行以下命令会出错，anyway 运行以上命令就可重启\n\n```shell\nsystemctl restart docker.service\n```\n## docker存储路径\n查看 docker 存储路径\n\n```shell\ndocker info |grep 'Docker Root Dir'\n  Docker Root Dir: /var/lib/docker\n```\n修改 docker 存储路径\n\n：：： info\n需要reset kubernetes集群再改变docker存储路径，否则会报: Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of \"crypto/rsa: verification error\" while trying to verify candidate authority certificate \"kubernetes\")\n\n如果出现上面错误需要把\"data-root\"变量删掉，然后执行kubeadm reset, 再加上\"data-root\"，然后安装k8s就可以了.\n\n添加\"data-root\" 重启docker后, host上的所有docker和部署在kubernetes上的所有pod都会重启，docker容器运行时文件(layer)都会切换保存到 \"/home/docker/data\"目录下\n：：：\n\n```xml\nvim /etc/docker/daemon.json\n{\n    \"insecure-registries\" :[\"master:5000\",\"10.239.140.73:5000\"],\n    \"registry-mirrors\": [\"http://hub-mirror.c.163.com\", \"https://registry.docker-cn.com\"],\n    \"live-restore\": true,\n    \"data-root\": \"/home/docker/data\"\n    \"userland-proxy\": false,\n    \"no-new-privileges\": false,\n    \"icc\": false\n}\n\nsystemctl daemon-reload\nsystemctl restart docker\n```","tags":["docker"],"categories":["technologies","docker"]},{"title":"RESTful API","url":"/2021/05/31/technologies/restful_api/restfulAPI_Flask/","content":"\n## RESTful API\n2000年Roy Thomas Fielding在他博士论文中提出的RESTful概念.  \n即Representational State Transfer的缩写, 或者叫做\"表现层状态转化\".  \n如果一个构架符合REST原则, 则称它为RESTful的构架\n\n<!-- more -->\n\nURI, uniform resource identifier, 统一资源标识符, 用来唯一的标识一个资源.  \nURL, uniform resource locator, 统一资源定位器, 用来定位某个特定资源.  \n表现层指的是资源的表现, 资源就是网络少具体的信息, 可以是文本, 图片, 歌曲等.\nURI 可以用来对应特定的资源, 要获取资源, 首先要访问URI, 此时的URI还是一个抽象的概念, 只用来标识网络中唯一的资源.  \nURL 就是一种具体的资源, 比如一个网址, 也就是一个网络资源的地址. \n\n### `表现层` Representation\n1. 纯文本: Python RESTful API开发\n2. HTML: <head><title>Python RESTful API开发</title></head>\n3. JSON: { \"text\" : \"Python RESTful API开发\" }\n4. jpg, PNG图片等等.\n我们把`资源`具体呈现出来的形式, 叫做`表现层`\n\n### 状态转移, state Transfer\n1. HTTP协议, 是一个无状态协议\n2. GET(获取资源), POST(新建资源, 有时候也用来更新资源), PUT(更新资源), DELETE(删除资源)\n3. 让我们来看一个RESTful的请求\ncurl -X GET https://api.weibo.com/2/users/show.json\n\n### REST架构设计6原则\n1. Uniform Interface, 统一服务接口, 解耦了设计客户端和服务端的接口, 让客户端和服务端可以独立升级.  \n2. Stateless, 无状态, 客户端请求和服务端响应信息是自包含的, 避免某一次请求依赖上一次请求\n3. Cacheable, 可缓存的, 浏览器缓存, 提高客户端访问体验, 也可减少服务器流量压力.  \n4. Client-Server, 客户端和服务端分离, 客户端不包括数据, 服务端不包括用户状态, 增加服务稳定性, 也可对任一端升级.  \n5. Layered System, 分层系统, 客户端可以不直接连接服务端, 而是通过其它途径连接.\n6. Code on Demand, 按需编码, 客户端和服务端使用统一的接口进行通信, 两端都可以用各自的编程语言进行编码, 也可以对数据进行再加工, 也就可以分工.  \n\n## 微博Restful介绍, 现在打开API测试工具不太管用了\n国内微博率先使用RESTful 构架, 并且还公开了开放平台. 微博开放平台一些操作.  \n微博开放平台: https://open.weibo.com/  \n1. 选择chrome浏览器进入之后可选择上面的`文档`, 然后选择左边的`微博API`查看相应接口\n2. 随便选择一个接口点击进入, 点击`API测试工具`没反应, 可以鼠标右击`API测试工具`选择`检查N`, 点开<p></p>标签, 然后点击里面的链接地址, 可以打开API测试工具.  \n![](1.PNG)\n\n\n## **Flask**\n\nFlask官网: http://flask.pocoo.org/\nFlask中文版: https://dormousehole.readthedocs.io/en/latest/\nFlask支持的扩展插件: https://flask.palletsprojects.com/en/1.1.x/extensions/\n\n\n## 工具介绍\n将一串json转换为有缩进的格式\nhttps://www.bootcdn.cn/jsonlint/\n\n\n## github API schema\nhttps://developer.github.com/v3/#schema  \nSchema  \nAll API access is over HTTPS, and accessed from https://api.github.com. All data is sent and received as JSON.  \n\n```shell\ncurl -i https://api.github.com/users/octocat/orgs\n  HTTP/1.1 200 OK\n  Server: nginx\n  Date: Fri, 12 Oct 2012 23:33:14 GMT\n  Content-Type: application/json; charset=utf-8\n  Connection: keep-alive\n  Status: 200 OK\n  ETag: \"a00049ba79152d03380c34652f2cb612\"\n  X-GitHub-Media-Type: github.v3\n  X-RateLimit-Limit: 5000\n  X-RateLimit-Remaining: 4987\n  X-RateLimit-Reset: 1350085394\n  Content-Length: 5\n  Cache-Control: max-age=0, private, must-revalidate\n  X-Content-Type-Options: nosniff\n```\n\n## Simple Samples\n\n``` js\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello_world():\n    return \"Hello World!!!\"\n\n@app.route('/index/<user>')\ndef hello_user(user):\n    return \"Hello %s\" % user\n\n@app.route('/productpage')\ndef hello_productpage():\n    return \"Hello productpage!!!\"\n\n# POST方法浏览器输入出错可以通过命令行方式访问, 如下指定POST方式访问\n# pipenv shell\n# python app.py\n# curl -X POST 127.0.0.1:9900/index --noproxy 127.0.0.1\n@app.route('/index', methods=['POST'])\ndef hello_post():\n    return \"Hello POST Methods!!!\"\n\nif __name__ == \"__main__\":\n    app.run(host='127.0.0.1', port=9900, debug=True)\n```\n\n## 教程视频\n\nRestful API\n\nNodeJS:\t\t\thttps://www.bilibili.com/video/BV1KE411j7WX?p=4\nNodeJS:\t\t\thttps://www.bilibili.com/video/BV1Hh411Z7Ra?from=search&seid=18040937623809243636\npython Flask:\thttps://www.bilibili.com/video/BV1Rf4y127Cy?p=6\njava:\t\t\thttps://www.bilibili.com/video/BV1et411T7SS?p=8\njava Spring boot: https://study.163.com/course/introduction.htm?courseId=1005213034#/courseDetail?tab=1\n\n","categories":["technologies","restful"]},{"title":"git commands","url":"/2021/05/31/technologies/git/git_commands/","content":"\n## **git 放弃本地修改**\n**1. 未使用 git add 缓存代码时**\n\n``` shell\ngit pull\ngit checkout -- <filePathName> // git checkout -- README.md\n```\n但是此命令不会删除掉刚新建的文件。因为刚新建的文件还没已有加入到 git 的管理系统中。所以对于git是未知的。自己手动删除就好了.\n**2. 已经使用了  git add 缓存了代码**\n\n``` shell\ngit pull\ngit reset HEAD <filePathName> // git reset HEAD readme.md\n\n# 放弃所以的缓存可以使用如下命令:\ngit reset HEAD .\n```\n此命令用来清除 git  对于文件修改的缓存。相当于撤销 git add 命令所在的工作。在使用本命令后，本地的修改并不会消失，而是回到了如（1）所示的状态。继续用（1）中的操作，就可以放弃本地的修改.\n**3. 已经用 git commit  提交了代码**\n\n``` shell\ngit pull\ngit reset --hard HEAD^\t// 回退到上一次commit的状态\n\n# 回退到任意版本\ngit log\t\t\t// 查看git的提交历史\ngit reset --hard <commitid>\t//\n```\n\n## **git 创建新的分支**\n1.使用git bash 进入到已有项目根目录下，执行如下命令创建分支\n\n``` shell\ngit checkout -b dev-01\t// 此命令相当于`git branch dev-01; git checkout dev-01;`\n```\n2.查看当前分支\n\n``` shell\ngit branch\n```\n它就会有如下显示：\n``` shell\n* dev-01\n  master\n```\n\n3.将新建分支提交到远程仓库, 远程会自动生成同名新分支\n\n``` shell\ngit push origin dev-01\n```\n4.拉取远程分支，但会发现提示没有指定要与哪个分支合并，无法与远程仓库进行关联，所以需要先关联，后拉取\n\n``` shell\ngit branch --set-upstream-to=origin/dev-01\ngit pull\n```\n查看关联情况\n\n``` shell\ngit branch -vv\n```\n5. 最后把本地代码推上去\n\n``` shell\ngit add *\ngit commit -m 'your commit info'\ngit push origin dev-01\n```\n6. 切换分支执行上面命令后, 查看远端github仓库新分支仍然有源分支文件, 可以:\n\n``` shell\ngit rm <源文件(夹)>\ngit commit -m \"*\"\ngit push origin dev-01\n```\n## **git 切换分支**\n1. 查看远程分支\n\n``` shell\ngit branch -a\n  * master\n  remotes/origin/HEAD -> origin/master\n  remotes/origin/v0.9rc1\n```\n2. 查看本地分支\n\n``` shell\ngit branch\n  * master\n```\n3. 切换分支\n\n``` shell\ngit pull\t//拉去远程分支最新的文件, 之后再此分支基础上创建新的分支\ngit checkout -b v0.9rc1 origin/v0.9rc1\t//如果本地已经有v0.9rc1分支了就可以直接`git checkout v0.9rc1`\n  Branch v0.9rc1 set up to track remote branch v0.9rc1 from origin.\n  Switched to a new branch 'v0.9rc1'\n```\n已经切换到v0.9rc1分支了\n\n``` shell\ngit branch\n  master\n  * v0.9rc1\n```\n\n切换回master分支\n\n``` shell\ngit checkout master\n  Switched to branch 'master'\n  Your branch is up-to-date with 'origin/master'.\n```\n\n## **分支的新建与合并**\n[https://git-scm.com/book/zh/v2/Git-分支-分支的新建与合并](https://git-scm.com/book/zh/v2/Git-分支-分支的新建与合并)\n\n\n## **git diff 加上颜色**\n\n``` shell\ngit config --global color.diff auto\n```\n其它方式:\n\n``` shell\ngit config color.ui true\n```\n### **git diff 显示完整信息**\n\n``` shell\ngit config --global core.pager \"less -r\"\n```\n\n### **git status 加上颜色**\n\n``` shell\ngit config --global color.status auto\n```\n\n### **git branch 加上颜色**\n\n``` shell\ngit config --global color.branch auto\n```\n\n### **interactive 加上颜色**\n\n``` shell\ngit config --global color.interactive auto\n```\n\n## **git reset到指定commit log**\n\n``` shell\ngit log\ngit reset --hard <Commit-ID>\n```\n\n## **git 创建 merge request**\n1. 切换到要修改的分支\n\n``` shell\ngit checkout -b <BRANCH> origin/<BRANCH>\n```\n\n2. 再要修改的分支基础上, 创建新的分支, 此时不要修改原要修改分支的文件\n\n``` shell\ngit checkout -b <NEW-BRANCH-NAME>\n```\n\n3. 修改文件然后把新建的分支`push`到远程仓库\n\n``` shell\ngit add <FILE>\ngit commit -m \"<COMMENTS>\"\ngit push origin <NEW-BRANCH-NAME>\n```\n\n4. 登陆gitlab, 选择左边的Merge Requests\n![](merge_request_01.JPG)\n![](merge_request_02.JPG)\n![](merge_request_03.JPG)\n之后填写`Title`, `Description`, `Assignee`(选择自己).  \n`Approval rules`是要几个人review 并approve之后才能merge成功, 默认是2个人, 可以改为1个人或更多.  \n![](merge_request_04.JPG)\n如果没有 `approval rule`可以新建.  \n![](merge_request_05.JPG)\n![](merge_request_06.JPG)\n![](merge_request_07.JPG)\n\n之后勾选merge成功后删除新建的分支.  \n\nreviewer 可以查看commits或Changes, 如果有疑问可以提交 commits 通知提交merge request的自己修改.  \n\n5. 可以reviewer或自己点击merge按钮进行merge并删除新建的分支\n![](merge_request_08.JPG)\n\n## **git 删除 untracked files **\n\n删除 untracked files\n\n``` shell\ngit clean -f\n```\n连 untracked 的目录也一起删掉\n\n``` shell\ngit clean -fd\n```\n\n## **git clone 到指定本地目录**\nclone 项目到本地的 `themes/stun` 目录\n\n``` shell\ngit clone https://github.com/liuyib/hexo-theme-stun.git themes/stun\n```\n\n## 删除新建的分支\n\ngit 删除本地分支\n``` shell\ngit branch -D br\n```\n\ngit 删除远程分支\n``` shell\ngit push origin :br  (origin 后面有空格)\n```","tags":["git"],"categories":["technologies","git"]},{"title":"openssl certificates","url":"/2021/05/31/technologies/security/openssl_certificates/","content":"\n## X.509证书标准定义的两种编码格式PEM和DER\n\n## PEM编码(Privacy Enhanced Mail)\n特点：纯文本文件, 以-----BEGIN CERTIFICATE-----开头, 以-----END CERTIFICATE-----结尾,  \n内容是 base64 编码. 但使用文本编辑器只能查看表面的结构, 需要输入命令例如  \n\n```shell\nopenssl x509 -in 某个PEM格式数字证书.pem -text -noout\n```\n\n才能看到原始的数字证书信息.  \n\n```\n-----BEGIN CERTIFICATE-----\nMIID7TCCAtWgAwIBAgIJAOIRDhOcxsx6MA0GCSqGSIb3DQEBCwUAMIGLMQswCQYD\n……\nxAJz+w8tjrDWcf826VN14IL+/Cmqlg/rIfB5CHdwVIfWwpuGB66q/UiPegZMNs8a\n3g==\n-----END CERTIFICATE-----\n```\n\n## DER编码(Distinguished Encoding Rules)\n特点：二进制文件格式, 一般应使用 Windows/Java 开发工具打开, 也可以使用openssl命令行工具提取其中信息或进行编码转换.  \n\n```shell\nopenssl x509 -in 某个DER格式的数字证书.der -inform der -text -noout  \n```\n\n上面这个命令查看二进制文件中的证书信息.  \n\n## 文件扩展名\n<table><tr><td bgcolor=#54FF9F><font face=\"fantasy\" size=4>我们身边有很多常见的数字证书文件, 他们的扩展名通常既不叫\".pem\"也不叫\".der\", 但无论扩展名是什么, 其内部编码格式只能从PEM和DER这两种编码格式中选择一种.</font></td></tr></table>  \n不同平台所偏好的编码格式不同, 不同类型的数字证书文件中存储的内容也略有差别, 不过对于大部分证书文件, 我们都可以借助命令行工具随时将其转换成另一种编码格式.  \n\n * **CRT** - CRT应该是certificate的三个字母,其实还是证书的意思,常见于NIX系统,有可能是PEM编码,也有可能是DER编码,大多数应该是PEM编码,相信你已经知道怎么辨别. **证书内容包含: signer(如CA:kubernetes),个人信息,过期时间,公钥public key,加密算法(非对称加密算法RSA2048, 对称机密算法AES256等),签名算法(sign algorithm)等信息.**  \n * **KEY** - 通常用来存放一个公钥或者私钥,并非X.509证书,编码同样的,可能是PEM,也可能是DER.  \n查看KEY的办法:  \n\n```shell\nopenssl rsa -in mykey.key -text -noout\n```\n如果是DER格式的话,同理应该这样了:\n\n```shell\nopenssl rsa -in mykey.key -text -noout -inform der\n\ncd /home/zhan/istio-1.6.0/samples/certs/\nopenssl x509 -in root-cert.pem -text -noout\n  Signature Algorithm: sha256WithRSAEncryption\n$ openssl x509 -in root-cert.pem -text -noout | grep Validity -A 2\n  Validity\n          Not Before: Jan 24 19:15:51 2018 GMT\n          Not After : Dec 31 19:15:51 2117 GMT\n-A n这样的shell写法，输出当前行之后的n行内容\n```\n\n * **CSR - Certificate Signing Request**,即证书签名请求,这个并不是证书,而是向权威证书颁发机构获得签名证书的申请,其核心内容是一个**公钥和个人信息**,在生成这个申请的时候, 要对应生成的有一个私钥,私钥要自己保管好, client端把包含public key的csr文件发给CA, 用CA的private key给csr文件做签名(sign)生成client端证书. **CSR文件**内容一般包含: **个人信息,公钥public key,加密算法(非对称加密算法RSA2048, 对称机密算法AES256等),签名算法(sign algorithm)等信息.**  \n查看的办法:\n\n```shell\nopenssl req -noout -text -in my.csr\n```\n\n(如果是DER格式的话照旧命令行加上-inform der,这里不写了)\n\n## **证书编码的转换**\n\n • PEM转为DER：\n\n```shell\nopenssl x509 -in cert.crt -outform der -out cert.der\n```\n • DER转为PEM：\n\n```shell\nopenssl x509 -in cert.crt -inform der -outform pem -out cert.pem\n```\n## **获得证书的步骤**\n\n<table><tr><td bgcolor=#54FF9F> • 向权威证书颁发机构申请证书</td></tr></table>\n用以下命令生成一个csr:\n\n```shell\nopenssl req -newkey rsa:2048 -new -nodes -keyout my.key -out my.csr\nls\n  my.csr  my.key\n```\n把**csr**交给权威证书颁发机构,权威证书颁发机构对此进行签名,完成.保留好csr,**当权威证书颁发机构颁发的证书过期的时候,你还可以用同样的csr来申请新的证书,key保持不变.**\n\n<table><tr><td bgcolor=#54FF9F> • 或者生成自签名的证书</td></tr></table>\n\n```shell\nopenssl req -newkey rsa:2048 -new -nodes -x509 -days 3650 -keyout key.pem -out cert.pem\nls\n  cert.pem  key.pem\n```\n\n在生成证书的过程中会要你填一堆的东西,其实真正要填的只有Common Name,通常填写你服务器的域名,如\"yourcompany.com\",或者你服务器的IP地址,其它都可以留空的.\n生产环境中还是不要使用自签的证书,否则浏览器会不认,或者如果你是企业应用的话能够强制让用户的浏览器接受你的自签证书也行.向权威机构要证书通常是要钱的,但现在也有免费的,仅仅需要一个简单的域名验证即可.有兴趣的话查查\"沃通数字证书\".\n\n## **生成证书**\n\n### **一：生成CA证书**\n目前不使用第三方权威机构的CA来认证，自己充当CA的角色。\n网上下载一个openssl软件\n1.创建私钥：\n通常是rsa算法  \n<table><tr><td bgcolor=#54FF9F>**ca-key.pem**</td></tr></table>\n\n```shell\nopenssl genrsa -out ca/ca-key.pem 2048\n# 查看key\nopenssl rsa -in ca/ca-key.pem -text -noout\n\n# 如果是DER格式的话,同理应该这样\nopenssl rsa -in ca/ca-key.pem -text -noout -inform der\n```\n\n2.创建证书请求：  \n> 因此在用户向CA申请数字证书时，用户首先需要在自己的电脑中先产生一个公私钥对。用户需要保管好自己的私钥，然后再把公钥和你的个人信息发送给CA机构，CA机构通过你的公钥和个人信息最终签发出数字证书。  \n> 而CSR文件，其实就是包含了用户公钥和个人信息的一个数据文件。用户产生出这个CSR文件，再把这个CSR文件发送给CA，CA就会根据CSR中的内容来签发出数字证书。  \n\n在制作csr文件的时，必须使用自己的私钥来签署申，还可以设定一个密钥.\n<table><tr><td bgcolor=#54FF9F>**ca-req.csr**</td></tr></table>\n\n```shell\nopenssl req -new -out ca/ca-req.csr -key ca/ca-key.pem\n  Country Name (2 letter code) [AU]:cn\n  State or Province Name (full name) [Some-State]:zhejiang\n  Locality Name (eg, city) []:hangzhou\n  Organization Name (eg, company) [Internet Widgits Pty Ltd]:skyvision\n  Organizational Unit Name (eg, section) []:test\n  Common Name (eg, YOUR name) []:root\n  Email Address []:sky\n```\n\n3.自签署证书 ：\n<table><tr><td bgcolor=#54FF9F>**ca/ca-cert.pem**</td></tr></table>\n\n```shell\nopenssl x509 -req -in ca/ca-req.csr -out ca/ca-cert.pem -signkey ca/ca-key.pem -days 3650\n\n# 查看证书格式:\nopenssl x509 -in ca/ca-cert.pem -text -noout\n```\n\n4.将证书导出成浏览器支持的.p12格式 ：\n\n```shell\nopenssl pkcs12 -export -clcerts -in ca/ca-cert.pem -inkey ca/ca-key.pem -out ca/ca.p12\n```\n密码：changeit\n\n### **二.生成server证书。**\n1.创建私钥 ：\n<table><tr><td bgcolor=#54FF9F>**server/server-key.pem**</td></tr></table>\n\n```shell\nopenssl genrsa -out server/server-key.pem 2048\n\n# 查看key\nopenssl rsa -in server/server-key.pem -text -noout\n\n# 如果是DER格式的话,同理应该这样\nopenssl rsa -in server/server-key.pem -text -noout -inform der\n```\n\n2.创建证书请求 ：\n<table><tr><td bgcolor=#54FF9F>**server/server-req.csr**</td></tr></table>\n\n```shell\nopenssl req -new -out server/server-req.csr -key server/server-key.pem\n```\n\n```text\nCountry Name (2 letter code) [AU]:cn\nState or Province Name (full name) [Some-State]:zhejiang\nLocality Name (eg, city) []:hangzhou\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:skyvision\nOrganizational Unit Name (eg, section) []:test\nCommon Name (eg, YOUR name) []:192.168.1.246 注释：一定要写服务器所在的ip地址\nEmail Address []:sky\n```\n\n查看csr文件内容:\n```shell\nopenssl req -in server-req.csr -text -noout // -noout 不用输出csr文件原始内容\n```\n\n``` text\nCertificate Request:\n    Data:\n        Version: 0 (0x0)\n        Subject: C=XX, L=Default City, O=Default Company Ltd, CN=10.239.140.186\n        Subject Public Key Info:\n            Public Key Algorithm: rsaEncryption\n                Public-Key: (2048 bit)\n                Modulus:\n                    00:e2:0c:a7:33:33:d9:9b:90:1b:29:30:3c:81:31:\n                    09:97:0a:a9:76:d5:54:be:63:17:21:0c:b9:3a:f0:\n                    a6:02:37:1a:d4:1e:53:4e:e0:c8:d9:5f:27:57:7f:\n                    f3:eb:7f:9d:ad:79:d6:e7:40:64:c8:bc:3d:f3:b4:\n                    16:d6:30:e9:16:04:b6:a0:0e:8f:75:e4:4b:d6:8e:\n                    0a:8e:75:d8:41:89:09:90:96:b0:8d:32:5f:b5:96:\n                    1d:65:d6:a6:b4:c7:eb:3d:3b:f9:62:36:69:7d:07:\n                    6d:05:89:ce:a6:a5:98:a0:b2:5f:ab:bc:25:ba:08:\n                    d8:86:0a:b9:c0:91:ca:f8:d3:bb:36:14:21:f9:c2:\n                    b5:53:43:a9:2c:03:39:9b:93:ef:1d:d9:20:ef:dd:\n                    ff:57:c6:b5:47:e8:bb:46:32:e3:1d:3b:2e:5b:15:\n                    11:80:72:f6:2e:f5:b2:cc:02:7f:b1:d6:e9:3d:8e:\n                    0e:66:f6:6d:45:0e:2f:8c:d5:c3:92:dc:a1:9a:d9:\n                    b0:33:82:30:69:0a:05:ee:08:1b:a6:81:f4:bb:31:\n                    0d:fa:26:37:eb:4f:c8:58:df:e5:be:cc:ac:9a:62:\n                    42:f1:af:8c:35:88:e4:f3:b4:76:8f:6c:13:1f:9a:\n                    61:e0:08:0f:f2:b1:d6:f3:61:b4:0a:5d:9a:61:5f:\n                    e1:0b\n                Exponent: 65537 (0x10001)\n        Attributes:\n            a0:00\n    Signature Algorithm: sha256WithRSAEncryption\n         5b:62:35:07:43:99:dc:af:7c:61:1e:76:4e:f8:ef:59:b2:27:\n         60:71:30:15:5d:f3:0b:b1:b4:53:29:ec:d1:7c:18:48:0a:b3:\n         fe:b7:6d:80:ef:dc:c6:24:04:3d:bd:c1:b8:61:49:f3:1e:fb:\n         22:0f:fb:06:99:ec:db:18:ac:34:ff:4b:15:f8:84:06:01:4d:\n         68:4f:0c:a2:a5:34:dc:1b:61:44:c7:ff:ef:5d:92:a1:09:3f:\n         11:27:1c:a7:30:8e:97:6a:08:03:99:e6:6a:8f:1d:d6:ea:e7:\n         cd:18:a7:eb:36:3d:e7:6b:5e:ef:72:85:ca:eb:89:97:02:cf:\n         fc:38:31:58:e1:66:85:d1:e7:49:e2:72:ef:b1:60:36:55:d7:\n         90:bd:8d:0e:d8:c6:8f:d2:bf:bf:43:85:36:04:2e:f1:ec:5f:\n         d8:1b:17:22:a4:6a:de:a7:b2:2b:00:30:27:e6:4b:32:4d:55:\n         70:b0:61:3d:3f:f2:9d:e7:24:f6:4c:1f:bf:63:6a:d9:16:ef:\n         cb:91:a3:a4:43:b5:1f:11:85:ad:0e:b1:57:39:f2:0a:56:ec:\n         52:90:b0:11:96:c6:28:e0:de:0c:eb:f2:b1:66:ce:04:48:7f:\n         11:90:09:1d:fd:ca:a7:25:66:32:a2:64:33:1a:5e:a9:85:50:\n         8a:2d:90:a5\n```\n\n3.自签署证书 ：\n<table><tr><td bgcolor=#54FF9F>**server/server-cert.pem**</td></tr></table>\n\n```shell\nopenssl x509 -req -in server/server-req.csr -out server/server-cert.pem -signkey server/server-key.pem -CA ca/ca-cert.pem -CAkey ca/ca-key.pem -CAcreateserial -days 3650\n```\n* -CA选项指明用于被签名的csr证书\n* -CAkey选项指明用于签名的密钥\n* -CAcreateserial指明文件不存在时自动生成\n\n查看证书格式:\n```shell\nopenssl x509 -in server/server-cert.pem -text -noout\n```\n\n可以查看到证书里所包含的public key等相关信息:\n```\nCertificate:\nData:\n    Version: 1 (0x0)\n    Serial Number:\n        cc:db:c0:f2:12:e8:09:27\nSignature Algorithm: sha256WithRSAEncryption\n    Issuer: C=XX, L=Default City, O=Default Company Ltd, CN=AI\t// 签发者(CA机构)\n    Validity\n        Not Before: Jul 16 07:01:18 2020 GMT\n        Not After : Jul 14 07:01:18 2030 GMT\n    Subject: C=XX, L=Default City, O=Default Company Ltd, CN=sky\n    Subject Public Key Info:\n        Public Key Algorithm: rsaEncryption\n            Public-Key: (2048 bit)\t\t\t// public key, 因此本地可以不需要再存储保留public key, 证书里已包含.\n            Modulus:\n                00:cd:d7:ed:e9:c6:5e:fa:bc:ef:1e:4e:92:52:99:\n                f0:34:96:67:7b:32:1b:f6:53:df:ca:7b:e5:72:6a:\n                29:e5:85:27:eb:71:00:c6:90:ac:c1:64:62:0d:2b:\n                b1:bc:b8:ee:e1:d4:54:b7:95:21:1e:de:56:c7:25:\n                4c:d4:2d:29:5f:48:19:8a:05:c4:33:d3:06:16:ec:\n                68:e2:81:07:cf:f9:d1:15:b2:68:3d:da:44:c3:d5:\n                ba:a3:0f:9e:34:34:71:53:4f:02:4b:eb:f8:de:fd:\n                94:3f:f4:ee:12:48:ea:b1:60:62:be:58:47:78:29:\n                59:5b:ae:57:53:23:31:aa:78:cc:6c:f0:f7:e9:76:\n                4a:b9:25:79:3f:9c:05:4e:f0:8e:87:32:df:87:72:\n                67:64:2e:9f:85:15:64:bf:ca:ce:33:71:ee:bb:1a:\n                d3:26:09:34:9b:65:b9:15:71:28:14:37:48:79:1b:\n                b1:99:a4:8c:cc:27:a1:a4:c4:28:8e:01:e5:08:db:\n                e6:45:6e:3d:d9:03:a9:cb:17:25:b7:c9:c9:4b:fb:\n                e5:93:d1:de:31:fe:a9:34:29:c3:29:a4:27:c2:eb:\n                66:99:c6:db:ba:52:07:30:97:d4:0a:1e:1b:5d:72:\n                f6:ff:19:92:22:c0:44:76:74:f7:a7:0d:c5:77:c8:\n                1c:55\n            Exponent: 65537 (0x10001)\nSignature Algorithm: sha256WithRSAEncryption\n     28:d1:d9:29:a5:40:f3:d3:d6:95:87:fd:2c:70:dc:0f:1c:86:\n     08:35:d0:a8:8e:d0:5d:78:28:ae:88:33:61:db:cd:b6:80:1c:\n     88:62:b8:ce:cf:87:14:15:bd:27:9a:3e:77:cb:a1:e0:11:0d:\n     89:ef:f2:e8:b2:2c:cf:96:26:bd:06:3a:7b:8f:4b:fa:b2:c3:\n     f9:14:3e:18:ef:57:b5:37:95:01:a0:0f:bf:6e:5c:c9:47:7b:\n     1a:ed:ca:7a:31:a1:89:e8:0d:4d:95:d2:61:e3:b8:48:e5:86:\n     19:91:3e:00:86:07:50:df:e2:57:29:69:61:c4:cc:55:8f:60:\n     de:20:c1:0d:7d:c7:98:52:f4:34:08:90:c5:90:34:ec:86:0f:\n     ad:9b:e7:1a:d4:7b:d9:dd:59:82:de:54:d3:87:8e:e7:82:ae:\n     22:70:cf:e7:d7:8c:f1:55:57:6d:41:e6:44:3c:83:b7:73:7e:\n     9a:d5:1d:af:72:e9:4e:88:d4:4f:9f:33:2f:dc:1b:50:10:8b:\n     db:cd:e4:0e:e6:96:cd:c6:27:c7:4b:c7:9f:05:74:32:35:7e:\n     99:78:36:30:ae:78:4b:c3:1a:6b:b8:db:62:23:b8:ab:22:11:\n     11:81:95:5d:46:f0:45:15:77:1f:6b:c0:bf:9d:a2:d2:b4:62:\n     c9:b5:2b:dd\n```\n\n4.将证书导出成浏览器支持的.p12格式 ：\n\n```shell\nopenssl pkcs12 -export -clcerts -in server/server-cert.pem -inkey server/server-key.pem -out server/server.p12\n```\n密码：changeit\n\nAdditional：\n因为server端的证书是由CA的private key签名(sign)server端的public key及其持有者的真实身份得到的, 因此server端证书里就包含server的public key.  \n就不需要用Openssl生成server端的public key. 用server端的**csr（certificate signing requests）**生成server证书即可. client端也一样.  \nclient端只需要用CA的public key(CA的public key所有人都能获取)解密server端存放在CA的证书文件就可以得到Server端的public key.  \n然后client端就可以用解密得到的server端的public key对server端用自身private key加密的信息附带的摘要A(digest)进行解密.  \nclient端将server端发过来的信息用Hash得到摘要B(digest)与解密得到的摘要A(digest)对比, 如果一直则说明信息没有被黑客篡改.  \n如果中间黑客把server端加密的摘要A(digest)修改了，则client用从CA的public key解密得到的server端的public key是解不开摘要A(digest)的, 说明摘要信息被黑客篡改, 内容不可靠.  \n\n如果要生成RSA公钥, command如下:\n\n```shell\nopenssl rsa -in server/server-key.pem -pubout -out server/server-public-key.pem\n```\n\n### **三.生成client证书。**\n1.创建私钥 ：\n<table><tr><td bgcolor=#54FF9F>**client/client-key.pem**</td></tr></table>\n\n```shell\nopenssl genrsa -out client/client-key.pem 2048\n\n# 查看key\nopenssl rsa -in client/client-key.pem -text -noout\n\n# 如果是DER格式的话,同理应该这样\nopenssl rsa -in client/client-key.pem -text -noout -inform der\n```\n\n2.创建证书请求 ：\n<table><tr><td bgcolor=#54FF9F>**client/client-req.csr**</td></tr></table>\n\n```shell\nopenssl req -new -out client/client-req.csr -key client/client-key.pem\n  Country Name (2 letter code) [AU]:cn\n  State or Province Name (full name) [Some-State]:zhejiang\n  Locality Name (eg, city) []:hangzhou\n  Organization Name (eg, company) [Internet Widgits Pty Ltd]:skyvision\n  Organizational Unit Name (eg, section) []:test\n  Common Name (eg, YOUR name) []:sky\n  Email Address []:sky 注释：就是登入中心的用户（本来用户名应该是Common Name，但是中山公安的不知道为什么使用的Email Address，其他版本没有测试）\n  Please enter the following ‘extra’ attributes\n  to be sent with your certificate request\n  A challenge password []:123456\n  An optional company name []:tsing\n```\n\n3.自签署证书 ：\n<table><tr><td bgcolor=#54FF9F>**client/client-cert.pem**</td></tr></table>\n\n```shell\nopenssl x509 -req -in client/client-req.csr -out client/client-cert.pem -signkey client/client-key.pem -CA ca/ca-cert.pem -CAkey ca/ca-key.pem -CAcreateserial -days 3650\n```\n* -CA选项指明用于被签名的csr证书\n* -CAkey选项指明用于签名的密钥\n* -CAcreateserial指明文件不存在时自动生成\n\n查看证书格式:\n``` shell\nopenssl x509 -in client/client-cert.pem -text -noout\n```\n\n4.将证书导出成浏览器支持的.p12格式 ：\n\n```shell\nopenssl pkcs12 -export -clcerts -in client/client-cert.pem -inkey client/client-key.pem -out client/client.p12\n```\n密码：changeit\n\nAdditional. 生成RSA公钥:\n\n```shell\nopenssl rsa -in client/client-key.pem -pubout -out client/client-public-key.pem\n```\n\n请一定严格根据里面的步骤来，待实验成功后，修改你自己想要修改的内容。我就是一开始没有安装该填写的来，结果生成的证书就无法配对成功。\n\n## linux信任证书\n\n``` shell\ncp *.pem /etc/pki/ca-trust/source/anchors/\nupdate-ca-trust\n```\n","tags":["security"],"categories":["technologies","security"]},{"title":"06 Deploy Vault, Consul, KES, MinIO on kubernetes","url":"/2021/05/31/storage/minio/minIO_06_KMS_k8s/","content":"\n## 总览\n\nVault, Consul, KES, MinIO 都部署在K8s.  \n\n## referenc link\n\nvault:\nhttp://docs.minio.org.cn/docs/master/minio-kms-quickstart-guide\nhttps://github.com/hashicorp/vault-helm/blob/master/values.yaml\nhttps://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes\nhttps://learn.hashicorp.com/tutorials/vault/troubleshooting-vault?in=vault/operations#server-gave-http-response-to-https-client\n\nKES:\nhttps://github.com/minio/kes\nhttps://github.com/minio/kes/blob/master/Dockerfile.release\n\n## 概念\n\nMinIO使用密钥管理系统（KMS）支持SSE-S3。如果客户端请求SSE-S3，或 启用了自动加密，则MinIO服务器会使用唯一的对象密钥对每个对象进行加密，该对象密钥受 KMS管理的主密钥保护。\n\nKMS将MinIO作为面向应用程序的存储系统与安全密钥存储区分开，并且 可以由专门的安全团队进行管理。MinIO 通过我们的KES project支持常用的KMS实现，例如 Hashicorp Vault。 通过KES，可以利用存储基础架构（MinIO群集）水平扩展KMS。 通常，MinIO-KMS基础结构如下所示：\n\n```\n     ┌─────────┐         ┌────────────┐         ┌─────────┐  \n     │  MinIO  ├─────────┤ KES Server ├─────────┤   KMS   │ \n     └─────────┘         └────────────┘         └─────────┘  \n```\n\n当您将存储基础架构扩展到多个MinIO群集时，您的架构应如下所示：\n\n```\n    ┌────────────┐\n    │ ┌──────────┴─┬─────╮          ┌────────────┐\n    └─┤ ┌──────────┴─┬───┴──────────┤ ┌──────────┴─┬─────────────────╮\n      └─┤ ┌──────────┴─┬─────┬──────┴─┤ KES Server ├─────────────────┤\n        └─┤   MinIO    ├─────╯        └────────────┘            ┌────┴────┐\n          └────────────┘                                        │   KMS   │\n                                                                └─────────┘\n```\n\n**`请注意`**, 所有MinIO群集均仅具有 **`其自己的`** KES实例的连接，而不能直接访问Vault（作为一种可能的KMS实现）。 每个KES实例将处理“其” MinIO群集发出的所有加密/解密请求，从而使中央KMS实现不必处理 大量流量。相反，每个KES实例都将使用中央KMS实现作为安全密钥存储，并从中获取所需的主密钥。\n\n该指南显示了如何使用Hashicorp Vault作为KMS实施来设置MinIO-KMS部署。 因此，它显示了如何设置和配置：\n * Vault服务器作为中央密钥库。\n * 一个KES服务器实例，作为MinIO和保险柜之间的中间件。\n * MinIO实例本身。\n\n:::info\n**`请注意,`** 为简便起见，本指南使用自签名证书。在生产部署中，应使用 由[“公共”]{.blue}（例如，让我们加密）或组织内部的CA颁发的X.509证书。\n:::\n\n本指南说明如何在同一台计算机上设置三台不同的服务器：\n\n * Vault服务器在K8S集群中为 https://vault.dev.svc.cluster.local:8200\n * KES服务器在K8S集群中为 https://kes-svc.minio.svc.cluster.local:7373\n * MinIO在K8s上部署后9000映射到主机端口位30007, K8S集群中服务地址:https://minio-hl-svc.minio.svc.cluster.local; 映射到本地30007端口，所以本地minio服务地址为:https://10.239.140.73:30007 or https://127.0.0.1:30007\n\n## Download\n安装MinIO，KES和Vault。对于MinIO，请参阅 MinIO 快速入门指南. 然后安装KES并下载 适用于您的操作系统和平台的最新Vault二进制文件.  \n\n### Vault:\n\n``` bash 命令行提示符 mark:1-3 command:(\"$\":1-3,6||\"#\":5)\nwget https://releases.hashicorp.com/vault/1.7.0-rc1/vault_1.7.0-rc1_linux_amd64.zip\nunzip vault_1.7.0-rc1_linux_amd64.zip\nmv vault /usr/local/bin/ # 如果cp过去的话, 下面启动vault server时候自动创建的vault文件夹会与此已存在的vault二进制文件冲突, 因此直接move过去.\n\n# vault 可以在Linux系统上使用 mlock syscall 来防止 OS 将内存中的数据写入磁盘（交换）\nsudo setcap cap_ipc_lock=+ep $(readlink -f $(which vault))\n```\n\n### KES:\nhttps://github.com/minio/kes#binary-releases\n```\nwget https://github.com/minio/kes/releases/latest/download/kes-linux-amd64\nmv kes-linux-amd64 kes\nchmod +x kes\ncp kes /usr/local/bin/\nkes -h\n\n# KES 可以在Linux系统上使用 mlock syscall 来防止 OS 将内存中的数据写入磁盘（交换）\nsudo setcap cap_ipc_lock=+ep $(readlink -f $(which kes))\n```\n\n### MinIO： \n```\nwget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\ncp minio /usr/local/bin/\n```\n\n### mc:\n```\nwget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\n./mc --help\n```\n\n## 机器配置\n\n| IP | 机器名 | 用途 |\n| :----- | :----- | :----- |\n| 10.239.140.73 | master | K8S集群节点, master(去除污点), 部署KMS, KES, MinIO 到K8S集群 |\n| 10.239.131.157 | laboratory | K8S集群节点, master(去除污点) |\n\nopenssl 版本：\n```\n$ openssl version -a\nOpenSSL 1.1.1j  16 Feb 2021\nbuilt on: Tue Mar 16 02:55:39 2021 UTC\nplatform: linux-x86_64\noptions:  bn(64,64) rc4(16x,int) des(int) idea(int) blowfish(ptr)\ncompiler: gcc -fPIC -pthread -m64 -Wa,--noexecstack -Wall -O3 -DOPENSSL_USE_NODELETE -DL_ENDIAN -DOPENSSL_PIC -DOPENSSL_CPUID_OBJ -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DKECCAK1600_ASM -DRC4_ASM -DMD5_ASM -DAESNI_ASM -DVPAES_ASM -DGHASH_ASM -DECP_NISTZ256_ASM -DX25519_ASM -DPOLY1305_ASM -DZLIB -DNDEBUG\nOPENSSLDIR: \"/usr/local/openssl/ssl\"\nENGINESDIR: \"/usr/local/openssl/lib/engines-1.1\"\nSeeding source: os-specific\n```\n\n## 生成证书\n\n### 根证书\n\n``` shell\nmkdir pki\ncd pki\n\nopenssl genrsa -out \"root-ca.key\" 4096\n\n# 注意, csr里subj的CN会覆盖下面生成证书配置文件的CN.\nopenssl req -new -key \"root-ca.key\" -out \"root-ca.csr\" -sha512 \\\n        -subj '/C=US/ST=CA/L=China/O=Hce/CN=Vault CA'\n\ncat > \"root-ca.conf\" <<EOF\n[root_ca]\nbasicConstraints = critical,CA:TRUE,pathlen:1\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\nsubjectKeyIdentifier=hash\nEOF\n\nopenssl x509 -req -days 3650 -in \"root-ca.csr\" -signkey \"root-ca.key\" -sha512 \\\n        -out \"root-ca.crt\" -extfile \"root-ca.conf\" -extensions root_ca\n```\n**[\"-extensions root_ca\"]{.blue}** 指向 minio-crt.conf配置文件中的 **[[root_ca]]{.blue}**.  \n\n * CA证书必须包含 **[basicConstraints]{.blue}**值，且CA参数需为TRUE。终端用户证书必须将CA参数设置为FALSE，或者不包含basicConstraints值。\n\n * **[pathlen]{.blue}** 参数指明了在证书链的当前证书之下允许存在的最大CA数量。当CA的pathlen值为0时，该CA仅能对终端用户证书进行签名，不能再对CA进行签名。\n\n### vault证书\n\n``` shell\n\n# 生成vault证书\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out vault-server.key\n\nopenssl req -new  -key \"vault-server.key\" -out \"vault-server.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=China/O=Hce/CN=vault.dev.svc.cluster.local'\n\nopenssl x509 -req -days 30  -in \"vault-server.csr\" -CA \"root-ca.crt\" -CAkey \"root-ca.key\"  -CAcreateserial -out \"vault-server.crt\" -extfile \"vault-crt.conf\" -extensions vault_server\n```\n\nvault-crt.conf 文件内容\n``` yaml\n[vault_server]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:localhost, DNS:vault.dev.svc.cluster.local DNS:master, IP:127.0.0.1, IP:10.239.140.73\nsubjectKeyIdentifier=hash\n```\n\n* **[subjectAltName]{.blue}** 至少需要包含 [DNS:vault.dev.svc.cluster.local]{.blue} 和 IP:[127.0.0.1]{.blue}, 前者是用于集群间通信认证, 后者是用于容器里带的 [valut]{.blude} 客户端初始化等操作时候的认证.\n\n### KES/minio证书\nKES和minio共用一样的密钥和证书, 原因是目前没办法把KES证书挂载到minio容器, 就先这样, 后续会更新.\n\n``` shell\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out private.key\n\n#生成csr, 此处CN可以随便指定\nopenssl req -new  -key \"private.key\" -out \"public.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=China/O=Hce/CN=service'\n\nopenssl x509 -req -days 30  -in \"public.csr\" -CA \"root-ca.crt\" -CAkey \"root-ca.key\"  -CAcreateserial -out \"public.crt\" -extfile \"minio-crt.conf\" -extensions minio_server\n```\n\n**[\"-extensions minio_server\"]{.blue}** 指向 minio-crt.conf配置文件中的 **[[minio_server]]{.blue}**.  \n\n\nminio-crt.conf文件内容\n\n``` yaml\n[minio_server]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:localhost, DNS:*.minio-hl-svc.minio.svc.cluster.local, DNS:kes-svc.minio.svc.cluster.local, DNS:master, DNS:laboratory,IP:127.0.0.1, IP:10.239.140.73,IP:10.239.131.157\nsubjectKeyIdentifier=hash\n```\n* **[subjectAltName]{.blue}** 至少需要包含 [DNS:*.minio-hl-svc.minio.svc.cluster.local]{.blue}, [DNS:kes-svc.minio.svc.cluster.local]{.blue}, [IP:127.0.0.1]{.blue}\n\n:::info\n **[\"subjectAltName\"]{.blue}** 表明此证书属于列出的服务A所属的域名和IP, 服务B访问此服务A时, 会将服务A的所属的域名和IP与服务A的有效证书中包含的域名和IP比较, 一致或包含则验证通过.  \n:::\n\n拷贝一份密钥和证书给KES用.  \n``` shell\ncp public.crt kes-server.crt\ncp private.key kes-server.key\n```\n\n## KMS(Vault)\n\n### 部署consul\n\n```shell\n# helm添加repo\nhelm del consul -n dev\n\n# 查看所有Vault版本:\nhelm search repo hashicorp/vault --versions\n# 安装指定版本:\n(reference)helm install vault hashicorp/vault --namespace vault --version 0.5.0\n# 安装默认最新版本\nhelm repo add hashicorp https://helm.releases.hashicorp.com\nhelm repo update\n\n# 安装 local-storage-class\ncat > \"local-storage.yaml\" <<EOF\n# Only create this for K8s 1.9+\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n# Supported policies: Delete, Retain\nreclaimPolicy: Delete\nEOF\nkubectl apply -f local-storage.yaml\n\n# 安装consul\nkubectl create namespace dev\nkubectl apply -f pv-consul.yaml\nhelm install consul hashicorp/consul -f consul-values.yaml --namespace dev\n\n# Verify the Consul cluster\nkubectl get po -n dev\nkubectl exec -it consul-server-0 -n dev -- consul members\n\n# 清理:\nhelm del consul -n dev\n\n# 清理pvc\nkubectl delete pvc -l app=consul -n dev\n\n# 清理pv\nkubectl delete pv -l app=consul\n\n# 清理 consul 在主机存储的文件\n# 登陆 master 机器\nrm -rf /home/zhan/storage/minio/disks/local-pv-master-01/*\nrm -rf /home/zhan/storage/minio/disks/local-pv-master-01/*\n# 登陆 laboratory 机器\nrm -rf /home/zhan/storage/minio/disks/local-pv-laboratory-01/*\nrm -rf /home/zhan/storage/minio/disks/local-pv-laboratory-01/*\n\n```\n\n### 部署vault\n\n``` shell\n# 创建secret资源\nkubectl --namespace='dev' create secret tls tls-ca --cert ./pki/root-ca.crt --key ./pki/root-ca.key\nkubectl --namespace='dev' create secret tls tls-server --cert ./pki/vault-server.crt --key ./pki/vault-server.key\n\n# 安装\nhelm install vault hashicorp/vault --namespace dev -f values.yaml\nkubectl get pods --selector='app.kubernetes.io/name=vault' --namespace='dev'\n\n# 初始化\nkubectl exec -it vault-0 -n dev -- vault operator init\n  Unseal Key 1: +1/bK0rrP5ntXjvKXX2tsmsvb4HvKahE8jX+ht5IQiO4\n  Unseal Key 2: glBmkDM950AO0rl89ckI9M+HEE2zAnT8PVKOv7yxW55Z\n  Unseal Key 3: mJO6ULlGwaV+mxAyDC+mj+ggp0MFle0qhOdMiw8GSWMT\n  Unseal Key 4: 1pq9vUpAnRwMVufuCG1D6lxLKiseTZsgN+P2J0/qFfAl\n  Unseal Key 5: n/XARk8In6cxOtzjtO27h5AYL/JqPnst/a5oe6X/Zwty\n  \n  Initial Root Token: s.vmMwIUVY8fMIIdXUB35uZvGD\n\n# 解封:\nkubectl exec -it vault-0 -n dev -- vault operator unseal +1/bK0rrP5ntXjvKXX2tsmsvb4HvKahE8jX+ht5IQiO4\nkubectl exec -it vault-0 -n dev -- vault operator unseal glBmkDM950AO0rl89ckI9M+HEE2zAnT8PVKOv7yxW55Z\nkubectl exec -it vault-0 -n dev -- vault operator unseal mJO6ULlGwaV+mxAyDC+mj+ggp0MFle0qhOdMiw8GSWMT\n\n# 查看解封状态\nkubectl exec -it vault-0 -n dev -- vault status\n  Key             Value\n  ---             -----\n  Seal Type       shamir\n  Initialized     true\n  Sealed          false\n  Total Shares    5\n  Threshold       3\n  Version         1.6.2\n  Storage Type    consul\n  Cluster Name    vault-cluster-a10c41b0\n  Cluster ID      3e10ba0e-0f81-6c2d-3ef6-3565713cdc62\n  HA Enabled      true\n  HA Cluster      https://vault-0.vault-internal:8201\n  HA Mode         active\n\n(optional)# 本地映射端口, 如果需要在本地vaule客户端操作k8s集群中搭建的vault服务, 需要再生成的vault证书里 [subjectAltName]{.blue} 字段加上 [本地主机域名]{.blue} 或 [本地主机IP]{.blue}\n(optional)kubectl apply -f  nodePort-vault.yaml\n\n# 使用:\nkubectl exec -it vault-0 -n dev -- /bin/sh -c \"export VAULT_TOKEN=s.vmMwIUVY8fMIIdXUB35uZvGD;vault secrets enable kv\"\nkubectl exec -it vault-0 -n dev -- /bin/sh -c \"export VAULT_TOKEN=s.vmMwIUVY8fMIIdXUB35uZvGD;vault auth enable approle\"\n\n# 登陆pod容器\nkubectl exec -it vault-0 -n dev -- /bin/sh\n\ncat > /tmp/minio-kes-policy.hcl <<EOF\npath \"kv/minio/*\" {\n  capabilities = [ \"create\", \"read\", \"delete\" ]\n}\nEOF\n\nexport VAULT_TOKEN=s.vmMwIUVY8fMIIdXUB35uZvGD\nvault policy write minio-key-policy /tmp/minio-kes-policy.hcl\n\nvault write auth/approle/role/kes-role token_num_uses=0  secret_id_num_uses=0  period=5m\nvault write auth/approle/role/kes-role policies=minio-key-policy\nvault read auth/approle/role/kes-role/role-id \n  Key        Value\n  ---        -----\n  role_id    1e82b3a5-d935-dd04-a5cd-480ec74791d2\n\n# 如果下面启动KES服务出现\"* Vault is sealed\"错误， 需要重新获取secret-id，并填写进KES配置文件\nvault write -f auth/approle/role/kes-role/secret-id    \n  Key                   Value\n  ---                   -----\n  secret_id             c1625424-d82d-17cc-ac6d-84844306574c\n  secret_id_accessor    cd6f5c1e-63f2-9b81-2637-56cb40a6c52b\n\n# Use the Vault CLI to inspect the K/V backend\n# 在下面部署KES生成新的key后才能运行下面命令看到\nvault kv list kv/\n  Keys\n  ----\n  minio/\nvault kv list kv/minio\n  Keys\n  ----\n  minio-key-1\n\n# 退出pod容器\nexit\n\n\n# 清理:\nhelm del vault -n dev\n\n# 清理secret\nkubectl delete secret tls-ca -n dev\nkubectl delete secret tls-server -n dev\n\n```\n\n\n### pv-consul.yaml\n\n``` yaml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: consul-local-pv-master-01\n  labels:\n    app: consul\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /home/zhan/storage/minio/disks/local-pv-master-01\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - master\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: consul-local-pv-master-02\n  labels:\n    app: consul\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /home/zhan/storage/minio/disks/local-pv-master-02\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - master\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: consul-local-pv-laboratory-01\n  labels:\n    app: consul\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /home/zhan/storage/minio/disks/local-pv-laboratory-01\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - laboratory\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: consul-local-pv-laboratory-02\n  labels:\n    app: consul\nspec:\n  capacity:\n    storage: 10Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /home/zhan/storage/minio/disks/local-pv-laboratory-02\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - laboratory\n```\n\n### vault-nodeport.yaml\n\n``` yaml\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: dev\n  name: vault-nodeport\nspec:\n  type: NodePort\n  selector:\n    app.kubernetes.io/instance: vault\n    app.kubernetes.io/name: vault\n    component: server\n  ports:\n      # By default and for convenience, the `targetPort` is set to the same value as the `port` field.\n    - port: 8200\n      targetPort: 8200\n      # Optional field\n      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)\n      nodePort: 30820\n```\n\n### values.yaml\n\nReferencd Link: https://github.com/hashicorp/vault-helm/blob/master/values.yaml\n\n``` raw\nglobal:\n  enabled: true\n  imagePullSecrets: []\n  tlsDisable: false\n  openshift: false\n  psp:\n    enable: false\n    annotations: |\n      seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default,runtime/default\n      apparmor.security.beta.kubernetes.io/allowedProfileNames: runtime/default\n      seccomp.security.alpha.kubernetes.io/defaultProfileName:  runtime/default\n      apparmor.security.beta.kubernetes.io/defaultProfileName:  runtime/default\ninjector:\n  enabled: true\n  replicas: 1\n  leaderElector:\n    enabled: true\n    image:\n      repository: \"gcr.io/google_containers/leader-elector\"\n      tag: \"0.4\"\n    ttl: 60s\n  metrics:\n    enabled: false\n  externalVaultAddr: \"\"\n  image:\n    repository: \"hashicorp/vault-k8s\"\n    tag: \"0.8.0\"\n    pullPolicy: IfNotPresent\n  agentImage:\n    repository: \"vault\"\n    tag: \"1.6.2\"\n  authPath: \"auth/kubernetes\"\n  logLevel: \"info\"\n  logFormat: \"standard\"\n  revokeOnShutdown: false\n  namespaceSelector: {}\n  failurePolicy: Ignore\n  certs:\n    secretName: null\n    caBundle: \"\"\n    certName: tls.crt\n    keyName: tls.key\n  resources: {}\n  extraEnvironmentVars: {}\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n{% raw %}\n              app.kubernetes.io/name: {{ template \"vault.name\" . }}-agent-injector\n              app.kubernetes.io/instance: \"{{ .Release.Name }}\"\n{% endraw %}\n              component: webhook\n          topologyKey: kubernetes.io/hostname\n  tolerations: null\n  nodeSelector: null\n  priorityClassName: \"\"\n  annotations: {}\n  extraLabels: {}\n  service:\n    annotations: {}\nserver:\n  image:\n    repository: \"vault\"\n    tag: \"1.6.2\"\n    pullPolicy: IfNotPresent\n  updateStrategyType: \"OnDelete\"\n  resources: {}\n  ingress:\n    enabled: false\n    labels: {}\n    annotations: {}\n    hosts:\n      - host: chart-example.local\n        paths: []\n    tls: []\n  route:\n    enabled: false\n    labels: {}\n    annotations: {}\n    host: chart-example.local\n  authDelegator:\n    enabled: true\n  extraInitContainers: null\n  extraContainers: null\n  shareProcessNamespace: false\n  extraArgs: \"\"\n  readinessProbe:\n    enabled: true\n    failureThreshold: 2\n    initialDelaySeconds: 5\n    periodSeconds: 5\n    successThreshold: 1\n    timeoutSeconds: 3\n  livenessProbe:\n    enabled: false\n    path: \"/v1/sys/health?standbyok=true\"\n    failureThreshold: 2\n    initialDelaySeconds: 60\n    periodSeconds: 5\n    successThreshold: 1\n    timeoutSeconds: 3\n  preStopSleepSeconds: 5\n  postStart: []\n  extraEnvironmentVars:\n    VAULT_CACERT: /vault/userconfig/tls-ca/tls.crt\n  extraSecretEnvironmentVars: [].\n  extraVolumes:\n    - type: secret\n      name: tls-server\n    - type: secret\n      name: tls-ca\n  volumes: null\n  volumeMounts: null\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n{% raw %}\n              app.kubernetes.io/name: {{ template \"vault.name\" . }}\n              app.kubernetes.io/instance: \"{{ .Release.Name }}\"\n{% endraw %}\n              component: server\n          topologyKey: kubernetes.io/hostname\n  tolerations: null\n  nodeSelector: null\n  networkPolicy:\n    enabled: false\n    egress: []\n  priorityClassName: \"\"\n  extraLabels: {}\n  annotations: {}\n  service:\n    enabled: true\n    port: 8200\n    targetPort: 8200\n    annotations: {}\n  dataStorage:\n    enabled: true\n    size: 10Gi\n    mountPath: \"/vault/data\"\n    storageClass: local-storage\n    accessMode: ReadWriteOnce\n    annotations: {}\n  auditStorage:\n    enabled: false\n    size: 10Gi\n    mountPath: \"/vault/audit\"\n    storageClass: local-storage\n    accessMode: ReadWriteOnce\n    annotations: {}\n  dev:\n    enabled: false\n    devRootToken: \"root\"\n  standalone:\n    enabled: false\n    config: |\n      ui = true\n      listener \"tcp\" {\n        tls_disable = 1\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n      }\n      storage \"file\" {\n        path = \"/vault/data\"\n      }\n  ha:\n    enabled: true\n    replicas: 2\n    apiAddr: null\n    raft:\n      enabled: false\n      setNodeId: false\n      config: |\n        ui = true\n        listener \"tcp\" {\n          tls_disable = 1\n          address = \"[::]:8200\"\n          cluster_address = \"[::]:8201\"\n        }\n        storage \"raft\" {\n          path = \"/vault/data\"\n        }\n        service_registration \"kubernetes\" {}\n    config: |\n      ui = true\n      listener \"tcp\" {\n        tls_disable = 0\n        address = \"[::]:8200\"\n        cluster_address = \"[::]:8201\"\n        tls_cert_file = \"/vault/userconfig/tls-server/tls.crt\"\n        tls_key_file = \"/vault/userconfig/tls-server/tls.key\"\n        tls_ca_cert_file = \"/vault/userconfig/tls-ca/tls.crt\"\n      }\n      storage \"consul\" {\n        path = \"vault\"\n        address = \"consul-server.dev:8500\"\n      }\n      service_registration \"kubernetes\" {}\n    disruptionBudget:\n      enabled: true\n      maxUnavailable: null\n  serviceAccount:\n    create: true\n    name: \"\"\n    annotations: {}\n  statefulSet:\n    annotations: {}\nui:\n  enabled: false\n  publishNotReadyAddresses: true\n  activeVaultPodOnly: false\n  serviceType: \"ClusterIP\"\n  serviceNodePort: null\n  externalPort: 8200\n  annotations: {}\n\n```\n\n### consul-values.yaml\n\n``` raw\nglobal:\n  enabled: true\n  name: consul\n  domain: consul\n  image: \"hashicorp/consul:1.9.2\"\n  imagePullSecrets: []\n  imageK8S: \"hashicorp/consul-k8s:0.23.0\"\n  datacenter: dc1\n  enablePodSecurityPolicies: false\n  gossipEncryption:\n    secretName: \"\"\n    secretKey: \"\"\n  tls:\n    enabled: false\n    enableAutoEncrypt: false\n    serverAdditionalDNSSANs: []\n    serverAdditionalIPSANs: []\n    verify: true\n    httpsOnly: true\n    caCert:\n      secretName: null\n      secretKey: null\n    caKey:\n      secretName: null\n      secretKey: null\n  enableConsulNamespaces: false\n  acls:\n    manageSystemACLs: false\n    bootstrapToken:\n      secretName: null\n      secretKey: null\n    createReplicationToken: false\n    replicationToken:\n      # The name of the Kubernetes secret.\n      secretName: null\n      # The key of the Kubernetes secret.\n      secretKey: null\n  federation:\n    enabled: false\n    createFederationSecret: false\n  lifecycleSidecarContainer:\n    resources:\n      requests:\n        memory: \"25Mi\"\n        cpu: \"20m\"\n      limits:\n        memory: \"50Mi\"\n        cpu: \"20m\"\n  imageEnvoy: \"envoyproxy/envoy-alpine:v1.16.0\"\n  openshift:\n    enabled: false\nserver:\n  enabled: \"-\"\n  image: null\n  replicas: 2\n  bootstrapExpect: null\n  enterpriseLicense:\n    secretName: null\n    secretKey: null\n  exposeGossipAndRPCPorts: false\n  ports:\n    serflan:\n      port: 8301\n  storage: 4.9Gi\n  storageClass: local-storage\n  connect: true\n  resources:\n    requests:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n    limits:\n      memory: \"100Mi\"\n      cpu: \"100m\"\n  securityContext:\n    runAsNonRoot: true\n    runAsGroup: 1000\n    runAsUser: 100\n    fsGroup: 1000\n  updatePartition: 0\n  disruptionBudget:\n    enabled: true\n    maxUnavailable: null\n  extraConfig: |\n    {}\n  extraVolumes: []\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n{% raw %}\n              app: {{ template \"consul.name\" . }}\n              release: \"{{ .Release.Name }}\"\n{% endraw %}\n              component: server\n          topologyKey: kubernetes.io/hostname\n  tolerations: \"\"\n  nodeSelector: null\n  priorityClassName: \"\"\n  extraLabels: null\n  annotations: null\n  service:\n    annotations: null\n  extraEnvironmentVars: {}\nexternalServers:\n  enabled: false\n  hosts: []\n  httpsPort: 8501\n  tlsServerName: null\n```\n\n## KES\n\n### 配置\n\n``` shell\n\n# 获取minio 证书 id\nkes tool identity of pki/public.crt\n Identity:  03ebeb2c14c4584b7ba3b487ad2cef8a57b4a8e40fb5d1ffd163b887082058b9\n```\n\n### kes-config.yaml\n\n``` yaml\naddress: 0.0.0.0:7373\nroot:    disabled  # We disable the root identity since we don't need it in this guide\n\ntls:\n  key:  /pki/kes-server.key\n  cert: /pki/kes-server.crt\n\npolicy:\n  minio:\n    paths:\n    - /v1/key/create/minio-*\n    - /v1/key/generate/minio-*\n    - /v1/key/decrypt/minio-*\n    identities:\n    - {APP_IDENTITY} # $ 最好不要写成 03ebeb2c14c4584b7ba3b487ad2cef8a57b4a8e40fb5d1ffd163b887082058b9, 否则证书变了就必须重新build image\n\ncache:\n  expiry:\n    any:    5m0s\n    unused: 20s\n\nkeys:\n  vault:\n    endpoint: https://vault.dev.svc.cluster.local:8200\n    prefix: minio\n    approle:\n      id: ${APP_ROLE_ID} # Your AppRole ID\n      secret: ${APP_SECRET_ID} # Your AppRole Secret ID\n      retry:  15s\n    status:\n      ping: 10s\n    tls:\n      ca: \"/pki/vault-server.crt\" # Since we use self-signed certificates\n```\n\n### 构建KES image\n\n``` shell\n# 构建image\ndocker build -t kes:0.1 --build-arg HTTP_PROXY=http://child-prc.intel.com:913 --build-arg HTTPS_PROXY=http://child-prc.intel.com:913 .\n# image拷贝到laboratory机器\ndocker save -o kes-0.1.tar kes:0.1\nscp kes-0.1.tar root@10.239.131.157:/home/zhan/docker-images/\nssh root@10.239.131.157\ndocker load -i kes-0.1.tar\nexit\n```\n\n### 部署KES\n\n``` shell\n# 创建namespace\nkubectl create ns minio\n\n# 创建secret\nReference:\nhttps://github.com/minio/operator/blob/master/docs/tls.md  \nhttps://kubernetes.io/docs/concepts/configuration/secret/#using-secrets  \nkubectl create secret generic kes-crts --from-file=pki/private.key --from-file=pki/public.crt --from-file=pki/kes-server.crt --from-file=pki/kes-server.key --from-file=pki/vault-server.crt -n minio\n\n# 部署kes\nkubectl apply -f kes-k8s.yaml\n\n# 查看log\nkubectl logs -l app=kes -n minio --tail=-1\n  Authenticating to Hashicorp Vault 'https://vault.dev.svc.cluster.local:8200' ...\n  Endpoint: https://127.0.0.1:7373        https://10.32.0.11:7373\n  \n  Root:     _     [ disabled ]\n  Auth:     off   [ any client can connect but policies still apply ]\n  \n  Keys:     Hashicorp Vault: https://vault.dev.svc.cluster.local:8200\n  \n  CLI:      export KES_SERVER=https://127.0.0.1:7373\n            export KES_CLIENT_KEY=<client-private-key>   // e.g. $HOME/root.key\n            export KES_CLIENT_CERT=<client-certificate>  // e.g. $HOME/root.cert\n            kes --help\n\n# 创建主密钥\nkubectl exec \"$(kubectl get pod -l app=kes -n minio -o jsonpath='{.items[0].metadata.name}')\" -n minio -it -- /bin/bash\nexport VAULT_SKIP_VERIFY=true\n./kes key create -k minio-key-1\n\n#  Derive data keys from the previously created minio-key-1\n./kes key derive -k minio-key-1\n  {\n    plaintext : TPq3GqKMYx8Y1TrzhI8kyKMlKnqLZjcpw3HQ2MU1YIg=\n    ciphertext: eyJhZWFkIjoiQUVTLTI1Ni1HQ00tSE1BQy1TSEEtMjU2IiwiaXYiOiJoOGRRbk9tSmdzRS9tQ0pDU21LdkV3PT0iLCJub25jZSI6Im0rNzdyTjBJMFRPT0Njd2YiLCJieXRlcyI6Imh5aDM5SVBXMEt3VGYxeEt6QWdSdTQ0QTlmSC9jQXR6ckNkWGk5TXo3My9EdkRxMXczTUZYaFlwVWNBVzVlbUYifQ==\n  }\n\n# Decrypt data keys from the previously created minio-key-1\n# ./kes key decrypt -k my-app-key <base64-ciphertext>\n./kes key decrypt -k minio-key-1 eyJhZWFkIjoiQUVTLTI1Ni1HQ00tSE1BQy1TSEEtMjU2IiwiaXYiOiJoOGRRbk9tSmdzRS9tQ0pDU21LdkV3PT0iLCJub25jZSI6Im0rNzdyTjBJMFRPT0Njd2YiLCJieXRlcyI6Imh5aDM5SVBXMEt3VGYxeEt6QWdSdTQ0QTlmSC9jQXR6ckNkWGk5TXo3My9EdkRxMXczTUZYaFlwVWNBVzVlbUYifQ==\n  plaintext: TPq3GqKMYx8Y1TrzhI8kyKMlKnqLZjcpw3HQ2MU1YIg=\n\n# 退出pod\nexit\n```\n::: primary\n**[KES derive]{.blue}** 命令的用法参考 [KES Github](https://github.com/harshavardhana/kes#3-generate-a-new-data-encryption-key-dek)\n[KES Github wiki](https://github.com/minio/kes/wiki/Getting-Started#33-derive-a-new-data-key-plaintextciphertext-pair)\n::: \n\n::: primary\nNow, you can use that master key to derive a new data encryption key.\nYou will get a plaintext and a ciphertext data key. The ciphertext data key is the encrypted version of the plaintext key. Your application would use the plaintext key to e.g. encrypt some application data but only remember the ciphertext key version.\n:::\n\n上面KES derive内容再结合 **[MinIO 官网](https://docs.min.io/docs/minio-security-overview.html)** 的 **[Key rotation - Basic Operation]{.blue}** 部分看.\n\n### Dockerfile\n\n``` docker\nFROM registry.access.redhat.com/ubi8/ubi-minimal:8.3\n\nLABEL name=\"MinIO\" \\\n      vendor=\"MinIO Inc <dev@min.io>\" \\\n      maintainer=\"MinIO Inc <dev@min.io>\" \\\n      version=\"v0.13.4\" \\\n      release=\"v0.13.4\" \\\n      summary=\"KES is a stateless and distributed key-management system for high-performance applications.\" \\\n      description=\"KES as the bridge between modern applications - running as containers on Kubernetes - and centralized KMS solutions. Therefore, KES has been designed to be simple, scalable and secure by default. It has just a few knobs to tweak instead of a complex configuration and does not require a deep understanding of secure key-management or cryptography.\"\n\nARG HTTP_PROXY\nARG HTTPS_PROXY\n\nRUN \\\n    microdnf update --nodocs && \\\n    microdnf install ca-certificates --nodocs && \\\n    microdnf clean all && \\\n    mkdir /licenses && \\\n    curl -s -q https://raw.githubusercontent.com/minio/kes/master/CREDITS -o /licenses/CREDITS && \\\n    curl -s -q https://raw.githubusercontent.com/minio/kes/master/LICENSE -o /licenses/LICENSE\n\nCOPY kes /kes\n\nCOPY ./kes-config.yaml /kes-config.yaml\n\nENV VAULT_SKIP_VERIFY true\nENV KES_CLIENT_CERT /pki/public.crt\nENV KES_CLIENT_KEY /pki/private.key\n\nEXPOSE 7373\n\nWORKDIR /\n\nENTRYPOINT [\"/kes\", \"server\", \"--config=kes-config.yaml\", \"--root=disabled\", \"--auth=off\"]\n\n```\n\n### kes-k8s.yaml\n\n``` yaml\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: minio\n  name: kes-svc\nspec:\n  selector:\n    app: kes\n  ports:\n  - protocol: TCP\n    port: 7373\n    targetPort: 7373\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: minio\n  name: kes-svc-node\nspec:\n  type: NodePort\n  selector:\n    app: kes\n  ports:\n  - protocol: TCP\n    port: 7373\n    targetPort: 7373\n    nodePort: 30737\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: minio\n  name: kes\n  labels:\n    app: kes\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kes\n  template:\n    metadata:\n      labels:\n        app: kes\n    spec:\n      containers:\n        - name: kes\n          image: kes:0.1\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: APP_IDENTITY\n            value: 03ebeb2c14c4584b7ba3b487ad2cef8a57b4a8e40fb5d1ffd163b887082058b9\n          - name: APP_ROLE_ID\n            value: 1e82b3a5-d935-dd04-a5cd-480ec74791d2\n          - name: APP_SECRET_ID\n            value: c1625424-d82d-17cc-ac6d-84844306574c\n          ports:\n          - containerPort: 7373\n          volumeMounts:\n          - name: kes-crts\n            mountPath: \"/pki\"\n      volumes:\n      - name: kes-crts\n        secret:\n          secretName: kes-crts\n\n```\n\n## MinIO\n\n### 磁盘分区\n\n参考linux parted 命令, 如下将/dev/sdb磁盘分出7个50GB大小的磁盘分区.  \n如果系统上没有 `parted` 工具需要用yum或apt-get下载.  \n\n``` shell\nparted --script /dev/sdb \\\n    mklabel gpt \\\n    mkpart primary 4096s 51200MiB \\\n    mkpart primary 51200MiB 102400MiB  \\\n    mkpart primary 102400MiB 153600MiB \\\n    mkpart primary 153600MiB 204800MiB \\\n    mkpart primary 204800MiB 256000MiB \\\n    mkpart primary 256000MiB 307200MiB \\\n    mkpart primary 307200MiB 358400MiB\n```\n\n### mount磁盘\n\n登陆到每台机器操作\n\n``` shell\nmkdir -p /mnt/minio\nmount /dev/sd* /mnt/minio\n\nmkdir -p /mnt/minio/data\n```\n\n:::info\nmount磁盘一定要在最初做，否则后边在部署direct-csi后再mount磁盘的话会导致minio数据存放不到mount到的磁盘.\n:::\n\n### mkdir_minio_node.sh\n用来自动登陆master和laboratory机器创建文件夹.  \n``` shell\n#!/bin/bash\n\n#usecase: ./bash_shell_commands.sh -u <username> -p <pwd>  --node01 <node01-ip> --node02 <node02-ip> --node03 <node03-ip> --node04=<node04-ip>\n#./mkdir_minio_node.sh -u root -p 123456 --node01 10.239.140.73 --node02 10.239.131.157\n\nARGS=`getopt -o u:p: --long username:,password:,node01_ip:,node02_ip:,node03_ip:,node04_ip:: -n 'mkdir_node_minio.sh' -- \"$@\"`\nif [ $? != 0 ]; then\n        echo \"Terminating...\"\n        exit 1\nfi\neval set -- \"${ARGS}\"\necho ${ARGS}\n\nwhile true\ndo\n        case \"$1\" in\n                -u|--username)\n                        username=$2\n                        shift 2 ;;\n                -p|--password)\n                        password=$2\n                        shift 2 ;;\n                --node01_ip)\n                        node01_ip=$2\n                        nodes+=(${node01_ip})\n                        shift 2 ;;\n                --node02_ip)\n                        node02_ip=$2\n                        nodes+=(${node02_ip})\n                        shift 2 ;;\n                --node03_ip)\n                        node03_ip=$2\n                        nodes+=(${node03_ip})\n                        shift 2 ;;\n                --node04_ip)\n                        case \"$2\" in\n                                \"\")\n                                        shift 2 ;;\n                                *)\n                                        node04_ip=$2\n                                        nodes+=(${node04_ip})\n                                        shift 2 ;;\n                        esac\n                        ;;\n                --)\n                        shift\n                        break ;;\n                *)\n                        echo \"Error!\"\n                        exit 1 ;;\n        esac\ndone\n\nfor node in ${nodes[*]}\ndo\nexpect <<-EOF\nspawn ssh $username@${node}\nexpect {\n        \"yes/no\" { send \"yes\\r\"; exp_continue }\n        \"password\" { send \"${password}\\r\"; exp_continue }\n        \"Last login\" { send \"\\r\" }\n}\nexpect \"*#\"\nsend \"rm -rf /mnt/minio/data \\r\"\nsend \"mkdir -p /mnt/minio/data \\r\"\nexpect \"*#\"\nsend \"exit\\r\"\nexpect \"eof\"\nEOF\ndone\n```\n\n### minioinstance.yaml\n\n``` yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-creds-secret\ntype: Opaque\ndata:\n  accesskey: aGNlbWluaW8= # base 64 encoded \"hceminio\" (echo -n 'hceminio' | base64)\n  secretkey: aGNlbWluaW8xMjM= # based 64 encoded \"hceminio123\" (echo -n 'hceminio123' | base64)\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9000\n      targetPort: 9000\n      protocol: TCP\n  selector:\n    app: minio\n---\napiVersion: operator.min.io/v1\nkind: MinIOInstance\nmetadata:\n  name: minio\nspec:\n  metadata:\n    labels:\n      app: minio\n    annotations:\n      prometheus.io/path: /minio/prometheus/metrics\n      prometheus.io/port: \"9000\"\n      prometheus.io/scrape: \"true\"\n  image: minio/minio:RELEASE.2020-06-18T02-23-35Z\n  imagePullPolicy: IfNotPresent\n  serviceName: minio-internal-service\n  zones:\n    - name: \"zone-0\"\n      servers: 4\n  volumesPerServer: 1\n  mountPath: /export\n  volumeClaimTemplate:\n    metadata:\n      name: data\n    spec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          #storage: 1Ti\n          storage: 1Gi\n      storageClassName: direct.csi.min.io\n  credsSecret:\n    name: minio-creds-secret\n  podManagementPolicy: Parallel\n  externalCertSecret:\n    name: tls-ssl-minio\n  requestAutoCert: false\n  certConfig:\n    commonName: \"\"\n    organizationName: []\n    dnsNames: []\n  env:\n    - name: MINIO_KMS_KES_ENDPOINT\n      value: https://kes-svc.minio.svc.cluster.local:7373\n    - name: MINIO_KMS_KES_KEY_FILE\n      value: /tmp/certs/private.key\n    - name: MINIO_KMS_KES_CERT_FILE\n      value: /tmp/certs/public.crt\n    - name: MINIO_KMS_KES_KEY_NAME    # 要根KES创建的 master key 一致\n      value: minio-key-1\n    - name: MINIO_KMS_KES_CA_PATH\n      value: /tmp/certs/public.crt\n  liveness:\n    initialDelaySeconds: 10\n    periodSeconds: 1\n    timeoutSeconds: 1\n```\n\n### minio-operator.yaml\n\n``` yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: minioinstances.operator.min.io\nspec:\n  group: operator.min.io\n  scope: Namespaced\n  names:\n    kind: MinIOInstance\n    singular: minioinstance\n    plural: minioinstances\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n      # openAPIV3Schema is the schema for validating custom objects.\n      # Refer https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema\n      # for more details\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              x-kubernetes-preserve-unknown-fields: true\n              properties:\n                replicas:\n                  type: integer\n                  minimum: 1\n                  maximum: 32\n                image:\n                  type: string\n                serviceName:\n                  type: string\n                volumesPerServer:\n                  type: integer\n                mountPath:\n                  type: string\n                podManagementPolicy:\n                  type: string\n                  enum: [Parallel,OrderedReady]\n                  default: Parallel\n                requestAutoCert:\n                  type: boolean\n                  default: false\n                version:\n                  type: string\n                mountpath:\n                  type: string\n                subpath:\n                  type: string\n                mcs:\n                  type: object\n                  x-kubernetes-preserve-unknown-fields: true\n                  properties:\n                    image:\n                      type: string\n                    replicas:\n                      type: integer\n                      default: 2\n                    mcsSecret:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                kes:\n                  type: object\n                  x-kubernetes-preserve-unknown-fields: true\n                  properties:\n                    image:\n                      type: string\n                    replicas:\n                      type: integer\n                      default: 2\n                    kesSecret:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n            status:\n              type: object\n              properties:\n                currentState:\n                  type: string\n      subresources:\n        # status enables the status subresource.\n        status: {}\n      additionalPrinterColumns:\n        - name: Current State\n          type: string\n          jsonPath: \".status.currentState\"\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: minio-operator-role\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  - secrets\n  - pods\n  - services\n  - events\n  verbs:\n  - get\n  - watch\n  - create\n  - list\n  - delete\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - deployments\n  verbs:\n  - get\n  - create\n  - list\n  - patch\n  - watch\n  - update\n  - delete\n- apiGroups:\n  - batch\n  resources:\n  - jobs\n  verbs:\n  - get\n  - create\n  - list\n  - patch\n  - watch\n  - update\n  - delete\n- apiGroups:\n  - \"certificates.k8s.io\"\n  resources:\n  - \"certificatesigningrequests\"\n  - \"certificatesigningrequests/approval\"\n  - \"certificatesigningrequests/status\"\n  verbs:\n  - update\n  - create\n  - get\n  - delete\n- apiGroups:\n  - operator.min.io\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n- apiGroups:\n  - min.io\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio-operator\n  namespace: minio\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: minio-operator-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: minio-operator-role\nsubjects:\n- kind: ServiceAccount\n  name: minio-operator\n  namespace: minio\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-operator\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minio-operator\n  template:\n    metadata:\n      labels:\n        name: minio-operator\n    spec:\n      serviceAccountName: minio-operator\n      containers:\n        - name: minio-operator\n          image: minio/k8s-operator:2.0.6\n          imagePullPolicy: IfNotPresent\n          # To specify cluster domain, un comment the following:\n          # env:\n          #   - name: CLUSTER_DOMAIN\n          #     value: mycluster.mydomain\n```\n\n### direct-csi\n用来自动生成pv\n\n``` shell\nvim default.env\n  DIRECT_CSI_DRIVES=data  # minio 数据存放位置\n  DIRECT_CSI_DRIVES_DIR=/mnt/minio  # direct-csi的一些控制组件存放路径\n  KUBELET_DIR_PATH=/var/lib/kubelet\n\ngit clone https://github.com/minio/direct-csi.git\ncd direct-csi/\ngit checkout v0.2.1\ncd ../\nexport $(cat default.env)\n```\n\n### 搭建minio\n\nvim build_minio.sh\n\n``` shell\n#!/bin/bash\n\nkubectl create ns minio\n\nkubectl create secret generic tls-ssl-minio --from-file=pki/private.key --from-file=pki/public.crt -n minio\n\n# Create key and crt\n:<<block\nrm -rf pki\nmkdir pki -p\nopenssl genrsa -out pki/private.key 2048\nsleep 1\nopenssl req -new -x509 -days 3650 -key pki/private.key -out pki/public.crt -subj \"/C=US/ST=state/L=location/O=organization/CN=*.minio-hl-svc.minio.svc.cluster.local\"\nsleep 1\nblock\n\n# Using Direct CSI Driver\n#cat << EOF > default.env\n## 这种写法表示一个minio server挂载4个volume, 如果想挂载一个volume就不要写成这样，直接写成下面的 `data`\n#DIRECT_CSI_DRIVES=data{1...4}\n#DIRECT_CSI_DRIVES_DIR=/mnt/minio\n#EOF\ncat << EOF > default.env\nDIRECT_CSI_DRIVES=data\nDIRECT_CSI_DRIVES_DIR=/mnt/minio\nEOF\n\nexport $(cat default.env)\nkubectl apply -k direct-csi\nsleep 3\n\n# Create Operator Deployment\nkubectl apply -f minio-operator.yaml -n minio\nkubectl apply -f minioinstance.yaml -n minio\n```\n\n### nodePort-minio.yaml\n\n``` yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service-nodeport\n  namespace: minio\nspec:\n  type: NodePort\n  ports:\n    - port: 9000\n      targetPort: 9000\n      protocol: TCP\n      nodePort: 30007\n  selector:\n    app: minio\n```\n\n### clean\n\nvim clean.sh\n\n``` shell\n#!/bin/bash\n\n# delete minio pod\nkubectl delete -f minioinstance.yaml -n minio\n\n# delete pvc\npvc_results=$(kubectl get pvc -n minio | grep minio | awk '{print $1}')\npvc_arr=(${pvc_results})\nfor ((i=0; i<${#pvc_arr[*]}; i++ ))\ndo\nkubectl delete pvc ${pvc_arr[i]} -n minio\ndone\n\n# delete pv\npv_results=$(kubectl get pv | grep minio | awk '{print $1}')\npv_arr=(${pv_results})\nfor ((i=0; i<${#pv_arr[*]}; i++ ))\ndo\nkubectl delete pv ${pv_arr[i]}\ndone\n\n# delete minio operator\nkubectl delete -f minio-operator.yaml -n minio\n\n# delete minio sc\nkubectl delete -k direct-csi\n# 登陆到每台机器删除direct-csi插件\nrm -rf /var/lib/kubelet/plugins/direct-*\nrm -rf /var/lib/kubelet/plugins_registry/direct.csi.min.io-reg.sock\n\n\n# delete secret\nkubectl delete secret tls-ssl-minio -n minio\n\n# delete ns\nkubectl delete ns minio\n```\n\n### 清理direct-csi Host上文件\n\n``` shell\nrm -rf /var/lib/kubelet/plugins/direct-csi-*\nrm -rf /var/lib/kubelet/plugins_registry/direct.csi.min.io-reg.sock\n```\n\n\n## mc实例\n\n### 加密minio数据\n\n``` shell\n# 查看minio host列表\n./mc config host list\n\n# 本地创建minio服务地址\n./mc alias set hceminio https://10.239.140.73:30007 hceminio hceminio123 --insecure\n\n# 上传数据加密\n./mc mb hceminio/test01 --insecure\n./mc encrypt set sse-s3 hceminio/test01 --insecure\n./mc encrypt info hceminio/test01 --insecure\n./mc cp k8s.jpg hceminio/test01 --insecure\n./mc stat hceminio/test01/k8s.jpg --insecure\n\n```\n\n### 查看key\nReference Link: https://github.com/minio/mc/blob/master/docs/minio-admin-complete-guide.md\nDisplay status information for the default master key.  \n``` shell\n./mc admin kms key status hceminio --insecure\n  Key: minio-key-1\n     - Encryption ✔\n     - Decryption ✔\n./mc admin kms key status hceminio minio-key-1 --insecure\n  Key: minio-key-1\n           • Encryption ✔\n           • Decryption ✔\n```\n\n### 查看minio日志\n\n``` shell\n./mc admin console hceminio --insecure\n```\n\n\n\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"kafka nifi","url":"/2021/05/31/technologies/bigdata/kafka_nifi/","content":"\n## kafka\n\n\n\n\n\n## nifi\n","tags":["bigdata"],"categories":["technologies","bigdata"]},{"title":"docker 网络","url":"/2021/05/31/technologies/docker/docker_network/","content":"\n## docker 网络\n\n在公司linux系统上配置完proxy后用 [docker build]{.red} 命令后发现下载文件下载不下来, 此时可以设置[docker build 网络]{.red}.  \n\n安装完Docker时，它会自动创建三个网络。你可以使用以下 [docker network ls]{.red} 命令列出这些网络\n\n``` shell\ndocker network ls\n  NETWORK ID          NAME                DRIVER              SCOPE\n  8dc309b605e2        bridge              bridge              local\n  455776962a1c        host                host                local\n  ada67f97c632        none                null                local\n```\n\nDocker内置这三个网络，运行容器时，你可以使用该来指定容器应连接到哪些网络.  \n\n我们在使用docker run/build 创建 Docker image/container 时，可以用--network标志 选项指定容器的网络模式，Docker有以下4种网络模式:  \n\n* [host模式]{.red}： 使用 --network=host 指定.\n\n* [none模式]{.red}： 使用 --network=none 指定.\n\n* [bridge模式]{.red}： 使用 --network=bridge 指定，默认设置.\n\n* [container模式]{.red}： 使用 --network=container:NAME_or_ID 指定.\n\n[host模式]{.red} 类似于Vmware的 [桥接模式]{.red}，与宿主机在同一个网络中，但没有独立IP地址.一个Docker容器一般会分配一个独立的Network Namespace。但如果启动容器的时候使用host模式，那么这个容器将不会获得一个独立的Network Namespace，而是和宿主机共用一个Network Namespace。容器将不会虚拟出自己的网卡，配置自己的IP等，而是使用宿主机的IP和端口.  \n\n\n\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"docker timeout","url":"/2021/05/31/technologies/docker/docker_timeout/","content":"\n## Set timeout\n\nsetting `DOCKER_CLIENT_TIMEOUT` and `COMPOSE_HTTP_TIMEOUT` environment variables.\n\n``` shell\nexport DOCKER_CLIENT_TIMEOUT=600\nexport COMPOSE_HTTP_TIMEOUT=600\n\n# Restart Docker\nsudo systemctl restart docker\n\n# Increase Docker CPU & memory\n参考如下方法, 不过如果不设置-m,--memory和--memory-swap，容器默认可以用完宿舍机的所有内存和 swap 分区.\n```\n\n## Increase CPU\n\nReference Link: https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler\n```\ndocker run -it --cpus=\"1.5\" ubuntu /bin/bash\n```\n::: info\nThis is the equivalent of setting `--cpu-period=\"100000\"` and `--cpu-quota=\"150000\"`.\n:::\n\n## Memory\n\n参考如下方法, 不过如果不设置-m,--memory和--memory-swap，容器默认可以用完宿舍机的所有内存和 swap 分区.\n\nReference Link: https://docs.docker.com/config/containers/resource_constraints/#memory\n\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"Deploy keycloak on kubernetes","url":"/2021/05/31/technologies/security/keycloak/","content":"\n\n## Prerequisites\n\n1. Kubernetes集群\n2. 需要安装helm3+\n3. Persistent volumes(PV) on local storage created for keycloak\n\n## 部署步骤\n\n``` shell\n# 部署local storage\ncat >local-storage.yaml <<EOF\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  name: local-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\nEOF\nkubectl apply -f local-storage.yaml\n\n\n# 设置 postgres password \nsed -i 's/PASSWORD/NEW_PASSWORD/' values.yaml\n\nkubectl create namespace dev\nhelm repo add codecentric https://codecentric.github.io/helm-charts\nhelm install my-keycloak codecentric/keycloak --values values.yaml -n dev\n\nkubectl get po,svc,statefulset -l app.kubernetes.io/instance=my-keycloak -n dev\n  NAME                                  READY   STATUS    RESTARTS   AGE\n  pod/my-keycloak-0                     1/1     Running   0          84m\n  pod/my-keycloak-1                     1/1     Running   0          86m\n  pod/my-keycloak-postgresql-master-0   1/1     Running   0          86m\n  pod/my-keycloak-postgresql-slave-0    1/1     Running   0          86m\n  pod/my-keycloak-postgresql-slave-1    1/1     Running   0          86m\n  \n  ……\n  NAME                                             READY   AGE\n  statefulset.apps/my-keycloak                     2/2     86m\n  statefulset.apps/my-keycloak-postgresql-master   1/1     86m\n  statefulset.apps/my-keycloak-postgresql-slave    2/2     86m\n\nkubectl port-forward svc/my-keycloak-http 8080:80 -n dev\n\n# 如果本机A办法打开浏览器访问，可以登陆到其它有浏览器机器B进行端口映射\nssh -L 8080:127.0.0.1:8080 root@10.67.117.192\n  root@10.67.117.192's password: <输入密码>\n  Last login: Thu Apr  8 18:37:16 2021 from yazhanma-mobl2.ccr.corp.intel.com\n  [root@inspire-dev-nodea ~]#\n\n# 再打开机器B另外一个终端\nfirefox\n\n# 浏览器输入如下网址\nhttp://127.0.0.7:8080 \n\n# 网页中创建User和设置密码，再登陆一遍就可以访问dashboard了.\n```\n\n## 清理\n\n``` shell\nhelm del my-keycloak -n dev\n\nkubectl delete pvc data-my-keycloak-postgresql-master-0 -n dev\nkubectl delete pvc data-my-keycloak-postgresql-slave-0 -n dev\nkubectl delete pvc data-my-keycloak-postgresql-slave-1 -n dev\n```\n\n## values.yaml\n\nvim values.yaml\n\n``` raw\n# Optionally override the fully qualified name\nfullnameOverride: \"\"\n\n# Optionally override the name\nnameOverride: \"\"\n\n# The number of replicas to create\nreplicas: 2\n\nimage:\n  # The Keycloak image repository\n  repository: docker.io/jboss/keycloak\n  # Overrides the Keycloak image tag whose default is the chart version\n  tag: \"\"\n  # The Keycloak image pull policy\n  pullPolicy: IfNotPresent\n\n# Image pull secrets for the Pod\nimagePullSecrets:\n  - name: myRegistrKeySecretName\n\n# Mapping between IPs and hostnames that will be injected as entries in the Pod's hosts files\nhostAliases: []\n# - ip: \"1.2.3.4\"\n#   hostnames:\n#     - \"my.host.com\"\n\n# Indicates whether information about services should be injected into Pod's environment variables, matching the syntax of Docker links\nenableServiceLinks: true\n\n# Pod management policy. One of `Parallel` or `OrderedReady`\npodManagementPolicy: Parallel\n\n# Pod restart policy. One of `Always`, `OnFailure`, or `Never`\nrestartPolicy: Always\n\nserviceAccount:\n  # Specifies whether a ServiceAccount should be created\n  create: true\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"\"\n  # Additional annotations for the ServiceAccount\n  annotations: {}\n  # Additional labels for the ServiceAccount\n  labels: {}\n  # Image pull secrets that are attached to the ServiceAccount\n  imagePullSecrets: []\n\nrbac:\n  create: false\n  rules: []\n  # RBAC rules for KUBE_PING\n  #  - apiGroups:\n  #      - \"\"\n  #    resources:\n  #      - pods\n  #    verbs:\n  #      - get\n  #      - list\n\n# SecurityContext for the entire Pod. Every container running in the Pod will inherit this SecurityContext. This might be relevant when other components of the environment inject additional containers into running Pods (service meshes are the most prominent example for this)\npodSecurityContext:\n  fsGroup: 1000\n\n# SecurityContext for the Keycloak container\nsecurityContext:\n  runAsUser: 1000\n  runAsNonRoot: true\n\n# Additional init containers, e. g. for providing custom themes\nextraInitContainers: \"\"\n\n# Additional sidecar containers, e. g. for a database proxy, such as Google's cloudsql-proxy\nextraContainers: \"\"\n\n# Lifecycle hooks for the Keycloak container\nlifecycleHooks: |\n#  postStart:\n#    exec:\n#      command:\n#        - /bin/sh\n#        - -c\n#        - ls\n\n# Termination grace period in seconds for Keycloak shutdown. Clusters with a large cache might need to extend this to give Infinispan more time to rebalance\nterminationGracePeriodSeconds: 60\n\n# The internal Kubernetes cluster domain\nclusterDomain: cluster.local\n\n## Overrides the default entrypoint of the Keycloak container\ncommand: []\n\n## Overrides the default args for the Keycloak container\nargs: []\n\n# Additional environment variables for Keycloak\nextraEnv: |\n  - name: JGROUPS_DISCOVERY_PROTOCOL\n    value: dns.DNS_PING\n  - name: JGROUPS_DISCOVERY_PROPERTIES\n{% raw %}\n    value: 'dns_query={{ include \"keycloak.serviceDnsName\" . }}'\n{% endraw %}\n  - name: CACHE_OWNERS_COUNT\n    value: \"2\"\n  - name: CACHE_OWNERS_AUTH_SESSIONS_COUNT\n    value: \"2\"\n  # - name: KEYCLOAK_LOGLEVEL\n  #   value: DEBUG\n  # - name: WILDFLY_LOGLEVEL\n  #   value: DEBUG\n  # - name: CACHE_OWNERS_COUNT\n  #   value: \"2\"\n  # - name: CACHE_OWNERS_AUTH_SESSIONS_COUNT\n  #   value: \"2\"\n\n# Additional environment variables for Keycloak mapped from Secret or ConfigMap\nextraEnvFrom: \"\"\n\n#  Pod priority class name\npriorityClassName: \"\"\n\n# Pod affinity\naffinity: |\n  podAntiAffinity:\n    requiredDuringSchedulingIgnoredDuringExecution:\n      - labelSelector:\n          matchLabels:\n{% raw %}\n            {{- include \"keycloak.selectorLabels\" . | nindent 10 }}\n{% endraw %}\n          matchExpressions:\n            - key: app.kubernetes.io/component\n              operator: NotIn\n              values:\n                - test\n        topologyKey: kubernetes.io/hostname\n    preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 100\n        podAffinityTerm:\n          labelSelector:\n            matchLabels:\n{% raw %}\n              {{- include \"keycloak.selectorLabels\" . | nindent 12 }}\n{% endraw %}\n            matchExpressions:\n              - key: app.kubernetes.io/component\n                operator: NotIn\n                values:\n                  - test\n          topologyKey: failure-domain.beta.kubernetes.io/zone\n\n# Node labels for Pod assignment\nnodeSelector: {}\n\n# Node taints to tolerate\ntolerations: []\n\n# Additional Pod labels\npodLabels: {}\n\n# Additional Pod annotations\npodAnnotations: {}\n\n# Liveness probe configuration\nlivenessProbe: |\n  httpGet:\n    path: /auth/\n    port: http\n  initialDelaySeconds: 300\n  timeoutSeconds: 5\n\n# Readiness probe configuration\nreadinessProbe: |\n  httpGet:\n    path: /auth/realms/master\n    port: http\n  initialDelaySeconds: 30\n  timeoutSeconds: 1\n\n# Pod resource requests and limits\nresources: {}\n  # requests:\n  #   cpu: \"500m\"\n  #   memory: \"1024Mi\"\n  # limits:\n  #   cpu: \"500m\"\n  #   memory: \"1024Mi\"\n\n# Startup scripts to run before Keycloak starts up\nstartupScripts:\n  # WildFly CLI script for configuring the node-identifier\n  keycloak.cli: |\n{% raw %}\n    {{- .Files.Get \"scripts/keycloak.cli\" }}\n{% endraw %}\n  # mystartup.sh: |\n  #   #!/bin/sh\n  #\n  # echo 'Hello from my custom startup script!'\n\n# Add additional volumes, e. g. for custom themes\nextraVolumes: \"\"\n\n# Add additional volumes mounts, e. g. for custom themes\nextraVolumeMounts: \"\"\n\n# Add additional ports, e. g. for admin console or exposing JGroups ports\nextraPorts: []\n\n# Pod disruption budget\npodDisruptionBudget: {}\n#  maxUnavailable: 1\n#  minAvailable: 1\n\n# Annotations for the StatefulSet\nstatefulsetAnnotations: {}\n\n# Additional labels for the StatefulSet\nstatefulsetLabels: {}\n\n# Configuration for secrets that should be created\nsecrets: {}\n  # mysecret:\n  #   annotations: {}\n  #   labels: {}\n  #   stringData: {}\n  #   data: {}\n\nservice:\n  # Annotations for headless and HTTP Services\n  annotations: {}\n  # Additional labels for headless and HTTP Services\n  labels: {}\n  # key: value\n  # The Service type\n  type: ClusterIP\n  # Optional IP for the load balancer. Used for services of type LoadBalancer only\n  loadBalancerIP: \"\"\n  # The http Service port\n  httpPort: 80\n  # The HTTP Service node port if type is NodePort\n  httpNodePort: null\n  # The HTTPS Service port\n  httpsPort: 8443\n  # The HTTPS Service node port if type is NodePort\n  httpsNodePort: null\n  # The WildFly management Service port\n  httpManagementPort: 9990\n  # The WildFly management Service node port if type is NodePort\n  httpManagementNodePort: null\n  # Additional Service ports, e. g. for custom admin console\n  extraPorts: []\n\ningress:\n  # If `true`, an Ingress is created\n  enabled: false\n  # The Service port targeted by the Ingress\n  servicePort: http\n  # Ingress annotations\n  annotations: {}\n  # Additional Ingress labels\n  labels: {}\n   # List of rules for the Ingress\n  rules:\n    -\n      # Ingress host\n{% raw %}\n      host: '{{ .Release.Name }}.keycloak.example.com'\n{% endraw %}\n      # Paths for the host\n      paths:\n        - /\n  # TLS configuration\n  tls:\n    - hosts:\n        - keycloak.example.com\n      secretName: keycloak-tls\n\nroute:\n  # If `true`, an OpenShift Route is created\n  enabled: false\n  # Path for the Route\n  path: /\n  # Route annotations\n  annotations: {}\n  # Additional Route labels\n  labels: {}\n  # Host name for the Route\n  host: \"\"\n  # TLS configuration\n  tls:\n    # If `true`, TLS is enabled for the Route\n    enabled: true\n    # Insecure edge termination policy of the Route. Can be `None`, `Redirect`, or `Allow`\n    insecureEdgeTerminationPolicy: Redirect\n    # TLS termination of the route. Can be `edge`, `passthrough`, or `reencrypt`\n    termination: edge\n\npgchecker:\n  image:\n    # Docker image used to check Postgresql readiness at startup\n    repository: docker.io/busybox\n    # Image tag for the pgchecker image\n    tag: 1.32\n    # Image pull policy for the pgchecker image\n    pullPolicy: IfNotPresent\n  # SecurityContext for the pgchecker container\n  securityContext:\n    allowPrivilegeEscalation: false\n    runAsUser: 1000\n    runAsGroup: 1000\n    runAsNonRoot: true\n  # Resource requests and limits for the pgchecker container\n  resources:\n    requests:\n      cpu: \"10m\"\n      memory: \"16Mi\"\n    limits:\n      cpu: \"10m\"\n      memory: \"16Mi\"\n\npostgresql:\n  # If `true`, the Postgresql dependency is enabled\n  enabled: true\n  # PostgreSQL User to create\n  postgresqlUsername: keycloak\n  # PostgreSQL Password for the new user\n  postgresqlPassword: hce123\n  # PostgreSQL Database to create\n  postgresqlDatabase: keycloak\n  # Persistent Volume Storage configuration\n  persistence:\n    storageClass: local-storage\n  replication:\n    enabled: true\n    user: repl_user\n    password: hce123\n    slaveReplicas: 2\n    ## Set synchronous commit mode: on, off, remote_apply, remote_write and local\n    ## ref: https://www.postgresql.org/docs/9.6/runtime-config-wal.html#GUC-WAL-LEVEL\n    synchronousCommit: \"on\"\n    ## From the number of `slaveReplicas` defined above, set the number of those that will have synchronous replication\n    ## NOTE: It cannot be > slaveReplicas\n    numSynchronousReplicas: 1\n    ## Replication Cluster application name. Useful for defining multiple replication policies\n    applicationName: my_application\n\nserviceMonitor:\n  # If `true`, a ServiceMonitor resource for the prometheus-operator is created\n  enabled: false\n  # Optionally sets a target namespace in which to deploy the ServiceMonitor resource\n  namespace: \"\"\n  # Annotations for the ServiceMonitor\n  annotations: {}\n  # Additional labels for the ServiceMonitor\n  labels: {}\n  # Interval at which Prometheus scrapes metrics\n  interval: 10s\n  # Timeout for scraping\n  scrapeTimeout: 10s\n  # The path at which metrics are served\n  path: /metrics\n  # The Service port at which metrics are served\n  port: http-management\n\nprometheusRule:\n  # If `true`, a PrometheusRule resource for the prometheus-operator is created\n  enabled: false\n  # Annotations for the PrometheusRule\n  annotations: {}\n  # Additional labels for the PrometheusRule\n  labels: {}\n  # List of rules for Prometheus\n  rules: []\n  # - alert: keycloak-IngressHigh5xxRate\n  #   annotations:\n  #     message: The percentage of 5xx errors for keycloak over the last 5 minutes is over 1%.\n  #   expr: |\n  #     (\n  #       sum(\n  #         rate(\n  #           nginx_ingress_controller_response_duration_seconds_count{exported_namespace=\"mynamespace\",ingress=\"mynamespace-keycloak\",status=~\"5[0-9]{2}\"}[1m]\n  #         )\n  #       )\n  #       /\n  #       sum(\n  #         rate(\n  #           nginx_ingress_controller_response_duration_seconds_count{exported_namespace=\"mynamespace\",ingress=\"mynamespace-keycloak\"}[1m]\n  #         )\n  #       )\n  #     ) * 100 > 1\n  #   for: 5m\n  #   labels:\n  #     severity: warning\n\ntest:\n  # If `true`, test resources are created\n  enabled: false\n  image:\n    # The image for the test Pod\n    repository: docker.io/unguiculus/docker-python3-phantomjs-selenium\n    # The tag for the test Pod image\n    tag: v1\n    # The image pull policy for the test Pod image\n    pullPolicy: IfNotPresent\n  # SecurityContext for the entire test Pod\n  podSecurityContext:\n    fsGroup: 1000\n  # SecurityContext for the test container\n  securityContext:\n    runAsUser: 1000\n    runAsNonRoot: true\n```\n","tags":["storage"],"categories":["technologies","security"]},{"title":"greenplum client","url":"/2021/05/31/storage/greenplum/greenplum_client/","content":"\n## python\n\n### **python3 install **\n\n```shell\nyum install python3-devel\npipenv --three\npipenv shell\npipenv install psycopg2==2.8.3\n```\n\n\n","tags":["storage"],"categories":["storage","greenplum"]},{"title":"greenplum(psql) 远程连接k8s部署的greenplum","url":"/2021/05/31/storage/greenplum/greenplum_K8s_远程连接/","content":"\nReference Link: https://docs.greenplum.org/6-8/security-guide/topics/Authenticate.html\n\n## 本地主机连接Kubernetes上部署的greenplum\n\n查看greenplum在kubernetes上部署的service\n\n```shell\nkubectl get svc -n greenplum\n  NAME        TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n  agent       ClusterIP      None            <none>        22/TCP           6m1s\n  greenplum   LoadBalancer   10.99.154.225   <pending>     5432:32518/TCP   6m1s\n```\n\n宿主主机安装psql客户端和服务端\nReference Link: https://www.cnblogs.com/zhi-leaf/p/11432054.html\n\n```shell\nyum install https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm\nyum install postgresql10\nyum install postgresql10-server\n/usr/pgsql-10/bin/postgresql-10-setup initdb\n(option) $ systemctl start postgresql-10\n```\n\n宿主主机安装完psql后会在如下路径生成配置文件\n\n```\n/var/lib/pgsql/10/data/\n```\n\n**遇到的问题**\n\n```shell\npsql -Ugpadmin -p32518 -h10.67.108.211\n  psql: FATAL:  no pg_hba.conf entry for host \"10.32.0.1\", user \"gpadmin\", database \"gpadmin\", SSL off\n```\n\n**解决方法**\n\n```shell\nkubectl exec po/master-0 -n greenplum -it -- /bin/bash\nvim /greenplum/data-1/pg_hba.conf\n\n# 添加如下内容只信任IP为10.32.0.1的client机器:\nhost     all         gpadmin         10.32.0.1/32    trust\n\n# 或者添加如下内容使所有client链接都trust\nhost     all         hce             0.0.0.0/0  trust\n\n# 或者添加如下内容使所有client链接都要输入密码\nhost     all         hce             0.0.0.0/0  md5\n```\n在上面登陆的greenplum的容器里运行如下命令Reload pg_hba.conf and postgresql.conf\n\n```shell\ngpstop -u\n```\n\n## 在docker里连接greenplum\n\n1. psql_lib.py内容如下\n\n``` py\nimport psycopg2\nimport sqlparse\n\nclass Greenplum_psql(object):\n    def __init__(self, database, host, user, port):\n        self.__database = database\n        self.__host = host\n        self.__user = user\n        self.__port = port\n\n    def connect(self):\n        conn = psycopg2.connect(\n            dbname=self.__database, host=self.__host, user=self.__user, port=self.__port)\n        cursor = conn.cursor()\n        return conn, cursor\n\n    def __parse_psql(self, sql_cmd):\n        parsed_cmd = sqlparse.split(sql_cmd)\n        return parsed_cmd\n\n    def process_psql(self, conn, cursor, cmd):\n        parsed_cmd = self.__parse_psql(cmd)\n        for cmd in parsed_cmd:\n            cursor.execute(cmd)\n        conn.commit()\n\n    def close(self, conn, cursor):\n        cursor.close()\n        conn.close()\n```\n\n2. main.py内容如下\n\n``` py\nimport psql_lib\n\nif __name__ == \"__main__\":\n    dbname = \"template1\"\n    host = \"10.99.154.225\"\n    user = \"gpadmin\"\n    port = \"5432\"\n    cmd = \"DROP TABLE test_conn1;\\\nCREATE TABLE test_conn1(id int, mediaURI text, date text);\\\nINSERT INTO test_conn1 values(111, 'URI-text', '2020-11-06, 10:05');\\\nselect * from test_conn1;\"\n\n    psql = psql_lib.Greenplum_psql(dbname, host, user, port)\n    conn, cursor = psql.connect()\n    psql.process_psql(conn, cursor, cmd)\n\n    rows = cursor.fetchall()\n    for row in rows:\n        print('id = ', row[0], 'name = ', row[1], 'date = ', row[2])\n\n    psql.close(conn, cursor)\n```\n3. Dockerfile 内容如下\n\nvim Dockerfile\n\n``` dockerfile\nFROM python:3.6.8\nLABEL storage=\"greenplum-sdk\"\nWORKDIR /home/\nCOPY psql_lib.py main.py ./\nARG proxy\nRUN pip install psycopg2==2.8.3 sqlparse==0.4.1 --proxy ${proxy}\nCMD [\"python\", \"main.py\"]\n```\n**Build the image**\n\n```shell\ndocker build --build-arg proxy=http://child-prc.intel.com:913 -t greenplum-client:0.1 .\n```\n**Run the container**\n\n```shell\ndocker run --name python-greenplum greenplum-client:0.1\n```\n\n\n\n\n\n","tags":["storage"],"categories":["storage","greenplum"]},{"title":"greenplum 遇到的问题","url":"/2021/05/31/storage/greenplum/greenplum_problems/","content":"\n\n## **问题1: greenplum pod异常重启后 用psql 登陆出现如下问题**\n```\npsql: could not connect to server: No such file or directory\n        Is the server running locally and accepting\n        connections on Unix domain socket \"/tmp/.s.PGSQL.5432\"?\n```\n\n解决方案: 运行`gpstart`命令, 重启服务, 中途会让选择默认配置, 选择y回车即可.\n\n\n","tags":["storage"],"categories":["storage","greenplum"]},{"title":"greenplum 01 deployment on Kubernetes","url":"/2021/05/31/storage/greenplum/greenplum_01_deployment/","content":"\n## **Prerequisites**\n1. [Kubernetes Node Configuration](http://greenplum-kubernetes.docs.pivotal.io/2-0/node-requirements.html) describes the Linux kernel configuration requirements for each Kubernetes node that is used in a Pivotal Greenplum cluster. These requirements are common to all Pivotal Greenplum deployments, regardless of which Kubernetes environment you use.\n2. Ensure that any previous Pivotal Greenplum installation has been uninstalled as described in Uninstalling Pivotal Greenplum.\n3. kubectl configured to refer to a Kubernetes cluster.\n\nmount磁盘, 使得greenplum存储的数据保存到指定的挂载磁盘目录.\n\n``` shell\nmount /dev/sdb1 /mnt/greenplum/greenplum-master-vol0/\nmount /dev/sdb2 /mnt/greenplum/greenplum-segment-vol0/\nmount /dev/sdb3 /mnt/greenplum/greenplum-segment-vol1/\n```\n\n将挂在的磁盘写到系统文件.\n``` shell\nvim /etc/fstab\n  /dev/sdb1               /mnt/greenplum/greenplum-master-vol0      ext4    defaults        0 0\n  /dev/sdb2               /mnt/greenplum/greenplum-segment-vol0     ext4    defaults        0 0\n  /dev/sdb3               /mnt/greenplum/greenplum-segment-vol1     ext4    defaults        0 0\n```\n\n## **Install Greenplum Operator for Kubernetes**\n1. Download the Pivotal Greenplum software from [VMware Tanzu Network](https://network.pivotal.io/products/greenplum-for-kubernetes) or skip to step 3 if have greenplum related files. The download file has the name: ```greenplum-for-kubernetes-<version>.tar.gz.```\n\n\n2. Go to the directory where you downloaded Greenplum for Kubernetes, and unpack the downloaded software. For example:\n\n```shell\ncd ~/Downloads\ntar xzf greenplum-for-kubernetes-*.tar.gz\n```\nThe above command unpacks the distribution into a new directory named ```greenplum-for-kubernetes-<version>```.\n3. Go into the new greenplum-for-kubernetes-<version> directory:\n\n```shell\ncd ./greenplum-for-kubernetes-*\n```\n4. Load the Greenplum for Kubernetes Docker image to the local Docker registry:\n\n```shell\ndocker load -i ./images/greenplum-for-kubernetes\n```\n5. Load the Greenplum Operator Docker image to the Docker registry:\n\n```shell\ndocker load -i ./images/greenplum-operator\n```\n6. Push the Greenplum docker images to the local container registry. For example:\n\n```shell\nIMAGE_REPO=\"hci-node01:5000\"\nGREENPLUM_IMAGE_NAME=\"${IMAGE_REPO}/greenplum-for-kubernetes:$(cat ./images/greenplum-for-kubernetes-tag)\"\ndocker tag $(cat ./images/greenplum-for-kubernetes-id) ${GREENPLUM_IMAGE_NAME}\ndocker push ${GREENPLUM_IMAGE_NAME}\n\nOPERATOR_IMAGE_NAME=\"${IMAGE_REPO}/greenplum-operator:$(cat ./images/greenplum-operator-tag)\"\ndocker tag $(cat ./images/greenplum-operator-id) ${OPERATOR_IMAGE_NAME}\ndocker push ${OPERATOR_IMAGE_NAME}\n```\n\n7. Create a new YAML file in the workspace subdirectory with two lines to indicate the registry where you pushed the images\n\n```xml\ncat <<EOF >workspace/operator-values-overrides.yaml\noperatorImageRepository: ${IMAGE_REPO}/greenplum-operator\ngreenplumImageRepository: ${IMAGE_REPO}/greenplum-for-kubernetes\noperatorWorkerSelector: {\ngreenplum-operator: \"default\"\n}\nEOF\n```\n8. Use helm to create a new Greenplum Operator release.\n\n```shell\n# 先给某台机器添加标签来部署greenplum-operator\n$ kubectl label node hci-node02  greenplum-operator=default\n\n$ kubectl create namespace greenplum\n$(不要用这个命令) helm install -f workspace/operator-values-overrides.yaml operator/ -n greenplum --generate-name\n$ helm install -name greenplum-operator -f workspace/operator-values-overrides.yaml operator/ -n greenplum\n  NAME: greenplum-operator\n  LAST DEPLOYED: Fri Jun 18 12:17:43 2021\n  NAMESPACE: greenplum\n  STATUS: deployed\n  REVISION: 1\n  TEST SUITE: None\n  NOTES:\n  greenplum-operator has been installed.\n  \n  Please see documentation at:\n  http://greenplum-kubernetes.docs.pivotal.io/\n```\n\n* -name, 指定部署的名字Name, 可供` helm uninstall <Name> -n namespace` 使用.\n* -n/--namespace, 指定部署到的namespace\n* -f workspace/operator-values-overrides.yaml, 里面的键值对会替换Chart所在同级目录的values.yaml中的键值对\n* operator/, Chart和values.yaml所在的目录\n* --generate-name, 会随机生成一个release name，此时就一定要记住输出的NAME, 方便后面uninstall\n\n## **Create Local Persistent Volumes for Greenplum**\n\n1. Create the directory, partition, or logical volume that you want to use as a Kubernetes local volume.\n\n2. Create the StorageClass definition, specifying no-provisioner in order to manually provision local persistent volumes. Using volumeBindingMode: WaitForFirstConsumer is also recommended to delay binding the local PersistenVolume until a pod requires.\n\nvim gpdb-storage-class.yaml\n\n```xml\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: gpdb-storage\nprovisioner: kubernetes.io/no-provisioner\nvolumeBindingMode: WaitForFirstConsumer\n```\n\n``` shell\n$ kubectl apply -f gpdb-storage-class.yaml\n```\n\n\n3. Create a PersistentVolume definition, specifying the local volume and the required NodeAffinity field. For example:\n\nvim pv-master.yaml\n\n```xml\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: greenplum-master-node02\nspec:\n  capacity:\n    storage: 1Gi\n  accessModes:\n  - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: gpdb-storage\n  local:\n    path: /mnt/disks/greenplum-master-vol0\t// 需要提前在相应机器上创建此目录\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n      - matchExpressions:\n        - key: kubernetes.io/hostname\n          operator: In\n          values:\n          - hci-node02\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: greenplum-master-node02\n...... // 重复上面的内容, 改变下local.path, nodeAffinity等在不同node机器上创建多个pv\n```\n\n4. Repeat the previous step for each PersistentVolume required for your cluster. Remember that each Greenplum segment host requires a dedicated storage volume.\n\n5. Use kubectl to apply the StorageClass and PersistentVolume configurations that you created.\n\n6. Specify the local storage StorageClass name when you deploy a new Greenplum cluster as below.\n\n## **Deploy a greenplum cluster**\n\n1. Go to the workspace subdirectory where you unpacked the Pivotal Greenplum distribution for Kubernetes:\n\n```shell\ncd ./greenplum-for-kubernetes-*/workspace\n```\n2. If necessary, create a Kubernetes manifest file to specify the configuration of your Greenplum cluster. A sample file is provided in workspace/my-gp-instance.yaml. my-gp-instance.yaml contains the minimal set of instructions necessary to create a demonstration cluster named “my-greenplum” with a single segment and default storage, memory, and CPU settings:\n\nvim my-gp-instance.yaml\n\n```xml\napiVersion: \"greenplum.pivotal.io/v1\"\nkind: \"GreenplumCluster\"\nmetadata:\n  name: my-greenplum\nspec:\n  masterAndStandby:\n    hostBasedAuthentication: |\n      # host   all   gpadmin   1.2.3.4/32   trust\n      # host   all   gpuser    0.0.0.0/0   md5\n      host    all   hce       0.0.0.0/0   md5\n    memory: \"800Mi\"\n    cpu: \"0.5\"\n    storageClassName: gpdb-storage\n    storage: 1G\n    antiAffinity: \"yes\"\n    workerSelector: {}\n  segments:\n    primarySegmentCount: 2\t\t# Expand the segment to 2\n    memory: \"800Mi\"\n    cpu: \"0.5\"\n    storageClassName: gpdb-storage\t\t# Use the specify storageclass\n    storage: 10G\t\t\t\t# Expand the storage to 10G\n    antiAffinity: \"yes\"\n    workerSelector: {}\n    mirrors: \"yes\"\n```\n::: primary\n在上面的[hostBasedAuthentication]{.red}添加认证信息后就不需要deploy greenplum后再登陆master-0 Pod手动添加认证信息到pg_hba.conf文件\n:::\n\n3. Use kubectl apply command and specify your manifest file to send the deployment request to the Greenplum Operator. For example, to use the sample my-gp-instance.yaml file:\n\n```shell\nkubectl apply -f ./my-gp-instance.yaml \n  greenplumcluster.greenplum.pivotal.io/my-greenplum created\n```\n\n## **Deploy multiple greenplum cluster**\n1. Create namespaces for greenplum cluster to deploy. Deploy two greenplum cluster instances:\n\n```shell\nkubectl create namespace gpinstance-1\nkubectl create namespace gpinstance-2\n```\n\n2. Deploy Greenplum cluster into the correspond namespace.\n\n```shell\ncd workspace\nkubectl apply -f ./my-gp-instance.yaml -n gpinstance-1\nkubectl apply -f ./my-gp-instance.yaml -n gpinstance-2\n```\n## **Test whether the Greenplum Cluster deployment is successful**\n查看greenplum logs\n\n```shell\nkubectl logs po/master-0 -n greenplum\n  ......\n  *******************************\n  Adding host based authentication to master-0 pg_hba.conf\n  *******************************\n  ......\n  *******************************\n  Running createdb\n  *******************************\n```\n需要等待一段时间才能执行下方操作\n\n```shell\nkubectl exec -it master-0 -n greenplum -- bash -c \"source /opt/gpdb/greenplum_path.sh; psql\"\n  psql (8.3.23)\n  Type \"help\" for help.\n \n  gpadmin=# select * from gp_segment_configuration;\n```\n如果报一些错误无法执行可以:\n1. 先进入master-0: kubectl exec -it master-0 -n greenplum -- bash, 再查找greenplum_path.sh\n2. 执行`$ source /opt/gpdb/greenplum_path.sh`, 再 `$ exit`退出, 然后就可以用以上命令了\n\n(Enter `\\q` to exit the psql utility.)\n\n**If you are redeploying a cluster that configured to use a standby master, wait until all pods reach the Running status. Then connect to the master-0 pod and execute the gpstart command manually. For example:**\n\n```shell\nkubectl exec -it master-0 -n greenplum -- bash -c \"source /opt/gpdb/greenplum_path.sh; gpstart\"\n```\n\n## trusted client\n\nReference Link: https://docs.greenplum.org/6-8/security-guide/topics/Authenticate.html\n\n``` shell\n# Login to the Greenplum master-0 pod \n$ kubectl exec po/master-0 -n greenplum -it -- /bin/bash \n\n# Add the following information\n# 添加如下内容使所有client链接都要输入密码\ngpadmin@master-0:~$ vim /greenplum/data-1/pg_hba.conf \n  …… \n  host     all         hce             0.0.0.0/0  md5\n  ……\n\n# reload the configuration of greenplum\ngpadmin@master-0:~$ gpstop –u \n\n# Exit the Greenplum master-0 pod \ngpadmin@master-0:~$ exit \n```\n\n## Create User\n\n``` shell\nyum install -y postgresql\nkubectl port-forward svc/greenplum -n greenplum 5555:5432\n\nkubectl -n greenplum exec -it po/master-0 -- bash\n\n//运行之后输入设置密码\ngpadmin@master-0:~$ createuser -P -s -e hce\ngpadmin@master-0:~$ echo \"host all hce 0.0.0.0/0 trust\" >> /greenplum/data-1/pg_hba.conf\ngpadmin@master-0:~$ gpstop –u\ngpadmin@master-0:~$ exit\n```\n\n\n## Create table\n\n``` shell\npsql -U gpadmin -p 5555 -h 127.0.0.1\ngpadmin=# create database hce;\n\n// 将database hce的所有权限赋给hce用户.\ngpadmin=# grant all privileges on database hce to hce;\ngpadmin=# \\q\n```\n\n\n## Upload data\n\n``` shell\npsql -U hce hce -p 5555 -h 127.0.0.1 < /path/to/hce_with_data.sql\n```\n\n\n## Clear table data\n\n``` shell\ngpadmin=# delete from media where 1=1;\n```\n\n\n## (推荐) 卸载greenplum\n查看helm版本:\n``` shell\n$ helm version\n  version.BuildInfo{Version:\"v3.3.1\", GitCommit:\"249e5215cde0c3fa72e27eb7a30e8d55c9696144\", GitTreeState:\"clean\", GoVersion:\"go1.14.7\"}\n```\n从kubernetes remove greenplum最简单命令如下:\n``` shell\n$ helm uninstall greenplum-operator -n greenplum\n```\n上面执行成功就不需要下面的一系列delete操作了\n\n\n## **Delete a greenplum cluster and uninstall pivotal greenplum for kubernetes**\n\n### **Delete a greenplum cluster**\n1. Navigate to the workspace directory of the Pivotal Greenplum distribution (or to the location of the Kubernetes manifest that you used to deploy the cluster). For example:\n\n```shell\ncd ./greenplum-for-kubernetes-*/workspace\n```\n2. Execute the kubectl delete command, specifying the manifest that you used to deploy the cluster. For example:\n\n```shell\nkubectl delete -f ./my-gp-instance.yaml --wait=false\n```\n**Note:** Use the optional --wait=false flag to return immediately without waiting for the deletion to complete.\n3. Use kubectl to describe the Greenplum cluster to verify Status.Phase and Events:\n\n```shell\nkubectl describe greenplumcluster my-greenplum\n  [...]\n  Status:\n    Instance Image:    greenplum-for-kubernetes:latest\n    Operator Version:  greenplum-operator:latest\n    Phase:             Deleting\n  Events:\n    Type    Reason                    Age   From               Message\n    ----    ------                    ----  ----               -------\n    Normal  CreatingGreenplumCluster  3m    greenplumOperator  Creating Greenplum cluster my-greenplum in default\n    Normal  CreatedGreenplumCluster   1m    greenplumOperator  Successfully created Greenplum cluster my-greenplum in default\n    Normal  DeletingGreenplumCluster  6s    greenplumOperator  Deleting Greenplum cluster my-greenplum in default\n```\nIf for any reason stopping the Greenplum instance fails, you should see a warning message in the greenplum-operator logs as shown below:\n\n```shell\nkubectl logs -l app=greenplum-operator\n  [...]\n  {\"level\":\"INFO\",\"ts\":\"2020-01-24T19:03:22.874Z\",\"logger\":\"controllers.GreenplumCluster\",\"msg\":\"DeletingGreenplumCluster\",\"name\":\"my-greenplum\",\"namespace\":\"default\"}\n  {\"level\":\"INFO\",\"ts\":\"2020-01-24T19:03:23.068Z\",\"logger\":\"controllers.GreenplumCluster\",\"msg\":\"initiating shutdown of the greenplum cluster\"}\n  {\"level\":\"INFO\",\"ts\":\"2020-01-24T19:03:31.971Z\",\"logger\":\"controllers.GreenplumCluster\",\"msg\":\"gpstop did not stop cleanly. Please check gpAdminLogs for more info.\"}\n  [...]\n  {\"level\":\"INFO\",\"ts\":\"2020-01-24T19:03:32.252Z\",\"logger\":\"controllers.GreenplumCluster\",\"msg\":\"DeletedGreenplumCluster\",\"name\":\"my-greenplum\",\"namespace\":\"default\"}\n```\n4. Use kubectl to monitor the progress of terminating Greenplum resources in your cluster. For example, if your cluster deployment was named my-greenplum:\n\n```shell\nkubectl get all -l greenplum-cluster=my-greenplum\nkubectl label nodes hci-node01 greenplum-affinity-greenplum-master-\t// 去掉node上关于greenplum的label\nkubectl label nodes hci-node01 greenplum-affinity-greenplum-segment-\n```\n### **Delete greenplum persistent volume claims**\n**Caution:** If the Persistent Volumes were created using dynamic provisioning, then deleting the PVCs will also delete the associated PVs. In this case, do not delete the PVCs unless you are certain that you no longer need the data.\n1. Verify that the PVCs are present for your cluster. For example, to show the Persistent Volume Claims created for a cluster named my-greenplum:\n\n```shell\nkubectl get pvc -l greenplum-cluster=my-greenplum\n```\n2. Use kubectl to delete the PVCs associated with the cluster. For example, to delete all PersistentVolume Claims created for a cluster named my-greenplum:\n\n```shell\nkubectl delete pvc -l greenplum-cluster=my-greenplum\n```\n3. If the Persistent Volumes were provisioned manually, then deleting the PVCs does not delete the associated PVs. (You can check for the PVs using kubectl get pv.) To delete any remaining Persistent Volumes, execute the command:\n\n```shell\nkubectl delete pv -l greenplum-cluster=my-greenplum\n```\n### **Uninstall pivotal greenplum for kubernetes**\n1. Use the helm delete command to delete the greenplum-operator release:\n\n```shell\nhelm delete greenplum-operator\nhelm del --purge greenplum-operator;\n```\n2. Delete the node label of greenplum operator when deployed on kubernetes\n\n```shell\nkubectl label nodes hci-node02 greenplum-operator-\n```\n3. Use docker rmi to delete images\n\n```shell\ndocker rmi <ImageName or ImgaeID>\n```\n## **FAQ**\nWhen uninstall greenplum, encountered this problem: Object is being deleted: customresourcedefinitions.apiextensions.k8s.io \"greenplumclusters.greenplum.pivotal.io\" already exists.\n\nsolution refer to this:[delete crd](https://github.com/kubernetes/kubernetes/issues/60538)\n\n## **删除greenplum集群脚本**\n\nvim clean_greenplum.sh\n\n```shell\n#!/bin/bash\n\nworkspace=workspace\nnode01=hci-node01\nnode02=hci-node02\nnode03=hci-node03\nnode04=hci-node04\ngreenplum_operator=hci-node02\n\ncd greenplum-for-kubernetes-v1.13.0\n\n# delete greenplum cluster\nkubectl delete -f ${workspace}/my-gp-instance.yaml\n\n# delete pvc\npvc_results=$(kubectl get pvc -n greenplum | grep greenplum | awk '{print $1}')\npvc_arr=(${pvc_results})\nfor ((i=0; i<${#pvc_arr[*]}; i++ ))\ndo\nkubectl delete pvc ${pvc_arr[i]} -n greenplum\ndone\n\n# delete pv\nkubectl delete -f ${workspace}/pv-master.yaml\nkubectl delete -f ${workspace}/pv-segment.yaml\n\n# delete sc\nkubectl delete -f ${workspace}/workspacegpdb-storage-class.yaml\n\n# delete greenplum operator\nhelm delete greenplum-operator\nhelm del --purge greenplum-operator;\n\n# delete label on nodes\nkubectl label nodes ${node01} greenplum-affinity-greenplum-master-\nkubectl label nodes ${node01} greenplum-affinity-greenplum-segment-\nkubectl label nodes ${node02} greenplum-affinity-greenplum-master-\nkubectl label nodes ${node02} greenplum-affinity-greenplum-segment-\nkubectl label nodes ${node03} greenplum-affinity-greenplum-master-\nkubectl label nodes ${node03} greenplum-affinity-greenplum-segment-\nkubectl label nodes ${node04} greenplum-affinity-greenplum-master-\nkubectl label nodes ${node04} greenplum-affinity-greenplum-segment-\nkubectl label nodes ${greenplum_operator} greenplum-operator-\n```\n\n## 删除clusterrole等资源\n\n``` shell\n# 查询系统还存在的greenplum相关组件\nkubectl get crd | grep greenplum\nkubectl get clusterrolebinding | grep greenplum\nkubectl get clusterrole | grep greenplum\n\n# 删除上面还存在的组件\nkubectl delete crd <correspond component name>\nkubectl delete clusterrolebinding <correspond component name>\nkubectl delete clusterrole <correspond component name>\n```\n\n## 删除数据\n\n``` shell\nrm -rf  /mnt/greenplum/greenplum-master-vol0/*\nrm -rf /mnt/greenplum/greenplum-segment-vol0/*\nrm -rf /mnt/greenplum/greenplum-segment-vol1/*\n```\n\n## 创建User\n\nReference: https://greenplum.org/user-and-schema-creation-in-greenplum-database/\n\n``` shell\n# 查看k8s上的greenplum服务端口\necho $(kubectl get svc greenplum -n greenplum -o jsonpath='{.spec.ports[0].nodePort}')\n#(optional)kubectl get svc -n greenplum | grep greenplum | awk '{print $5}'\n  31905\n\n\n\n# 查看部署到 k8s 上用来访问同样部署到 k8s 上的 greenplum 的 pod 的 IP\nkubectl get po -n storage-rest -o wide\nNAME                            READY   STATUS    RESTARTS   AGE   IP           NODE                NOMINATED NODE   READINESS GATES\nstorage-rest-7799775d48-45m7l   2/2     Running   0          15h   10.32.0.13   inspire-dev-nodea   <none>           <none>\n\n\n\n# 从Host主机登陆psql数据库, 需要提前在Host下载好 postgresql10.\npsql -h127.0.0.1 -p31905 -Uhce -dhce\n  psql: FATAL:  no pg_hba.conf entry for host \"10.32.0.1\", user \"hce\", database \"hce\", SSL off\n\n# 修改 k8s 上的 greenplum 的 pg_hba.conf 文件\nkubectl exec po/master-0 -n greenplum -it -- /bin/bash\nvim /greenplum/data-1/pg_hba.conf\n  ......\n  host     all         hce             0.0.0.0/0  md5\n  ......\n\n * 10.32.0.13 是在K8S集群里部署的 POD 的IP, 用来访问也在k8s上部署的greenplum psql数据库\n# 加载greenplum修改的配置\ngpstop -u\n\n# 退出POD\nexit\n\n\n# 从 Host 主机登陆greenplum 创建User\npsql -h127.0.0.1 -p31905 -Ugpadmin\npsql (10.16, server 8.3.23)\nType \"help\" for help.\n\ngpadmin=# CREATE USER hce WITH PASSWORD 'hce123';\nNOTICE:  resource group required -- using default resource group \"default_group\"\nCREATE ROLE\ngpadmin=# GRANT ALL PRIVILEGES ON DATABASE hce TO hce;\nERROR:  database \"hce\" does not exist\ngpadmin=# CREATE DATABASE hce;\nCREATE DATABASE\ngpadmin=# GRANT ALL PRIVILEGES ON DATABASE hce TO hce;\nGRANT\ngpadmin=# \\q\n\n\n\n# 从 Host 主机重新登陆 psql 数据库\npsql -h127.0.0.1 -p31905 -Uhce -dhce\nPassword for user hce: <输入密码: hce123>\npsql (10.16, server 8.3.23)\nType \"help\" for help.\n\nhce=>\n\n\n\n# 删除 user\npsql -h127.0.0.1 -p31905 -Ugpadmin\npsql (10.16, server 8.3.23)\nType \"help\" for help.\n\ngpadmin=# DROP USER hce\ngpadmin-# ;\nDROP ROLE\n\n# 查看user\nselect rolname,oid from pg_roles;\n```\n\n\n\n","tags":["storage"],"categories":["storage","greenplum"]},{"title":"MinIO 02 deployment on kubernetes","url":"/2021/05/31/storage/minio/minIO_02_deployment_on_kubernetes/","content":"\n在storage仓库里也存储了一份当时部署所使用的yaml等资源文件.\n\nQuick start:  \n  [https://docs.min.io/docs/minio-quickstart-guide.html](https://docs.min.io/docs/minio-quickstart-guide.html)\nDeployment on kubernetes:  \n  [https://docs.min.io/docs/deploy-minio-on-kubernetes.html](https://docs.min.io/docs/deploy-minio-on-kubernetes.html)\n  [https://github.com/minio/operator/blob/master/README.md](https://github.com/minio/operator/blob/master/README.md)\nEnable TLS:  \n  [https://github.com/minio/operator/blob/master/docs/tls.md](https://github.com/minio/operator/blob/master/docs/tls.md)\n\n(csdn)https://www.cnblogs.com/aloneysir/p/12874984.html\n\n部署分布式MinIO至少需要部署4个实例.\n\n## Prerequisites\n1. Kubernetes version v1.17.0 and above for compatibility. MinIO Operator uses k8s/client-go v0.18.0.\n2. kubectl configured to refer to a Kubernetes cluster.\n3. Create the required PVs as [direct CSI driver.](https://github.com/minio/minio-operator/blob/master/docs/using-direct-csi.md) or use the following installation step.\n\n## mount disk\nCreate MinIO storage folder and mount to the specified disk.\n\n``` shell\nyum install expect \n\n// Please refer to the comments in the mkdir_minio_node.sh script below to run a command similar to the following\nbash mkdir_minio_node.sh -u <user> -p <pwd> --node01 …\n\n// Log in to each machine and mount the created file to the specified disk\nmount /dev/sd* /mnt/minio_data\n```\n\n## Using Direct CSI Driver\n\nDownload the direct-csi from github and checkout to the v0.2.1 tag.\n\n``` shell\ngit clone https://github.com/minio/direct-csi.git\ncd direct-csi/\ngit checkout v0.2.1\ncd ../\nexport $(cat default.env)\nkubectl apply -k direct-csi\n```\n\n配置csi driver\n```shell\ncat << EOF > default.env\nDIRECT_CSI_DRIVES=data{1...4}\nDIRECT_CSI_DRIVES_DIR=/mnt\nKUBELET_DIR_PATH=/var/lib/kubelet\nEOF\n\nexport $(cat default.env)\nkubectl apply -k direct-csi\n```\n\n执行完上述操作后会在work node的/var/lib/kubelet/plugins_registry/生成下列sock文件\n\n```shell\n$ ls /var/lib/kubelet/plugins_registry/\ndirect.csi.min.io-reg.sock\n```\n\n## Create Operator Deployment\n1. Create namespace minio for minIO to deploy.\n\n```shell\nkubectl create ns minio\n```\n\n2. To start MinIO-Operator with default configuration, use the minio-operator.yaml file.\n\n```shell\nkubectl apply -f minio-operator.yaml -n minio\n```\n\n## Create MinIO instances without tls\nOnce MinIO-Operator deployment is running, you can create MinIO instances using the below command\n\n```shell\nkubectl apply -f minioinstance.yaml\n```\n\n## Create NodePort service for minIO\n\n```shell\nkubectl apply -f NodePort-minio.yaml\n```\n\n## Access minIO from the browser\n\n```shell\n// Minio Server没有配置TLS\n浏览器输入: http://127.0.0.1:30007\ndefault username: minio\ndefault password: minio123\n// Minio Server配置TLS\n浏览器输入: https://127.0.0.1:30007\n```\n\n## **Secure access to MinIO server with TLS**\n**1.1. Generate a private key with ECDSA**\n\n```shell\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out private.key\nread EC key\nwriting EC key\n```\n\nAlternatively, use the following command to generate a private ECDSA key protected by a password:\n\n```shell\nopenssl ecparam -genkey -name prime256v1 | openssl ec -aes256 -out private.key -passout pass:<PASSWORD>\n```\n**1.2 Generate a private key with RSA.**\nUse the following command to generate a private key with RSA:\n\n```shell\nopenssl genrsa -out private.key 2048\nGenerating RSA private key, 2048 bit long modulus\n............................................+++\n...........+++\ne is 65537 (0x10001)\n\n// openssl查看证书内容\n openssl rsa -in private.key -text -noout\n```\n\nAlternatively, use the following command to generate a private RSA key protected by a password:\n\n```shell\nopenssl genrsa -aes256 -out private.key 2048 -passout pass:<PASSWORD>\n```\n**Note:** When using a password-protected private key, the password must be provided through the environment variable MINIO_CERT_PASSWD using the following command:\n\n```shell\n$ export MINIO_CERT_PASSWD=<PASSWORD>\n// 也就是在minioinstance.yaml中添加如下信息\nenv:\n  - MINIO_CERT_PASSWD\n    value: \"<YourPassword>\"\t\t// 一定要加上双引号\"\"\n```\n**2. Generate a self-signed certificate.**\nUse the following command to generate a self-signed certificate and enter a passphrase when prompted:\n\n```shell\nopenssl req -new -x509 -days 3650 -key private.key -out public.crt -subj \"/C=US/ST=state/L=location/O=organization/CN=<domain.com>\"\n\n// 查看证书签名,算法,public key等信息\nopenssl x509 -in public.crt -text -noout\n```\n\n**Note:** Replace <domain.com> with the development domain name.\nAlternatively, use the command below to generate a self-signed wildcard certificate that is valid for all subdomains under <domain.com>. Wildcard certificates are useful for `deploying distributed MinIO instances`, where each instance runs on a subdomain under a single parent domain.\n\n```shell\nopenssl req -new -x509 -days 3650 -key private.key -out public.crt -subj \"/C=US/ST=state/L=location/O=organization/CN=*.minio-hl-svc.minio.svc.cluster.local\"\n```\n**Note: 若是不知道怎么确定`<*.domain.com>`信息, 添加TLS后部署minio发现pod启动异常或者访问minio服务不正常, 查看pod的日志,`$ kubectl logs po/minio-0 -n minio`, 即可看到pod出现error信息，显示需要的certificates的commanName(简称CN), 然后将不同pod证书不同的内容用`*`替代, 其它的照搬，如上的例子`*.minio-hl-svc.minio.svc.cluster.local`.**\n\n**3. Use a Kubernetes Secret resource to store this information, create a Kubernetes Secret using:**\n\n```shell\n\t$ kubectl create secret generic tls-ssl-minio --from-file=***/pki/private.key --from-file=***/pki/public.crt -n minio\n```\n再将tenant.yaml或者改过名的minioinstance.yaml文件中的下列注释内容打开\n\n```\nexternalCertSecret:\n  name: tls-ssl-minio\n```\n然后再重新部署minio\n\n```shell\nkubectl apply -f minioinstance.yaml\n```\n执行完上述操作后会在work node生成以下文件夹\n\n```shell\nls /var/lib/kubelet/plugins\ndirect-csi-min-io/  kubernetes.io/\n```\n\n## **查看pvc和pv,查看WorkNode pvc volume存储**\n\n```shell\n$ kubectl get pvc -n minio\n[root@hci-node01 minIO]# k get pvc -n minio\nNAME            STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE\ndata0-minio-0   Bound    pvc-fc3a42f2-78d7-4dfe-8ad5-a0e3c745f638   10Gi       RWO            direct.csi.min.io   19h\ndata1-minio-0   Bound    pvc-b9f227cd-7d6c-4728-b27f-00d4d5c42a16   10Gi       RWO            direct.csi.min.io   19h\n```\n转到minio-0这个pod部署到的work node机器上查看\n\n```shell\n$ cd /var/lib/kubelet/plugins/kubernetes.io/csi/pv\n$ ls\n  pvc-fc3a42f2-78d7-4dfe-8ad5-a0e3c745f638/ pvc-b9f227cd-7d6c-4728-b27f-00d4d5c42a16/\n$ cd pvc-b9f227cd-7d6c-4728-b27f-00d4d5c42a16/globalmount/mybucket-01/ // 自己创建了一个buckt: mybucket-01\n$ ls\n  file-Name/\t\t\t\t// file-Name是通过mc命令行工具加进mybucket-01\n$ touch test.txt\n$ ls\n  file-Name/  test.txt\t\t// test.txt是登陆work node后手动复制或创建的文件.\n```\n但是在此work node机器上的 /mnt/data1/目录下也能查看到文件\n\n```shell\ncd /mnt/data1/\n\nls\nb1ac2515-e6ba-11ea-98f8-064de4d3a4ce/  b1ac2983-e6ba-11ea-98f8-064de4d3a4ce/\n\nls b1ac2515-e6ba-11ea-98f8-064de4d3a4ce/\nfile-Name/ test.txt\t\t// 发现此目录也出现了test.txt文件\n```\n上面登陆work node向pv volume添加的文件在master机器上用mc客户端是查不到的.\n\n## **删除minio Cluster**\nchean.sh\n\n``` shell\n#!/bin/bash\n\n# delete minio pod\nkubectl delete -f minioinstance.yaml\n\n# delete pvc\npvc_results=$(kubectl get pvc -n minio | grep minio | awk '{print $1}')\npvc_arr=(${pvc_results})\nfor ((i=0; i<${#pvc_arr[*]}; i++ ))\ndo\nkubectl delete pvc ${pvc_arr[i]} -n minio\ndone\n\n# delete pv\npv_results=$(kubectl get pv | grep minio | awk '{print $1}')\npv_arr=(${pv_results})\nfor ((i=0; i<${#pv_arr[*]}; i++ ))\ndo\nkubectl delete pv ${pv_arr[i]}\ndone\n\n# delete minio operator\nkubectl delete -f minio-operator.yaml -n minio\n\n# delete minio sc\nkubectl delete -k direct-csi\n\n# delete secret\nkubectl delete secret tls-ssl-minio -n minio\n\n# delete ns\nkubectl delete ns minio\n```\n执行完上面删除操作后再到work node删除minio生成的指定文件如data,pvc等\n\n```shell\n// 到work node上删除data\nrm -rf /mnt/minio_data0/ /mnt/minio_data1/\n\n// 删除pv\nrm -rf /var/lib/kubelet/plugins/direct-csi-controller-min-io\nrm -rf /var/lib/kubelet/plugins/direct-csi-min-io\nrm -rf /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-***<pv about minio>\nrm -rf /var/lib/kubelet/plugins_registry/<minio>.sock\n```\n\n## **遇到的问题**\n\n### **问题1:** 删除其它机器pvc绑定的 /var/lib/kubelet/plugins/* 出错\n\n```shell\nrm -rf /var/lib/kubelet/plugins/*\nrm: cannot remove ‘/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-d6ea6990-2a0b-4f4b-9838-3a971424732d/globalmount’: Device or resource busy\nrm: cannot remove ‘/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-a08fed0b-b16d-4a74-a052-7936d6fb8340/globalmount’: Device or resource busy\n```\n解决方法:\n\n```shell\numount /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-d6ea6990-2a0b-4f4b-9838-3a971424732d/globalmount\numount /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-a08fed0b-b16d-4a74-a052-7936d6fb8340/globalmount\n```\n再进行删除即可\n\n### **问题2:** minio pod删除不掉一直处于Terminating状态\n强制删除pod命令:\n\n```shell\nkubectl delete pod <PodName> -n <NAMESPACE> --force --grace-period=0\n```\n\n### **问题3:** 有个node上pod部署不成功\n场景描述： 清理minio重新部署时应该有一步是要删除$ rm -rf /var/lib/kubelet/plugins/*, 但是不小心执行$ rm -rf /var/lib/kubelet/plugins/, 把plugins文件夹给删除了.  \n之后再部署minio pod发现如下错误\n\n```shell\nkubectl describe po/minio-2 -n minio\n......\nWarning  FailedMount  21s (x2 over 2m23s)  kubelet, hci-node03  MountVolume.MountDevice failed for volume \"pvc-a7f404b2-d1a0-4d8a-a00a-ee40abb85760\" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name direct.csi.min.io not found in the list of registered CSI drivers\nWarning  FailedMount  21s (x2 over 2m23s)  kubelet, hci-node03  MountVolume.MountDevice failed for volume \"pvc-d8d4ba24-f6e9-4a86-ac8a-3ce1cd6c36b6\" : kubernetes.io/csi: attacher.MountDevice failed to create newCsiDriverClient: driver name direct.csi.min.io not found in the list of registered CSI drivers\n```\n\n解决方法是登陆hci-node03机器, 重启kubelet服务, 因为kubelet服务默认挂载pod路径是上面的plugins/, 手动删除可能会有影响但没深究.  \n\n```shell\nsystemctl restart kubelet\n```\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"greenplum authorization and encryption","url":"/2021/05/31/storage/greenplum/greenplum_authorization_encryption/","content":"\nReference Link:\n\nhttps://docs.greenplum.org/6-8/security-guide/topics/Authorization.html\n\nhttps://docs.greenplum.org/6-8/security-guide/topics/Encryption.html\n\n\n\n\n","tags":["storage"],"categories":["storage","greenplum"]},{"title":"istio cert-manager","url":"/2021/05/31/microService/istio/istio_cert_manager/","content":"\n## cert-manager\n\n默认情况下直接部署Istio后会自动生成默认的根证书, 此文章展示如何配置自己生成的根证书.\n\nOfficial website: \nhttps://istio.io/latest/zh/docs/ops/integrations/certmanager/\n\n[cert-manager](https://cert-manager.io/) 是一种自动执行证书管理的工具，它可以与 Istio Gateway 集成以管理 TLS 证书。\n\n\n## Download Istio\n\n``` shell\ncurl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.8.1 TARGET_ARCH=x86_64 sh -\ncd istio-1.8.1\n\n# Add the istioctl client to your path (Linux or macOS)\nexport PATH=$PWD/bin:$PATH\n```\n\n## Plug in certificates and key into the cluster\n\n1. In the top-level directory of the Istio installation package, create a directory to hold certificates and keys:\n\n``` shell\nmkdir -p certs\npushd certs\n```\n\n2. Generate the root certificate and key:\n\n``` shell\nmake -f ../tools/certs/Makefile.selfsigned.mk root-ca\n```\n\nThis will generate the following files:\n\n* `root-cert.pem`: the generated root certificate\n* `root-key.pem`: the generated root key\n* `root-ca.conf`: the configuration for `openssl` to generate the root certificate\n* `root-cert.csr`: the generated CSR for the root certificate\n\n3. For each cluster, generate an intermediate certificate and key for the Istio CA. The following is an example for `hce`:\n``` shell\nmake -f ../tools/certs/Makefile.selfsigned.mk hce-cacerts\n```\n\nThis will generate the following files in a directory named `hce`:\n\n* `ca-cert.pem`: the generated intermediate certificates\n* `ca-key.pem`: the generated intermediate key\n* `cert-chain.pem`: the generated certificate chain which is used by istiod\n* `root-cert.pem`: the root certificate\nYou can replace `hce` with a string of your choosing. For example, with the argument `cluster2-cacerts`, you can create certificates and key in a directory called `cluster2`.\n\nIf you are doing this on an offline machine, copy the generated directory to a machine with access to the clusters.\n\n4. In each cluster, create a secret `cacerts` including all the input files `ca-cert.pem`, `ca-key.pem`, `root-cert.pem` and `cert-chain.pem`. For example, for `hce`:\n``` shell\nkubectl create namespace istio-system\nkubectl create secret generic cacerts -n istio-system \\\n    --from-file=hce/ca-cert.pem \\\n    --from-file=hce/ca-key.pem \\\n    --from-file=hce/root-cert.pem \\\n    --from-file=hce/cert-chain.pem\n```\n\n5. Return to the top-level directory of the Istio installation:\n``` shell\npopd\n```\n\n## Deploy Istio\n\n1. Deploy Istio using the `demo` profile.\n\nIstio’s CA will read certificates and key from the secret-mount files.\n``` shell\nistioctl install --set profile=demo -y\n```\n\n## Deploying example services\n\n1. Deploy the httpbin and sleep sample services.\n``` shell\nkubectl create ns foo\nkubectl apply -f <(istioctl kube-inject -f samples/httpbin/httpbin.yaml) -n foo\nkubectl apply -f <(istioctl kube-inject -f samples/sleep/sleep.yaml) -n foo\n```\n\n2. Deploy a policy for workloads in the foo namespace to only accept mutual TLS traffic.\n``` shell\nkubectl apply -n foo -f - <<EOF\napiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"default\"\nspec:\n  mtls:\n    mode: STRICT\nEOF\n```\n\n## Verifying the certificates\n\nIn this section, we verify that workload certificates are signed by the certificates that we plugged into the CA. This requires you have `openssl` installed on your machine.\n\n`Sleep 20 seconds` for the mTLS policy to take effect before retrieving the certificate chain of `httpbin`. As the CA certificate used in this example is self-signed, the `verify error:num=19:self signed certificate in certificate chain` error returned by the openssl command is expected.\n``` shell\nsleep 20; kubectl exec \"$(kubectl get pod -l app=sleep -n foo -o jsonpath={.items..metadata.name})\" -c istio-proxy -n foo -- openssl s_client -showcerts -connect httpbin.foo:8000 > httpbin-proxy-cert.txt\n```\n\nParse the certificates on the certificate chain.\n``` shell\nsed -n '/-----BEGIN CERTIFICATE-----/{:start /-----END CERTIFICATE-----/!{N;b start};/.*/p}' httpbin-proxy-cert.txt > certs.pem\nawk 'BEGIN {counter=0;} /BEGIN CERT/{counter++} { print > \"proxy-cert-\" counter \".pem\"}' < certs.pem\n```\n\nVerify the root certificate is the same as the one specified by the administrator:\n``` shell\nopenssl x509 -in certs/cluster1/root-cert.pem -text -noout > /tmp/root-cert.crt.txt\nopenssl x509 -in ./proxy-cert-3.pem -text -noout > /tmp/pod-root-cert.crt.txt\ndiff -s /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt\n  Files /tmp/root-cert.crt.txt and /tmp/pod-root-cert.crt.txt are identical\n```\n\nVerify the CA certificate is the same as the one specified by the administrator:\n``` shell\nopenssl x509 -in certs/cluster1/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt\nopenssl x509 -in ./proxy-cert-2.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt\ndiff -s /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt\nFiles /tmp/ca-cert.crt.txt and /tmp/pod-cert-chain-ca.crt.txt are identical\n```\n\nVerify the certificate chain from the root certificate to the workload certificate:\n``` shell\nopenssl verify -CAfile <(cat certs/cluster1/ca-cert.pem certs/cluster1/root-cert.pem) ./proxy-cert-1.pem\n./proxy-cert-1.pem: OK\n```\n\n## Cleanup\n\nRemove the certificates, keys, and intermediate files from your local disk:\n``` shell\nrm -rf certs\nrm -rf proxy-cert-*\n```\n\nRemove the secret cacerts, and the foo and istio-system namespaces:\n``` shell\nkubectl delete secret cacerts -n istio-system\nistioctl manifest generate --set profile=demo | kubectl delete --ignore-not-found=true -f -\nkubectl delete ns foo istio-system\n```\n\nRemove the httpbin and sleep service\n``` shell\nkubectl delete -f samples/httpbin/httpbin.yaml -n foo\nkubectl delete -f samples/sleep/sleep.yaml -n foo\nkubectl delete ns foo\n```\n\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"Istio 查看证书","url":"/2021/05/31/microService/istio/istio查看证书/","content":"\nreference:\nhttps://preliminary.istio.io/latest/docs/ops/common-problems/security-issues/#keys-and-certificates-errors\n\n<!-- more -->\n\n## 查看pod对应的envoy证书配置\n\n``` shell\nkubectl get po -n foo\n  NAME                      READY   STATUS    RESTARTS   AGE\n  httpbin-5b6477fb8-5t5gs   2/2     Running   0          72m\n  sleep-866b7dc94-92rw8     2/2     Running   0          72m\n\nistioctl proxy-config secret httpbin-5b6477fb8-5t5gs -n foo\n  RESOURCE NAME     TYPE           STATUS     VALID CERT     SERIAL NUMBER                               NOT AFTER                NOT BEFORE\n  default           Cert Chain     ACTIVE     true           184437040641666769168369109524598657285     2021-04-26T05:29:31Z     2021-04-25T05:29:31Z\n  ROOTCA            CA             ACTIVE     true           173938461372044230585237743290786401784     2031-04-23T05:28:39Z     2021-04-25T05:28:39Z\n\n\n# Centos 安装 jq 工具\nyum install epel-release\nyum list jq\nyum install jq\n\n# 查看证书\nistioctl proxy-config secret httpbin-5b6477fb8-5t5gs -n foo -o json\nistioctl proxy-config secret httpbin-5b575fd4-n8kp2 -n foo -o json | jq '[.dynamicActiveSecrets[] | select(.name == \"default\")][0].secret.tlsCertificate.certificateChain.inlineBytes' -r | base64 -d | openssl x509 -noout -text\nistioctl proxy-config secret httpbin-5b6477fb8-5t5gs -n foo -o json | jq '[.dynamicActiveSecrets[] | select(.name == \"ROOTCA\")][0].secret.validationContext.trustedCa.inlineBytes' -r | base64 -d | openssl x509 -noout -text\n  Certificate:\n      Data:\n          Version: 3 (0x2)\n          Serial Number:\n              82:db:54:41:73:82:b0:70:0e:a8:53:34:fe:52:4d:f8\n      Signature Algorithm: sha256WithRSAEncryption\n          Issuer: O=cluster.local\n          Validity\n              Not Before: Apr 25 05:28:39 2021 GMT\n              Not After : Apr 23 05:28:39 2031 GMT\n\n# 查看证书编号与上面的proxy-config secret获取的SERIAL NUMBER进行对比\npip3 install pyopenssl\npython3\nPython 3.6.8 (default, Nov 16 2020, 16:55:22)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> '{0:x}'.format(int(173938461372044230585237743290786401784))\n'82db54417382b0700ea85334fe524df8'\n>>>\n# 可以看到上面返回的证书数值与CA证书的Serial Number一致.\n\n```\n\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"01 Kubernetes build with kubeadm","url":"/2021/05/31/microService/kubernetes/01_kubernetes_build/","content":"\n## **Kubernetes 简介**\n<!-- more -->\n\n ![](1.JPG)\n ![](2.JPG)\n \n## **主要特征**\n:::info\n* 以服务为中心: 不关心服务运行的环境和细节，所以构建在kubernetes上的系统可以部署在物理机、虚拟机、公有云、私有云，在什么地方运行都是无差别的.\n* 自动化: 在kubernetes里的系统可以自动扩缩容、自动升级、更新、部署. 比如:\n  - K8s收到某个指令后，会触发调度流程，选中目标节点，部署或者停止响应服务.\n  - 如果有新的pod启动，会被自动加入负载均衡器，自动生效\n  - 服务运行过程中，K8s会定期的检查它们的实例数，以及这些实例的状态是否正常，当发现某个实例不可用的时候会自动销毁不可用的实例然后重新调度一个新的实例，以上所有都是自动化完成，不需要人工参与.\n:::\n\n## **架构**\n![](K8s_arch1.JPG)\n\n## **Kubernetes VS Docker**\n\n:::info\nK8s可以看成是Docker的上层架构, 就像是javaee和java的关系,Java是一问语言，J2EE是Java语言的一门使用技术，Java为J2EE提供了库和语法，J2EE使用Java的库和语法应用在WEB上。这是概念性的区别。\n* Java SE（Java Platform，Standard Edition）。Java SE 以前称为 J2SE。它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 包含了支持 Java Web 服务开发的类，并为 Java Platform，Enterprise Edition（Java EE）提供基础。\n* Java EE（Java Platform，Enterprise Edition）。这个版本以前称为 J2EE。企业版本帮助开发和部署可移植、健壮、可伸缩且安全的服务器端 Java 应用程序。Java EE 是在 Java SE 的基础上构建的，它提供 Web 服务、组件模型、管理和通信 API，可以用来实现企业级的面向服务体系结构（service-oriented architecture，SOA）和 Web 2.0 应用程序。\n* Java ME（Java Platform，Micro Edition）。这个版本以前称为 J2ME。Java ME 为在移动设备和嵌入式设备（比如手机、PDA、电视机顶盒和打印机）上运行的应用程序提供一个健壮且灵活的环境。Java ME 包括灵活的用户界面、健壮的安全模型、许多内置的网络协议以及对可以动态下载的连网和离线应用程序的丰富支持。基于 Java ME 规范的应用程序只需编写一次，就可以用于许多设备，而且可以利用每个设备的本机功能。\n\nK8s是以Docker技术的标准为基础去打造一个全新的分布式架构系统，K8s不是一定要依赖Docker，Docker是一个产品，而Docker技术是一些列的标准，只要实现了这些标准的产品都可以替代Docker，所以说K8s在底层可以支持它自己的容器技术并且经过Google的持续优化，号称在某些方面做得比Docker更加优秀，所以用不用Docker可以自己选择.\n:::\n\n## **核心概念**\n![](3.JPG)\n![](4.JPG)\n![](5.JPG)\n\n## **Label 标签**\nPOD，Deployment，Node等都可以打标签启到标识作用.\n\n## **POD (可以称为实例)**\n:::info\n* 所有的服务，所有的应用最终都是跑在Pod中,Pod是Kubernetes概念中最小的单元，可以理解为是Kubernetes的一个原子.\n* POD 里面可以有一个或多个容器，\n* POD里面所有的容器都是运行在一台机器上\n* POD里面的容器共享网络，有一个唯一的IP\n* POD里面都会有一个容器叫做Pause容器\n  - 有特定的image镜像比如pause:v1.0\n  - 作为根容器，把POD里其它的容器都link到一起，当我们的业务里面有两个或多个容器关系非常紧密，这时候就可以考虑把它们放到同一个POD里\n  - 负责整个POD的健康检查，然后汇报给K8s\n:::\n![](K8s_arch2.JPG)\n![](K8s_arch3.JPG)\n \n## **Pod 通讯**\n\n1. Pod内容器之间通讯: 通过localhost加上端口就可以访问.\n![](Pod_communication1.JPG)\n\n2. 同一个Node上不同Pod之间的通讯: 同一个Node上的Pod，它们默认的路由都是Docker0，都关联在同一个Docker0网桥，地址网段是相同的，它们之间可以直接通过网桥进行通讯，访问方式是可以通过 Pod IP 直接进行访问.\n![](Pod_communication2.JPG)\n\n3. 不同Node不同Pod直接通讯: Pod的IP不能冲突，Pod的IP和Node的IP关联起来，通过关联让Pod之间可以通讯.\n![](Pod_communication3.JPG)\n\n## **Service**\n![](K8s_arch4.JPG)\n:::info\nPod具体运行在某个Node上\nService在Pod外再包一层IP\n当某个Pod提供服务出现问题，会在其它地方再启动一个Pod和新的Pod的IP，我们还可以通过Service IP找到新的Pod\n上面2台Node，3个Pod可以看做同一个应用的多个副本，对一个应用进行扩容，从一个实例扩成三个实例对外提供相同服务\nService除了上面可以定位到Pod地址外还可以对Pod地址进行负载均衡，比如轮训访问每个Pod\nPod也不一定是一模一样的，也可以是同一个应用的不同版本\n\n通过什么方式来确定哪些Pod是一个Service? 怎么定位哪个Pod或哪几个Pod属于某个Service?\nKubernetes使用的是Laber Selector\n通过配置好的Service的Select()，选择标签然后自动寻找POD, Service 对外有一个ClusterIP(Kube-proxy)，其它服务或者Client客户端就可以通过ClusterIP访问到这个Service，进而访问到最底层的POD服务\n:::\n\n## **ReplicaSet(RS)副本集 (副本集这一层运行的程序可以称为应用)**\nRS是POD的上一层, 管理关联POD，如果应用运行过程中某个POD出现了异常或异常退出，RS就会保证副本始终为R，会在另一台机器重新调度一个POD\n\n## **Deployment**\n* 扩容: 如对一个应用(Pod)扩容，把1个Pod扩容成四个实例，扩容的是Pod而不是Service, 4个Pod拥有相同标签，ServiceIP不变并对这4个Pod实行负载均衡\n![](Pod_Scaling.JPG)\n\n* 滚动更新: 一个旧的应用(RS这一层)运行了两个实例(两个POD), 更新这个应用的时候，Deployment会自动帮我们创建一个RS，并且滚动的先启动一个新版本的不改变服务的POD, 修改的可能是image(login-image:V1->V2),这时候Deployment管理的是三个实例(3个POD)，新的POD启动完成，健康检查结束，会停掉原来的POD并删掉，然后RS会再新建一个与另一个旧版本提供相同服务的POD，然后停掉另外一个旧版本POD，旧版POD停掉之后Deployment会清理掉管理旧版本的RS, 服务更新完成.\n![](Rolling_update1.JPG)\n![](Rolling_update2.JPG)\n\n## **架构设计**\n![](k8s_architecture1.JPG)\n![](k8s_architecture2.JPG)\n\n## **密码学原理**\n* 对称加密:\n![](Symmetric_encryption.JPG)\n \n* 非对称加密:\n![](Asymmetric_encryption.JPG)\n\n## **服务之间通信加密:**\n![](6.JPG)\n\n:::info\n非对称加密非常复杂，不管是加密还是解密都非常耗时， 如果每次通信都进行非对称加密性能损耗是无法接受的\n对称加密性能非常高，因此考虑把两者结合在一起来通信\n* Server B 公开了自己的公钥pub_key, 任何人都可以看到\n* 第一次通信，Server A 用Server B的pub_key 加密自己的秘钥，把秘钥变成密文，然后发到Server B，除了Server B可以用自己的私钥解密看到是对称加密的秘钥，中间黑客因为没有Server B的私钥因此无法解密, Server B 就知道要跟 Server A 进行对称加密的通信，并且使用的就是这个秘钥\n* 之后通信, Server A就可以用发送给Server B 的秘钥对要发送的信息使用对称加密算法进行加密变成密文，然后发送给Server B， Server B收到信息后再用第一次拿到的秘钥进行对称加密算法解密.\n- 上面的就是 SSL/TLS 协议， https底层就是通过这两个协议进行通信.\n+ 上面有个不完美地方，Server B公开pub_key， 黑客截获后再把黑客自己Server的pub_key发送给Server A, Server A拿着这个pub_key加密了自己的私钥，之后又被黑客截获并解密，虽然这些工作对黑客来说很复杂，但这种情况是有可能发生的.\n+ 解决方法: CA 证书认证机构，一个中间商，给所有Server颁发证书,所有正常网站的证书都在这一个地方存储，当Server A 拿到 pub_key之后会向 CA 查询这个公钥是不是合法的是不是可以信任的，CA会查自己的数据库这个pub_key是哪个公司的，它的域名是什么，包括所有人是谁等各种信息在CA都有备案，CA告诉Server A这个pub_key是我颁发的没有问题，Server A再拿着这个公钥去通信, 有时候我们访问一些网站时候，https会显示红色警告，这就说明这个网站的证书不是通过CA认证过的，一般是自己生成的.\n:::\n\n## **服务发现**\n* Kube-proxy(ClusterIP)： 为Pod创建虚拟IP，只能在集群内部访问，并且是固定的, 只要Service不删除，这个IP是不变的.\n* Kube-proxy(NodePort): 在每个Node上都启一个线程端口，把服务暴露在节点上，这样就可以让集群外的服务通过Node IP 和 NodePort去访问集群内的服务.\n* Kube-DNS: Kubernetes的一个插件，负责集群内部的DNS解析，目的是让集群内部的Pod之间通过名字去访问\n\n## **环境搭建一篇blog**\n* 官方推荐使用Kubeadmin进行方便快捷的搭建.\n* 网上找的个人搭建的Kubernetes, 在绿色网络环境下安装kubernetes集群，并在安装过程中加深对Kubernetes组件以及架构的理解.\n - [https://github.com/liuyi01/kubernetes-starter](https://github.com/liuyi01/kubernetes-starter)\n\n## **Download & Install**\nkubernetes集群部署三种方式\n;;;id00 1. kubeadm(安装起来简单)\nkubeadm也是一个工具，提供kubeadm init和kubeadm join，用于快速部署kubernetes集群, 官网步骤参考: \nhttps://kubernetes.io/docs/tasks/tools/install-kubectl/\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n;;;id00\n\n;;;id00 2. 二进制包 (生产环境安装方式,比较稳定)\n从官方下载发行版的二进制包，手动部署每个组件，组成kubernetes集群\n地址：https://github.com/kubernetes/kubernetes/releases\nhttps://github.com/kubernetes/kubernetes/releases?after=v1.13.1\n;;;\n\n;;;id00 3. minikube\nminikube是一个工具，可以在本地快速运行一个单点的kubernetes，仅用于尝试K8S或日常开发的测试环境使用\n部署地址：https://kubernetes.io/docs/setup/minkube/\n;;;\n\n:::info\n`这里我们使用kubeadm安装的方式搭建 kubernetes`\n:::\n\n\n搭建K8s集群只用了一台安装Ubuntu18.04的酷睿机器.\n1. Ubuntu18.04宿主主机上 download & install virtualbox\n``` shell\n $ apt-get install virtualbox\n```\n2. 下载Ubuntu18.04镜像: http://releases.ubuntu.com/18.04/ 选择 ubuntu-18.04.4-live-server-amd64.iso 2020-02-03 18:36 870M\tServer install image for 64-bit PC (AMD64) computers (standard download)\n3. 用virtualbox安装两台Ubuntu18.04虚拟机\n  * 虚拟机server01 作为 master; \n  * 虚拟机server02 作为 worker01; \n  * 宿主主机 作为 worker02\n每台虚拟机 内存要大于等于 2 G ，CPU核数需要大于等于 4 核\n\n### **每个node都在 /etc/environment 添加如下信息**\n\n``` shell\nhttp_proxy=\"http://child-prc.intel.com:913/\"\nhttps_proxy=\"http://child-prc.intel.com:913/\"\nftp_proxy=\"ftp://child-prc.intel.com:913/\"\nno_proxy=\"K8S_MASTER_IP,K8S_MASTER_HostName\"  如: no_proxy=\"10.67.108.200,hci-node01\"\nNO_PROXY=$no_proxy\nHTTP_PROXY=$http_proxy\nHTTPS_PROXY=$https_proxy\n\nsource /etc/environment\n```\n\n### **安装docker**\n#### **删除旧版本docker**\n\n```shell\n# 步骤1:\nrpm -qa | grep docker – – 列出包含docker字段的软件的信息\n  docker-ce-cli-19.03.12-3.el7.x86_64\nyum remove docker-ce-cli-19.03.12-3.el7.x86_64\n\n# (optional)步骤2:\nyum remove -y docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine\n```\n\n#### **配置docker源**\n\n``` shell\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\nsudo yum-config-manager --add-repo  https://download.docker.com/linux/centos/docker-ce.repo\n```\n\n#### **yum 查看docker可用版本**\n\n```shell\nyum list docker-ce --showduplicates | sort -r\nyum list docker-ce-cli --showduplicates | sort -r\nyum list containerd.io --showduplicates | sort -r\n```\n\n#### **开始安装docker**\n;;;id0 第一种方法:\n```shell\n# 安装最新版docker\nyum install docker-ce docker-ce-cli containerd.io\n\n# 安装指定版docker\nyum install docker-ce-19.03.14-3.el7 docker-ce-cli-19.03.14-3.el7 containerd.io-1.3.9-3.1.el7\n```\n;;;\n\n;;;id0 第二种方法: 使用curl升级到最新版\n\n```shell\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsh get-docker.sh\n```\n;;;\n\n#### **启动docker**\n\n```shell\nsystemctl daemon-reload\nsystemctl start docker\nsystemctl enable docker\n```\n\n#### **docker的proxy**\n\n**NOTE:**\n如果在系统的**`/etc/environment`**中添加proxy, 则k8s安装过程api-server等组件会先读取/etc/environment文件中的proxy信息.\n```\ncat /etc/environment\nhttp_proxy=\"http://child-prc.intel.com:913\"\nhttps_proxy=\"http://child-prc.intel.com:913\"\nno_proxy=\"10.67.108.211,10.67.109.142,10.67.109.147,10.67.109.144,10.67.108.220,127.0.0.1,hce-node01,hce-node02,hce-node03,hce-node04\"\nNO_PROXY=$no_proxy\nHTTP_PROXY=$http_proxy\nHTTPS_PROXY=$https_proxy\n\nsource /etc/environment\n```\n;;;id1 第一种:\n\n```shell\nmkdir /etc/systemd/system/docker.service.d\ntouch /etc/systemd/system/docker.service.d\n\n# Add proxy in this newly created file\nvim /etc/systemd/system/docker.service.d/proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"HTTPS_PROXY=http://child-prc.intel.com:913\"\nEnvironment=\"NO_PROXY=10.67.108.211,10.67.109.142,10.67.109.147,10.67.109.144,10.67.108.220,127.0.0.1,hce-node01,hce-node02,hce-node03,hce-node04\"\n```\n;;;\n\n;;;id1 第二种:\n\n```shell\nmkdir /etc/systemd/system/docker.service.d\nvim /etc/systemd/system/docker.service.d/http-proxy.conf\n[Service]\nEnvironment=\"HTTP_PROXY=http://child-prc.intel.com:913/\"\n\nvim /etc/systemd/system/docker.service.d/https-proxy.conf\n[Service]\nEnvironment=\"HTTPS_PROXY=http://child-prc.intel.com:913/\"\n\nvim /etc/systemd/system/docker.service.d/no-proxy.conf\n  [Service]\n  Environment=\"NO_PROXY=10.239.140.133,10.239.141.123,master-node,node-1\"\n```\n;;;\n\n之后再次加载os系统配置项然后重启docker\n\n```shell\nsystemctl daemon-reload\nsystemctl start docker\n```\n\n#### **docker daemon**\n\n```shell\nvim /etc/docker/daemon.json\n{\n\"insecure-registries\" :[\"hce-node01:5000\"],\n\"registry-mirrors\": [\"http://hub-mirror.c.163.com\", \"https://registry.docker-cn.com\"], // 或者\"registry-mirrors\": [\"https://uxk0ognt.mirror.aliyuncs.com\"]\n\"live-restore\": true,\n\"data-root\": \"/home/zhan/docker/data\"\n}\n\n# 重新加载docker daemon, 以后每次修改docker 的 daemon.json后可以只执行下面的reload操作就可以了\nsystemctl reload docker\n\n# 下面方法会重启doker运行时.\nsystemctl daemon-reload\nsystemctl restart docker\n```\n\n::: info\n**[\"data-root\"]{.blue}**, 最好设置下，否则docker下载的image和image运行时的容器layer挂在在系统上的目录是在 [/var/lib/docker/...]{.blue}, 而此目录数据默认是存储在根 [“/”]{.blue}目录下，我们装系统默认给根目录分区是50G大小，根目录快用完时, 会发现系统上有些docker容器被默认退出, 下载的image也被删除.\n:::\n\n#### 镜像源\n可以同事添加多个镜像源, 如上所示\nDocker 官方中国区：\nhttps://registry.docker-cn.com\n\n网易：\nhttp://hub-mirror.c.163.com\n\n中国科技大学：\nhttps://docker.mirrors.ustc.edu.cn\n\n阿里云：\nhttps://pee6w651.mirror.aliyuncs.com\n\n:::info\n每台机器都安装kubeadm(二进制文件工具), kubelet(服务), master上安装kubectl(二进制文件工具), 也可以在需要kubectl控制k8s资源的worknode上也安装(也就是下载或拷贝)kubectl二进制文件工具.\n:::\n\n### 卸载旧版K8S\n\n```shell\nkubectl delete node --all\n\nfor service in kube-apiserver kube-controller-manager kubectl kubelet kube-proxy kube-scheduler; do\nsystemctl stop $service\ndone\n\nyum -y remove kubeadm kubectl kubelet\n```\n\n### 安装新版kubernetes\n\nReference Link: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n\n;;;id2 Ubuntu安装k8s\nreference: https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/\n\n```shell\n\n# 关闭交换区\nswapoff -a\n# 永久关闭, 注释掉带有swap的那一行\nvim /etc/fstab\n\n# 查看防火墙状态\nsudo ufw status\n  Status: inactive\n# 开启防火墙\nsudo ufw enable\n# 关闭防火墙\nsudo ufw disable\n\n\n# 如果在公司, 配置公司proxy\nexport http_proxy=http://<Proxy>:port\nexport https_proxy=http://<Proxy>:port\n\n# 安装k8s包\nsudo apt-get update\nsudo apt-get install -y apt-transport-https ca-certificates curl\ncurl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\necho \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee /etc/apt/sources.list.d/kubernetes.list\napt-get update\n\n# 查看都有哪些版本\napt-cache madison kubectl\napt-cache madison kubectl\napt-cache madison kubectl\n\n# 安装指定版本的kubernetes\napt-get install -y kubectl=<version> kubelet=<version> kubeadm=<version>\napt-get install -y kubectl=1.19.0-00 kubelet=1.19.0-00 kubeadm=1.19.0-00\napt-mark hold kubelet kubeadm kubectl\n```\nThe kubelet is now restarting every few seconds, as it waits in a crashloop for kubeadm to tell it what to do.\n;;;\n\n;;;id2 Centos安装k8s\n```shell\n# 关闭交换区\nswapoff -a\n# Edit /etc/fstab to comment out swap partition line so that it remains disabled after reboot\n\nvim /etc/sysctl.d/k8s.conf\n  net.bridge.bridge-nf-call-ip6tables = 1\n  net.bridge.bridge-nf-call-iptables = 1\n\nsysctl --system\n\n# Centos 关闭防火墙\nsystemctl stop firewalld.service\nsystemctl disable firewalld\n\n# Ubuntu 关闭防火墙\nsudo apt-get install ufw \n# inactive状态是防火墙关闭状态 active是开启状态\nsudo ufw status\n  Status: active\n# 关闭防火墙\nsudo ufw disable\n# 开启防火墙\nsudo ufw enable\n\n# Set SELinux in permissive mode (effectively disabling it)\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\n# 配置kubernetes安装源\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nexclude=kubelet kubeadm kubectl\nEOF\n\n# 查看可安装的 kubeadm 版本\nyum list kubelet kubeadm kubectl --showduplicates|sort -r\n\n# 安装kubernetes组件\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes // 禁掉除了这个之外的别的仓库,也就是用这个新加的kubernetes仓库来安装kubeadm等.  \n(特定版本)yum install -y kubectl-1.19.0 kubelet-1.19.0 kubeadm-1.19.0 --disableexcludes=kubernetes\nsystemctl enable --now kubelet\n\nkubeadm version\t\t// 通过 kubectl 命令行客户端向运行在主节点上的 Kubemetes API 服务器发出 REST 请求以与集群交互\nkubectl version\t\t// 客户端工具\n# kubelet服务配置文件路径: /var/lib/kubelet/config.yaml\n$ kubelet --version\t\t// kubelet是一个服务，可通过systemctl restart kubelet重启服务，每台master和worker节点都需要安装\n$ systemctl enable --now kubelet\n$ kubeadm reset\n$ sudo hostnamectl set-hostname master-node //修改机器名字, 重开终端就可以看到机器名变了\n```\n;;;\n\n### **k8s配置自动补全命令**\n\n```shell\n# 安装bash自动补全插件\nyum install bash-completion -y\n\n# 设置kubectl与kubeadm命令补全，下次login生效\nkubectl completion bash >/etc/bash_completion.d/kubectl\nkubeadm completion bash > /etc/bash_completion.d/kubeadm\n```\n\n## **机器配置**\n\n### **环境配置**\nmaster-node和worknode都需要设置.  \n关闭交换区, K8s认为swap性能开销比较大, 性能会大幅降低, 使用swap做云基础架构会减少性能, 因此k8s关闭swap.  \n另外重新装系统OS时候就可以不给swap分配分区.  \n\n```shell\nswapoff -a\t\t\t// 临时关闭交换区，$ free -h 可以查看 Swap: 0B...\nvim /etc/fstab  // 设置重启后自动关闭swapoff, 将含有swap的那一行前面加\"#\"注释掉就可以了\n  /dev/mapper/centos-swap swap                    swap    defaults        0 0\nsed -i '/swap/d' /etc/fstab\t//永久关闭\n\n# 关闭防火墙\nsystemctl stop firewalld.service\nsystemctl status firewalld.service\t// 查看防火墙是否有 Active: inactive (dead) since......\nsystemctl disable firewalld \t\t\t// 设置开机不启动防火墙\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables=1\n```\n有的说明还可以关闭网络管理器,关闭核心防护,清空iptabels, 编辑主机名\n\n```shell\n// systemctl list-unit-files --type=service | grep NetworkManager // 查看NetworkManager是否enabled\n// systemctl status NetworkManager\t// 查看NetworkManager是否running\n// systemctl stop NetworkManager\t\t// 关闭网络, 没有IP地址无法远程连接终端, 慎用.\n// systemctl disable NetworkManager\nsetenforce 0\nsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n# 清理 iptables规则\nyum install ipvsadm -y  # apt-get install ipvsadm -y\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\nipvsadm -C\nipvsadm --clear\n\n```\n::: info\nIptables原理\nlinux的防火墙由netfilter和iptables组成\n用户空间的iptables制定防火墙规则，内核空间的netfilter实现防火墙功能netfilter（内核空间）位于Linux内核中的包过滤防火墙功能体系，称为Linux防火墙的“内核态”\niptables(用户空间)位于/sbin/iptables，是用来管理防火墙的命令的工具，为防火墙体系提供过滤规则/策略，决定如何过滤或处理到达防火墙主机的数据包，称为Linux防火墙的“用户态\"\n\n$ getenforce // 获得当前SELinux的模式。\n* －enforcing        强制模式：SELinux 被启动，并强制执行所有的安全策略规则。\n* －permissive     宽容模式：SELinux 被启用，但安全策略规则并没有被强制执行。当安全策略规则应该拒绝访问时，访问仍然被允许。然而，此时会向日志文件发送一条消息，表示该访问应该被拒绝。\n* －disabled         禁用模式：SELinux 被关闭，默认的 DAC 访问控制方式被使用。对于那些不需要增强安全性的环境来说，该模式是非常有用的。\n$ setenforce // 修改当前SELinux的模式\n:::\n\n```shell\n# 关闭selinux:\t\t// 限制访问linux资源文件上下文\ngetenforce\t\t\t// 查看是否disabled\nsetenforce 0\t\t\t//临时关闭selinux(Security-Enhanced Linux), 终端会输出\"setenforce: SELinux is disabled\"\nvim /etc/selinux/config --> 将 SELINUX=permissive 改为 SELINUX=disabled, 设置重启后自动关闭selinux\nsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\t//永久关闭\nsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config(另一种)\n```\n### **同步系统时间**\n\n:::info\n涉及到验证签发的证书的有效性, 如果签发证书的服务器时间比使用证书的服务器时间早, 就会导致校验不成功或证书错误, 一直等到使用证书的服务器时间也运行到证书开始生效的时间后才会解决这个问题.\n:::\n\n;;;id3 第一种: 手动修改时间\n\n```shell\n# 查看当前系统时间\ndate\n  Fri Apr  2 09:58:24 EDT 2021\n\n# 查看时区\ndate -R\n  Fri, 02 Apr 2021 13:40:30 -0400  // 西四区\n# 修改系统时区\ncp -f /usr/share/zoneinfo/Asia/Shanghai     /etc/localtime\n# 提示是否覆盖,输入Y回车\n# 再次查看系统时间, CST, China Time Zone\ndate\n  Sat Apr  3 02:05:06 CST 2021\ndate -R\n  Sat, 03 Apr 2021 02:08:23 +0800  // 东八区\n\n# 修改当前系统时间\ndate -s “2018-2-22 19:10:30”\n\n# 查看硬件时间\nhwclock --show\n\n# 修改硬件时间\nhwclock –set –date “2018-2-22 19:10:30”\n\n# 同步硬件时间和系统时间\nhwclock –hctosys\n(optional)# 同步系统时间和硬件时间\n(optional)hwclock --systohc\n\n# 保存时钟\nclock -w\n\n重启系统（init 6）后便发现系统时间被修改了\n```\n;;;\n\n;;;id3 第二种chrony\n1. 配置master机器\n\n\n```shell\n# master-node安装\nyum install chrony -y\n\nvim /etc/chrony.conf\n......\n# Please consider joining the pool (http://www.pool.ntp.org/join.html)\n#server 0.centos.pool.ntp.org iburst\t//注释掉或删掉\n#server 1.centos.pool.ntp.org iburst\t//注释掉或删掉\n#server 2.centos.pool.ntp.org iburst\t//注释掉或删掉\n#server 3.centos.pool.ntp.org iburst\t//注释掉或删掉\nserver 127.127.1.0 iburst\t\t//1. 添加master充当server\n......\n# Allow NTP client access from local network\nallow 10.239.0.0/16\t\t\t\t// 2. # allow 192.168.31.0/24\n# Serve time even if not synchronized to a time source.\nlocal stratum 10\t\t\t\t// 3. \n```\n\n重启chrony\n```shell\nsystemctl start chronyd\nsystemctl restart chronyd\nsystemctl enable chronyd\n```\n查看chrony端口，判断服务是否起来\n\n```shell\nss -unl | grep 123\n  UNCONN     0      0            *:123                      *:*\n```\n2. 配置node机器\n\n\n```shell\n# work node安装\nyum install chrony -y\n\nvim /etc/chrony.conf\t//只添加一行，指定从master获取时间\n# Please consider joining the pool (http://www.pool.ntp.org/join.html).\n#server 0.centos.pool.ntp.org iburst\t//注释\n#server 1.centos.pool.ntp.org iburst\t//注释\n#server 2.centos.pool.ntp.org iburst\t//注释\n#server 3.centos.pool.ntp.org iburst\t//注释\nserver 10.239.140.133 iburst\t\t\t//添加冲master获取时间\n```\n重启chrony服务, 服务重启后就与master时间同步了\n\n```shell\nsystemctl start chronyd\nsystemctl restart chronyd\nsystemctl enable chronyd\n```\nwork node上不需要查看端口, 因为node的chrony不需要开启接受请求时间端口, 因此可以没有\n\n3. work node上执行执行chronyc命令查看与master机器时间同步情况\n\n\n```shell mark:1 command:(\"$\":1)\nchronyc sources\n  210 Number of sources = 1\n  MS Name/IP address         Stratum Poll Reach LastRx Last sample\n  ^* master-node                  10   6    77    40    -10us[ -111us] +/-   67us\n```\n^* 表示时间已经同步完成\n^? 表示还没有同步完成, 需要等一会, 如果等一会还不行说明配置出错需要找原因\n\n;;;\n\n;;;id3 第三种timedatectl(实测没啥效果):\n\n```shell\n# 设置系统时区为 中国/上海\ntimedatectl set-timezone Asia/Shanghai\n\n# 将当前的UTC时间写入硬件时钟\ntimedatectl set-local-rtc 0\n\n# 重启依赖于系统时间的服务\nsystemctl restart rsyslog\nsystemctl restart crond\n```\n;;;\n\n;;;id3 第四种ntpdate:\n\n```shell\nntpdate time.windows.com \t\t// 同步 windows 系统时间\n```\n;;;\n\n### **安装镜像(可跳过)**\n\n```shell\nkubeadm config images list // 查看kubeadm 下载过的images\ndocker images\ndocker pull gcr.io/google_containers/kube-apiserver-amd64:v1.9.3\ndocker pull gcr.io/google_containers/kube-controller-manager-amd64:v1.9.3\ndocker pull gcr.io/google_containers/kube-scheduler-amd64:v1.9.3\n```\n\n### **添加机器到K8s集群**\n> 1. 在Master主机 server01 上运行\n\n```shell\nkubeadm init\n```\n返回部分数据如下\n\n```\n......\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 10.239.141.112:6443 --token uvm0zr.ndg144wcga276j16 \\\n    --discovery-token-ca-cert-hash sha256:e1535452b32ed4039fa2f261197c0b91179fb168e8da3dd58b99fc11fe2213b8\nroot@server01:~#\n```\n\n:::info\n添加kubeadm部署k8s后生成的administrator访问证书到环境变量或~/.kube目录, 使得root或其它user登陆后可以通过kubectl访问或生成k8s资源如pod等, 有如下两种方式.\n:::\n\n;;;id4 第一种配置k8s证书\n\n```shell\nexport KUBECONFIG=/etc/kubernetes/admin.conf\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" | tee -a ~/.bashrc\nsource ~/.bashrc\n```\n;;;\n\n;;;id4 第二种配置k8s证书\n(其它user而非root登陆后需要做如下操作才能通过kubectl访问或生成k8s资源如pod等):\n\n```shell\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n;;;\n\n添加容器之间的通信网络, 第三方资源weave, 官网上也推荐部署其它几种通信网络方式\n\n```shell\nkubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\n```\n\n> 2. 之后用上面命令返回的 kubeadm join 10.239.141.112:6443 --t ... 复制 并 在其它node机器(server02和宿主主机) 上运行就可以把node加进上面创建的Cluster了\n\n### **在master server01 机器上查看集群节点信息**\n\n```shell\nkubectl get nodes\nkubectl get namespaces\n```\n\n### **查看node节点信息**\n\n```shell\nkubectl describe node server02\n```\n## **重新(reset)在原来机器上搭建k8s集群操作**\n> 主机名和IP解析, 通过主机名访问机器, 修改下各个节点 /etc/hosts 文件内容(实验环境没有修改，跳过这个步骤), 也可以只在master上配置, 因为很多操作都是在master上执行\n\n```\n......\n10.239.141.106 server01\n10.239.140.184 server02\n10.239.140.186 alpha\n```\n\n> 1.需要在master节点上执行 \n\n```shell\n# reset k8s 集群\nrm -rf /etc/kubernetes/pki/etcd/\nrm -rf /var/lib/etcd\nrm -rf $HOME/.kube\nkubeadm reset\t\t// 出现有什么没有清理干净的可以手动删除掉, 如cni等,再reset, 如果还出现,可以忽略掉没有清理干净的信息提示, 执行kubeadm init.\n\n# 清理iptables规则\n(optional)yum install ipvsadm -y  # apt-get install ipvsadm -y\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\nipvsadm -C\nipvsadm --clear\n\n# 再次设置环境\nsystemctl stop kubelet\nsystemctl restart kubelet\nsystemctl daemon-reload\nsystemctl stop docker\nsystemctl restart docker\nswapoff -a\n# Edit /etc/fstab to comment out swap partition line so that it remains disabled after reboot\nsetenforce 0\nsystemctl stop firewalld.service\nsystemctl disable firewalld\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables = 1\n\nvim /etc/sysctl.d/k8s.conf\n  net.bridge.bridge-nf-call-ip6tables = 1\n  net.bridge.bridge-nf-call-iptables = 1\nsysctl --system\n\nkubeadm init\t\t//再用返回的 \"kubeadm join...\" 在其它节点执行\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" | tee -a ~/.bashrc\nsource ~/.bashrc\n```\n\n> 2.在worker节点执行:\n检查/etc/systemd/system/kubelet.service.d/20-etcd-service-manager.conf有没有残留的kubelet服务配置文件, 有的话删掉.\nenvironment_initialization.sh\n\n```shell\nsystemctl enable docker.service\nkubeadm reset\n\n# 清理iptables规则\n(optional)yum install ipvsadm -y  # apt-get install ipvsadm -y\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\nipvsadm -C\nipvsadm --clear\n\nsystemctl stop kubelet\nsystemctl stop docker\nsystemctl restart kubelet\nsystemctl restart docker\nswapoff -a\nsetenforce 0\nsystemctl stop firewalld.service\nsysctl net.bridge.bridge-nf-call-iptables=1\nsystemctl daemon-reload\n```\n加入集群\n\n```shell\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\t\t// will reset iptables\nkubeadm join ......\n```\n\n> 3.再次在master节点上执行\n如果不执行下面命令安装weave pod, kube-system命名空间下的coredns会一直处于containercreating状态.\n\n```shell\nkubectl apply -f https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\nkubectl get cs\n```\n\n## **k8s重新生成token**\n主机上执行如下命令，主机IP:10.239.140.186\n\n```shell\nkubeadm token create\nv6rgnu.ydqgkuujayykkanv\n\nkubeadm token list\nTOKEN                     TTL   EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS\nv6rgnu.ydqgkuujayykkanv   23h   2020-05-30T13:24:41+08:00   authentication,signing   <none>        system:bootstrappers:kubeadm:default-node-token\n\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\nbe6606e3e081afc6f9785fbe0e129e048e5a2a5557cb2e7747d727edd20c6ed4\n```\n用上面master主机上生成的token在worker节点执行如下命令:\n\n```shell\nkubeadm reset\nswapoff -a\nsetenforce 0\nsystemctl stop firewalld.service\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables=1\nkubeadm join --token v6rgnu.ydqgkuujayykkanv --discovery-token-ca-cert-hash sha256:be6606e3e081afc6f9785fbe0e129e048e5a2a5557cb2e7747d727edd20c6ed4  10.239.140.186:6443\n```\n\n## **k8s命令自动补全**\n\n```shell\nyum install bash-completion\necho \"source <(kubectl completion bash)\" >> ~/.bashrc\nsource ~/.bashrc\nbash /usr/share/bash-completion/bash_completion\nbash\n```\n试试 输入 `kubectl get n` 按 `tab` 查看提示.\n\n## **reset iptables**\nhttps://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/\n\n```shell\n#The reset process does not reset or clean up iptables rules or IPVS tables. If you wish to reset iptables, you must do so manually:\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n\n#If you want to reset the IPVS tables, you must run the following command:\nipvsadm -C\nipvsadm --clear\n```\n\n## **Additional**\n\n### **重新reset K8s集群，然后kubeadm init遇到如下问题**\n### **问题1**\n> [kubelet-check] The HTTP call equal to 'curl -sSL http://localhost:10248/healthz' failed with error: Get http://localhost:10248/healthz: dial tcp [::1]:10248: connect: connection refused.\n解决方法:\n\n```shell\nsystemctl restart docker\nrm -rf /etc/systemd/system/kubelet.service.d/*\nsystemctl daemon-reload\n```\n### **问题2**\nUnable to connect to the server: x509: certificate signed by unknown authority\n需要删除上一次部署后cp到~/.kube的证书文件, 再重新部署一遍k8s集群\n\n```shell\nrm -rf $HOME/.kube\n```\n### **问题3**\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n需要添加administrator访问证书\n第一种:\n\n```shell\nexport KUBECONFIG=/etc/kubernetes/admin.conf\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" | tee -a ~/.bashrc\nsource ~/.bashrc\n```\n第二种:\n\n```shell\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n### **问题n**\nhttps://istio.io/docs/examples/bookinfo/\nIstio 部署bookinfo 到bookinfo命名空间， 发现只部署了svc，RS，但是没有部署pod.\n\n```shell\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfo\n```\n用以下命令可以查看出错信息, 发现是webhook相关错误\n\n```shell\nkubectl describe rs/RS-NAME -n bookinfo\n```\n解决方法是注释掉kubernetes的proxy\n\n```xml\nvim /etc/kubernetes/manifests/kube-apiserver.yaml\n\n env:\t\t\t\t\t\t\t\t\t// K8s安装会用系统的proxy，加#注释掉.\n#- name: HTTP_PROXY\n#  value: http://child-prc.intel.com:913\n#- name: https_proxy\n#  value: http://child-prc.intel.com:913\n#- name: http_proxy\n#  value: http://child-prc.intel.com:913\n#- name: HTTPS_PROXY\n#  value: http://child-prc.intel.com:913\n - name: no_proxy\n   value: 10.239.140.186,10.239.140.200\t\t// master和一个worker节点的NodeIP.\n```\n\n稍等一会$ kubectl get po -n bookinfo 就可以看到pod慢慢部署成功了.\n\n### **部署网络weave出错**\n\nUnable to update cni config: No networks found in /etc/cni/net.d\n\n由于设置了代理导致的错误, kubelet 无法通过代理链接到 kube-apiserve\n\n解决办法:\n\n```shell\n$ unset http_proxy https_proxy\n\n# or\n$ export no_proxy=<your_kube_apiserver_ip>\n```\n\n### **去掉污点Taints**\n\n```shell\n# 允许调度 pod\nkubectl taint node {node name} node-role.kubernetes.io/master-\n\n# example\nkubectl taint node host1 node-role.kubernetes.io/master-\n\n# 禁止调度 pod\nkubectl taint node {node name} node-role.kubernetes.io/master=master\n\n# example\nkubectl taint node host1 node-role.kubernetes.io/master=master\n\n# 去掉所有控制平面host污点\nkubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n### **重新生成证书**\n\n;;;id5 生成证书方法1(推荐)\n```\n# 在master机器上执行如下命令\nkubeadm token create --print-join-command\n  W1202 13:34:46.942799   17329 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\n  kubeadm join 10.239.140.201:6443 --token 3vm6e8.wjlspdqpjau62riz     --discovery-token-ca-cert-hash sha256:99f1f55a10e439883030b810be5d3d364d12c508765984cc9ab633db6dbfada9\n\n# 再在要加入的node机器上执行如上生成的join命令就可以了.\n```\n;;;\n\n;;;id5 生成证书方法2\n```shell\n有时可能过了一段时间需要添加新的 node\n# 生成一个 token\nkubeadm token generate\n\n# 获取证书的 hash 值\nopenssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | \\\n    openssl dgst -sha256 -hex | sed 's/^.* //'\n\n# kubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>\n# example\nkubeadm join --token xf96mj.aq2c5v14r62rf2aw 172.16.50.10:6443  --discovery-token-ca-cert-hash sha256:a18c59189884451f71305a0107d15b79a8ac091ef9a8b9e394cad5d4b9f18162\n```\n;;;\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"01 Kubernetes build high availability","url":"/2021/05/31/microService/kubernetes/01_kubernetes_high_availablity_build/","content":"\n## **准备机器**\n\n| IP地址 | 主机名 | 角色 |\n| :------: | :------: | :------: |\n| 10.239.140.133 | master-node | master |\n| 10.239.131.156 | laboratory | master |\n| 10.239.141.123 | node-1 | master |\n| 10.239.141.194 | node-2 | worker |\n| 10.239.140.51 | k8s-vip | Virtual IP |\n\n总共四台机器，三台做master, 一台做work node, 部署好后可以把master上污点去掉, 照样可以部署k8s资源.\nVirtual IP是部署过程中在机器网卡上添加的虚拟IP, 操作下方有涉及到.\n**`Note: k8s-vip地址需要独一无二, 不能与能ping同的所有其它机器的IP地址冲突, 因此, 设置k8s-vip ip地址之前线ping一下改地址看是否能ping通, 如果能平通就换成其它不能ping通, 也就是其它机器还没有占用的IP地址.`**\n\n![](finish_setup.PNG)\n\n## **环境配置**\n\n修改机器名字, 重开终端就可以看到机器名变了\n```\nsudo hostnamectl set-hostname master-node\n```\n\n每台机器(master和node)都要配置\n\n\n```shell\n\n# 同步各个机器系统时间, 否则其它机器通过token加入时候可能因为证书时间不到或过期而不能加入\n# 同步时间方法请参考 [01 Kubernetes build with kubeadm]\n\n# 修改 /etc/hosts\nvim /etc/hosts\n  ......\n  10.239.140.133 master-node\n  10.239.131.156 laboratory\n  10.239.141.123 node-1\n  10.239.141.194 node-2\n  10.239.140.51  k8s-vip\n\n\n# 设置k8s相关系统内核参数\ncat << EOF > /etc/sysctl.d/k8s.conf\nnet.bridge.bridege-nf-call-iptables = 1\nnet.bridge.bridege-nf-call-ip6tables = 1\nEOF\n\nsysctl -p\necho 1 > /proc/sys/net/bridge/bridge-nf-call-iptables\necho 1 > /proc/sys/net/bridge/bridge-nf-call-ip6tables\n\nswapoff -a\nsed -i '/swap/d' /etc/fstab\n\nsystemctl stop firewalld.service\nsystemctl disable firewalld\nsetenforce 0\nsed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config\n\nsystemctl stop kubelet\nsystemctl stop docker\nsystemctl restart kubelet\nsystemctl restart docker\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables=1\n\n# 清理 iptables规则\nyum install ipvsadm -y  # apt-get install ipvsadm -y\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\nipvsadm -C\nipvsadm --clear\n```\n\n**`Note:`**\n\n```shell\n# 有时候在公司开发机上部署不成功, 需要在~/.bashrc添加NO_PROXY\n# 不要忘了添加 127.0.0.1 和 虚拟出来的 Virtual IP\ncat << EOF >> ~/.bashrc\nexport NO_PROXY=127.0.0.1,<master-node-IP>,<laboratory-IP>,<Node01-IP>,<Node02-IP>,<k8s-vip-IP>, master-node,laboratory,Node01,Node02,k8s-vip\nEOF\nsource ~/.bashrc\n```\n\n如果在系统的**`/etc/environment`**中添加proxy, 则k8s安装过程api-server等组件会先读取/etc/environment文件中的proxy信息.\n```\ncat /etc/environment\nhttp_proxy=\"http://child-prc.intel.com:913\"\nhttps_proxy=\"http://child-prc.intel.com:913\"\nno_proxy=\"127.0.0.1,<master-node-IP>,<laboratory-IP>,<Node01-IP>,<Node02-IP>,<k8s-vip-IP>, master-node,laboratory,Node01,Node02,k8s-vip\"\nNO_PROXY=$no_proxy\nHTTP_PROXY=$http_proxy\nHTTPS_PROXY=$https_proxy\n\nsource /etc/environment\n```\n\n\n修改 /etc/hosts文件内容\n\n```shell\nvim /etc/hosts\n......\n10.239.140.133 master-node\n10.239.131.156 laboratory\n10.239.141.123 node-1\n10.239.141.194 node-2\n10.239.140.51 k8s-vip\n```\n**`Note: `** 下面部署0.1.5版本的kube-vip时候 上面的/etc/hosts文件里的k8s-vip修改成如下内容\n\n```\n10.239.140.201 k8s-vip\n```\n\n## **kube-vip方式部署高可用k8s集群**\nofficial website:  \nhttps://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip  \nhttps://github.com/plunder-app/kube-vip/blob/master/kubernetes-control-plane.md  \n\n## **在三台master机器上添加kube-vip配置文件**\n\n### kube-vip 0.1.1本地安装config.yaml版本\n**master-node机器上**\n\n``` shell\nmkdir /etc/kube-vip\ntouch /etc/kube-vip/config.yaml\n```\n\nvim config.yaml\n\n``` xml\nlocalPeer:\n  id: master-node\t\t\t// 机器hostname, 通过$ hostnamectl set-hostname <HostName>修改\n  address: 10.239.140.133\t// 指定本地IP地址\n  port: 10000\nremotePeers:\n- id: laboratory\n  address: 10.239.131.156\t// 另外一台充当master机器IP地址\n  port: 10000\n- id: node-1\n  address: 10.239.141.123\t// 另外一台充当master机器IP地址\n  port: 10000\n# [...]\nvip: 10.239.140.51\t\t// 手动写的IP, 但必须在集群子网内, 部署集群后 $ ip addr 查看p8p1网卡下多出了此IP\ngratuitousARP: true\nsingleNode: false\nstartAsLeader: true\t\t\t// 设置作为三台master机器的leader\ninterface: p8p1\t\t\t\t// 用本机的网卡名字, $ ip addr可查看\nloadBalancers:\n- name: API Server Load Balancer\n  type: tcp\n  port: 6444\t\t\t\t// configure the load balancer to sit on the standard API-Server port 6443\n  bindToVip: true\n  backends:\n  - port: 6443\t\t// configure the backends to point to the API-servers that will be configured to run on port 6444\n    address: 10.239.140.133\n  - port: 6443\n    address: 10.239.131.156\n  - port: 6443\n    address: 10.239.141.123\n  # [...]\n```\n**laboratory机器上**\n\n```\ntouch /etc/kube-vip/config.yaml\n```\n\nvim touch /etc/kube-vip/config.yaml\n\n```xml\nlocalPeer:\n  id: laboratory\t\t\t// 改成本机的\n  address: 10.239.131.156\n  port: 10000\nremotePeers:\n- id: master-node\n  address: 10.239.140.133\n  port: 10000\n- id: node-1\n  address: 10.239.141.123\n  port: 10000\n# [...]\nvip: 10.239.140.51\ngratuitousARP: true\nsingleNode: false\nstartAsLeader: false\t\t// 不要设置成为leader\ninterface: eno1\t\t\t\t// 改成本机的IP地址网卡名\nloadBalancers:\n- name: API Server Load Balancer\n  type: tcp\n  port: 6444\n  bindToVip: true\n  backends:\n  - port: 6443\n    address: 10.239.140.133\n  - port: 6443\n    address: 10.239.131.156\n  - port: 6443\n    address: 10.239.141.123\n  # [...]\n```\n\n**node-1机器上**\n\n```\ntouch /etc/kube-vip/config.yaml\n```\n\nvim touch /etc/kube-vip/config.yaml\n\n```xml\nlocalPeer:\n  id: node-1\t\t\t\t// 改成本机的\n  address: 10.239.141.123\n  port: 10000\nremotePeers:\n- id: master-node\n  address: 10.239.140.133\n  port: 10000\n- id: laboratory\n  address: 10.239.131.156\n  port: 10000\n# [...]\nvip: 10.239.140.51\ngratuitousARP: true\nsingleNode: false\nstartAsLeader: false\t\t// 不要设置成为leader\ninterface: enp0s3\t\t\t// 改成本机的IP地址网卡名\nloadBalancers:\n- name: API Server Load Balancer\n  type: tcp\n  port: 6444\n  bindToVip: true\n  backends:\n  - port: 6443\n    address: 10.239.140.133\n  - port: 6443\n    address: 10.239.131.156\n  - port: 6443\n    address: 10.239.141.123\n  # [...]\n```\n> Use 6443 for both the VIP and the API-Servers, in order to do this we need to specify that the api-server is bound to it's local IP. To do this we use the --apiserver-advertise-address flag as part of the init, this means that we can then bind the same port to the VIP and we wont have a port conflict.\n\n### **kube-vip 0.1.5本地安装config.yaml版本**\n**master-node机器上**\n\n```\ntouch /etc/kube-vip/config.yaml\n```\n\nvim touch /etc/kube-vip/config.yaml\n\n```xml\nlocalPeer:\n  id: master-node\n  address: 10.239.140.137\n  port: 10000\nremotePeers:\n- id: laboratory\n  address: 10.239.131.157\n  port: 10000\n- id: node01\n  address: 10.239.140.50\n  port: 10000\n# [...]\nvip: 10.239.140.201\ngratuitousARP: true\nsingleNode: false\nstartAsLeader: true\ninterface: p8p1\nloadBalancers:\n- name: API Server Load Balancer\n  type: tcp\n  port: 6444\n  bindToVip: true\n  backends:\n  - port: 6443\n    address: 10.239.140.137\n  - port: 6443\n    address: 10.239.131.157\n  - port: 6443\n    address: 10.239.140.50\n  # [...]\n```\n\n## **部署High Availability K8s集群**\n### **1. master-node机器上**\n现在master-node机器上配置好K8s集群, 然后再把其它两个master加进来就可以了.  \n\n#### **0.1.1版本:**\n\n```shell\ndocker run -it --rm plndr/kube-vip:0.1.1 /kube-vip sample manifest \\\n    | sed \"s|plndr/kube-vip:'|plndr/kube-vip:0.1.1'|\" \\\n    | sudo tee /etc/kubernetes/manifests/kube-vip.yaml\n```\n#### **0.1.5版本:**\n\n```shell\nsudo docker run -it --rm plndr/kube-vip:0.1.5 sample manifest | sudo tee /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n> Ensure that image: plndr/kube-vip:<x> is modified to point to a specific version (0.1.5 at the time of writing), refer to docker hub for details.  \n> Also ensure that the hostPath points to the correct kube-vip configuration, if it isn’t the above path.  \n\nvim /etc/kubernetes/manifests/kube-vip.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  creationTimestamp: null\n  name: kube-vip\n  namespace: kube-system\nspec:\n  containers:\n  - command:\n    - /kube-vip\n    - start\n    - -c\n    - /vip.yaml\n    image: 'plndr/kube-vip:0.1.1'\n    name: kube-vip\n    resources: {}\n    securityContext:\n      capabilities:\n        add:\n        - NET_ADMIN\n        - SYS_TIME\n    volumeMounts:\n    - mountPath: /vip.yaml\n      name: config\n  hostNetwork: true\n  volumes:\n  - hostPath:\n      path: /etc/kube-vip/config.yaml\t// 跟上面的conf.yaml文件路径对应\n    name: config\nstatus: {}\n```\n#### **执行部署K8s集群命令**\n\n##### (0.1.1版本)\n\n```shell\n\nkubeadm init --control-plane-endpoint \"10.239.140.51:6444\" --apiserver-bind-port 6443 --upload-certs --kubernetes-version \"v1.19.0\"\n\n# api-server 和 kube-vip公用6443端口, 需要把/etc/kube-vip/config里loadBalancers.port也改成6443:\nkubeadm init --control-plane-endpoint \"10.239.140.51:6443\" --apiserver-advertise-address 10.239.140.133 --apiserver-bind-port 6443 --upload-certs --kubernetes-version \"v1.19.0\"\n * --control-plane-endpoin: 指定设置的Virtual IP和端口.\n * --apiserver-advertise-address: 指定第一台宿主机IP, 当Virtual IP所用port端口与apiserver port端口设置成相同时需要此参数.\n * --apiserver-bind-port: 指定apiserver运行所在的port, 此处与Virutal IP(做load balancing)所运行的port相同都是6443\n * --upload-certs: kubeadm部署方式下能够让证书自动上传.\n```\n\n##### (0.1.5版本)\n\n```shell\nkubeadm init --control-plane-endpoint \"10.239.140.201:6444\" --apiserver-bind-port 6443 --upload-certs --kubernetes-version \"v1.19.0\"\n```\n * --control-plane-endpoint: 指定Virtual IP地址和port为6444\n * --apiserver-bind-port: 指定apiserver运行所在的port为6443\n这个 --upload-certs 标志用来将在所有控制平面实例之间的共享证书上传到集群.  \n当 --upload-certs 与 kubeadm init 一起使用时，主控制平面的证书被加密并上传到 kubeadm-certs 密钥中.  \n\n查看部署情况\n\n\n```shell\nkubectl get pods -A\n  NAMESPACE     NAME                                     READY   STATUS    RESTARTS   AGE\n  <...>\n  kube-system   kube-vip-controlplane01                  1/1     Running   0          10m\n```\n\n**查看网卡地址上是否多出了一个虚拟IP为:10.239.140.51**\n\n```shell\nip addr\n```\n### **2. laboratory和node-1机器上**\n> 先不要在路径/etc/kubernetes/manifests/添加 kube-vip.yaml 文件, this is due to some bizarre kubeadm/kubelet behaviour.  \n> 等laboratory和node-1机器都添加进master集群后再添加kube-vip.yaml, kubeadm会自动检测/etc/kubernetes/manifests/文件变化并部署pod.  \n直接运行如下命令添加进master集群.\n\n```shell\nkubeadm join 10.239.140.133:6443 --token <tkn> \\\n    --discovery-token-ca-cert-hash sha256:<hash> \\\n    --control-plane --certificate-key <key> \n```\n**配置k8s访问环境变量**  \n这样就能在laboratory和node-1机器上执行kubectl命令了.  \n第一种:\n\n```shell\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n```\n\n第二种:\n\n```shell\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n```\n在master-node机器上查看laboratory和node-1机器已经加入master控制层面后, 再在laboratory和node-1机器上添加/etc/kubernetes/manifests/kube-vip.yaml文件.  \n**修改api-server访问地址为本机**\n这样某一台master机器挂了其它机器照样可以正常访问api-server.\n\n```shell\n# laboratory机器上\nvim /etc/kubernetes/admin.conf\n  server: https://10.239.131.156:6443\n# node-1机器上\nvim /etc/kubernetes/admin.conf\n  server: https://10.239.141.123:6443\n```\n\n#### **配置0.1.5(或者0.1.1)版本kube-vip.yaml**\n\n```shell\nsudo docker run -it --rm plndr/kube-vip:0.1.5 sample manifest | sudo tee /etc/kubernetes/manifests/kube-vip.yaml\n```\n\n### **3. master-node机器上**\n在master-node机器上运行查看pod运行情况.  \n\n```shell\nkubectl get pods -A | grep vip\n  kube-system   kube-vip-controlplane01                  1/1     Running             1          16m\n  kube-system   kube-vip-controlplane02                  1/1     Running             0          18m\n  kube-system   kube-vip-controlplane03                  1/1     Running             0          20m\n```\n\n查看 pod/kube-vip-master-node 运行日志\n\n```shell\nkubectl logs po/kube-vip-master-node -n kube-system\n  time=“2020-08-28T15:33:09Z” level=info msg=“The Node [10.239.140.133:10000] is leading”\n  time=“2020-08-28T15:33:09Z” level=info msg=“The Node [10.239.140.133:10000] is leading”\n```\n**部署CNI网络**\n\n```shell\nkubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n```\n\n### **4. node-2机器上**\n添加work node节点\n\n```shell\nsudo kubeadm join 10.239.140.133:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866\n```\n\n## **查看kube-vip, api-server服务进程和监听端口**\n\n```shell\nnetstat -nltp | grep 10000\t// 列出监听端口10000的进程\n  Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n  tcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip\n\nnetstat -nltp | grep 6443\t\t// 列出监听端口6443的进程\n  Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n  tcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve\n\nnetstat -antp\t\t// 列出所有tcp进程, State不仅包括Listen的, 还包括已建立链接状态为Established的进程.\n  Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n  tcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve\n  tcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip\n  tcp        0      0 10.239.140.133:10000    10.239.141.145:48554    ESTABLISHED 21353/kube-vip\n```\n\n`Local Address`可以看作是服务端IP和提供服务的监听端口, `Foreign Address`可以看作是客户端IP和发起链接请求的IP地址和请求端口.  \n`ESTABLISHED`表示客户端与服务端已经建立tcp长链接.  \n`LISTEN`表示服务端提供服务的端口仍处于监听状态, 等待客户端发起请求.  \nTCP才能在Foreign Address看到链接的客户端IP和端口, 而UDP无状态是没有的.  \n由以上输出可看到:\n * kube-vip服务进程编号为21353, 监听端口为10000, 所在本机IP为10.239.140.133\n * api-server服务进程编号为15700, 监听端口为6443\n\n查看所有链接本机6443服务端口的客户端IP地址, 地址一致的合并, 然后连接数从高到底排序.\n\n```shell\nnetstat -antp | grep :6443 | awk '{print $5}' | awk -F \":\" '{print $1}' | sort | uniq -c | sort -r -n\n      4 10.239.4.100\t// 表示从IP地址为10.239.4.100的客户端请求访问本机6443服务端口的进程数为4\n      3 10.239.4.80\n      3 10.239.141.194\n      3 10.239.141.145\n      3\n      2 10.40.0.6\n      2 10.239.140.53\n      2 10.239.140.133\n      2 10.109.19.69\n      1 10.40.0.9\n      1 10.40.0.2\n      1 10.40.0.1\n      1 10.109.19.68\n```\n\n## **去掉apiserver配置的proxy**\n部署完集群后在公司环境一定要去掉apiserver的proxy配置, 否则会遇到如下问题\n\n问题1: The connection to the server 10.239.140.200:6443 was refused - did you specify the right host or port?\n问题2: 执行systemctl status kubelet发现 类似如下错误\n\n```\nFailed to get status for pod \"kube-controller-manager-master-node_kube-system(185ec5bf52273f72fe5c4a72e3fbab62)\": Get \"https://10.239.140.200:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master-node\": dial tcp 10.239.140.200:6443: connect: connection refused\n```\n问题2： 执行kubectl get po -n kube-system 发现 controller-manager 和 scheduler 组件运行不正常\n解决方案如下就是登陆每台master注释掉如下内容\n\n```xml\n# 登陆每台master注释如下内容\nvim /etc/kubernetes/manifests/kube-apiserver.yaml\n  ......\n      #env:\n      #- name: NO_PROXY\n      #  value: node-1,laboratory,node-2,k8s-vip,127.0.0.1,10.239.140.200\n      #- name: http_proxy\n      #  value: http://child-prc.intel.com:913\n      #- name: HTTPS_PROXY\n      #  value: http://child-prc.intel.com:913\n      #- name: https_proxy\n      #  value: http://child-prc.intel.com:913\n      #- name: HTTP_PROXY\n      #  value: http://child-prc.intel.com:913\n  ......\n```\n\n**Note:**添加注释保存退出后apiserver, controller manager, scheduler组件会重启, 如果没有重启可以执行 `kubectl delete po/<组件名> -n kube-system` 删掉然后就发现重启了.\n\n\n## 查看并去掉node污点(taint)\n\n```shell\n# 查看node机器污点\nkubectl describe node/<Node-Name> | grep Taint\n  Taints:             node-role.kubernetes.io/master:NoSchedule\n\n# 去掉污点\nkubectl taint nodes <Node-Name> node-role.kubernetes.io/master:NoSchedule-\n# 去掉所有控制平面host污点\nkubectl taint nodes --all node-role.kubernetes.io/master-\n```\n\n## 网卡上添加删除虚拟网址\n网卡上增加一个IP\n\n```shell\nifconfig eth0:1 192.168.0.1 netmask 255.255.255.0\n```\n\n删除网卡的第二个IP地址\n```shell\nip addr del 192.168.0.1/32 dev eth0\n```\n上面IP后面加上 `/32` 否则会报 Warning: Executing wildcard deletion to stay compatible with old scripts.\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"istio dashboard","url":"/2021/05/31/microService/istio/istio_dashboard/","content":"\n## Kiali\n\nKiali 的 graphics 显示的 svc 和 pod 在一个方框里原因是 svc 的 label 和 pod label一致.  \nKiali 目前看是会将同一 namespace 下的 所有相同 label的资源框起来.  \n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"k8s cert manager","url":"/2021/05/31/microService/kubernetes/k8s_cert_manager/","content":"\nReference Link: https://cert-manager.io/docs/installation/kubernetes/\n\n## Installing with regular manifests\n\nAll resources (the CustomResourceDefinitions, cert-manager, namespace, and the webhook component) are included in a single YAML manifest file.  \n\n::: info\nNote: If you’re using a kubectl version below v1.19.0-rc.1 you will have issues updating the CRDs. For more info see the v0.16 upgrade notes\n:::\n\nInstall the CustomResourceDefinitions and cert-manager itself:\n``` shell\nkubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.3.1/cert-manager.crds.yaml\n```\n\n## Verifying the installation\n\n``` shell\nkubectl get pods --namespace cert-manager\n  NAME                                       READY   STATUS    RESTARTS   AGE\n  cert-manager-5c6866597-zw7kh               1/1     Running   0          2m\n  cert-manager-cainjector-577f6d9fd7-tr77l   1/1     Running   0          2m\n  cert-manager-webhook-787858fcdb-nlzsq      1/1     Running   0          2m\n```\nYou should see the cert-manager, cert-manager-cainjector, and cert-manager-webhook pod in a Running state. It may take a minute or so for the TLS assets required for the webhook to function to be provisioned. This may cause the webhook to take a while longer to start for the first time than other pods. If you experience problems, please check the [FAQ guide](https://cert-manager.io/docs/faq/)\n\n## Setting up CA Issuers\n\nThe following command will create a Secret containing a signing key pair in the default namespace.  \n\n``` shell\nkubectl create secret tls ca-key-pair \\\n   --cert=ca.crt \\\n   --key=ca.key \\\n   --namespace=cert-manager\n```\n::: info\nWhen referencing a [Secret]{.red} resource in [ClusterIssuer]{.red} resources (eg [apiKeySecretRef]{.red}) the [Secret]{.red} needs to be in the same namespace as the [cert-manager]{.red} controller pod.\n:::\n\nThe following steps will confirm that cert-manager is set up correctly and able to issue basic certificate types\n\nCreate a [ClusterIssuer]{.red} to test the webhook works okay\n\n``` shell\ncat <<EOF > test-resources.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: cert-manager-test\n---\n# You can specify a ClusterIssuer resource by changing the kind attribute of an Issuer to ClusterIssuer, and removing the metadata.namespace attribute\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: ca-issuer\nspec:\n  ca:\n    secretName: ca-key-pair\n---\napiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: selfsigned-cert\n  namespace: cert-manager-test\nspec:\n  dnsNames:\n    - example.com\n  secretName: selfsigned-cert-tls\n  issuerRef:\n    name: ca-issuer\n    # We can then reference a ClusterIssuer from a Certificate resource by setting the spec.issuerRef.kind field to ClusterIssuer\n    kind: ClusterIssuer\nEOF\n```\n\nCreate the test resources.\n\n``` shell\nkubectl apply -f test-resources.yaml\n```\n\nCheck the status of the newly created certificate. You may need to wait a few seconds before cert-manager processes the certificate request.\n\n``` shell\nkubectl get ClusterIssuer -o wide\n  NAME         READY   STATUS                AGE\n  ca-inssuer   True    Signing CA verified   2m44s\n\nkubectl describe certificate -n cert-manager-test\n  ......\n  Spec:\n    Dns Names:\n      example.com\n    Issuer Ref:\n      Kind:       ClusterIssuer\n      Name:       ca-issuer\n    Secret Name:  selfsigned-cert-tls\n  Status:\n    Conditions:\n      Last Transition Time:  2021-04-23T09:07:36Z\n      Message:               Certificate is up to date and has not expired\n      Observed Generation:   1\n      Reason:                Ready\n      Status:                True\n      Type:                  Ready\n    Not After:               2021-07-22T09:07:36Z\n    Not Before:              2021-04-23T09:07:36Z\n    Renewal Time:            2021-06-22T09:07:36Z\n    Revision:                1\n  Events:\n    Type    Reason     Age   From          Message\n    ----    ------     ----  ----          -------\n    Normal  Issuing    3m6s  cert-manager  Issuing certificate as Secret does not exist\n    Normal  Generated  3m6s  cert-manager  Stored new private key in temporary Secret resource \"selfsigned-cert-26qw7\"\n    Normal  Requested  3m6s  cert-manager  Created new CertificateRequest resource \"selfsigned-cert-ptpsb\"\n    Normal  Issuing    3m6s  cert-manager  The certificate has been successfully issued\n```\n\n## Clean up\n\nClean up the test resources.\n\n``` shell\nkubectl delete -f test-resources.yaml\nkubectl delete secret ca-key-pair -n cert-manager\n```\n\nIf all the above steps have completed without error, you are good to go!\n\n\n## Configuring your first Issuer\n\nBefore you can begin issuing certificates, you must configure at least one [Issuer]{.red} or [ClusterIssuer]{.red} resource in your cluster.\n\nYou should read the [configuration](https://cert-manager.io/docs/configuration/) guide to learn how to configure cert-manager to issue certificates from one of the supported backends.\n\n\n## Installing the kubectl plugin\n\ncert-manager also has a kubectl plugin which can be used to help you to manage cert-manager resources in the cluster. Installation instructions for this can be found in the [kubectl plugin](https://cert-manager.io/docs/usage/kubectl-plugin/) documentation.\n\n\n\n","tags":["k8s"],"categories":["microService","kubernetes"]},{"title":"affinity","url":"/2021/05/31/microService/kubernetes/affinity/","content":"\n## Attach a label to the node\n\nAdd a label to the node you've chosen\n``` shell\nkubectl label nodes <node-name> <label-key>=<label-value>\n```\n\nCheck the node now has a label you set.\n``` shell\nkubectl get nodes --show-labels\n```\n\n## nodeSelector\n\nTake whatever pod config file you want to run, and add a nodeSelector section to it.\n\ncat pod-nginx.yaml\n\n``` yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  namespace: test\n  name: nginx\n  labels:\n    env: test\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    imagePullPolicy: IfNotPresent\n  nodeSelector:\n    kubernetes.io/hostname: laboratory\n```\n\n## Affinity and anti-affinity\n\n### Node affinity\n\nNode affinity is conceptually similar to [nodeSelector]{.blue} -- it allows you to constrain which nodes your pod is eligible to be scheduled on, based on labels on the node.\n\nThere are currently two types of node affinity, called:  \n* requiredDuringSchedulingIgnoredDuringExecution \n* preferredDuringSchedulingIgnoredDuringExecution\n\nYou can think of them as [\"hard\"]{.blue} and [\"soft\"]{.blue} respectively, in the sense that the former specifies rules that must be met for a pod to be scheduled onto a node (similar to [nodeSelector]{.blue} but using a more expressive syntax), while the latter specifies preferences that the scheduler will try to enforce but will not guarantee. The [\"IgnoredDuringExecution\"]{.blue} part of the names means that, similar to how [nodeSelector]{.blue} works, if labels on a node change at runtime such that the affinity rules on a pod are no longer met, the pod continues to run on the node. In the future we plan to offer [requiredDuringSchedulingRequiredDuringExecution]{.blue} which will be identical to [requiredDuringSchedulingIgnoredDuringExecution]{.blue} except that it will evict pods from nodes that cease to satisfy the pods' node affinity requirements.\n\nThus an example of [requiredDuringSchedulingIgnoredDuringExecution]{.red} would be \"only run the pod on nodes with Intel CPUs\" and an example [preferredDuringSchedulingIgnoredDuringExecution]{.red} would be \"try to run this set of pods in failure zone XYZ, but if it's not possible, then allow some to run elsewhere\".\n\nNode affinity is specified as field [nodeAffinity]{.red} of field [affinity]{.red} in the PodSpec.\n\n``` yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: with-node-affinity\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/e2e-az-name\n            operator: In\n            values:\n            - e2e-az1\n            - e2e-az2\n      preferredDuringSchedulingIgnoredDuringExecution:\n      - weight: 1\n        preference:\n          matchExpressions:\n          - key: another-node-label-key\n            operator: In\n            values:\n            - another-node-label-value\n  containers:\n  - name: with-node-affinity\n    image: k8s.gcr.io/pause:2.0\n```\n\nThis node affinity rule says the pod can only be placed on a node with a label whose key is [kubernetes.io/e2e-az-name]{.red} and whose value is either [e2e-az1]{.red} or [e2e-az2]{.red}. In addition, among nodes that meet that criteria, nodes with a label whose key is [another-node-label-key]{.red} and whose value is [another-node-label-value]{.red} should be preferred.\n\nYou can see the operator In being used in the example. The new node affinity syntax supports the following operators: [In]{.red}, [NotIn]{.red}, [Exists]{.red}, [DoesNotExist]{.red}, [Gt]{.red}, [Lt]{.red}. You can use [NotIn]{.red} and [DoesNotExist]{.red} to achieve node anti-affinity behavior, or use node taints to repel pods from specific nodes.\n\n1. If you specify both [nodeSelector]{.red} and [nodeAffinity]{.red}, both must be satisfied for the pod to be scheduled onto a candidate node.\n\n2. If you specify multiple [nodeSelectorTerms]{.red} associated with [nodeAffinity]{.red} types, then the pod can be scheduled onto a node if one of the [nodeSelectorTerms]{.red} can be satisfied.\n\n3. If you specify multiple [matchExpressions]{.red} associated with [nodeSelectorTerms]{.red}, then the pod can be scheduled onto a node only if all [matchExpressions]{.red} is satisfied.\n\n4. If you remove or change the label of the node where the pod is scheduled, the pod won't be removed. In other words, the affinity selection works only at the time of scheduling the pod.\n\nThe [weight]{.red} field in [preferredDuringSchedulingIgnoredDuringExecution]{.red} is in the range [1-100]{.red}. For each node that meets all of the scheduling requirements (resource request, RequiredDuringScheduling affinity expressions, etc.), the scheduler will compute a sum by iterating through the elements of this field and adding \"weight\" to the sum if the node matches the corresponding MatchExpressions. This score is then combined with the scores of other priority functions for the node. The node(s) with the highest total score are the most preferred.\n\n\n### Pod affinity\n\nPod 间亲和性与反亲和性需要大量的处理，这可能会显著减慢大规模集群中的调度。 我们不建议在超过数百个节点的集群中使用它们.  \n\nInter-pod affinity and anti-affinity allow you to constrain which nodes your pod is eligible to be scheduled based on labels on pods that are already running on the node rather than based on labels on nodes. The rules are of the form \"this pod should (or, in the case of anti-affinity, should not) run in an X if that X is already running one or more pods that meet rule Y\". Y is expressed as a LabelSelector with an optional associated list of namespaces; unlike nodes, because pods are namespaced (and therefore the labels on pods are implicitly namespaced), a label selector over pod labels must specify which namespaces the selector should apply to. Conceptually X is a topology domain like node, rack, cloud provider zone, cloud provider region, etc. You express it using a topologyKey which is the key for the node label that the system uses to denote such a topology domain.  \n\nAs with node affinity, there are currently two types of pod affinity and anti-affinity, called requiredDuringSchedulingIgnoredDuringExecution and preferredDuringSchedulingIgnoredDuringExecution which denote \"hard\" vs. \"soft\" requirements. See the description in the node affinity section earlier. An example of requiredDuringSchedulingIgnoredDuringExecution affinity would be \"co-locate the pods of service A and service B in the same zone, since they communicate a lot with each other\" and an example preferredDuringSchedulingIgnoredDuringExecution anti-affinity would be \"spread the pods from this service across zones\" (a hard requirement wouldn't make sense, since you probably have more pods than zones).\n\ncat redis.yaml\n\n``` yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-cache\nspec:\n  selector:\n    matchLabels:\n      app: store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: redis-server\n        image: redis:3.2-alpine\n```\n\ncat web-server.yaml\n\n``` yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\nspec:\n  selector:\n    matchLabels:\n      app: web-store\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-store\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - web-store\n            topologyKey: \"kubernetes.io/hostname\"\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - store\n            topologyKey: \"kubernetes.io/hostname\"\n      containers:\n      - name: web-app\n        image: nginx:1.16-alpine\n```\n\n\n\n\n\n","tags":["k8s"],"categories":["microService","kubernetes"]},{"title":"删除crd,pod,pv,pvc等资源","url":"/2021/05/31/microService/kubernetes/delete_pod_pv_pvc等资源/","content":"\n## **pod,pv,pvc删除顺序**\n\n一般删除步骤为\n1. 先删pod\n2. 再删pvc\n3. 最后删pv\n\n如果遇到pv始终处于 `Terminating` 状态，而且 delete 不掉.  \n\n解决方法是直接删除k8s中相对应的pvc和pv的记录\n\n## 强制删除pod\n\n加参数 `--force --grace-period=0` , `grace-period` 表示过渡存活期，默认 `30s`，在删除 POD 之前允许 POD 慢慢终止其上的容器进程，从而优雅退出，`0` 表示立即终止 POD\n\n``` shell\nkubectl delete po <your-pod-name> -n <name-space> --force --grace-period=0\n```\n\n## 强制删除pvc和pv\n\n```shell\n1. 删除pvc\nkubectl describe pvc PVC_NAME -n NameSpace| grep Finalizers\n  Finalizers:    [kubernetes.io/pvc-protection]\n\nkubectl patch pvc PVC_NAME -n NameSpace -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n\n2. 删除pv\nkubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}'\n```\n\n## **强制删除crd**\n\n```shell\n# remove the CRD finalizer blocking on custom resource cleanup\nkubectl patch crd/greenplumclusters.greenplum.pivotal.io -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n  customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co patched\n\n# 再次删除资源 \nkubectl delete crd greenplumclusters.greenplum.pivotal.io\n# OK搞定了~\n```\n\n## **删除某个namespace**\n在k8s中会无法删除某个 namespace，它会一直卡在 `terminating` 状态.  \n这个指令找到阻碍这个 namespace 删除的资源，然后手动删除这些资源.  \n\n```shell\nkubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <namespace> \n```\n\n\n\n\n\n\n","tags":["istio"],"categories":["microService","kubernetes"]},{"title":"k8s操作命令","url":"/2021/05/31/microService/kubernetes/k8s_commands/","content":"\n## 获取所有某种资源\n\n``` shell\nkubectl get peerauthentication --all-namespaces\nkubectl get pa -A\n```\n\n<!-- more -->\n\n\n","tags":["k8s"],"categories":["microService","kubernetes"]},{"title":"k8s部署遇到的问题","url":"/2021/05/31/microService/kubernetes/k8s_encounter_problem/","content":"\n## 如果机器配置不行导致运行命令很卡, 很容易造成各种问题, 请酌情判断\n因为pod 里有配置Readiness probe和Liveness probe, 如果网速不行或机器很卡很容易造成 HTTP probe failed with statuscode: 500等错误.\n\n\n## **(仍需确认)问题01 failed to get cgroup stats for \"/system.slice/kubelet.service\"**\n\n通过`kubectl get po -n kube-system` 查看api-server启动异常一直重启\n通过`systemctl status kubelet` 查看kubelet服务出现如下类似异常\n\n**failed to get cgroup stats for \"/system.slice/kubelet.service\"**\n**解决方法:** https://github.com/kubernetes/kubernetes/issues/56850\n参考最新的：\n\n```shell\n# ubuntu\ncat << EOF | sudo tee /etc/systemd/system/kubelet.service.d/12-after-docker.conf\n[Unit]\nAfter=docker.service\nEOF\n\n# centos\ncat << EOF | sudo tee /usr/lib/systemd/system/kubelet.service.d/12-after-docker.conf\n[Unit]\nAfter=docker.service\nEOF\n\nsystemctl daemon-reload && systemctl restart kubelet // restart kubelet, after adding this file\n```\n\n实际重启kubelet后仍然会报同样的异常, 没有管, 当时也没观察api-server是否正常启动, 过一段时间后发现api-server正常启动.\n\n## **修改kube-apiserver文件后发现某台master的kube-apiserver组件异常**\n\n```shell\nkubectl logs po/kube-apiserver-master-node -n kube-system\n  Error from server: Get \"https://10.239.140.137:10250/containerLogs/kube-system/kube-apiserver-master-node/kube-apiserver\": dial tcp 10.239.140.137:10250: connect: connection refused\n```\n\n登陆master-node机器查看\n\n```shell\nnetstat -lutpn | grep 6443\n  tcp6       0      0 :::6443                 :::*                    LISTEN      14983/kube-apiserve\n\nkubectl logs po/kube-apiserver-master-node -n kube-system\n  Flag --insecure-port has been deprecated, This flag will be removed in a future version.\n  I1202 04:13:33.729232       1 server.go:625] external host was not specified, using 10.239.140.137\n  I1202 04:13:33.729387       1 server.go:163] Version: v1.19.0\n  Error: failed to create listener: failed to listen on 0.0.0.0:6443: listen tcp 0.0.0.0:6443: bind: address already in use\n```\n解决方法:\nReference Link: https://stackoverflow.com/questions/48734524/kubernetes-api-server-and-controller-manager-cant-start\n\n```shell\nnetstat -lutpn | grep 6443\n  tcp6       0      0 :::6443                 :::*                    LISTEN      11395/some-service\n\nkill 11395\n\nservice kubelet restart\n```\n实际操作中还删除了kube-apiserver pod, deployment会再重新创建\n\n```shell\nkubectl delete po/kube-apiserver-master-node -n kube-system\n```\n\n## **强制删除**\n\n一般删除步骤为：先删 pod 再删 pvc 最后删 pv.\n\n### **POD强制删除**\n\n```shell\nkubectl -n <namespace> delete po <podName> --grace-period=0 --force\n```\n\n### **pv/pvc强制删除**\n\n```shell\nkubectl patch pv opspv -p '{\"metadata\":{\"finalizers\":null}}'\nkubectl patch pvc opspvc  -p '{\"metadata\":{\"finalizers\":null}}' -n kube-ops\n```\n\n## **Node机器重启后重新加入K8s集群**\n\n### **实际观察到如果IP没变, 重启的Master节点等待一段时间会自动加入集群**\n但重启的Master节点机器需要保证如下配置重启后也都生效, 否则可以再执行一遍, **实际操作中也是重新执行了一遍**\n\n```shell\nsystemctl stop kubelet\nsystemctl stop docker\nsystemctl restart kubelet\nsystemctl restart docker\nswapoff -a\nsetenforce 0\nsystemctl stop firewalld.service\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables = 1\n\nvim /etc/sysctl.d/k8s.conf\n  net.bridge.bridge-nf-call-ip6tables = 1\n  net.bridge.bridge-nf-call-iptables = 1\nsysctl --system\n\n```\n\n### **重新加入Node机器**\n重启后如果Node机器未自动加入集群, 运行如下命令重新加入\n\n现在重启的Node机器上执行如下命令reset\n\n```shell\nrm -rf /etc/kubernetes/pki/etcd/\nrm -rf /var/lib/etcd\nrm -rf $HOME/.kube\nkubeadm reset\t\t// 出现有什么没有清理干净的可以手动删除掉, 如cni等,再reset, 如果还出现,可以忽略掉没有清理干净的信息提示, 执行kubeadm init.\n\n# 清理iptables规则\n(optional)yum install ipvsadm -y  # apt-get install ipvsadm -y\niptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\nipvsadm -C\nipvsadm --clear\n\n# 再次设置环境\nsystemctl stop kubelet\nsystemctl restart kubelet\nsystemctl daemon-reload\nsystemctl stop docker\nsystemctl restart docker\nswapoff -a\n# Edit /etc/fstab to comment out swap partition line so that it remains disabled after reboot\nsetenforce 0\nsystemctl stop firewalld.service\nsystemctl disable firewalld\n\n# Ubuntu 关闭防火墙\nsudo apt-get install ufw \n# inactive状态是防火墙关闭状态 active是开启状态\nsudo ufw status\n  Status: active\n# 关闭防火墙\nsudo ufw disable\n# 开启防火墙\nsudo ufw enable\n\nsysctl net.bridge.bridge-nf-call-iptables=1\nsysctl net.bridge.bridge-nf-call-ip6tables = 1\n\nvim /etc/sysctl.d/k8s.conf\n  net.bridge.bridge-nf-call-ip6tables = 1\n  net.bridge.bridge-nf-call-iptables = 1\nsysctl --system\n\n```\n\n再在master 机器上执行如下命令获取加入token\n\n```shell\nkubeadm token create --print-join-command\n  W1202 13:34:46.942799   17329 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]\n  kubeadm join 10.239.140.201:6443 --token 3vm6e8.wjlspdqpjau62riz     --discovery-token-ca-cert-hash sha256:99f1f55a10e439883030b810be5d3d364d12c508765984cc9ab633db6dbfada9\n```\n然后将获取到的token在重启的Node机器上执行\n\n```shell\nkubeadm join 10.239.140.201:6443 --token 3vm6e8.wjlspdqpjau62riz     --discovery-token-ca-cert-hash sha256:99f1f55a10e439883030b810be5d3d364d12c508765984cc9ab633db6dbfada9\n```\n\n### **重新加入master机器**\n\n最好把iptables也清理一下 `iptables -F`\n\n去到现有的master节点上生成token\n\n``` shell\n#生成token\nkubeadm  token create --print-join-command\n  kubeadm join 10.239.140.201:6443 --token 9ks5ps.g0fhcxbzl604k8v0     --discovery-token-ca-cert-hash sha256:2d291498e3c0739c53f33b85c4498fc7ef2ab362926e970671159b4f392d43dc\n\n#生成key\nkubeadm init phase upload-certs --upload-certs\n  W0805 14:41:18.070434   16460 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n  W0805 14:41:18.070565   16460 version.go:102] falling back to the local client version: v1.16.2\n  [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace\n  [upload-certs] Using certificate key:\n  6314a9877893263374fdf33bedf9a225640a97215784c8c8f387549966d0565d\n```\n\n拿到上述内容之后，拼接；前面的token加上-control-plane --certificate-key ,在要加入的master机器节点上运行，加入集群。\n\n```shell\nkubeadm join 172.31.17.49:9443 --token kjjguy.pmqxvb1nmgf1nq4q     --discovery-token-ca-cert-hash sha256:dcadd5b87024c304e5e396ba06d60a4dbf36509a627a6a949c126172e9c61cfb --control-plane --certificate-key 6314a9877893263374fdf33bedf9a225640a97215784c8c8f387549966d0565d\n```\n\n## **机器重启IP改变重新加入集群**\n机器重启后重新加入集群\n\n;;;id0 (不推荐)第一种:\n * 修改/etc/hosts里的机器IP\n * 修改/etc/systemd/system/docker.service.d/no-proxy.conf或proxy.conf里的机器IP\n * 修改~/.bashrc里的export NO_PROXY=***对应的机器IP\n * 修改/etc/kubernetes/manifests/kube-apiserver.yaml的机器IP\n * 修改/etc/kubernetes/manifests/etcd.yaml的机器IP\n ......\n以上可见非常麻烦，因此推荐如下方法\n;;;\n\n;;;id0 第二种: 重新reset机器再加入集群\n参考本文章上面方法\n;;;\n\n## weave运行出现错误\n\n执行如下命令安装weave组件后发现 weave pod一直运行不起来\n``` shell\n# 安装weave插件\nkubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\nkubectl get po -n kube-system\n  ......\n  weave-net-d77s8                             1/2     CrashLoopBackOff    13        42m\n  ......\n# 查看weave版本\nkubectl describe po/weave-net-d77s8 -n kube-system\n  ......\n  docker.io/weaveworks/weave-kube:2.8.1\n  ......\n\n# 查看weave日志\nkubectl logs po/weave-net-d77s8 -n kube-system -c weave\n  ......\n  FATA: 2018/03/15 15:38:37.870839 Existing bridge type \"bridge\" is different than requested \"bridged_fastdp\". Please do 'weave reset' and try again\n  ......\n```\n\n解决方法: 参考链接\nhttps://github.com/weaveworks/weave/issues/3259  \nhttps://gist.github.com/carlosedp/5040f4a1b2c97c1fa260a3409b5f14f9  \n\n``` shell\n# 登陆到出问题的weave所在的机器上执行如下操作\nsudo curl -L git.io/weave -o /usr/local/bin/weave\nsudo chmod a+x /usr/local/bin/weave\n\n# 确定weave文件里的weave版本与k8s部署里的weave版本一致, 否则要修改一致\nvim /use/local/bin/weave\n  ......\n  6 SCRIPT_VERSION=\"2.8.1\"\n  ......\n\n# 执行如下命令 remove the bridge\nsudo weave reset --force\n\n(方法1:实测)# 重新登陆master机器重新部署weave插件\nkubectl delete -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\nkubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')\"\n\n(方法2) # 重新部署出问题的weave pod\nkubectl get pod weave-net-d77s8 -n kube-system -o yaml | kubectl replace --force -f -\n```\n\n## api-server端口冲突\n\nmaster或node机器重启后查看到 ‘kube-apiserver’ 或其它组件一直处于重启状态\n``` shell\nkubectl get po -n kube-system\n  ......\n  kube-apiserver-hci-node02            0/1     CrashLoopBackOff   132        3d\n  ......\nkubectl describe po/kube-apiserver-hci-node02 -n kube-system\n  ......\n  <6443端口已经占用的意思>\n  ......\n```\n解决方法:\n\n``` shell\n# 修改apiserver的yaml文件, 把所有6443改成6444或其它host机器没有占用的端口号.\nvim /etc/kubernetes/manifests/kube-apiserver.yaml\n  ......\n  metadata:\n    annotations:\n      kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.67.109.147:6444\n  ......\n  - --secure-port=6444\n  ......\n  port: 6444\n  ......\n```\n\n## kubeadm join出错\n\n### proxy 导致\n在其它node上执行kubeadm join命令时出错，信息如下:\n``` shell\n$ kubeadm join 10.67.108.220:6444 --token 29at7m.ocoa403n14l36cr2     --discovery-token-ca-cert-hash sha256:315189819e85a9c8854fdc8e015f07e3003cbdb798f4279c9647fd26b798591c     --control-plane --certificate-key 142d532570f8660461f4f475a97853b0bc6f530875262e868a50b8c549c1f157 --v=5\n  I0602 13:49:57.536302    8780 token.go:215] [discovery] Failed to request cluster-info, will try again: Get \"https://10.67.108.220:6444/api/v1/namespaces/kube-public/configmaps/cluster-info?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n```\n解决方法是去掉proxy\n``` shell\nunset http_proxy\nunset https_proxy\nunset HTTP_PROXY\nunset HTTPS_PROXY\n```\n\n### node重启boot文件没有mount\n在其它node上执行kubeadm join命令时出错，信息如下:\n``` shell\n$ kubeadm join 10.67.108.220:6444 --token 29at7m.ocoa403n14l36cr2     --discovery-token-ca-cert-hash sha256:315189819e85a9c8854fdc8e015f07e3003cbdb798f4279c9647fd26b798591c     --control-plane --certificate-key 142d532570f8660461f4f475a97853b0bc6f530875262e868a50b8c549c1f157 --v=5\n[preflight] Running pre-flight checks\n        [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/\n[preflight] The system verification failed. Printing the output from the verification:\nKERNEL_VERSION: 3.10.0-1062.el7.x86_64\nDOCKER_VERSION: 19.03.14\nDOCKER_GRAPH_DRIVER: overlay2\nOS: Linux\nCGROUPS_CPU: enabled\nCGROUPS_CPUACCT: enabled\nCGROUPS_CPUSET: enabled\nCGROUPS_DEVICES: enabled\nCGROUPS_FREEZER: enabled\nCGROUPS_MEMORY: enabled\nCGROUPS_HUGETLB: enabled\nCGROUPS_PIDS: enabled\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR SystemVerification]: failed to parse kernel config: unable to load kernel module: \"configs\", output: \"modprobe: FATAL: Module configs not found.\\n\", err: exit status 1\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n```\n解决方法\n``` shell\n# 查看其它正常node的mount信息\n$ lsblk\nnvme0n1         259:0    0   477G  0 disk\n├─nvme0n1p1     259:1    0   200M  0 part /boot/efi\n├─nvme0n1p2     259:2    0     1G  0 part /boot\n└─nvme0n1p3     259:3    0 475.8G  0 part\n  ├─centos-root 253:0    0 271.8G  0 lvm  /\n  ├─centos-swap 253:1    0     4G  0 lvm\n  └─centos-home 253:2    0   200G  0 lvm  /home\n\n# 登陆有问题的node查看mount信息\n$ lsblk\nnvme0n1         259:0    0   477G  0 disk\n├─nvme0n1p1     259:1    0   200M  0 part\n├─nvme0n1p2     259:2    0     1G  0 part\n└─nvme0n1p3     259:3    0 475.8G  0 part\n  ├─centos-root 253:0    0   250G  0 lvm  /\n  ├─centos-swap 253:1    0     4G  0 lvm\n  └─centos-home 253:2    0 221.8G  0 lvm  /home\n$ mount /dev/nvme0n1p1 /boot/efi\n$ mount /dev/nvme0n1p2 /boot\n\n# 再次执行kubeadm join成功\n```\n\n## k8s repo包问题\n\n遇到问题如下:\n\n``` shell\n 1. Contact the upstream for the repository and get them to fix the problem.\n\n 2. Reconfigure the baseurl/etc. for the repository, to point to a working\n    upstream. This is most often useful if you are using a newer\n    distribution release than is supported by the repository (and the\n    packages for the previous distribution release still work).\n\n 3. Run the command with the repository temporarily disabled\n        yum --disablerepo=kubernetes ...\n\n 4. Disable the repository permanently, so yum won't use it by default. Yum\n    will then just ignore the repository until you permanently enable it\n    again or use --enablerepo for temporary usage:\n\n        yum-config-manager --disable kubernetes\n    or\n        subscription-manager repos --disable=kubernetes\n\n 5. Configure the failing repository to be skipped, if it is unavailable.\n    Note that yum will try to contact the repo. when it runs most commands,\n    so will have to try and fail each time (and thus. yum will be be much\n    slower). If it is a very temporary problem though, this is often a nice\n    compromise:\n\n        yum-config-manager --save --setopt=kubernetes.skip_if_unavailable=true\n```\n解决方法:\n``` txt\nYou can get it to work by setting `repo_gpgcheck=0` in `/etc/yum.repos.d/kubernetes.repo` but that is obviously not a real solution.\n```\n\n## kubelet报node找不到\n\n......\nApr 02 17:51:15 inspire-dev-nodea kubelet[68113]: E0402 17:51:15.481463   68113 kubelet.go:2183] node \"inspire-dev-nodea\" not found\n......\n\n``` shell\nsystemctl status kubelet -l\njournalctl -xefu kubelet\n\n```\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"jekins","url":"/2021/05/31/microService/cicd/jekins/","content":"\n## jekins\n\n\n\n\n\n\n\n\n\n\n","tags":["cicd"],"categories":["microService","cicd"]},{"title":"ssh端口映射","url":"/2021/05/31/linux/ssh端口映射/","content":"\n## ssh命令简介\n可以将远端服务器一个端口remote_port绑定到本地端口port，其中-C是进行数据压缩，-f是后台操作，只有当提示用户名密码的时候才转向前台。-N是不执行远端命令，在只是端口转发时这条命令很有用处。-g 是允许远端主机连接本地转发端口。-R表明是将远端主机端口映射到本地端口。如果是-L，则是将本地端口映射到远端主机端口。\nssh的三个强大的端口转发命令：\n转发到远端：ssh -C -f -N -g -L 本地端口:目标IP:目标端口 用户名@目标IP\n转发到本地：ssh -C -f -N -g –R 本地端口:目标IP:目标端口 用户名@目标IP\n\n```shell\nssh -C -f -N -g -D listen_port user@Tunnel_Host\n```\n* -C: 压缩数据传输。\n* -f: 后台认证用户/密码，通常和-N连用，不用登录到远程主机。\n* -N: 不执行脚本或命令，通常与-f连用。\n* -g: 在-L/-R/-D参数中，允许远程主机连接到建立的转发的端口，如果不加这个参数，只允许本地主机建立连接。\n* -L: localport:remotehost:remotehostport sshserver(本地机器端口:目标机器IP:目标机器端口 中转机器IP)\n\n## **本地端口转发**\n将访问本机的80端口访问转发到192.168.1.1的8080端口\n```shell\nssh -C -f -N -g -L 80:192.168.1.1:8080  user@192.168.1.1\n```\n\n如下图，假如host3和host1、host2都同互相通信，但是host1和host2之间不能通信，如何从host1连接上host2？\n\n对于实现ssh连接来说，实现方式很简单，从host1 ssh到host3，再ssh到host2，也就是将host3作为跳板的方式。但是如果不是ssh，而是http的80端口呢？如何让host1能访问host2的80端口？\n![](local_port_transmit.png)\nssh支持本地端口转发，语法格式为：\n\n```shell\nssh -L [local_bind_addr:]local_port:remote:remote_port middle_host\n```\n以上图为例，实现方式是在host1上执行:\n\n```shell\nssh -g -L 2222:host2:80 host3\n```\n\n其中**\"-L\"**选项表示本地端口转发，其工作方式为：在本地指定一个由ssh监听的转发端口(2222)，将远程主机的端口(host2:80)映射为本地端口(2222)，当有主机连接本地映射端口(2222)时，本地ssh就将此端口的数据包转发给中间主机(host3)，然后host3再与远程主机的端口(host2:80)通信。\n现在就可以通过访问host1的2222端口来达到访问host2:80的目的了。例如：\n![](local_port_transmit1.png)\n再来解释下\"-g\"选项，指定该选项表示允许外界主机连接本地转发端口(2222)，如果不指定\"-g\"，则host4将无法通过访问host1:2222达到访问host2:80的目的。甚至，host1自身也不能使用172.16.10.5:2222，**而只能使用localhost:2222或127.0.0.1:2222这样的方式达到访问host2:80的目的，**之所以如此，是因为本地转发端口默认绑定在回环地址上。可以使用bind_addr来改变转发端口的绑定地址，例如：  \n\n```shell\nssh -L 172.16.10.5:2222:host2:80 host3\n```\n\n这样，**host1自身就能通过访问172.16.10.5:2222的方式达到访问host2:80的目的。**  \n一般来说，使用转发端口，都建议同时使用\"-g\"选项，否则将只有自身能访问转发端口.  \n\n### **再来分析下转发端口通信的过程**\n![](port_transmit_protocol.png)\n当host4发起172.16.10.5:2222的连接时(即步骤①)，数据包的目标地址和端口为\"172.16.10.5:2222\"。由于host1上ssh已经监听了2222端口，并且知道该端口映射自哪台主机哪个端口，所以将会把该数据包目标地址和端口替换为\"172.16.10.3:80\"，并将此数据包通过转发给host3。当host3收到该数据包时，发现是host1转发过来请求访问host2:80的数据包，所以host3将代为访问host2的80端口。  \n\n所以，**host1和host3之间的通信方式是SSH协议**，这段连接是安全加密的，因此称为\"安全隧道\"，**而host3和host2之间通信协议则是HTTP而不是ssh。**\n\n### **现在再来考虑下，通过本地端口转发的方式如何实现ssh跳板的功能呢？仍以上图为例**\n\n```shell\nssh -g -L 22333:host2:22 host3]\n```\n这样只需使用ssh连上host1的22333端口就等于连接了host2的22端口。\n\n### **最后，关于端口转发有一个需要注意的问题：ssh命令中带有要执行的命令。考虑了下面的三条在host1上执行的命令的区别。**\n\n```shell\nssh -g -L 22333:host2:22 host3\nssh -g -L 22333:host2:22 host3 \"ifconfig\"\nssh -g -L 22333:host2:22 host3 \"sleep 10\"\n```\n\n第一条命令开启了本地端口转发，且是以登录到host3的方式开启的，所以执行完该命令后，将跳到host3主机上，当退出host3时，端口转发功能将被关闭。另外，host1上之所以要开启端口转发，目的是为了与host2进行通信，而不是跳到host3上，所以应该在ssh命令行上加上\"-f\"选项让ssh在本机host1上以后台方式提供端口转发功能，而不是跳到host3上来提供端口转发功能。  \n\n第二条命令在开启本地转发的时候还指定了要在host3上执行\"ifconfig\"命令，但是ssh的工作机制是远程命令执行完毕的那一刻，ssh关闭连接，所以此命令开启的本地端口转发功能有效期只有执行ifconfig命令的一瞬间。  \n\n第三条命令和第二条命令类似，只不过指定的是睡眠10秒命令，所以此命令开启的本地转发功能有效期只有10秒。  \n\n结合上面的分析，开启端口转发功能时，建议让ssh以后台方式提供端口转发功能，且明确指示不要执行任何ssh命令行上的远程命令。即最佳开启方式为：  \n\n```shell\nssh -f -N -g -L 22333:host2:22 host3\n```\n\n## **远程端口转发**\n将访问192.168.1.1的8080访问转发到本机的80端口\n```shell\nssh -C -f -N -g -R 80:192.168.1.1:8080 user@192.168.1.1\n```\n请求开启的转发端口是在远程主机上的，所以称为\"远程端口转发\"\n\n\n## K8s:\n查看Server机上k8s部署的Pod\n\n```shell\nkubectl get po -n rook-ceph\n  NAME                               READY   STATUS      RESTARTS   AGE     IP              NODE         NOMINATED NODE   READINESS GATES\n  rook-ceph-mgr-a-778576cbbc-mxvcp   1/1     Running     0          4h24m   10.36.0.11      hci-node04   <none>           <none>            app=rook-ceph-mgr,ceph_daemon_id=a,instance=a,mgr=a,pod-template-hash=778576cbbc,rook_cluster=rook-ceph\n\nkubectl get svc -n rook-ceph\n  NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE     LABELS\n  rook-ceph-mgr-dashboard    ClusterIP   10.108.115.137   <none>        8443/TCP            4h25m   app=rook-ceph-mgr,rook_cluster=rook-ceph\n```\n将server机器的上部署的pod端口或svc的ClusterIP改为NodePort后的端口映射到自己的开发机上，并用浏览器访问两种方法:\n第一种: 也可以修改svc的ClusterIP类型为NodePort，这样就不用再运行下面的Pod端口映射到Node上了\n\n```shell\nkubectl edit svc -n rook-ceph rook-ceph-mgr-dashboard\n  spec:\n    clusterIP: 10.108.115.137\n    ......\n    type: NodePort\t// 把ClusterIP改为NodePort\n\nkubectl get svc -n rook-ceph\n  NAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE\n  rook-ceph-mgr-dashboard    NodePort    10.101.80.177    <none>        8443:31180/TCP      46m\n```\n在server机上用curl访问看看\n\n```shell\ncurl -v https://127.0.0.1:31180 --noproxy \"*\" -k\t//不通过机器配置的proxy访问\n```\n可以看到映射到server的31180端口, 因此端口映射改为31180而不是8443，因为8443是svc的端口\n\n```shell\nssh -L MyLabIP:31180:10.67.108.211:31180 root@10.67.108.211\n```\n\n第二种: Server机上将Pod端口映射到Node上\n\n```shell\nkubectl port-forward rook-ceph-mgr-a-778576cbbc-mxvcp -n rook-ceph 8443:8443\n```\nServer机上访问\n\n```shell\ncurl -v https://127.0.0.1:8443 --noproxy \"*\" -k\t// 访问不经过设置的公司Proxy, 直接用局域网访问\n```\n\n在开发机上设置端口转发\n```shell\n在自己开发机上运行下面command，将访问自己开发机的8443端口流量转发到server机器10.67.108.211的8443端口\n浏览器或curl访问路径为 https://10.67.108.211:8443\nssh -L 8443:10.67.108.211:8443 root@10.67.108.211\n\n(管用!!!自己开发机上运行)运行下面command后浏览器或curl访问路径为 https://127.0.0.1:8443\nssh -L 8443:127.0.0.1:8443 root@10.67.108.211\t\t// 将访问本机8443(第一个)端口的路由转发到10.67.108.211机器8443端口\nssh -L MyLabIP:8443:127.0.0.1:8443 root@10.67.108.211\t//如果不在本机运行上面命令需要加上要转发8443端口的主机IP\n浏览器输入https://127.0.0.1:31440\n```\n\n## istio dashboard端口转发\n### 如在master机器A上运行istio dashboard\n\n```shell\nistioctl dashboard kiali\nhttp://localhost:38610/kiali\n```\n查看kiali服务监听端口\n\n```shell\nnetstat -nltp | grep 38610\n  tcp        0      0 127.0.0.1:38610         0.0.0.0:*               LISTEN      303611/istioctl\n  tcp6       0      0 ::1:38610               :::*                    LISTEN      303611/istioctl\n```\n\n### 在可打开浏览器的机器B上运行如下端口转发命令\n\n```shell\nssh -fL 38610:127.0.0.1:38610 root@<master-IP>\n  root@<master-IP>'s password:  //输入密码登陆即可\n```\n\n然后再打开机器B终端, 运行终端打开浏览器命令如下, 或直接在浏览器输入http://127.0.0.1:38610\n\n```shell\nfirefox\n```\n\n## windows端口转发\n\n``` shell\n# cmd打开windows bash窗口, 输入端口流量转发命令.\n# 这里的remote机器和middle_host机器是同一台.\nssh -L [local_bind_addr:]local_port:remote:remote_port middle_host\nssh -L 20001:127.0.0.1:20001 root@10.67.117.192\n# 打开windows上的google或chrome浏览器访问如下地址就可访问remote机器20001端口提供的服务\nhttp://127.0.0.1:20001\n```\n\n\n\n\n","tags":["SSH-port"],"categories":["linux"]},{"title":"sftp transfer files","url":"/2021/05/31/linux/sftp_transfer_files/","content":"\nwindows自带fstp, 可以打开 `powershell` or `cmd` 输入 `sftp` 查看命令提示.\n\n[FTP]{.red} traffic is not encrypted. For a secure data transfer, use [SCP]{.red} or [SFTP]{.red} \n\n[SFTP (SSH File Transfer Protocol)]{.red} is a secure file protocol that is used to access, manage, and transfer files over an encrypted SSH transport.\n\nWhen compared with the traditional [FTP protocol]{.red}, SFTP offers all the functionality of FTP, but it is more secure and easier to configure.\n\nUnlike [SCP]{.red} , which supports only file transfers, the SFTP allows you to perform a range of operations on remote files and resume file transfers.\n\nIn this tutorial, we will show you how to use the Linux [`sftp`]{.red} command\n\n## Before you Begin\n\nTo open an SFTP connection to a remote system, use the sftp command followed by the remote server username and the IP address or domain name:\n\nWhen transferring large files, it is recommended to run the sftp command inside a [screen](https://linuxize.com/post/how-to-use-linux-screen/) or [tmux](https://linuxize.com/post/getting-started-with-tmux/) session.\n\n++ [The directory from where you run the sftp command is the local working directory.]{.red} ++{.wavy}\n\n在windows上打开 `powershell` or `cmd` 先 `cd` 到指定目录在执行 `sftp 命令`, 如 `cd \"C:\\Users\\User\\OneDrive - Intel Corporation\\Desktop\\Tasks\\DiamondPark\\images_dockerfile_yaml\\alpha\"`\n\n![](01.JPG)\n\n## Establishing an SFTP connection\n\n[SFTP]{.red} works on a [client-server model]{.red}. It is a subsystem of SSH and supports all SSH authentication mechanisms.\n\nTo open an SFTP connection to a remote system, use the sftp command followed by the remote server username and the IP address or domain name:\n\n``` shell\nsftp remote_username@server_ip_or_hostname\n```\nIf you are connecting to the host using password authentication, you will be prompted to enter the user password.\n\nOnce connected, you will be presented with the sftp prompt, and you can start interacting with the remote server:\n\n``` shell\nConnected to remote_username@server_ip_or_hostname.\nsftp>\n```\n\nIf the remote SSH server is not listening on the [++default port 22++{.wavy}]{.red} , use the [++-P++{.wavy}]{.red} option to specify the SFTP port\n\n``` shell\nsftp -P custom_port remote_username@server_ip_or_hostname\n```\n\n## SFTP Commands\n\nMost of the SFTP commands are similar or identical to the Linux shell commands.\n\nTo get a list of all available SFTP commands, type [help]{.red}, or [?]{.red}.\n\n``` shell\nsftp> help\n```\n\n## Downloading Files with the SFTP Command\n\nTo download a single file from the remote server, use the get command:\n``` shell\nsftp> get filename.zip\n```\n\nIf you want to save the downloaded file with a different name, specify the new name as the second argument:\n``` shell\nsftp> get filename.zip local_filename.zip\n```\n\nTo download a directory from the remote system, use the recursive -r option:\n``` shell\nsftp> get -r remote_directory\n```\n\nIf a file transfer fails or is interrupted, you can resume it using the [reget]{.red} command.\nThe syntax of [reget]{.red} is the same as the syntax of [get]{.red}:\n``` shell\nsftp> reget filename.zip\n```\n\n## Uploading Files with the SFTP Command\n\nTo upload a file from the local machine to the remote SFTP server, use the [put]{.red} command\n``` shell\nsftp> put filename.zip\n```\n::: info\nIf the file you want to upload is not located in your current working directory, use the absolute path to the file.\n:::\n\nTo upload a local directory, you would type:\n``` shell\nsftp> put -r locale_directory\n```\n\nTo resume an interrupted upload:\n``` shell\nsftp> reput filename.zip\n```\n\n## File Manipulations with SFTP\n\nSFTP allows you to perform some basic file manipulation commands. Below are some examples of how to use the SFTP shell:\n\n1. Get information about the remote system’s [disk usage](https://linuxize.com/post/how-to-check-disk-space-in-linux-using-the-df-command/) :\n``` shell\nsftp> df\n        Size         Used        Avail       (root)    %Capacity\n    20616252      1548776     18002580     19067476           7%\n```\n\n2. Create a new directory on the remote server:\n``` shell\nsftp> mkdir directory_name\n```\n\n3. Rename a file on the remote server:\n``` shell\nsftp> rename file_name new_file_name\n```\n\n4. Delete a file on the remote server:\n``` shell\nsftp> rm file_name\n```\n\n5. Delete a directory on the remote server:\n``` shell\nsftp> rmdir directory_name\n```\n\n6. Change the permissions of a file on the remote system:\n``` shell\nsftp> chmod 644 file_name\n```\n\n7. Change the owner of a file on the remote system:\n``` shell\nsftp> chown user_id file_name\n```\nYou must supply the user ID to the [chown](https://linuxize.com/post/linux-chown-command/) and [chgrp](https://linuxize.com/post/chgrp-command-in-linux/) commands.\n\n8. Change the group owner of a remote file with:\n``` shell\nsftp> chgrp group_id file_name\n```\n\nOnce you are done with your work, close the connection by typing [`bye`]{.red} or [`quit`]{.red}.\n\n","tags":["k8s"],"categories":["linux"]},{"title":"磁盘分区fdisk, parted, mount操作","url":"/2021/05/31/linux/磁盘分区_mount/","content":"\n## **查看分区**\n查看磁盘格式类型, UUID, 挂载点\n\n```shell\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ├─sda2          xfs               47f36b35-c3fd-4374-86d6-0e56bb72eb02   /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02\n  ├─sda3          xfs               fea5b28a-8ac4-4736-9487-42517fd242de   /var/lib/kubelet/pods/e447c269-bbfd-4d59-9e01-432f14b75b26/volumes/kubernetes.io~local-volume/local-pv-3\n  ├─sda4          xfs               516c6d3c-c639-4092-82b6-0a82d09edff3   /var/lib/kubelet/pods/794961eb-3eb7-4cc2-b252-8d39d496b298/volumes/kubernetes.io~local-volume/local-pv-b\n  └─sda5          xfs               a7b1ca2d-5d08-411e-83e5-9fff59d554be   /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be\n  sdb             LVM2_member       OvpWBO-5AmD-7b4H-Plsn-twvB-k2oC-A5Aaca\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n  \n  sdc             LVM2_member       3x1Vwv-POeH-igqu-yuRn-ex2l-oDp3-9FaFPi\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n  \n  nvme0n1\n  ├─nvme0n1p1     vfat              3E80-8EF9                              /boot/efi\n  ├─nvme0n1p2     xfs               d4e9a380-5394-47ff-8682-9f3ec689060c   /boot\n  └─nvme0n1p3     LVM2_member       2JVUdy-83cp-vH1n-LLAV-vMwZ-twiw-bhBNDH\n    ├─centos-root xfs               51083aeb-a167-4fe9-aced-fa14c2a18953   /\n    ├─centos-swap swap              ebcd1018-d259-4d2c-bfd0-46abdc143201\n    └─centos-home xfs               ada9d310-ffee-4b08-9350-3e222614c11b   /home\n```\n\n查看磁盘大小等信息\n\n```shell\nlsblk\n  NAME                                    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n  sda                                       8:0    0   1.8T  0 disk\n  ├─sda1                                    8:1    0    10G  0 part /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f902c4d0\n  ├─sda2                                    8:2    0    10G  0 part /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02\n  ├─sda3                                    8:3    0    30G  0 part /var/lib/kubelet/pods/e447c269-bbfd-4d59-9e01-432f14b75b26/volumes/kubernetes.io~local-volume/local-pv-3f8c242c\n  ├─sda4                                    8:4    0   200G  0 part /var/lib/kubelet/pods/794961eb-3eb7-4cc2-b252-8d39d496b298/volumes/kubernetes.io~local-volume/local-pv-bcf24291\n  └─sda5                                    8:5    0   500G  0 part /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be\n  sdb                                       8:16   0   1.8T  0 disk\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n                                          253:3    0   1.8T  0 lvm\n  sdc                                       8:32   0   1.8T  0 disk\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n                                          253:4    0   1.8T  0 lvm\n  nvme0n1                                 259:0    0   477G  0 disk\n  ├─nvme0n1p1                             259:1    0   200M  0 part /boot/efi\n  ├─nvme0n1p2                             259:2    0     1G  0 part /boot\n  └─nvme0n1p3                             259:3    0 475.8G  0 part\n    ├─centos-root                         253:0    0    50G  0 lvm  /\n    ├─centos-swap                         253:1    0     4G  0 lvm\n    └─centos-home                         253:2    0 421.8G  0 lvm  /home\n```\n\n## fdisk\n\n### 分区\n\n```shell\nfdisk /dev/sda\n  WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.\n  Welcome to fdisk (util-linux 2.23.2).\n  \n  Changes will remain in memory only, until you decide to write them.\n  Be careful before using the write command.\n  \n  Command (m for help): `p`\n  \n  Disk /dev/sda: 1920.4 GB, 1920383410176 bytes, 3750748848 sectors\n  Units = sectors of 1 * 512 = 512 bytes\n  Sector size (logical/physical): 512 bytes / 4096 bytes\n  I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n  Disk label type: gpt\n  Disk identifier: 6427BFAE-2D0B-4DBB-91E7-80D20C8A1DA7\n  \n  #         Start          End    Size  Type            Name\n   1         4096     20971519     10G  Microsoft basic primary\n   2     20971520     41943039     10G  Microsoft basic primary\n   3     41943040    104857599     30G  Microsoft basic primary\n   4    104857600    524287999    200G  Microsoft basic primary\n   5    524288000   1572863999    500G  Microsoft basic primary\n  \n  Command (m for help): `n`\n  Partition number (6-128, default 6):\n  First sector (34-3750748814, default 1572864000):\n  Last sector, +sectors or +size{K,M,G,T,P} (1572864000-3750748814, default 3750748814): `+10G`\n  Created partition 6\n  \n  Command (m for help): `p`\n  \n  Disk /dev/sda: 1920.4 GB, 1920383410176 bytes, 3750748848 sectors\n  Units = sectors of 1 * 512 = 512 bytes\n  Sector size (logical/physical): 512 bytes / 4096 bytes\n  I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n  Disk label type: gpt\n  Disk identifier: 6427BFAE-2D0B-4DBB-91E7-80D20C8A1DA7\n  \n  #         Start          End    Size  Type            Name\n   1         4096     20971519     10G  Microsoft basic primary\n   2     20971520     41943039     10G  Microsoft basic primary\n   3     41943040    104857599     30G  Microsoft basic primary\n   4    104857600    524287999    200G  Microsoft basic primary\n   5    524288000   1572863999    500G  Microsoft basic primary\n   6   1572864000   1593835519     10G  Linux filesyste\n  \n  Command (m for help): `w`\n  The partition table has been altered!\n  \n  Calling ioctl() to re-read partition table.\n  \n  WARNING: Re-reading the partition table failed with error 16: Device or resource busy.\n  The kernel still uses the old table. The new table will be used at\n  the next reboot or after you run partprobe(8) or kpartx(8)\n  Syncing disks.\n```\n\n### Kernel重新加载磁盘table\n查看磁盘文件\n```shell\nls /dev/sda*\n  /dev/sda  /dev/sda1  /dev/sda2  /dev/sda3  /dev/sda4  /dev/sda5 \t// 没有sda6磁盘\n```\n\n进行Kernel sync disks\n\n```shell\npartprobe /dev/sda\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ......\n  └─sda6\n  sdb             LVM2_member       OvpWBO-5AmD-7b4H-Plsn-twvB-k2oC-A5Aaca\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n  \n  sdc             LVM2_member       3x1Vwv-POeH-igqu-yuRn-ex2l-oDp3-9FaFPi\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n  ......\n```\n\n### 删除分区\n\n```shell\nfdisk /dev/sda\n  m \t\t// 查看命令\n  d \t\t// 删除分区\n  6 \t\t// 选择要删除的partition\n  w \t\t// 输入 w  保存，这个时候分区以及删除了\n```\n\n## **格式化分区**\n在设备上格式化成指定格式的文件系统；  centos 7以后的版本默认使用xfs格式； 也可以指定 ext3\\4格式\n\n```shell\nmkfs.ext4 /dev/sdb1\n  mke2fs 1.42.9 (28-Dec-2013)\n          Using EXT2FS Library version 1.42.9\n```\n * fs：指定建立文件系统时的参数；\n * -t<文件系统类型>：指定要建立何种文件系统；\n * -v：显示版本信息与详细的使用方法；\n * -V：显示简要的使用方法；\n * -c：在制做档案系统前，检查该partition是否有坏轨。\n\n\n格式为xfs,所以使用mkfs.xfs命令。**`如果已有其他文件系统创建在此分区，必须加上\"-f\"参数来覆盖它`**\n\n```shell\nmkfs.xfs -f  -i size=512 -l size=128m,lazy-count=1 -d agcount=64 /dev/xvda3\n```\n\n * -i size=512 : 默认的值是256KB，当内容小于这个值时，写到inode中，超过这个值时，写到block中。\n * -l size=128m :默认值的是10m，修改这个参数成128m，可以显著的提高xfs文件系统删除文件的速度，当然还有其它，如拷贝文件的速度。\n * -l lazy-count=1: 值可以是0或者1；默认值是0;在一些配置上显著提高性能；\n * -d agcount=4 : 默认值是根据容量自动设置的。可以设置成1/2/4/16等等，这个参数可以调节对CPU的占用率，值越小，占用率越低；因为我的硬盘为2T的大硬盘，所以设置64；\n**Use Case:**\n\n```shell\nmkfs.xfs -f /dev/sda6        格式化sda6磁盘\n  mkfs.xfs -f /dev/sda6\n  meta-data=/dev/sda6              isize=512    agcount=4, agsize=655360 blks\n           =                       sectsz=4096  attr=2, projid32bit=1\n           =                       crc=1        finobt=0, sparse=0\n  data     =                       bsize=4096   blocks=2621440, imaxpct=25\n           =                       sunit=0      swidth=0 blks\n  naming   =version 2              bsize=4096   ascii-ci=0 ftype=1\n  log      =internal log           bsize=4096   blocks=2560, version=2\n           =                       sectsz=4096  sunit=1 blks, lazy-count=1\n  realtime =none                   extsz=4096   blocks=0, rtextents=0\nlsblk\n  NAME                                    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n  sda                                       8:0    0   1.8T  0 disk\n  ├─sda1                                    8:1    0    10G  0 part /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f902c4d0\n  ......\n  └─sda6                                    8:6    0    10G  0 part\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ......\n  └─sda6          xfs               8c653ffc-4b28-47d1-9e7f-32f8d969757a\n```\n\n## **挂载分区**\n\n```shell\nmkdir /d1 \nmount /dev/sda6 /d1 \n```\n\n**设置开机自动挂载新建分区**\n\n```shell\nvim /etc/fstab\n  #\n  # /etc/fstab\n  # Created by anaconda on Fri Apr  3 16:29:28 2020\n  #\n  # Accessible filesystems, by reference, are maintained under '/dev/disk'\n  # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n  #\n  /dev/mapper/centos-root /                       xfs     defaults        0 0\n  /dev/mapper/centos-home /home                   xfs     defaults        0 0\n  # /dev/mapper/centos-swap swap                    swap    defaults        0 0\n  UUID=a48b360e-37b1-4ca3-b1b1-a1101429091f /mnt/minio_data         ext4    defaults        0 0\n  UUID=eadc654f-b75a-4d81-8e17-910031209006 /mnt/disks/eadc654f-b75a-4d81-8e17-910031209006 xfs defaults 0 2\n  UUID=47f36b35-c3fd-4374-86d6-0e56bb72eb02 /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02 xfs defaults 0 2\n  UUID=fea5b28a-8ac4-4736-9487-42517fd242de /mnt/disks/fea5b28a-8ac4-4736-9487-42517fd242de xfs defaults 0 2\n  UUID=516c6d3c-c639-4092-82b6-0a82d09edff3 /mnt/disks/516c6d3c-c639-4092-82b6-0a82d09edff3 xfs defaults 0 2\n  UUID=a7b1ca2d-5d08-411e-83e5-9fff59d554be /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be xfs defaults 0 2\n  UUID=8c653ffc-4b28-47d1-9e7f-32f8d969757a /mnt/disks/My-Directory xfs default 0 2\n```\n\n执行如下命令生效：\n\n```\nmount -a\n```\n\n/etc/fstab文件负责配置Linux开机时自动挂载的分区\n\n```\n第一列可以是实际分区名，也可以是实际分区的卷标（Lable）\n第二列是挂载点,挂载点必须为当前已经存在的目录\n第三列为此分区的文件系统类型\n第四列是挂载的选项，用于设置挂载的参数\n* auto: 系统自动挂载，fstab默认就是这个选项\n* defaults: rw, suid, dev, exec, auto, nouser, and async.\n* noauto 开机不自动挂载\n* nouser 只有超级用户可以挂载\n* ro 按只读权限挂载\n* rw 按可读可写权限挂载\n* user 任何用户都可以挂载\n请注意光驱和软驱只有在装有介质时才可以进行挂载，因此它是noauto\n第五列是dump备份设置,当其值设置为1时，将允许dump备份程序备份；设置为0时，忽略备份操作；\n第六列是fsck磁盘检查设置,其值是一个顺序。当其值为0时，永远不检查；而 / 根目录分区永远都为1。其它分区从2开始，数字越小越先检查，如果两个分区的数字相同，则同时检查。\n```\n\n## **卸载分区**\n\n```shell\numount /dev/sda6\n```\n\n## 查看磁盘挂载信息\n\nReference Link: https://www.shuzhiduo.com/A/1O5E3Ww4z7/\n\n### ext文件系统\n\n如果文件系统类型为ext3、ext4（ext2没有测试，这么老的文件系统也没有多少意义了），那么可以使用命令tune2fs查看文件系统最后一次挂载信息\n\n``` shell\ntune2fs -l /dev/sda8\n  tune2fs 1.42.9 (28-Dec-2013)\n  Filesystem volume name:   <none>\n  Last mounted on:          /export\n  Filesystem UUID:          7401e577-a745-4e4e-a9e8-20cbb88bc657\n  Filesystem magic number:  0xEF53\n  Filesystem revision #:    1 (dynamic)\n  Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize\n  Filesystem flags:         signed_directory_hash\n  Default mount options:    user_xattr acl\n  Filesystem state:         clean\n  Errors behavior:          Continue\n  Filesystem OS type:       Linux\n  Inode count:              3276800\n  Block count:              13107200\n  Reserved block count:     655360\n  Free blocks:              12855410\n  Free inodes:              3276789\n  First block:              0\n  Block size:               4096\n  Fragment size:            4096\n  Group descriptor size:    64\n  Reserved GDT blocks:      1024\n  Blocks per group:         32768\n  Fragments per group:      32768\n  Inodes per group:         8192\n  Inode blocks per group:   512\n  Flex block group size:    16\n  Filesystem created:       Thu Mar 18 14:01:14 2021\n  Last mount time:          Thu Mar 18 14:01:35 2021\n  Last write time:          Thu Mar 18 14:01:35 2021\n  Mount count:              1\n  Maximum mount count:      -1\n  Last checked:             Thu Mar 18 14:01:14 2021\n  Check interval:           0 (<none>)\n  Lifetime writes:          132 MB\n  Reserved blocks uid:      0 (user root)\n  Reserved blocks gid:      0 (group root)\n  First inode:              11\n  Inode size:               256\n  Required extra isize:     28\n  Desired extra isize:      28\n  Journal inode:            8\n  Default directory hash:   half_md4\n  Directory Hash Seed:      0de9f37f-4275-407a-b97d-a1e3c8c6d228\n  Journal backup:           inode blocks\n```\n\n### xfs文件系统\n\n如果文件类型是[xfs]{.red}的话，可以在日志[/var/log/messages]{.red} 中搜索[Mounting]{.red}关键字。当然也可以搜索[XFS]{.red}关键字.  \n\n``` shell\ngrep Mounting  /var/log/messages\ngrep -ri Mounting /var/log/* | more\ngrep XFS  /var/log/messages\n```\n\n如下面, 先查找到某个 **[sda* 磁盘]{.red}**, 然后下面距离它 [最近的挂载目录]{.red} 就是此磁盘得挂载目录.  \n\n``` shell\nlsblk -f\n  NAME        FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1      xfs               79ee7207-89e1-40d1-aae9-ac7418d88d34   /var/lib/kubelet/pods/4fc6ff4f-523f-4e3d-b3aa-3969c83\n  ├─sda2      xfs               ded32002-32a2-4a53-8fcf-d69d133c51b4   /var/lib/kubelet/pods/404c1d5a-4f0a-43ee-a03c-50e41f7\n  ├─sda3      xfs               b40dbd6b-1040-4fbe-bf54-9b42555543c3   /var/lib/kubelet/pods/6e210848-5dbe-4a86-8a13-1c96084\n  ├─sda4      xfs               cd5573f8-92ce-4584-aaae-9994d6b5df34   /mnt/disks/cd5573f8-92ce-4584-aaae-9994d6b5df34\n  ├─sda5      xfs               b5c2c5b5-a3ca-42c3-9bfb-3797590d038d   /var/lib/kubelet/pods/7f6840a4-2a09-47f7-ac33-25446da\n  ├─sda6      xfs               05102643-67d4-4674-ab6d-125430e1836e   /hdfs-data\n  ├─sda7\n  └─sda8      ext4              7401e577-a745-4e4e-a9e8-20cbb88bc657\nsdb\n└─sdb1      xfs               02c8c1b9-beec-42b8-99ab-d2e41dff07aa   /mnt/disks/02c8c1b9-beec-42b8-99ab-d2e41dff07aa\n\ngrep Mounting  /var/log/messages\n  ......\n  Apr 14 13:40:21 hci-node01 systemd: Mounting Configuration File System...\n  Apr 14 13:40:23 hci-node01 systemd: Mounting /sysroot...                                         // 挂载目录\n  Apr 14 13:40:23 hci-node01 kernel: XFS (dm-0): Mounting V5 Filesystem                            // 磁盘\n\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/02c8c1b9-beec-42b8-99ab-d2e41dff07aa...  // 挂载目录\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sdb1): Mounting V5 Filesystem                            // 磁盘\n\n  Apr 14 13:40:26 hci-node01 kernel: XFS (nvme0n1p2): Mounting V5 Filesystem                       // 磁盘\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /boot...                                            // 挂载目录\n\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda1): Mounting V5 Filesystem                            // sda1 磁盘\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda6): Mounting V5 Filesystem                            // sda6 磁盘\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/79ee7207-89e1-40d1-aae9-ac7418d88d34...  // sda1 挂载目录\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /hdfs-data...                                       // sda6 挂载目录\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/ded32002-32a2-4a53-8fcf-d69d133c51b4...  // sda2 挂载目录\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda2): Mounting V5 Filesystem                            // sda2 磁盘\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/b40dbd6b-1040-4fbe-bf54-9b42555543c3...  // sda3 挂载目录\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda3): Mounting V5 Filesystem                            // sda3 磁盘\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/b5c2c5b5-a3ca-42c3-9bfb-3797590d038d...  // sda5 挂载目录\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda5): Mounting V5 Filesystem                            // sda5 磁盘\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /mnt/disks/cd5573f8-92ce-4584-aaae-9994d6b5df34...\n  Apr 14 13:40:26 hci-node01 kernel: XFS (sda4): Mounting V5 Filesystem\n  Apr 14 13:40:26 hci-node01 systemd: Mounting /boot/efi...\n  Apr 14 13:40:30 hci-node01 systemd: Mounting /home...\n  Apr 14 13:40:30 hci-node01 kernel: XFS (dm-2): Mounting V5 Filesystem\n  Apr 14 13:40:30 hci-node01 systemd: Mounting RPC Pipe File System...\n```\n\n\n### nfs文件系统\n\n如果文件格式是nfs的话，可以用命令 cat /proc/self/mountstats，通过age来判断，这个表示nfs挂载后的时间，单位为秒.  \n\n\n\n## parted\n\n``` shell\nparted --script /dev/sdb \\\n    mklabel gpt \\\n    mkpart primary 4096s 51200MiB \\\n    mkpart primary 51200MiB 102400MiB  \\\n    mkpart primary 102400MiB 153600MiB \\\n    mkpart primary 153600MiB 204800MiB \\\n    mkpart primary 204800MiB 256000MiB \\\n    mkpart primary 256000MiB 307200MiB \\\n    mkpart primary 307200MiB 358400MiB \\\n    mkpart primary 358400MiB 870400MiB\n```\n\n\n\n","categories":["linux"]},{"title":"fdisk磁盘分区 mount操作","url":"/2021/05/31/linux/fdisk磁盘分区_mount/","content":"\n## **查看分区**\n查看磁盘格式类型, UUID, 挂载点\n\n```shell\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ├─sda2          xfs               47f36b35-c3fd-4374-86d6-0e56bb72eb02   /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02\n  ├─sda3          xfs               fea5b28a-8ac4-4736-9487-42517fd242de   /var/lib/kubelet/pods/e447c269-bbfd-4d59-9e01-432f14b75b26/volumes/kubernetes.io~local-volume/local-pv-3\n  ├─sda4          xfs               516c6d3c-c639-4092-82b6-0a82d09edff3   /var/lib/kubelet/pods/794961eb-3eb7-4cc2-b252-8d39d496b298/volumes/kubernetes.io~local-volume/local-pv-b\n  └─sda5          xfs               a7b1ca2d-5d08-411e-83e5-9fff59d554be   /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be\n  sdb             LVM2_member       OvpWBO-5AmD-7b4H-Plsn-twvB-k2oC-A5Aaca\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n  \n  sdc             LVM2_member       3x1Vwv-POeH-igqu-yuRn-ex2l-oDp3-9FaFPi\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n  \n  nvme0n1\n  ├─nvme0n1p1     vfat              3E80-8EF9                              /boot/efi\n  ├─nvme0n1p2     xfs               d4e9a380-5394-47ff-8682-9f3ec689060c   /boot\n  └─nvme0n1p3     LVM2_member       2JVUdy-83cp-vH1n-LLAV-vMwZ-twiw-bhBNDH\n    ├─centos-root xfs               51083aeb-a167-4fe9-aced-fa14c2a18953   /\n    ├─centos-swap swap              ebcd1018-d259-4d2c-bfd0-46abdc143201\n    └─centos-home xfs               ada9d310-ffee-4b08-9350-3e222614c11b   /home\n```\n\n查看磁盘大小等信息\n\n```shell\nlsblk\n  NAME                                    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n  sda                                       8:0    0   1.8T  0 disk\n  ├─sda1                                    8:1    0    10G  0 part /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f902c4d0\n  ├─sda2                                    8:2    0    10G  0 part /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02\n  ├─sda3                                    8:3    0    30G  0 part /var/lib/kubelet/pods/e447c269-bbfd-4d59-9e01-432f14b75b26/volumes/kubernetes.io~local-volume/local-pv-3f8c242c\n  ├─sda4                                    8:4    0   200G  0 part /var/lib/kubelet/pods/794961eb-3eb7-4cc2-b252-8d39d496b298/volumes/kubernetes.io~local-volume/local-pv-bcf24291\n  └─sda5                                    8:5    0   500G  0 part /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be\n  sdb                                       8:16   0   1.8T  0 disk\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n                                          253:3    0   1.8T  0 lvm\n  sdc                                       8:32   0   1.8T  0 disk\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n                                          253:4    0   1.8T  0 lvm\n  nvme0n1                                 259:0    0   477G  0 disk\n  ├─nvme0n1p1                             259:1    0   200M  0 part /boot/efi\n  ├─nvme0n1p2                             259:2    0     1G  0 part /boot\n  └─nvme0n1p3                             259:3    0 475.8G  0 part\n    ├─centos-root                         253:0    0    50G  0 lvm  /\n    ├─centos-swap                         253:1    0     4G  0 lvm\n    └─centos-home                         253:2    0 421.8G  0 lvm  /home\n```\n\n## **执行分区**\n\n```shell\nfdisk /dev/sda\n  WARNING: fdisk GPT support is currently new, and therefore in an experimental phase. Use at your own discretion.\n  Welcome to fdisk (util-linux 2.23.2).\n  \n  Changes will remain in memory only, until you decide to write them.\n  Be careful before using the write command.\n  \n  Command (m for help): `p`\n  \n  Disk /dev/sda: 1920.4 GB, 1920383410176 bytes, 3750748848 sectors\n  Units = sectors of 1 * 512 = 512 bytes\n  Sector size (logical/physical): 512 bytes / 4096 bytes\n  I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n  Disk label type: gpt\n  Disk identifier: 6427BFAE-2D0B-4DBB-91E7-80D20C8A1DA7\n  \n  #         Start          End    Size  Type            Name\n   1         4096     20971519     10G  Microsoft basic primary\n   2     20971520     41943039     10G  Microsoft basic primary\n   3     41943040    104857599     30G  Microsoft basic primary\n   4    104857600    524287999    200G  Microsoft basic primary\n   5    524288000   1572863999    500G  Microsoft basic primary\n  \n  Command (m for help): `n`\n  Partition number (6-128, default 6):\n  First sector (34-3750748814, default 1572864000):\n  Last sector, +sectors or +size{K,M,G,T,P} (1572864000-3750748814, default 3750748814): `+10G`\n  Created partition 6\n  \n  Command (m for help): `p`\n  \n  Disk /dev/sda: 1920.4 GB, 1920383410176 bytes, 3750748848 sectors\n  Units = sectors of 1 * 512 = 512 bytes\n  Sector size (logical/physical): 512 bytes / 4096 bytes\n  I/O size (minimum/optimal): 4096 bytes / 4096 bytes\n  Disk label type: gpt\n  Disk identifier: 6427BFAE-2D0B-4DBB-91E7-80D20C8A1DA7\n  \n  #         Start          End    Size  Type            Name\n   1         4096     20971519     10G  Microsoft basic primary\n   2     20971520     41943039     10G  Microsoft basic primary\n   3     41943040    104857599     30G  Microsoft basic primary\n   4    104857600    524287999    200G  Microsoft basic primary\n   5    524288000   1572863999    500G  Microsoft basic primary\n   6   1572864000   1593835519     10G  Linux filesyste\n  \n  Command (m for help): `w`\n  The partition table has been altered!\n  \n  Calling ioctl() to re-read partition table.\n  \n  WARNING: Re-reading the partition table failed with error 16: Device or resource busy.\n  The kernel still uses the old table. The new table will be used at\n  the next reboot or after you run partprobe(8) or kpartx(8)\n  Syncing disks.\n```\n\n## **不重启机器,Kernel重新加载磁盘table**\n查看磁盘文件\n```shell\nls /dev/sda*\n  /dev/sda  /dev/sda1  /dev/sda2  /dev/sda3  /dev/sda4  /dev/sda5 \t// 没有sda6磁盘\n```\n\n进行Kernel sync disks\n\n```shell\npartprobe /dev/sda\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ......\n  └─sda6\n  sdb             LVM2_member       OvpWBO-5AmD-7b4H-Plsn-twvB-k2oC-A5Aaca\n  └─ceph--7ee2d5aa--c451--44d1--8ff8--0d6394d5fb48-osd--data--9ca735f4--03c4--4f1d--89d8--bcd78778ce6c\n  \n  sdc             LVM2_member       3x1Vwv-POeH-igqu-yuRn-ex2l-oDp3-9FaFPi\n  └─ceph--c0d1ccc4--5130--4697--b9a2--e9c58cbc5f7b-osd--data--91d305f7--cfc5--4ca1--b787--35fefcc6f938\n  ......\n```\n\n## **删除分区**\n\n```shell\nfdisk /dev/sda\n  m \t\t// 查看命令\n  d \t\t// 删除分区\n  6 \t\t// 选择要删除的partition\n  w \t\t// 输入 w  保存，这个时候分区以及删除了\n```\n\n## **格式化分区**\n在设备上格式化成指定格式的文件系统；  centos 7以后的版本默认使用xfs格式； 也可以指定 ext3\\4格式\n\n```shell\nmkfs.ext4 /dev/sdb1\n  mke2fs 1.42.9 (28-Dec-2013)\n          Using EXT2FS Library version 1.42.9\n```\n * fs：指定建立文件系统时的参数；\n * -t<文件系统类型>：指定要建立何种文件系统；\n * -v：显示版本信息与详细的使用方法；\n * -V：显示简要的使用方法；\n * -c：在制做档案系统前，检查该partition是否有坏轨。\n\n\n格式为xfs,所以使用mkfs.xfs命令。**`如果已有其他文件系统创建在此分区，必须加上\"-f\"参数来覆盖它`**\n\n```shell\nmkfs.xfs -f  -i size=512 -l size=128m,lazy-count=1 -d agcount=64 /dev/xvda3\n```\n\n * -i size=512 : 默认的值是256KB，当内容小于这个值时，写到inode中，超过这个值时，写到block中。\n * -l size=128m :默认值的是10m，修改这个参数成128m，可以显著的提高xfs文件系统删除文件的速度，当然还有其它，如拷贝文件的速度。\n * -l lazy-count=1: 值可以是0或者1；默认值是0;在一些配置上显著提高性能；\n * -d agcount=4 : 默认值是根据容量自动设置的。可以设置成1/2/4/16等等，这个参数可以调节对CPU的占用率，值越小，占用率越低；因为我的硬盘为2T的大硬盘，所以设置64；\n**Use Case:**\n\n```shell\nmkfs.xfs -f /dev/sda6        格式化sda6磁盘\n  mkfs.xfs -f /dev/sda6\n  meta-data=/dev/sda6              isize=512    agcount=4, agsize=655360 blks\n           =                       sectsz=4096  attr=2, projid32bit=1\n           =                       crc=1        finobt=0, sparse=0\n  data     =                       bsize=4096   blocks=2621440, imaxpct=25\n           =                       sunit=0      swidth=0 blks\n  naming   =version 2              bsize=4096   ascii-ci=0 ftype=1\n  log      =internal log           bsize=4096   blocks=2560, version=2\n           =                       sectsz=4096  sunit=1 blks, lazy-count=1\n  realtime =none                   extsz=4096   blocks=0, rtextents=0\nlsblk\n  NAME                                    MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\n  sda                                       8:0    0   1.8T  0 disk\n  ├─sda1                                    8:1    0    10G  0 part /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f902c4d0\n  ......\n  └─sda6                                    8:6    0    10G  0 part\nlsblk -f\n  NAME            FSTYPE      LABEL UUID                                   MOUNTPOINT\n  sda\n  ├─sda1          xfs               eadc654f-b75a-4d81-8e17-910031209006   /var/lib/kubelet/pods/0ef567bf-3636-455b-a585-6a9d8ab1b2dd/volumes/kubernetes.io~local-volume/local-pv-f\n  ......\n  └─sda6          xfs               8c653ffc-4b28-47d1-9e7f-32f8d969757a\n```\n\n## **挂载分区**\n\n```shell\nmkdir /d1 \nmount /dev/sda6 /d1 \n```\n\n**设置开机自动挂载新建分区**\n\n```shell\nvim /etc/fstab\n  #\n  # /etc/fstab\n  # Created by anaconda on Fri Apr  3 16:29:28 2020\n  #\n  # Accessible filesystems, by reference, are maintained under '/dev/disk'\n  # See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info\n  #\n  /dev/mapper/centos-root /                       xfs     defaults        0 0\n  /dev/mapper/centos-home /home                   xfs     defaults        0 0\n  # /dev/mapper/centos-swap swap                    swap    defaults        0 0\n  UUID=a48b360e-37b1-4ca3-b1b1-a1101429091f /mnt/minio_data         ext4    defaults        0 0\n  UUID=eadc654f-b75a-4d81-8e17-910031209006 /mnt/disks/eadc654f-b75a-4d81-8e17-910031209006 xfs defaults 0 2\n  UUID=47f36b35-c3fd-4374-86d6-0e56bb72eb02 /mnt/disks/47f36b35-c3fd-4374-86d6-0e56bb72eb02 xfs defaults 0 2\n  UUID=fea5b28a-8ac4-4736-9487-42517fd242de /mnt/disks/fea5b28a-8ac4-4736-9487-42517fd242de xfs defaults 0 2\n  UUID=516c6d3c-c639-4092-82b6-0a82d09edff3 /mnt/disks/516c6d3c-c639-4092-82b6-0a82d09edff3 xfs defaults 0 2\n  UUID=a7b1ca2d-5d08-411e-83e5-9fff59d554be /mnt/disks/a7b1ca2d-5d08-411e-83e5-9fff59d554be xfs defaults 0 2\n  UUID=8c653ffc-4b28-47d1-9e7f-32f8d969757a /mnt/disks/My-Directory xfs default 0 2\n```\n\n/etc/fstab文件负责配置Linux开机时自动挂载的分区\n\n```\n第一列可以是实际分区名，也可以是实际分区的卷标（Lable）\n第二列是挂载点,挂载点必须为当前已经存在的目录\n第三列为此分区的文件系统类型\n第四列是挂载的选项，用于设置挂载的参数\n* auto: 系统自动挂载，fstab默认就是这个选项\n* defaults: rw, suid, dev, exec, auto, nouser, and async.\n* noauto 开机不自动挂载\n* nouser 只有超级用户可以挂载\n* ro 按只读权限挂载\n* rw 按可读可写权限挂载\n* user 任何用户都可以挂载\n请注意光驱和软驱只有在装有介质时才可以进行挂载，因此它是noauto\n第五列是dump备份设置,当其值设置为1时，将允许dump备份程序备份；设置为0时，忽略备份操作；\n第六列是fsck磁盘检查设置,其值是一个顺序。当其值为0时，永远不检查；而 / 根目录分区永远都为1。其它分区从2开始，数字越小越先检查，如果两个分区的数字相同，则同时检查。\n```\n\n## **卸载分区**\n\n```shell\numount /dev/sda6\n```\n\n\n\n","categories":["linux"]},{"title":"mstsc xrdp 远程连接linux桌面","url":"/2021/05/31/linux/mstsc_remote_desktop/","content":"\nXrdp中的 [rdp]{.blue} 可以理解为 [Remote Desktop]{.blue} 的缩写.  \n\n## Centos上操作\n\n1. First, install Gnome GUI on CentOS 7 / RHEL 7\n\n``` shell\n# Run the following command to list down the available package groups for CentOS 7.\nyum group list\n\n# Install Gnome GUI packages using the YUM command.\n# CentOS 7:\nyum groupinstall \"GNOME Desktop\" \"Graphical Administration Tools\"\n# RHEL 7:\nyum groupinstall \"Server with GUI\"\n\n# Enable GUI on system startup\nln -sf /lib/systemd/system/runlevel5.target /etc/systemd/system/default.target\n\n# Reboot the machine to start the server in the graphical mode.\nreboot\n\n# 登陆机器页面选择License Agreement\nAccept the license by clicking on the “LICENSE INFORMATION“.\nTick mark the “I accept the license agreement” and click on “Done“.\nClick on “FINISH CONFIGURATION” to complete the setup.\nYou may need to do some post configuration tasks, like creating first user (local account), language, etc.\nThen finally you will get the desktop.\nThat’s All. You have successfully installed GUI on CentOS 7 / RHEL 7.\n```\n\n2. Install and configure EPEL repository\n\n``` shell\nrpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm\n```\n\n3. Use YUM command to install xrdp package on CentOS 7 / RHEL 7.\n\n``` shell\nyum -y install xrdp tigervnc-server\n```\n\n4. start the xrdp service\n\n``` shell\nsystemctl start xrdp\n```\n\n5. xrdp should now be listening on 3389. You can confirm this by using netstat command.\n\n``` shell\nnetstat -antup | grep xrdp\n  tcp        0      0 0.0.0.0:3389            0.0.0.0:*               LISTEN      1508/xrdp\n  tcp        0      0 127.0.0.1:3350          0.0.0.0:*               LISTEN      1507/xrdp-sesman\n```\n\n6. Enable the xrdp service at system startup\n\n``` shell\nsystemctl enable xrdp\n```\n\n7. Configure the firewall to allow RDP connection from external machines\n\nThe following command will add the exception for RDP port (3389). \n\n``` shell\nfirewall-cmd --permanent --add-port=3389/tcp\nfirewall-cmd --reload\n```\n\n8. Configure SELinux\n\n``` shell\nchcon --type=bin_t /usr/sbin/xrdp\nchcon --type=bin_t /usr/sbin/xrdp-sesman\n```\n\n9. Now take RDP from any windows machine using Remote Desktop Connection. Enter the ip address of Linux server in the computer field and then click on connect\n\n![](01.JPG)\n\nYou may need to ignore the warning of RDP certificate name mismatch.\n\n![](02.jpg)\n\nYou would be asked to enter the username and password. You can either use root or any user that you have it on the system. Make sure you use module “Xvnc“.\n\n![](03.jpg)\n\nIf you click ok, you will see the processing. In less than a half minute, you will get a desktop.\n\n![](04.jpg)\n\nThat’s All. You have successfully configured xRDP on CentOS 7 / RHEL 7.\n\n\n## Ubuntu上操作\n\n### install\n\n1. Install Gnome:\n\n``` shell\nsudo apt update\nsudo apt install ubuntu-desktop\n```\n\n2. Install Xfce:\n\n``` shell\nsudo apt update\nsudo apt install xubuntu-desktop\n```\nDepending on your system, downloading and installing GUI packages will take some tim\n\n3. Installing Xrdp\n\nXrdp is incuded in the default Ubuntu repositories. To install it, run:\n\n``` shell\nsudo apt install xrdp \n```\n\nOnce the installation is complete, the Xrdp service will automatically start. You can verify it by typing:\n\n``` shell\nsudo systemctl status xrdp\n```\n\nThe output will look something like this:\n\n``` shell\n● xrdp.service - xrdp daemon\n     Loaded: loaded (/lib/systemd/system/xrdp.service; enabled; vendor preset: enabled)\n     Active: active (running) since Fri 2020-05-22 17:36:16 UTC; 4min 41s ago\n  ...\n```\n\nBy default Xrdp uses the /etc/ssl/private/ssl-cert-snakeoil.key file that is readable only by members of the “ssl-cert” group. Run the following command to add the xrdp user to the group :\n\n``` shell\nsudo adduser xrdp ssl-cert  \n```\n\nRestart the Xrdp service for changes to take effect:\n\n``` shell\nsudo systemctl restart xrdp\n```\n\nThat’s it. Xrdp has been installed on your Ubuntu server, and you can start using it.\n\n### Xrdp Configuration\n\nThe Xrdp configuration files are located in the `/etc/xrdp` directory. For basic Xrdp connections, you do not need to make any changes to the configuration files.\n\nXrdp uses the default X Window desktop environment (Gnome or XFCE).\n\nThe main configuration file is named `xrdp.ini` . This file is divided into sections and allows you to set global configuration settings such as security and listening addresses and create different xrdp login sessions.\n\nWhenever you make any changes to the configuration file, you need to restart the Xrdp service.\n\nXrdp uses `startwm.sh` file to launch the X session. If you want to use another X Window desktop, edit this file\n\n### Configuring Firewall \n\nThe Xrdp daemon listens on port `3389` on all interfaces. If you run a [firewall on your Ubuntu server](https://linuxize.com/post/how-to-setup-a-firewall-with-ufw-on-ubuntu-18-04/) , you’ll need to open the Xrdp port.\n\n``` shell\n# Install UFW\nsudo apt install ufw\n\n# Check UFW Status\nsudo ufw status verbose\n# UFW is disabled by default. If you never activated UFW before, the output will look like this:\n  Status: inactive\n# If UFW is activated, the output will look similar to the following:\n  Status: active\n```\n\nTo allow access to the Xrdp server from a specific IP address or IP range, for example, `192.168.33.0/24`, you would run the following command:\n\n``` shell\nsudo ufw allow from 192.168.33.0/24 to any port 3389\n```\n\nIf you want to allow access from anywhere (which is highly discouraged for security reasons), run:\n\n``` shell\nsudo ufw allow 3389\n```\n\nFor increased security, you may consider setting up Xrdp to listen only on localhost and creating an SSH tunnel that securely forwards traffic from your local machine on port 3389 to the server on the same port.\n\n### Connecting to the Xrdp Server\n\nNow that you have set up your Xrdp server, it is time to open your Xrdp client and connect to the server.\n\nIf you have a Windows PC, you can use the default RDP client. Type **[“remote”]{.red}** in the Windows search bar and click on **[“Remote Desktop Connection”].{.red}**. This will open up the RDP client. In the “Computer” field, enter the remote server IP address and click “Connect”.\n\nOn the login screen, enter your username and password and click “OK”.\n\nOnce logged in, you should see the default Gnome or Xfce desktop.\n\n\n","categories":["linux"]},{"title":"Install python3 & pip3 & pipenv","url":"/2021/05/31/language/python/linux_install_python_pip_pipenv/","content":"## 查看python库\n\n``` shell\npython -m site\t// python 2 版本\npython3 -m site\t// python 3 版本\n```\n\n## 安装python\n\n;;;id0 Centos\n``` shell\nyum install epel-release\nyum search python | grep python36\nyum install python36 postgresql postgresql-devel python-devel python3-devel -y\n\n# 安装uwsgi\nyum install openssl openssl-devel\npip install psycopg2==2.8.3 sqlparse==0.4.1 flask==1.1.1 uwsgi==2.0.18\n```\n;;;\n\n\n;;;id0 Ubuntu\nubuntu18.04+python3，这个系统是默认自带了python3，且版本是python 3.6.8\n该python 3.6.8中并没有对应的pip3，于是执行命令 sudo apt-get install python3-pip，即可成功安装pip3\n然后即可通过 pip3 install pipenv , 安装自己需要的pipenv模块\n;;;\n\n\n;;;id0 源码安装(非常不推荐，容易踩各种坑)\nInstall Python3\n\n``` shell\nyum install zlib-devel.x86_64\nyum -y install zlib1g-dev\nyum -y install libffi-devel\nyum install openssl-devel -y\n\nwget https://www.python.org/ftp/python/3.7.2/Python-3.7.2.tar.xz\ntar -xvJf  Python-3.7.2.tar.xz\nmkdir /usr/local/python3 \ncd Python-3.7.2\n./configure --prefix=/usr/local/python3 --enable-optimizations --with-ssl \n\n#第一个指定安装的路径,不指定的话,安装过程中可能软件所需要的文件复制到其他不同目录,删除软件很不方便,复制软件也不方便.\n#第二个可以提高python10%-20%代码运行速度.\n#第三个是为了安装pip需要用到ssl,后面报错会有提到.\nmake -j && make install\n```\n;;;\n\n## 创建软链接\n``` shell\nln -s /usr/local/python3/bin/python3 /usr/bin/python3\nln -s /usr/local/python3/bin/pip3 /usr/bin/pip3\n```\nPATH路径需要有/usr/local/bin, 重启客户端\n\n## 验证是否成功\n```shell\npython3 -V\npip3 -V\n```\n## 修改pip安装源\n```shell\ncd ~\nmkdir .pip\ncd .pip\nvim pip.conf\n// 进入后添加以下内容,保存退出.\n[global]\nindex-url = https://mirrors.aliyun.com/pypi/simple\n```\n\n## 安装pipenv\npython里如果多个多个项目同时引用包，就会涉及到包版本的问题，包不同版本管理的问题可以用虚拟环境来管理\n创建虚拟环境，这里是用官方推荐的pipenv来创建\n安装pipenv\n\n``` shell\n第一种: pip3 install pipenv (亲测可用)\n第二种: pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple pipenv 使用国内源安装pipenv\n$ pip3 list\t// 查看 pipnev是否安装完成\npipenv --version\t//报错如下:\n * -bash: pipenv: command not found\n// 原因：未建立软链接:\nln -s /usr/local/python3/bin/pipenv /usr/bin/pipenv\n```\n\n在使用pipenv之前，必须彻底的忘记pip这个东西\n新建一个准备当环境的文件夹pipenvtest，并cd进入该文件夹：\n确定python版本\n\n``` shell\n$ pipenv --three 会使用当前系统的Python3创建环境\n$ pipenv --python 3.6 指定某一Python版本创建环境\n// 然后该目录下会有一个Pipfile文件, 内容为:\n[[source]]\t\t\t\t// 安装包时下载的地址\nname = \"pypi\"\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\n\n[dev-packages]\t\t\t// 依赖包\n\n[packages]\t\t\t\t// 已将安装的安装包\n\n[requires]\t\t\t\t// 环境配置要求\npython_version = \"3.6\"\n```\n\n![](pipfile.JPG)\n\n * 激活并查看环境\n\n``` shell\npipenv shell 激活虚拟环境\npipenv --where 显示目录信息\npipenv --venv 显示虚拟环境信息\npipenv --py 显示Python解释器信息\n\n*. 安装第三方包\npipenv install [第三方库名] 安装相关模块并加入到Pipfile\npipenv install django==1.11 安装固定版本模块并加入到Pipfile\n\npipenv graph 查看目前安装的库及其依赖\n\n*. 卸载第三方包\npipenv uninstall [第三方库名]\npipenv uninstall --all  卸载当前环境下所有包\n\n*. 更新\npipenv  update  --更新当前环境下所有包，升级到最新版本\n\n*. 退出\nexit  推出虚拟环境\n\n*. 删除虚拟环境\npipenv --rm  删除虚拟环境\n```\n\n","categories":["language","python"]},{"title":"vnc 远程连接Centos桌面","url":"/2021/03/25/linux/vnc/","content":"\nReference:  \nhttps://blog.csdn.net/llag_haveboy/article/details/84960479  \nhttps://www.cnblogs.com/xiaodangshan/p/7446975.html  \n\n## VNC简介\n\nVNC (Virtual Network Console)，即 虚拟网络控制台。它是一款优秀的远程控制工具软件，而且是基于 UNIX 和 Linux 操作系统的免费开源的.  \n\n## VNC 服务端安装\n\n``` shell\n# 1. 检查服务器是否安装了 VNC\nrpm -qa | grep vnc\n\n# 2. 如果没有安装VNC，输入以下命令进行安装\nyum install tigervnc tigervnc-server -y\n\n# 3. 安装之后，编辑文件进行配置\nvim /etc/sysconfig/vncservers\n  # THIS FILE HAS BEEN REPLACED BY /lib/systemd/system/vncserver@.service\n  VNCSERVERS=\"1:root\"\n\n# 4. 运行并设置密码\nvncserver\n\n```\n\n:::info\n1.VNCSERVERS 配置登录远程桌面的用户名;\n2.VNC 的默认监听端口是 5900，监听端口规则为590+usernumber[如 1:root对应端口号5901]\n3.VNCSERVERARGS[1] 登录桌面配置；[1 为用户序号，1366x768 为分辨率，-nolisten tcp 为阻止tcp包，-nohttpd 为阻止http包，-localhost 代表只监听本地。如：VNCSERVERARGS[2]=\"-geometry 800x600 -nolisten tcp -nohttpd -localhost\"]\n:::\n\n## windows连接\n\n1. windows上打开MobaXterm, 选择上面Session.\n2. 选择VNC.\n3. 输入linux机器的IP或域名, Port默认时5900,不过根据上面往vncservers上填写的1,此处Port填写5901.\n4. 点击OK连接即可.\n\n","categories":["linux"]},{"title":"删除crd,pod,pv,pvc等资源","url":"/2021/03/21/linux/删除crd,pod,pv,pvc等资源/","content":"\n## **pod,pv,pvc删除顺序**\n\n一般删除步骤为：\n * 先删pod\n * 再删pvc\n * 最后删pv.  \n如果遇到pv始终处于 **`[Terminating]{.blue}`** 状态，而且delete不掉.  \n解决方法是直接删除k8s中相对应的pvc和pv的记录：\n\n## **强制删除pod**\n加参数 **`--force --grace-period=0`**, grace-period表示过渡存活期，默认30s，在删除POD之前允许POD慢慢终止其上的容器进程，从而优雅退出，0表示立即终止POD\n\n``` shell\nkubectl delete po <your-pod-name> -n <name-space> --force --grace-period=0\n```\n\n## **强制删除pvc和pv**\n\n```shell\n# 1. 删除pvc\nkubectl describe pvc PVC_NAME -n NameSpace| grep Finalizers\n  Finalizers:    [kubernetes.io/pvc-protection]\nkubectl patch pvc PVC_NAME -n NameSpace -p '{\"metadata\":{\"finalizers\": []}}' --type=merge\n\n# 2. 删除pv\nkubectl patch pv xxx -p '{\"metadata\":{\"finalizers\":null}}'\n```\n\n## **强制删除crd**\n\n```shell\n# remove the CRD finalizer blocking on custom resource cleanup\nkubectl patch crd/greenplumclusters.greenplum.pivotal.io -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n  customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co patched\n\n// 再次删除资源 \n$ kubectl delete crd greenplumclusters.greenplum.pivotal.io\nOK搞定了~\n```\n\n## **删除某个namespace**\n在k8s中会无法删除某个namespace，它会一直卡在terminating状态.  \n这个指令找到阻碍这个namespace删除的资源，然后手动删除这些资源.  \n\n```shell\nkubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n <namespace> \n```\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"KMS vault","url":"/2021/03/21/technologies/security/kms_vault/","content":"\n\n## Install vault on kubernetes\n在kubernetes上安装vault\n\nReference link:\n* https://learn.hashicorp.com/  \n* (consul与vault本机部署)https://learn.hashicorp.com/tutorials/vault/ha-with-consul?in=vault/operations  \n\n* https://www.vaultproject.io/docs/platform/k8s  \n* https://www.vaultproject.io/docs/platform/k8s/helm/run  \n\n\n## Install consul\n\nReference Link:\n* (部署步骤)https://github.com/hashicorp/consul-helm/tree/v0.30.0  \n* (参数设置)https://www.consul.io/docs/k8s/helm  \n* (详细文档)https://learn.hashicorp.com/tutorials/consul/service-mesh-deploy?in=consul/gs-consul-service-mesh  \n* (TLS,ACL- access control)https://www.consul.io/docs/k8s/installation/install  \n\n``` shell\nkubectl create namespace dev\nhelm repo add hashicorp https://helm.releases.hashicorp.com\nhelm search repo hashicorp/consul\n\n# Install Consul server cluster and Consul client agent by helm\nhelm install consul hashicorp/consul -f consul-values.yaml --namespace dev\n\n# Verify the Consul cluster\nkubectl exec -it consul-server-0 -n dev -- consul members\n  Node             Address          Status  Type    Build  Protocol  DC   Segment\n  consul-server-0  10.32.0.14:8301  alive   server  1.9.2  2         dc1  <all>\n  consul-server-1  10.36.0.7:8301   alive   server  1.9.2  2         dc1  <all>\n  laboratory       10.32.0.13:8301  alive   client  1.9.2  2         dc1  <default>\n  master           10.36.0.6:8301   alive   client  1.9.2  2         dc1  <default>\n```\n\n## Install vault\n\nReference Link:\nhttps://learn.hashicorp.com/tutorials/vault/kubernetes-raft-deployment-guide?in=vault/kubernetes\n\n``` shell\n# Setup Vault servers\nhelm install vault hashicorp/vault --namespace dev -f values.yaml\nkubectl get pods --selector='app.kubernetes.io/name=vault' --namespace='dev'\n\n# Initialize the vault and unseal servers\nkubectl exec -it vault-0 -n dev -- vault operator init\n\n# Unseal each server. After that, you need to unseal vault-1 and vault-2.\nkubectl exec -it vault-0 -n dev -- vault operator unseal <unseal_key_1>\nkubectl exec -it vault-0 -n dev -- vault operator unseal <unseal_key_2>\nkubectl exec -it vault-0 -n dev -- vault operator unseal <unseal_key_3>\n\n# Check the vault status and ensure that the servers are initialized and unsealed\nkubectl exec -it vault-0 -n dev -- vault status\n  Key             Value\n  ---             -----\n  Seal Type       shamir\n  Initialized     true\n  Sealed          false\n  Total Shares    5\n  Threshold       3\n  Version         1.6.2\n  Storage Type    consul\n  Cluster Name    vault-cluster-3df8e36a\n  Cluster ID      49c1449f-3cbb-470d-d183-6ccce8bfb738\n  HA Enabled      true\n  HA Cluster      https://vault-0.vault-internal:8201\n  HA Mode         active\n```\n\n\n\n\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"重启linux机器","url":"/2021/03/17/linux/重启linux机器/","content":"\n## reboot命令\n\nreboot命令是最简单的方法，也不需要额外的其他参数, 通常，你需要在命令前加上sudo以获得权限.\n``` shell\nsudo reboot\n```\n\n## shutdown命令\n\n * 立即重启\n``` shell\nsudo shutdown -r\n```\n\n * 延迟一段时间重启(分钟为单位)\n``` shell\nsudo shutdown -r +10\n```\n\n指定重启的具体时间\n``` shell\nsudo shutdown -r 16:15\n```\n\n * 取消之前指定的重启命令\n``` shell\nshutdown -c\n```\n\n## init命令\n\n重启的命令为：\n``` shell\ninit 6\n```\n\n也可以直接用于关闭系统：\n``` shell\ninit 0\n```\n\n\n","categories":["linux"]},{"title":"openssl update","url":"/2021/03/16/technologies/security/openssl_update/","content":"\n## openssl官网下载\n\n * 官方网站：https://www.openssl.org\n * 下载:https://www.openssl.org/source/\n * GitHub：https://github.com/openssl/openssl\n\nopenssl版本\n``` shell\nopenssl version\nopenssl version -a\n```\n\n<!-- more -->\n\n## Centos上更新Openssl\n\n``` shell\n# 查看centos版本\ncat /etc/redhat-release\n  CentOS Linux release 7.9.2009 (Core)\n\n# 查看openssl版本\nopenssl version\n  OpenSSL 1.0.2k-fips  26 Jan 2017\n\n# 删除旧版本包\nyum remove openssl -y\n\n# 安装依赖\n(原始操作)yum install gcc gcc-c++ autoconf automake zlib zlib-devel pcre-devel -y\n# 实际安装依赖过程中因为本机已经安装更新过gcc, 因此用了如下命令安装依赖\nyum install autoconf automake zlib zlib-devel pcre-devel -y\n\n# 下载包\nwget https://www.openssl.org/source/openssl-1.1.1j.tar.gz\ntar -zxf openssl-1.1.1j.tar.gz\n\n# 编译\ncd openssl-1.1.1j/\n./config --prefix=/usr/local/openssl shared zlib\nmake -j\nmake install\n\n# 创建软链接\nln -sv /usr/local/openssl/bin/openssl /usr/bin/openssl\nln -sv /usr/local/openssl/include/openssl /usr/include/openssl\n\n# 设置动态库地址\necho '/usr/local/openssl/lib' > /etc/ld.so.conf.d/openssl-ld.conf\nldconfig -v\n\n# 查看版本\nopenssl version\n  OpenSSL 1.1.1j  16 Feb 2021\n\n# 查看某个证书信息\nopenssl x509 -in public.crt -text -noout\n\n```\n升级完成， 最好重启下服务器.\n\n\n## debian9上更新Openssl\n\n``` shell\n# 当前版本\n# openssl version\nOpenSSL 1.0.2k-fips  26 Jan 2017\n\n# 删除旧版本包\napt-get --purge remove openssl\n\n# 安装依赖\napt-get update\napt-get install build-essential checkinstall zlib1g-dev -y\n\n# 下载包\nwget https://www.openssl.org/source/openssl-1.1.1d.tar.gz\ntar -zxf openssl-1.1.1d.tar.gz \n\n# 编译\ncd ./openssl-1.1.1d/\n./config --prefix=/usr/local/openssl shared zlib\nmake\nmake install\n\n# 创建软链接\nln -sv /usr/local/openssl/bin/openssl /usr/bin/openssl\nln -sv /usr/local/openssl/include/openssl /usr/include/openssl\n\n# 设置动态库地址\necho '/usr/local/openssl/lib' > /etc/ld.so.conf.d/openssl-ld.conf\nldconfig -v\n\n# 查看版本\nopenssl version\nOpenSSL 1.1.1d  10 Sep 2019\n```\n\n升级完成， 最好重启下服务器。","tags":["security"],"categories":["technologies","security"]},{"title":"05 Deploy MinIO with KMS(vault)","url":"/2021/03/15/storage/minio/minIO_05_KMS/","content":"\n## 总览\n\nVault, Consul, MinIO 部署在K8s.  \nKES 部署在本地.  \n\n\n## referenc link\nhttps://github.com/minio/kes/wiki\nhttps://github.com/minio/kes/wiki/Hashicorp-Vault-Keystore\nhttps://github.com/minio/kes/wiki/MinIO-Object-Storage#minio-configuration\nhttps://github.com/minio/minio/blob/master/docs/kms/README.md\n\n<!-- more -->\n\nhttps://docs.min.io/docs/minio-security-overview.html\nhttp://docs.minio.org.cn/docs/master/minio-kms-quickstart-guide\nhttps://docs.min.io/docs/minio-client-quickstart-guide\n\nhttps://blog.csdn.net/peterwanghao/article/details/83181932\n\n\nvalut:\nhttps://learn.hashicorp.com/tutorials/vault/ha-with-consul?in=vault/operations\n\n\n## 概念\n\nMinIO使用密钥管理系统（KMS）支持SSE-S3。如果客户端请求SSE-S3，或 启用了自动加密，则MinIO服务器会使用唯一的对象密钥对每个对象进行加密，该对象密钥受 KMS管理的主密钥保护。\n\nKMS将MinIO作为面向应用程序的存储系统与安全密钥存储区分开，并且 可以由专门的安全团队进行管理。MinIO 通过我们的KES project支持常用的KMS实现，例如 Hashicorp Vault。 通过KES，可以利用存储基础架构（MinIO群集）水平扩展KMS。 通常，MinIO-KMS基础结构如下所示：\n\n```\n     ┌─────────┐         ┌────────────┐         ┌─────────┐  \n     │  MinIO  ├─────────┤ KES Server ├─────────┤   KMS   │ \n     └─────────┘         └────────────┘         └─────────┘  \n```\n\n当您将存储基础架构扩展到多个MinIO群集时，您的架构应如下所示：\n\n```\n    ┌────────────┐\n    │ ┌──────────┴─┬─────╮          ┌────────────┐\n    └─┤ ┌──────────┴─┬───┴──────────┤ ┌──────────┴─┬─────────────────╮\n      └─┤ ┌──────────┴─┬─────┬──────┴─┤ KES Server ├─────────────────┤\n        └─┤   MinIO    ├─────╯        └────────────┘            ┌────┴────┐\n          └────────────┘                                        │   KMS   │\n                                                                └─────────┘\n```\n\n**`请注意`**, 所有MinIO群集均仅具有 **`其自己的`** KES实例的连接，而不能直接访问Vault（作为一种可能的KMS实现）。 每个KES实例将处理“其” MinIO群集发出的所有加密/解密请求，从而使中央KMS实现不必处理 大量流量。相反，每个KES实例都将使用中央KMS实现作为安全密钥存储，并从中获取所需的主密钥。\n\n该指南显示了如何使用Hashicorp Vault作为KMS实施来设置MinIO-KMS部署。 因此，它显示了如何设置和配置：\n * Vault服务器作为中央密钥库。\n * 一个KES服务器实例，作为MinIO和保险柜之间的中间件。\n * MinIO实例本身。\n\n:::info\n**`请注意,`** 为简便起见，本指南使用自签名证书。在生产部署中，应使用 由[“公共”]{.blue}（例如，让我们加密）或组织内部的CA颁发的X.509证书。\n:::\n\n本指南说明如何在同一台计算机上设置三台不同的服务器：\n\n * Vault服务器为 https://127.0.0.1:8200\n * KES服务器为 https://127.0.0.1:7373\n * MinIO在K8s上部署后9000映射到主机端口位30007, 则服务器为 https://127.0.0.1:30007\n\n## 先决条件\n安装MinIO，KES和Vault。对于MinIO，请参阅 MinIO 快速入门指南. 然后安装KES并下载 适用于您的操作系统和平台的最新Vault二进制文件.  \n\n### * Vault:\n\n``` bash 命令行提示符 mark:1-3 command:(\"$\":1-3,6||\"#\":5)\nwget https://releases.hashicorp.com/vault/1.7.0-rc1/vault_1.7.0-rc1_linux_amd64.zip\nunzip vault_1.7.0-rc1_linux_amd64.zip\nmv vault /usr/local/bin/ # 如果cp过去的话, 下面启动vault server时候自动创建的vault文件夹会与此已存在的vault二进制文件冲突, 因此直接move过去.\n\n# vault 可以在Linux系统上使用 mlock syscall 来防止 OS 将内存中的数据写入磁盘（交换）\nsudo setcap cap_ipc_lock=+ep $(readlink -f $(which vault))\n```\n\n### * KES:\nhttps://github.com/minio/kes#binary-releases\n```\nwget https://github.com/minio/kes/releases/latest/download/kes-linux-amd64\nmv kes-linux-amd64 kes\nchmod +x kes\ncp kes /usr/local/bin/\nkes -h\n\n# KES 可以在Linux系统上使用 mlock syscall 来防止 OS 将内存中的数据写入磁盘（交换）\nsudo setcap cap_ipc_lock=+ep $(readlink -f $(which kes))\n```\n\n### * MinIO： \n```\nwget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\ncp minio /usr/local/bin/\n```\n\n### * mc:\n```\nwget https://dl.min.io/client/mc/release/linux-amd64/mc\nchmod +x mc\n./mc --help\n```\n\n## 机器配置\n\n| IP | 机器名 | 用途 |\n| :----- | :----- | :----- |\n| 10.239.140.73 | master | K8S集群节点, master(去除污点), 部署KMS, KES到本机, MinIO 部署到K8S集群 |\n| 10.239.131.157 | laboratory | K8S集群节点, master(去除污点) |\n\nopenssl 版本：\n```\n$ openssl version\nOpenSSL 1.0.2k-fips  26 Jan 2017\nbuilt on: reproducible build, date unspecified\nplatform: linux-x86_64\noptions:  bn(64,64) md2(int) rc4(16x,int) des(idx,cisc,16,int) idea(int) blowfish(idx)\ncompiler: gcc -I. -I.. -I../include  -fPIC -DOPENSSL_PIC -DZLIB -DOPENSSL_THREADS -D_REENTRANT -DDSO_DLFCN -DHAVE_DLFCN_H -DKRB5_MIT -m64 -DL_ENDIAN -Wall -O2 -g -pipe -Wall -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches   -m64 -mtune=generic -Wa,--noexecstack -DPURIFY -DOPENSSL_IA32_SSE2 -DOPENSSL_BN_ASM_MONT -DOPENSSL_BN_ASM_MONT5 -DOPENSSL_BN_ASM_GF2m -DRC4_ASM -DSHA1_ASM -DSHA256_ASM -DSHA512_ASM -DMD5_ASM -DAES_ASM -DVPAES_ASM -DBSAES_ASM -DWHIRLPOOL_ASM -DGHASH_ASM -DECP_NISTZ256_ASM\nOPENSSLDIR: \"/etc/pki/tls\"\nengines:  rdrand dynamic\n```\n\n## KMS(Vault)\n\n### 为保险柜生成TLS证书\n\nvault-crt.conf\n\n```\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nC = US\nST = state\nL = location\nO = organization\nCN = localhost\n\n[v3_req]\nsubjectAltName = @alt_names\n\n[alt_names]\n[alt_names]\nIP.1 = 127.0.0.1\nIP.2 = 10.239.140.73\nDNS.1 = localhost\nDNS.2 = master\n```\n生成证书\n```\n# 使用ECDSA生成密钥\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out vault-tls.key\nopenssl req -new -x509 -nodes -days 30 -key vault-tls.key -sha384 -out vault-tls.crt -config vault-crt.conf\n\n# 使用RSA生成密钥\nopenssl genrsa -out vault-tls.key 2048\nopenssl req -new -x509 -nodes -days 30 -key private.key -sha384 -out vault-tls.crt -config vault-crt.conf\n```\n\n查看密钥和证书\n```\n# 查看ecdsa生成的密钥\nopenssl ec -in vault-tls.key -text -noout\n# 查看rsa生成的密钥\nopenssl rsa -in vault-tls.key -text -noout\n\n(Optional)# 查看csr\n(Optional)openssl req -in root-cert.csr -text -noout\n\n# 查看证书\nopenssl x509 -in vault-tls.crt -text -noout\n```\n\n### 创建保管库配置文件\n\n```\ncat > vault-config.json <<EOF\n{\n   \"api_addr\": \"https://127.0.0.1:8200\",\n   \"backend\": {\n     \"file\": {\n       \"path\": \"vault/file\"\n     }\n   },\n  \"default_lease_ttl\": \"168h\",\n  \"max_lease_ttl\": \"720h\",\n  \"listener\": {\n    \"tcp\": {\n      \"address\": \"0.0.0.0:8200\",\n      \"tls_cert_file\": \"vault-tls.crt\",\n      \"tls_key_file\": \"vault-tls.key\",\n      \"tls_min_version\": \"tls12\"\n    }\n  }\n}\nEOF\n```\n\n请注意，我们使用文件后端运行Vault。为了获得高可用性，您可能需要使用其他 后端，例如[etcd](https://www.vaultproject.io/docs/configuration/storage/etcd)或[consul](https://learn.hashicorp.com/tutorials/vault/ha-with-consul).\n\n### 启动Vault服务器：\n启动vault server, 测试完终止后, 下次直接运行此命令重启vault server, 再用下面 三条unseal 命令解封即可.  \n```\nvault server -config vault-config.json\n```\n\n### 初始化和解封保管箱\n切换终端, 在单独的终端窗口中设置环境VAULT_ADDR。保险柜服务器的变量:\n```\nexport VAULT_ADDR='https://127.0.0.1:8200'\n// 如果Vault使用自签名TLS 证书，则可能要运行如下命令, 当Vault提供 由计算机信任的CA颁发的TLS证书（例如，让我们加密）时，则无需运行此命令.\nexport VAULT_SKIP_VERIFY=true\n// 初始化保险柜\nvault operator init\n```\n**`保管箱将打印n（默认情况下为5）解封密钥共享，其中至少m（至少3） 为重新生成实际解封密钥才能解封保管库.`** 因此，请务必 记住它们。特别是，请将那些未密封的密钥共享放在安全且持久的位置\n您应该看到类似于以下内容的输出：\n```\nCopyUnseal Key 1: eyW/+8ZtsgT81Cb0e8OVxzJAQP5lY7Dcamnze+JnWEDT\nUnseal Key 2: 0tZn+7QQCxphpHwTm6/dC3LpP5JGIbYl6PK8Sy79R+P2\nUnseal Key 3: cmhs+AUMXUuB6Lzsvgcbp3bRT6VDGQjgCBwB2xm0ANeF\nUnseal Key 4: /fTPpec5fWpGqWHK+uhnnTNMQyAbl5alUi4iq2yNgyqj\nUnseal Key 5: UPdDVPto+H6ko+20NKmagK40MOskqOBw4y/S51WpgVy/\n\nInitial Root Token: s.zaU4Gbcu0Wh46uj2V3VuUde0\n\nVault is initialized with 5 key shares and a key threshold of 3. Please securely\ndistribute the key shares printed above. When the Vault is re-sealed,\nrestarted, or stopped, you must supply at least 3 of these keys to unseal it\nbefore it can start servicing requests.\n\nVault does not store the generated master key. Without at least 3 key to\nreconstruct the master key, Vault will remain permanently sealed!\n\nIt is possible to generate new unseal keys, provided you have a quorum of\nexisting unseal keys shares. See \"vault operator rekey\" for more information.\n```\n\n设置环境, 变量 **`VAULT_TOKEN`** 到命令之前打印的根令牌\n```\nexport VAULT_TOKEN=s.zaU4Gbcu0Wh46uj2V3VuUde0\n```\n\n然后, 使用任何先前生成的密钥共享来打开Vault的密封.\n```\nvault operator unseal eyW/+8ZtsgT81Cb0e8OVxzJAQP5lY7Dcamnze+JnWEDT\nvault operator unseal 0tZn+7QQCxphpHwTm6/dC3LpP5JGIbYl6PK8Sy79R+P2\nvault operator unseal cmhs+AUMXUuB6Lzsvgcbp3bRT6VDGQjgCBwB2xm0ANeF\n```\n提交足够的有效密钥共享后，保管箱将被解封 并能够处理请求。\n\n### 启用保险柜的K/V后端.\n加密主密钥（而不是对象加密密钥）将存储 在Vault中。因此，我们需要启用Vault的K/V后端。为此，请运行：\n```\nvault secrets enable kv\n```\n为 K/V引擎 创建访问策略.  \n以下策略确定应用程序（即KES服务器）如​​何 与Vault 交互.  \n```\ncat > minio-kes-policy.hcl <<EOF\npath \"kv/minio/*\" {\n  capabilities = [ \"create\", \"read\", \"delete\" ]\n}\nEOF\n```\n观察在 **`kv/minio/*`** 的路径前缀 **`minio`**, 此前缀确保 KES 服务器只能在 **`minio/-`** 下进行读取，而只能在-下进行写入 `some-app/`. 如何在 K/V 引擎上分隔域取决于您的基础结构 和安全要求。\n\n然后，我们将政策上传到保险柜：\n```\nvault policy write minio-key-policy ./minio-kes-policy.hcl\n```\n\n### 启用AppRole身份验证\n由于我们希望稍后将一个/多个KES服务器连接到Vault，因此必须启用 AppRole身份验证。为此，请运行：\n```\nvault auth enable approle\n```\n创建一个新的AppRole ID并将其绑定到策略.  \n创建一个新的AppRole ID并授予该ID特定的权限。 该应用程序（即KES服务器）将通过AppRole角色ID 和机密ID 向Vault进行身份验证，并且仅允许执行特定策略授予的操作.  \n```\n# 我们首先为KES服务器创建一个新角色:\nvault write auth/approle/role/kes-role token_num_uses=0  secret_id_num_uses=0  period=5m\n\n# 然后，我们将策略绑定到角色：\nvault write auth/approle/role/kes-role policies=minio-key-policy\n\n# 最后，我们从Vault请求AppRole角色ID和秘密ID。\n首先是角色ID：\nvault read auth/approle/role/kes-role/role-id \n\tfe46e329-bb70-2342-9fac-f3964a447d07\n\n# 然后是秘密ID：\nvault write -f auth/approle/role/kes-role/secret-id\n\t50646d24-5589-bba9-3f99-15ef2ae7e100\n```\n我们只对secret_id感兴趣, 不包括secret_id_accessor.\n\n\n\n\n## KES\n\n### 生成KES证书和私钥\n**`此部署文档将Kes和MinIO共用相同的private.key和public.crt`**\n上面私匙和证书名字不要改变, 因为MinIO的私匙和证书必须是private.key和public.crt,\n\nkes-minio-crt.conf:\n\n```\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nC = US\nST = state\nL = location\nO = organization\nCN = localhost\n\n[v3_req]\nsubjectAltName = @alt_names\n\n[alt_names]\nIP.1 = 127.0.0.1\nIP.2 = 10.239.140.73\nDNS.1 = *.minio-hl-svc.minio.svc.cluster.local\nDNS.2 = master\nDNS.3 = localhost\n```\n连接到KES服务器（mTLS）时，每个用户或应用程序必须出示有效的X.509证书。 KES服务器将接受/拒绝连接尝试，并根据证书应用策略。\n\n因此，每个MinIO群集都需要一个X.509 TLS证书来进行客户端身份验证. 可以通过运行以下命令来创建MinIO（自签名）证书. \n```\n# 使用ECDSA生成密钥\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out minio-pki/private.key\nopenssl req -new -x509 -nodes -days 730 -key minio-pki/private.key -sha384 -out minio-pki/public.crt -config kes-minio-crt.conf\n\n# 使用RSA生成密钥\nopenssl genrsa -out private.key 2048\nopenssl req -new -x509 -nodes -days 730 -key minio-pki/private.key -out minio-pki/public.crt -config kes-minio-crt.conf\n\n# KES和MinIO共用相同的证书和私钥, 因此拷贝一份MinIO密钥和证书给KES用:\ncp minio-pki/private.key ./kes-server.key\ncp minio-pki/public.crt ./kes-server.crt\n```\n\n注意，MinIO是[subject name](https://en.wikipedia.org/wiki/X.509#Structure_of_a_certificate)。 您可以为您的部署方案选择一个更合适的名称。此外，对于生产部署，我们 建议获取由CA颁发的用于客户端身份验证的TLS证书。\n\n要获取 MinIO 的 X.509 证书的身份，请运行：\n```\nkes tool identity of minio-pki/public.crt\n# 此命令可与任何（有效）X.509证书一起使用-无论如何创建它-并 产生类似于以下内容的输出：\n  Identity: dd46485bedc9ad2909d2e8f9017216eec4413bc5c64b236d992f7ec19c843c5f\n\n# 设置环境变量\nexport APP_IDENTITY=dd46485bedc9ad2909d2e8f9017216eec4413bc5c64b236d992f7ec19c843c5f\n```\n\n### 创建KES配置文件\n创建KES配置文件并启动KES服务器.  \n```\ncat > kes-server-config.yml <<EOF\naddress: 0.0.0.0:7373\nroot:    disabled  # We disable the root identity since we don't need it in this guide\n\ntls:\n  key:  kes-server.key\n  cert: kes-server.crt\n\npolicy:\n  my-app:\n    paths:\n    - /v1/key/create/minio-*\n    - /v1/key/generate/minio-*\n    - /v1/key/decrypt/minio-*\n    identities:\n    - ${APP_IDENTITY}\n\ncache:\n  expiry:\n    any:    5m0s\n    unused: 20s\n\nkeys:\n  vault:\n    endpoint: https://127.0.0.1:8200\n    prefix: minio\n    approle:\n      id:     \"fe46e329-bb70-2342-9fac-f3964a447d07\" # Your AppRole ID: $ vault read auth/approle/role/kes-role/role-id\n      secret: \"50646d24-5589-bba9-3f99-15ef2ae7e100\" # Your AppRole Secret ID: $ vault write -f auth/approle/role/kes-role/secret-id\n      retry:  15s\n    status:\n      ping: 10s\n    tls:\n      ca: \"vault-tls.crt\" # Since we use self-signed certificates\n\nEOF\n```\n请identities在policy部分中将的值更改为您的身份 minio-pki/public.crt, 或者在终端运行 **`export APP_IDENTITY=dd46485bedc9ad2909d2e8f9017216eec4413bc5c64b236d992f7ec19c843c5f`**  \n另外，插入您在保险柜设置过程中之前创建的AppRole角色ID和密码ID。 您可以[在此处](https://github.com/minio/kes/blob/master/server-config.yaml)找到包含所有可用参数的文档化配置文件.  \n\n最后，通过以下方式启动KES服务器：\n```\nkes server --mlock --config=kes-server-config.yml --auth=off\n```\n**`--auth=off`** is required since our root.cert and app.cert certificates are self-signed.\nThe **`--mlock`** option is currently only available on Linux.\n\n有关KES访问控制模型和身份验证的更多信息:[KES Concepts](https://github.com/minio/kes/wiki/Concepts)\n\n### 创建一个新的主密钥\n在继续进行MinIO设置之前，我们需要创建一个新的主密钥。因此，我们使用 MinIO身份和KES CLI。\n\n在新的终端窗口中，通过以下方式成为MinIO身份：\n```\nexport KES_CLIENT_CERT=minio-pki/public.crt\nexport KES_CLIENT_KEY=minio-pki/private.key\n```\n然后运行以下命令来创建主密钥：\n```\n# 设置访问vault变量:\nexport VAULT_SKIP_VERIFY=true\nexport VAULT_TOKEN=s.zaU4Gbcu0Wh46uj2V3VuUde0\n\n# 创建主密钥:\nkes key create -k minio-key-1\n\n# 查看所有key-value文件夹\nvault kv list kv/\n  Keys\n  ----\n  minio/\n  test01\n\n# 查看某个文件夹下有哪些主密钥\nvault kv list kv/minio\n  Keys\n  ----\n  minio-key-1\n  minio-key-2\n\n# 获取某个主密钥数据信息\nvault kv get kv/minio/minio-key-1\n  ======= Data =======\n  Key            Value\n  ---            -----\n  minio-key-1    {\"bytes\":\"rrjxB3THaSzSGaJ2mUwWDjT/+Tvgb5XbECPi3P9rgBs=\"}\n\n```\n由于我们使用自签名证书，因此仅需要 **`-k`** 标志.  \n另外，请注意，基于服务器配置文件， 仅允许MinIO标识创建/使用以开头的主密钥 `minio-`. 因此，尝试创建密钥（例如） `kes key create my-key-1 -k` 将失败，并出现策略错误禁止的消息.  \n\n\n\n## MinIO\n\nminio的证书和私钥请参考上面KES生成 minio-pki/private.key 和 minio-pki/public.crt 步骤.  \n\nMinIO服务器将需要知道KES服务器端点， 用于身份验证和授权的mTLS客户端证书以及默认的主密钥名称.  \n\n### 搭建本地minio服务\nMinIO服务器将需要知道KES服务器端点， 用于身份验证和授权的mTLS客户端证书以及默认的主密钥名称.  \n\n```\nexport MINIO_KMS_KES_ENDPOINT=https://10.239.140.73:7373\nexport MINIO_KMS_KES_CERT_FILE=minio-pki/public.crt\nexport MINIO_KMS_KES_KEY_FILE=minio-pki/private.key\nexport MINIO_KMS_KES_CA_PATH=kes-server.crt\nexport MINIO_KMS_KES_KEY_NAME=minio-key-1\n```\n由于我们使用自签名证书, 因此只需要**`MINIO_KMS_KES_CA_PATH`**\n\n然后启动MinIO服务器：\n```\nexport MINIO_ACCESS_KEY=minio\nexport MINIO_SECRET_KEY=minio123\nrm -rf /root/.mc\nrm -rf /root/.minio\nminio server --certs-dir ./minio-pki ./data\n * 指定上传minio server端证书存放路径文件夹为: ./minio-pki\n * minio存储的数据保存到data目录\n```\n\n切换终端,启动自动加密, 如果在K8S上部署MinIO server enable了密钥和证书, 则执行下面类似命令时候都要加上 **`--insecure`**, 信任自签的证书.\n```\n# 设置server服务地址\nmc alias set myminio https://10.239.140.73:9000 minio minio123 --insecure\n\n# 创建bucket\nmc mb myminio/test01 --insecure\n\n# 加密minio 已存在的某个 bucket\nmc encrypt set sse-s3 myminio/test01/ --insecure\nmc encrypt info myminio/test01/ --insecure\n  Auto encryption 'sse-s3' is enabled\n\n# 将本地文件synk.jpg上传到minio\nmc cp synk.jpg myminio/test01/ --insecure\n\n# 查看上传文件状态, 可以看到已自动加密\nmc stat myminio/test01/synk.jpg --insecure\n  Name      : synk.jpg\n  Date      : 2021-03-15 11:54:36 CST\n  Size      : 112 B\n  ETag      : 2ced7fb62d324c643aeb21eccbe86eeb\n  Type      : file\n  Metadata  :\n    Content-Type: application/x-sh\n  Encrypted :\n    X-Amz-Server-Side-Encryption: AES256\n\n# 将上传的文件下载到本地\nmc cp myminio/test01/synk.jpg 001.jpg --insecure\n```\n\n### k8s上搭建minio\n\n#### 所有文件\n\n+++primary build_minio.sh\n``` shell\n#!/bin/bash\n\nkubectl create ns minio\n\nkubectl create secret generic tls-ssl-minio --from-file=pki/private.key --from-file=pki/public.crt -n minio\n\n# Sample: Create key and crt\n:<<block\nrm -rf pki\nmkdir pki -p\nopenssl genrsa -out pki/private.key 2048\nsleep 1\nopenssl req -new -x509 -days 3650 -key pki/private.key -out pki/public.crt -subj \"/C=US/ST=state/L=location/O=organization/CN=*.minio-                     hl-svc.minio.svc.cluster.local\"\nsleep 1\nblock\n\n# Using Direct CSI Driver\n#cat << EOF > default.env\n#DIRECT_CSI_DRIVES=minio_data{1...4}\n#DIRECT_CSI_DRIVES_DIR=/mnt/minio\n#EOF\ncat << EOF > default.env\nDIRECT_CSI_DRIVES=minio_data\nDIRECT_CSI_DRIVES_DIR=/mnt\nEOF\n\nexport $(cat default.env)\nkubectl apply -k direct-csi\nsleep 3\n\n# Create Operator Deployment\nkubectl apply -f minio-operator.yaml -n minio\nkubectl apply -f minioinstance.yaml -n minio\n\n# Create NodePort service for minIO\nkubectl apply -f nodePort-minio.yaml\n```\n+++\n\n+++info clean.sh\n``` shell\n#!/bin/bash\n\n# delete minio pod\nkubectl delete -f minioinstance.yaml -n minio\n\n# delete pvc\npvc_results=$(kubectl get pvc -n minio | grep minio | awk '{print $1}')\npvc_arr=(${pvc_results})\nfor ((i=0; i<${#pvc_arr[*]}; i++ ))\ndo\nkubectl delete pvc ${pvc_arr[i]} -n minio\ndone\n\n# delete pv\npv_results=$(kubectl get pv | grep minio | awk '{print $1}')\npv_arr=(${pv_results})\nfor ((i=0; i<${#pv_arr[*]}; i++ ))\ndo\nkubectl delete pv ${pv_arr[i]}\ndone\n\n# delete minio operator\nkubectl delete -f minio-operator.yaml -n minio\n\n# delete minio sc\nkubectl delete -k direct-csi\n\n# delete secret\nkubectl delete secret tls-ssl-minio -n minio\n\n# delete ns\nkubectl delete ns minio\n```\n+++\n\n+++success default.env\n``` text\nDIRECT_CSI_DRIVES=minio_data\nDIRECT_CSI_DRIVES_DIR=/mnt\nKUBELET_DIR_PATH=/var/lib/kubelet\n```\n+++\n\n+++warning direct-csi/\n``` shell\ngit clone https://github.com/minio/direct-csi.git\ncd direct-csi/\ngit checkout v0.2.1\ncd ../\nexport $(cat default.env)\nkubectl apply -k direct-csi\n```\n+++\n\n+++primary minio-crt.conf\n``` yaml\n[req]\ndistinguished_name = req_distinguished_name\nx509_extensions = v3_req\nprompt = no\n\n[req_distinguished_name]\nC = US\nST = state\nL = location\nO = organization\nCN = localhost\n\n[v3_req]\nsubjectAltName = @alt_names\n\n[alt_names]\nIP.1 = 127.0.0.1\nIP.2 = 10.239.140.73\nDNS.1 = *.minio-hl-svc.minio.svc.cluster.local\nDNS.2 = master\nDNS.3 = localhost\n```\n+++\n\n+++info minioinstance.yaml\n``` yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: minio-creds-secret\ntype: Opaque\ndata:\n  accesskey: aGNlbWluaW8= # base 64 encoded \"hceminio\" (echo -n 'hceminio' | base64)\n  secretkey: aGNlbWluaW8xMjM= # based 64 encoded \"hceminio123\" (echo -n 'hceminio123' | base64)\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service\nspec:\n  type: ClusterIP\n  ports:\n    - port: 9000\n      targetPort: 9000\n      protocol: TCP\n      # Optional field\n      # By default and for convenience, the Kubernetes control plane will allocate a port from a range (default: 30000-32767)\n      # nodePort: 30007\n  selector:\n    app: minio\n---\napiVersion: operator.min.io/v1\nkind: MinIOInstance\nmetadata:\n  name: minio\n## If specified, MinIOInstance pods will be dispatched by specified scheduler.\n## If not specified, the pod will be dispatched by default scheduler.\n# scheduler:\n#  name: my-custom-scheduler\nspec:\n  ## Add metadata to the all pods created by the StatefulSet\n  metadata:\n    ## Optionally pass labels to be applied to the statefulset pods\n    labels:\n      app: minio\n    annotations:\n      prometheus.io/path: /minio/prometheus/metrics\n      prometheus.io/port: \"9000\"\n      prometheus.io/scrape: \"true\"\n  ## Registry location and Tag to download MinIO Server image\n  image: minio/minio:RELEASE.2020-06-18T02-23-35Z\n  imagePullPolicy: IfNotPresent\n  ## A ClusterIP Service will be created with the given name\n  serviceName: minio-internal-service\n  zones:\n    - name: \"zone-0\"\n      ## Number of MinIO servers/pods in this zone.\n      ## For standalone mode, supply 1. For distributed mode, supply 4 or more.\n      ## Note that the operator does not support upgrading from standalone to distributed mode.\n      servers: 4\n  ## Supply number of volumes to be mounted per MinIO server instance.\n  volumesPerServer: 1\n  ## Mount path where PV will be mounted inside container(s). Defaults to \"/export\".\n  mountPath: /export\n  ## Sub path inside Mount path where MinIO starts. Defaults to \"\".\n  # subPath: /data\n  ## This VolumeClaimTemplate is used across all the volumes provisioned for MinIO cluster.\n  ## Please do not change the volumeClaimTemplate field while expanding the cluster, this may\n  ## lead to unbound PVCs and missing data\n  volumeClaimTemplate:\n    metadata:\n      name: data\n    spec:\n      accessModes:\n        - ReadWriteOnce\n      resources:\n        requests:\n          #storage: 1Ti\n          storage: 1Gi\n      storageClassName: direct.csi.min.io\n  ## Secret with credentials to be used by MinIO instance.\n  credsSecret:\n    name: minio-creds-secret\n  ## PodManagement policy for pods created by StatefulSet. Can be \"OrderedReady\" or \"Parallel\"\n  ## Refer https://kubernetes.io/docs/tutorials/stateful-application/basic-stateful-set/#pod-management-policy\n  ## for details. Defaults to \"Parallel\"\n  podManagementPolicy: Parallel\n  ## Secret with certificates to configure TLS for MinIO certs. Create secrets as explained\n  ## here: https://github.com/minio/minio/tree/master/docs/tls/kubernetes#2-create-kubernetes-secret\n  externalCertSecret:\n    name: tls-ssl-minio\n  ## Enable Kubernetes based certificate generation and signing as explained in\n  ## https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster\n  requestAutoCert: false\n  ## Used when \"requestAutoCert\" is set to true. Set CommonName for the auto-generated certificate.\n  ## Internal DNS name for the pod will be used if CommonName is not provided.\n  ## DNS name format is minio-{0...3}.minio.default.svc.cluster.local\n  certConfig:\n    commonName: \"\"\n    organizationName: []\n    dnsNames: []\n  ## Used to specify a toleration for a pod\n  # tolerations:\n  #  - effect: NoSchedule\n  #    key: dedicated\n  #    operator: Equal\n  #    value: storage\n  ## Add environment variables to be set in MinIO container (https://github.com/minio/minio/tree/master/docs/config)\n  # env:\n    # - name: MINIO_BROWSER\n    #   value: \"off\" # to turn-off browser\n    # - name: MINIO_STORAGE_CLASS_STANDARD\n    #   value: \"EC:2\"\n  env:\n    - name: MINIO_KMS_KES_ENDPOINT\n      value: https://10.239.140.73:7373\n    - name: MINIO_KMS_KES_KEY_FILE\n      value: /tmp/certs/private.key\n    - name: MINIO_KMS_KES_CERT_FILE\n      value: /tmp/certs/public.crt\n    - name: MINIO_KMS_KES_KEY_NAME\n      value: minio-key-2\n    - name: MINIO_KMS_KES_CA_PATH\n      value: /tmp/certs/public.crt\n  ## Configure resource requests and limits for MinIO containers\n  # resources:\n    # requests:\n      # memory: 20Gi\n  ## Liveness probe detects situations where MinIO server instance\n  ## is not working properly and needs restart. Kubernetes automatically\n  ## restarts the pods if liveness checks fail.\n  liveness:\n    initialDelaySeconds: 10\n    periodSeconds: 1\n    timeoutSeconds: 1\n  ## nodeSelector parameters for MinIO Pods. It specifies a map of key-value pairs. For the pod to be\n  ## eligible to run on a node, the node must have each of the\n  ## indicated key-value pairs as labels.\n  ## Read more here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/\n  # nodeSelector:\n  #   disktype: ssd\n\n#  nodeSelector:\n#    kubernetes.io/hostname: hci-node02\n\n  ## Affinity settings for MinIO pods. Read more about affinity\n  ## here: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity.\n  # affinity:\n\n```\n+++\n\n+++success minio-operator.yaml\n``` yaml\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: minioinstances.operator.min.io\nspec:\n  group: operator.min.io\n  scope: Namespaced\n  names:\n    kind: MinIOInstance\n    singular: minioinstance\n    plural: minioinstances\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n      # openAPIV3Schema is the schema for validating custom objects.\n      # Refer https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#specifying-a-structural-schema\n      # for more details\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              x-kubernetes-preserve-unknown-fields: true\n              properties:\n                replicas:\n                  type: integer\n                  minimum: 1\n                  maximum: 32\n                image:\n                  type: string\n                serviceName:\n                  type: string\n                volumesPerServer:\n                  type: integer\n                mountPath:\n                  type: string\n                podManagementPolicy:\n                  type: string\n                  enum: [Parallel,OrderedReady]\n                  default: Parallel\n                requestAutoCert:\n                  type: boolean\n                  default: false\n                version:\n                  type: string\n                mountpath:\n                  type: string\n                subpath:\n                  type: string\n                mcs:\n                  type: object\n                  x-kubernetes-preserve-unknown-fields: true\n                  properties:\n                    image:\n                      type: string\n                    replicas:\n                      type: integer\n                      default: 2\n                    mcsSecret:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n                kes:\n                  type: object\n                  x-kubernetes-preserve-unknown-fields: true\n                  properties:\n                    image:\n                      type: string\n                    replicas:\n                      type: integer\n                      default: 2\n                    kesSecret:\n                      type: object\n                      properties:\n                        name:\n                          type: string\n            status:\n              type: object\n              properties:\n                currentState:\n                  type: string\n      subresources:\n        # status enables the status subresource.\n        status: {}\n      additionalPrinterColumns:\n        - name: Current State\n          type: string\n          jsonPath: \".status.currentState\"\n---\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRole\nmetadata:\n  name: minio-operator-role\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - namespaces\n  - secrets\n  - pods\n  - services\n  - events\n  verbs:\n  - get\n  - watch\n  - create\n  - list\n  - delete\n- apiGroups:\n  - apps\n  resources:\n  - statefulsets\n  - deployments\n  verbs:\n  - get\n  - create\n  - list\n  - patch\n  - watch\n  - update\n  - delete\n- apiGroups:\n  - batch\n  resources:\n  - jobs\n  verbs:\n  - get\n  - create\n  - list\n  - patch\n  - watch\n  - update\n  - delete\n- apiGroups:\n  - \"certificates.k8s.io\"\n  resources:\n  - \"certificatesigningrequests\"\n  - \"certificatesigningrequests/approval\"\n  - \"certificatesigningrequests/status\"\n  verbs:\n  - update\n  - create\n  - get\n  - delete\n- apiGroups:\n  - operator.min.io\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n- apiGroups:\n  - min.io\n  resources:\n  - \"*\"\n  verbs:\n  - \"*\"\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: minio-operator\n  namespace: minio\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: minio-operator-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: minio-operator-role\nsubjects:\n- kind: ServiceAccount\n  name: minio-operator\n  namespace: minio\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: minio-operator\n  namespace: minio\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: minio-operator\n  template:\n    metadata:\n      labels:\n        name: minio-operator\n    spec:\n      serviceAccountName: minio-operator\n      containers:\n        - name: minio-operator\n          image: minio/k8s-operator:2.0.6\n          imagePullPolicy: IfNotPresent\n          # To specify cluster domain, un comment the following:\n          # env:\n          #   - name: CLUSTER_DOMAIN\n          #     value: mycluster.mydomain\n```\n+++\n\n+++primary mkdir_minio_node.sh\n``` shell\n#!/bin/bash\n\n#usecase: ./bash_shell_commands.sh -u <username> -p <pwd>  --node01 <node01-ip> --node02 <node02-ip> --node03 <node03-ip> --node04=<node04-ip>\n#./mkdir_minio_node.sh -u root -p 123456 --node01 10.239.140.73 --node02 10.239.131.157\n\nARGS=`getopt -o u:p: --long username:,password:,node01_ip:,node02_ip:,node03_ip:,node04_ip:: -n 'mkdir_node_minio.sh' -- \"$@\"`\nif [ $? != 0 ]; then\n        echo \"Terminating...\"\n        exit 1\nfi\neval set -- \"${ARGS}\"\necho ${ARGS}\n\nwhile true\ndo\n        case \"$1\" in\n                -u|--username)\n                        username=$2\n                        shift 2 ;;\n                -p|--password)\n                        password=$2\n                        shift 2 ;;\n                --node01_ip)\n                        node01_ip=$2\n                        nodes+=(${node01_ip})\n                        shift 2 ;;\n                --node02_ip)\n                        node02_ip=$2\n                        nodes+=(${node02_ip})\n                        shift 2 ;;\n                --node03_ip)\n                        node03_ip=$2\n                        nodes+=(${node03_ip})\n                        shift 2 ;;\n                --node04_ip)\n                        case \"$2\" in\n                                \"\")\n                                        shift 2 ;;\n                                *)\n                                        node04_ip=$2\n                                        nodes+=(${node04_ip})\n                                        shift 2 ;;\n                        esac\n                        ;;\n                --)\n                        shift\n                        break ;;\n                *)\n                        echo \"Error!\"\n                        exit 1 ;;\n        esac\ndone\n\nfor node in ${nodes[*]}\ndo\nexpect <<-EOF\nspawn ssh $username@${node}\nexpect {\n        \"yes/no\" { send \"yes\\r\"; exp_continue }\n        \"password\" { send \"${password}\\r\"; exp_continue }\n        \"Last login\" { send \"\\r\" }\n}\nexpect \"*#\"\nsend \"rm -rf /mnt/minio_data \\r\"\nsend \"mkdir -p /mnt/minio_data \\r\"\nexpect \"*#\"\nsend \"exit\\r\"\nexpect \"eof\"\nEOF\ndone\n```\n+++\n\n+++info nodePort-minio.yaml\n``` yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: minio-service-nodeport\n  namespace: minio\nspec:\n  type: NodePort\n  ports:\n    - port: 9000\n      targetPort: 9000\n      protocol: TCP\n      nodePort: 30007\n  selector:\n    app: minio\n```\n+++\n\n+++success pki/\n存在生成MinIO密钥和证书的配置文件等.\n```\nminio-crt.conf  private.key  public.crt\n```\n+++\n\n#### minio证书\nminio证书跟上面的KES一致, 可以直接复制上面的KES证书, 或者用下面命令生成.  \n\n``` shell\n# 使用ECDSA生成MinIO密钥\nopenssl ecparam -genkey -name prime256v1 | openssl ec -out pki/private.key\nopenssl req -new -x509 -nodes -days 730 -key pki/private.key -sha384 -out pki/public.crt -config minio-crt.conf\n\n# 使用RSA生成MinIO密钥\nopenssl genrsa -out private.key 2048\nopenssl req -new -x509 -nodes -days 730 -key pki/private.key -out pki/public.crt -config minio-crt.conf\n```\n\n#### 数据存放目录\n``` shell\n./mkdir_minio_node.sh\n\n# 登陆到各个部署minio的机器, mount MinIO的数据存放目录到指定磁盘\nmount /dev/sd* /mnt/minio_data\n```\n\n#### 部署\n``` shell \n./build_minio.sh\n```\n\n#### 查看\n``` shell\nkubectl get po -n minio\n  NAME                              READY   STATUS    RESTARTS   AGE\n  minio-0                           1/1     Running   0          6d23h\n  minio-1                           1/1     Running   0          6d23h\n  minio-2                           1/1     Running   0          6d23h\n  minio-3                           1/1     Running   0          6d23h\n  minio-operator-54d8bd774b-4h9sm   1/1     Running   0          6d23h\n\n\nkubectl logs po/minio-0 -n minio\n  ......\n  Formatting 1st zone, 1 set(s), 4 drives per set.\n  Waiting for all MinIO sub-systems to be initialized.. lock acquired\n  Attempting encryption of all config, IAM users and policies on MinIO backend\n  All MinIO sub-systems initialized successfully\n  Status:         4 Online, 0 Offline.\n  Endpoint:  https://10.32.0.11:9000  https://127.0.0.1:9000\n  \n  Browser Access:\n     https://10.32.0.11:9000  https://127.0.0.1:9000\n  \n  Object API (Amazon S3 compatible):\n     Go:         https://docs.min.io/docs/golang-client-quickstart-guide\n     Java:       https://docs.min.io/docs/java-client-quickstart-guide\n     Python:     https://docs.min.io/docs/python-client-quickstart-guide\n     JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide\n     .NET:       https://docs.min.io/docs/dotnet-client-quickstart-guide\n```\n\n#### 上传下载\n跟上面在本地搭建的minio服务操作几乎一样, 就是把MinIO server端的9000端口换成30007.  \n```\n# 设置server服务地址\nmc alias set myminio https://10.239.140.73:30007 minio minio123 --insecure\n\n# 创建bucket\nmc mb myminio/test01 --insecure\n\n# 加密minio 已存在的某个 bucket\nmc encrypt set sse-s3 myminio/test01/ --insecure\nmc encrypt info myminio/test01/ --insecure\n  Auto encryption 'sse-s3' is enabled\n\n# 将本地文件synk.jpg上传到minio\nmc cp synk.jpg myminio/test01/ --insecure\n\n# 查看上传文件状态, 可以看到已自动加密\nmc stat myminio/test01/synk.jpg --insecure\n  Name      : synk.jpg\n  Date      : 2021-03-15 11:54:36 CST\n  Size      : 112 B\n  ETag      : 2ced7fb62d324c643aeb21eccbe86eeb\n  Type      : file\n  Metadata  :\n    Content-Type: application/x-sh\n  Encrypted :\n    X-Amz-Server-Side-Encryption: AES256\n\n# 将上传的文件下载到本地\nmc cp myminio/test01/synk.jpg 001.jpg --insecure\n```\n\n#### 清理\n``` shell\n./clean.sh\n```\n\n\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"test","url":"/2021/03/13/entertainment/test/","content":"\n## test 001 ","categories":["entertainment"]},{"title":"Windows10_VScode远程连接linux编辑调试","url":"/2021/03/13/windows/Windows10_VScode远程连接linux编辑调试/","content":"\n1. 查看Windows10 是否已安装或开启ssh-client，默认Windows10自带的有\nWindows 10 : 设置 -> 应用(APPS) -> 应用和功能(APP & features) -> 管理可选功能(Manage app execution aliases)\n![](1.png)\n\n<!-- more -->\n\n没有的话需要点击如图上面的Add a feature，install一下.\n\n2. Centos\n\n```shell\n\t1. //安装 yum install -y openssl openssh-server \n\t//重启sshd服务 systemctl restart sshd.service \n\t//自动启动 systemctl enable sshd\n\t2. $cd ~/.ssh/\n```\n此目录如果没有authorized_keys文件需要touch新建一个，里面需要存放Window10的公匙(id_rsa.pub,另外id_rsa是Window10的密匙).\n\n3. 安装VS code， 安装扩展(Extensions)\"Remote Developoment\"插件，会自动安装其他的Remote插件，其中会包含Remote-SSH\n安装完成出现如下选项\n![](2.png)\n\n添加config文件\n![](3.png)\n\n添加linux主机\n\n```\nHost 后面接空格，名字随便写，显示在左边\nHostName 主机IP\nUser root\n```\n![](4.png)\n\n右击要连接的linux，选择在当前页面或新打开Vscode\n![](5.png)\n\n输入linux登录密码，如果出现需要输入密码多次可能之前链接过, 在linux `/root/.vscode-server` 生成有文件，删掉, 再重新用Vscode链接…\n![](6.png)\n观察VScode右下角等待连接成功\nSetting up SSH Host UserName:(details) Downloading VS Code Server\n![](7.png)\n\n最后点击Open folder就可以了\n\n后边遇到vscode一直连不上linux情况\n解决方法一:\n```shell\n$ df -hl 查看linux ~/ 等主目录是否已占满，删除一些文件释放空间后再连接就可以了\n```\n\n解决方法二:\n是查看linux /tmp 临时文件发现占满了，全部删掉，再用windows上得VS code连接就可以了\n原因是vscode连接linxu会自动在linux的/tmp生成一些文件\n\n## Linux 重装系统后再用windowsshangVScode连接报如下错误:\n```\nCould not establish connection to \"IP\". The process tried to write to a nonexistent pipe.\n```\n原因是windows与linux连接成功后会在C:\\Users\\用户名\\.ssh\\known_hosts添加对应Linux的密匙信息，把它相关的内容删掉.\n\n## VScode 连接Linux Waiting for /root/.vscode-server/bin/***/vscode-scp-done.flag and vscode-server.tar.gz to exist\n解决方法如下链接:\n[参考链接](https://blog.csdn.net/Ding19950107/article/details/103713556)\n\n```shell\nps -aux | grep vscode\nkill -9 PID\nrm -rf ~/.vscode-server\n```\n\n再重新用Vscode链接\n\n## VScode链接远程机器一致让输入远程机器密码\n解决方法是登陆远程机器然后删除/root/.vscode-server/bin/ 下最新的文件夹如\n\n```shell\n$ cd /root/.vscode-server/bin\n$ ls -alh \n  drwxr-xr-x 2 root root 106 Oct  9 10:00 58bb7b2331731bf72587010e943852e13e6fd3cf\n  drwxr-xr-x 6 root root 150 Sep 13 18:28 a0479759d6e9ea56afa657e454193f72aef85bd0\n  drwxr-xr-x 6 root root 150 Sep 16 13:44 e790b931385d72cf5669fcefc51cdf65990efa5d\n$ rm -rf 58bb7b2331731bf72587010e943852e13e6fd3cf\n```\n之后再次尝试用VScode连接远程机器就可以了\n","categories":["windows"]},{"title":"windows批量修改文件名","url":"/2021/03/13/windows/windows批量修改文件名/","content":"\n## **批量删除文件名中的空格**\n在要批量修改文件名的文件夹路径下创建新bat文件`delete_space.bat`, 内容如下:\n\n```shell\n@echo off\nSetlocal Enabledelayedexpansion\nset \"str= \"\nfor /f \"delims=\" %%i in ('dir /b *.*') do (\nset \"var=%%i\" & ren \"%%i\" \"!var:%str%=!\")\n```\n\n保存后直接双击即可\n\n<!-- more -->\n\n## **批量修改文件名**\n如上操作在要批量修改文件名的文件夹下创建`rename_or_delete_specified_str.bat`, 内容如下:\n\n```shell\n@echo off\nset /p str1= 请输入要替换的文件(文件夹)名字符串（可替换空格）：\nset /p str2= 请输入替换后的文件(文件夹)名字符串（去除则直接回车）：\necho.\necho 正在修改文件(夹)名中，请稍候……\nfor /f \"delims=\" %%a in ('dir /s /b ^|sort /+65535') do (\nif \"%%~nxa\" neq \"%~nx0\" (\nset \"file=%%a\"\nset \"name=%%~na\"\nset \"extension=%%~xa\"\ncall set \"name=%%name:%str1%=%str2%%%\"\nsetlocal enabledelayedexpansion\nren \"!file!\" \"!name!!extension!\" 2>nul\nendlocal\n)\n)\nexit\n```\n\n直接双击执行, 输入`要修改的文件名中的字符串`, 再输入`要替换成的字符串`, 如果不输入要替换的字符串而是直接回车则相当于`删除要修改的字符串`.  \n\n\n","categories":["windows"]},{"title":"笔记本内存信息查询","url":"/2021/03/13/windows/笔记本内存信息查询/","content":"\n\n## 内存频率和可用插槽查询\n\n右键菜单栏 打开任务管理器, 点击 `性能` 栏目, 点击 `内存`  \n\n已使用插槽后的数字分别对应就是已用内存插槽和可用内存插槽.  \n\n速度:展示内存条速率\n\n![](1.PNG)\n\n\n## 命令行查看内存详细信息\n\nWin + R 输入powershell或cmd打开终端\n\n在命令行界面输入wmic进入命令行系统管理执行脚本界面；\n\n可以通过命令memorychip进行查看内存型号，可以看出，有1条内存，存储大小, 型号, PN号也可以看出\n\n``` shell\n\tPS C:\\Users\\hp> wmic\n\twmic:root\\cli>memorychip\n\tAttributes  BankLabel  Capacity    Caption   ConfiguredClockSpeed  ConfiguredVoltage  CreationClassName     DataWidth  Description  DeviceLocator         FormFactor  HotSwappable  InstallDate  InterleaveDataDepth  InterleavePosition  Manufacturer  MaxVoltage  MemoryType  MinVoltage  Model  Name      OtherIdentifyingInfo  PartNumber        PositionInRow  PoweredOn  Removable  Replaceable  SerialNumber  SKU  SMBIOSMemoryType  Speed  Status  Tag                TotalWidth  TypeDetail  Version\n\t1           BANK 2     8589934592  物理内存   2400                  1200               Win32_PhysicalMemory  64         物理内存     Bottom-Slot 2(right)   12                                     1                    2                   Samsung       1350        0           1250               物理内存                        M471A1K43CB1-CRC  1                                                 9757621F           26                2400           Physical Memory 1  64          128\n\t\n\twmic:root\\cli>\n```\n","categories":["windows"]},{"title":"git problems encountered","url":"/2021/03/13/technologies/git/git_problems_encountered/","content":"\n## 查看github 贡献最多和增长最快链接:\nhttps://octoverse.github.com/#fastest-growing-oss-projects-by-contributors\n\n<!-- more -->\n## git branch只能看到master分支\n执行`git init`命令后再执行`git clone ...`, 之后执行`git branch -a`, 发现只有master分支, 远程还有其它分支但是看不到.  \n\n解决方法:\n\n```xml\n\t$ vim .git/config\n\t// 添加如下内容\n\t[remote \"origin\"]\n\t\turl = https://***.git\t// 跟git clone 后面跟的地址一样, 也就是项目所在地址.\n\t\tfetch = +refs/heads/*:refs/remotes/origin/*\t\t// 必须加上这一行\n```\n\n之后回到上一层目录, 然后fetch 远程分支\n\n``` shell\n\t$ git fetch origin\n\t输入用户名密码\n\t$ git branch -a\n\t即可查看到远程所有分支\n```\n\n## 解决git bash 终端显示中文乱码\n\n在git bash的界面中右击空白处，弹出菜单，选择选项->文本->本地Locale，设置为zh_CN，而旁边的字符集选框选为UTF-8\n```\n1. git bash打开鼠标右击 -> 选项 -> 文本 -> 本地Locale，设置为zh_CN，而旁边的字符集选框选为UTF-8\n2. `git config --global core.quotepath false` 可以不用重启gitbash终端可解决\n```\n\n补充说明:\n```\ngit config --global core.quotepath false          # 显示 status 编码\ngit config --global gui.encoding utf-8            # 图形界面编码\ngit config --global i18n.commit.encoding utf-8    # 提交信息编码\ngit config --global i18n.logoutputencoding utf-8  # 输出 log 编码\nset LESSCHARSET=utf-8\n```\n另外, **`Local`** 设置为 **`zh_CN`** 编码格式转换成 **`gbk(Chinese)`** 也行\n\n## git push免密登陆方法\n\n1. 创建文件 C:\\Users\\hp\\.git-credentials(Windows系统, 惠普电脑, 其它种类OS和厂商路径类似)\n\n打开并添加内容 https://{username}:{password}@github.com\n\n2.添加git config内容\n\n``` shell\ngit config --global credential.helper store\n```\n\n执行此命令后，用户主目录下的.gitconfig文件会多了一项：[credential]\nhelper = store\n\n重新git push就不需要用户名密码了\n\n## git提示“warning: LF will be replaced by CRLF”\n遇到此问题场景: 在windows上用git提交linux文件, 这是因为在文本处理中，CR（CarriageReturn），LF（LineFeed），CR/LF是不同操作系统上使用的换行符.\n\n * Dos和Windows平台: 使用回车（CR）和换行（LF）两个字符来结束一行，回车+换行(CR+LF)，即“\\r\\n”；\n * Mac 和 Linux平台: 只使用换行（LF）一个字符来结束一行，即“\\n”；\n所以我们平时在windows上编写文件的`回车符`应该确切来说叫做`回车换行符`.\n\n许多 Windows 上的编辑器会悄悄把行尾的换行（LF）字符转换成回车（CR）和换行（LF），或在用户按下 Enter 键时，插入回车（CR）和换行（LF）两个字符.  \n\n**影响：**\n * Unix/Mac系统下的文件在Windows里打开的话，所有文字会变成一行.  \n * 而Windows里的文件在Unix/Mac下打开的话，在每行的结尾可能会多出一个^M符号.  \n * Linux保存的文件在windows上用记事本看的话会出现黑点.  \n通过一定方式进行转换统一:\n\n```\n\t在linux下，命令unix2dos 是把linux文件格式转换成windows文件格式\n\t命令dos2unix 是把windows格式转换成linux文件格式.\n```\n\n**情况一:**\nGit 可以在你提交时自动地把回车(CR)和换行(LF)转换成换行(LF), 而在检出代码时把换行(LF)转换成回车(CR)和换行(LF).  \n如果是在 Windows 系统上，把它设置成 true，这样在检出代码时，换行会被转换成回车和换行.  \n\n**提交时转换为LF，检出时转换为CRLF**\n\n``` shell\n\t$ git config --global core.autocrlf true\n```\n\n**情况二:**\n可以把 core.autocrlf 设置成 input 来告诉 Git 在提交时把回车和换行转换成换行，检出时不转换, 这样在 Windows 上的检出文件中会保留回车和换行，而在 Mac 和 Linux 上，以及版本库中会保留换行.  \n\n**提交时转换为LF，检出时不转换**\n\n``` shell\n\t$ git config --global core.autocrlf input\n```\n\n**情况三:**\n如果你是 Windows 程序员，且正在开发仅运行在 Windows 上的项目，可以设置 false 取消此功能，把回车保留在版本库中.  \n\n**提交检出均不转换**\n\n``` shell\n\t$ git config --global core.autocrlf false\n```\n\n**你也可以在文件提交时进行safecrlf检查**\n\n**拒绝提交包含混合换行符的文件**\n\n``` shell\n\tgit config --global core.safecrlf true   \n```\n\n**允许提交包含混合换行符的文件**\n\n``` shell\n\tgit config --global core.safecrlf false   \n```\n\n**提交包含混合换行符的文件时给出警告**\n\n``` shell\n\tgit config --global core.safecrlf warn\n```\n\n**通俗解释**\n * git 的 Windows 客户端基本都会默认设置 core.autocrlf=true，设置core.autocrlf=true, 只要保持工作区都是纯 CRLF 文件，编辑器用 CRLF 换行，就不会出现警告了.  \n * Linux 最好不要设置 core.autocrlf，因为这个配置算是为 Windows 平台定制.  \n * Windows 上设置 core.autocrlf=false，仓库里也没有配置 .gitattributes，很容易引入 CRLF 或者混合换行符（Mixed Line Endings，一个文件里既有 LF 又有CRLF）到版本库，这样就可能产生各种奇怪的问题.  \n\n\n\n\n\n","tags":["git"],"categories":["technologies","git"]},{"title":"Maven_eclipse_tomcat_JDK1.8_configuration","url":"/2021/03/13/technologies/maven/maven_eclipse_tomcat_JDK1.8_configuration/","content":"\nWindows 环境:\n\n下载Maven，搜集网上配置PATH环境变量等\n$ vim apache-maven-3.6.3\\conf\\settings.xml\n$ C:\\Users\\UserName\\.m2\\settings.xml (没有.m2路径就不设)\n\n<!-- more -->\n\n1. 设置local repository: // maven从中央仓库下载到本地仓库\n\n```xml\n    <localRepository>C:\\Users\\UserName\\***\\software_package\\Maven\\maven_repository</localRepository>\n```\n\n2. 设置proxy:\n\n```xml\n    <proxy>\n       <id>my-proxy1</id>\n       <active>true</active>\n       <protocol>http</protocol>\n       <host>child-prc.intel.com</host>\n       <port>913</port>\n    </proxy>\n    <proxy>\n       <id>my-proxy2</id>\n       <active>true</active>\n       <protocol>https</protocol>\n       <host>child-prc.intel.com</host>\n       <port>913</port>\n    </proxy>\n```\n\n3. 设置tomcat用户名和密码，如果tomcat安装时候或安装后tomcat的配置文件没有设置用户名和密码此处可忽略\n\n```xml\n    <server>\n      <id>tomcat8</id>\n      <username>admin</username>\n      <password>123456</password>\n    </server>\n```\n\n设置aliyun 镜像:\n\n```xml\n  <mirrors>\n    <mirror>\n        <id>nexus-aliyun</id>\n        <mirrorOf>central</mirrorOf>\n        <name>Nexus aliyun</name>\n        <url>http://maven.aliyun.com/nexus/content/repositories/central</url>\n        <!--<url>http://maven.aliyun.com/nexus/content/groups/public</url>-->\n    </mirror>\n  </mirrors>\n```\n\n打开cmd控制台输入：mvn -v 查看版本\n\n```shell\n$ mvn help:system\t\t// 可看到数据正常下载即为成功\n// $ ping repo1.maven.org //此远程repo好像不能访问，不过没关系，上面成功即可\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n安装tomcat服务器\nhttp://tomcat.apache.org/\n然后配置到eclipse\n\n打开C:\\Program Files\\Tomcat 8.5\\conf\\server.xml\n\t网上说需把<Server port=\"-1\" shutdown=\"SHUTDOWN\"> 改为 <Server port=\"8005\" shutdown=\"SHUTDOWN\">\n\t这里没改, 运行OK\n\n打开C:\\Program Files\\Tomcat 8.5\\conf\\tomcat-users.xml\n添加如下内容, 设置tomcat密码，也可以不设置，设置后需要在maven的**\\config\\settings.xml配置文件中也添加tomcat密码\n\n``` xml\n<role rolename=\"manager\"/>\n<role rolename=\"manager-gui\"/>\n<role rolename=\"admin\"/>\n<role rolename=\"admin-gui\"/>\n<role rolename=\"manager-script\"/>\n<user username=\"admin\" password=\"123456\" roles=\"admin-gui,admin,manager-gui,manager,manager-script\"/>\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n官网下载eclipse（免费）\nhttps://www.eclipse.org/downloads/\n第一种: 下载安装包:\n\t直接点击下载，下载后安装选择Eclipse IDE for Enterprise Java Developers就可以了(提前安装配置好JDK1.8等版本环境变量)\n第二种： 下载压缩包(解压缩时候有问题),\n\tDownload Packages -> Eclipse IDE for Enterprise Java Developers (includes Incubating components)压缩包\n\t-> 选择中国镜像 -> 下载\n1. 打开eclipse, Window -> Preferences -> Maven -> installations -> Add -> 选择自己装的Maven，否则内嵌的Maven没有proxy\n2. 打开eclipse, Window -> Preferences -> Maven -> User Settings \n  -> Global/User Settings都设置为 ***/apache-maven-3.6.3\\conf\\settings.xml -> Update Settings -> Apply -> Apply and Close\n\n\n创建 maven web项目并运行:\n1. File -> New -> maven Project -> Create a simple project(skip archetype selection) \n\t-> Next -> 输入Group Id: com.test ,Artifact Id 输入 test_demo, Packaging： war(选择建立web服务此处必须选为war)\n2. 右击项目名 点击最下面Properties -> Maven下面的Project Facets -> 先不勾选Dynamic Web Module，选择右边的Runtimes并选中安装过的tomcat8.5\n\t-> Apply -> Apply and close -> 重新打开上面的页面 -> 选中Dynamic Web Module 并 修改右边 Version 试着选各个版本使支持\n\t-> 点击下面出现的 \"i Furtherconfiguration available...\" -> Conten directory: 内容改为 \"src/main/webapp\" \n\t-> 勾选中下面的Generate web.xml deployment descripter -> Apply -> Apply and Close\n3. 在 src/main/webapp 目录下新建个index.js文件内容如下:\n``` html\n<%@ page language=\"java\" contentType=\"text/html; charset=ISO-8859-1\"\n    pageEncoding=\"ISO-8859-1\"%>\n<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"ISO-8859-1\">\n<title>Insert title here</title>\n</head>\n<body>\n<p>Hello test demo</p>\n</body>\n</html>\n```\n\n4. 右击项目名 -> Run As -> Run on Server， 如果没有Run on Server选项重新打开Project Facets页面再Apply -> Apply and Close\n5. 浏览器输入http://localhost:8080/test_demo/即可看到信息: Hello test demo\n\n创建Parent/jar/web项目:\n\n```\nFile -> New -> Other -> Maven -> Maven Project 按照网上操作即可创建maven pom/jar/war三种项目，其中pom是父类管理其它jar和war(web)等project\n```\n\n> 创建好maven项目后，修改jar或war的pom.xml(项目对象模型(Project Object Modet,POM))文件后\n\t鼠标右击pom.xml -> Maven -> Update Project... -> 可勾选下面的Force Update of Snapshots/Releases -> OK\n\n```xml\n#> eclipse for java ee 创建好maven web项目后会出错，原因是缺少webDemo/src/main/webapp/WEB-INF/web.xml\n#第一种：\t手动创建文件夹WEB-INF和文件web.xml,然后添加如下内容\n#<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n#<web-app version=\"2.5\" xmlns=\"http://java.sun.com/xml/ns/javaee\"\n#\txmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n#\txsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee \n#\thttp://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\">\n#</web-app>\n#\n#第二种: 鼠标右击webDemo -> Java EE Tools -> Generate Deployment Descriptor Stub 即可自动生成上面的web.xml文件\n```\n\n\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\nVScode 配置 Maven 安装环境 创建Maven项目\n\nFile -> Perferences -> Settings -> 搜索Maven -> 点击打开 \"Edit in settings.json\"文件\n\n``` java\n\"remote.SSH.remotePlatform\": {\n        \"10.239.85.244\": \"linux\",\n        \"10.239.65.163\": \"linux\"\n    },\n\n * Windows 添加配置如下内容:\n\"java.home\": \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_241\",\n\"java.configuration.maven.userSettings\": \"C:\\\\Users\\\\UserName\\\\Desktop\\\\software_package\\\\apache-maven-3.6.3\\\\conf\\\\settings.xml\",\n\"maven.executable.path\": \"C:\\\\Users\\\\UserName\\\\Desktop\\\\software_package\\\\apache-maven-3.6.3\\\\bin\\\\mvn\",\n\n * Linux 添加配置如下内容:\n\"java.home\": \"/usr/local/jdk1.8/\",\n\"java.configuration.maven.userSettings\": \"/usr/maven/apache-maven-3.6.3/conf/settings.xml\",\n\"maven.executable.path\": \"/usr/maven/apache-maven-3.6.3/bin/mvn\",\n\n\"maven.terminal.customEnv\": [\n        {\n            \"environmentVariable\": \"JAVA_HOME\",\n            //\"value\": \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_241\"\t\t// Windows\n            \"value\": \"/usr/local/jdk1.8\"\t\t\t\t\t\t\t// Linux\n        }\n    ]\n\n\"remote.SSH.configFile\": \"C:\\\\Users\\\\UserName\\\\.ssh\\\\config\"\n```\n\n\nCtrl + shift + P 选择 \"Maven:Create Maven Project\" 创建 Maven 项目\nSelect an archetype: 选择 maven-archetype-quickstart -> 选择版本 -> 选择生成目录\n-> VScode 终端 输入Group Id: 如com.imooc -> 输入Artifact Id: 如microservice -> 输入package： 直接回车 -> 输入Version: 直接回车\n\n1. 第一此创建Maven 父 项目pom.xml 没有 <packaging>pom</packaging> 需要手动添加, 右击 pom.xml -> Update project configuration\n2. 创建子项目， 右击上面项目名， 选择 Create Maven Project ......'groupId': com.imooc -> 'artifactId': test -> 'version' 1.0-SNAPSHOT: :回车\n3. 创建子项目后查看pom.xml可以发现自动添加了<parent></parent>标签\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n\n\n\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n\n\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n\n\n\n","categories":["technologies","maven"]},{"title":"maven project","url":"/2021/03/13/technologies/maven/maven_project/","content":"\n## Ubuntu18.04 安装 IntelliJ idea\n```\n\thttps://blog.csdn.net/weixx3/article/details/81136822\n```\n\n<!-- more -->\n\n## 手动下载jar包并通过mvn安装到Linux或windows环境，供项目引用\n```\nhttps://repo1.maven.org/maven2/ 或者https://mvnrepository.com/\n```\n\n如在pom.xml添加依赖但是下载不下来\n\n```xml\n\t    <dependency>\n\t      <groupId>org.apache.thrift</groupId>\n\t      <artifactId>libthrift</artifactId>\n\t      <version>0.10.0</version>\n\t    </dependency>\n```\n浏览器连接https://repo1.maven.org/maven2/org/apache/thrift/libthrift/ 查看指定版本如0.10.0, 点击进去下载libthrift-0.10.0.jar\n执行如下命令\n\n```shell\n\tcd /home/ai/IdeaProjects/microservice/user-thrift-service\n\tmvn install:install-file -Dfile=/usr/maven/maven_repository/org/apache/thrift/libthrift/libthrift-0.10.0.jar -DgroupId=org.apache.thrift -DartifactId=libthrift -Dversion=0.10.0 -Dpackaging=jar\n```\n\nMaven 安装 JAR 包的命令是\n\n```shell\n\tmvn install:install-file -Dfile=本地jar包的位置  -DgroupId=上面的groupId  -DartifactId=上面的artifactId  -Dversion=上面的version  -Dpackaging=jar\n```\n\n到maven设置的repository里查看\n\n```shell\n\t$ cd /usr/maven/maven_repository/org/apache/thrift/libthrift#\n\t$ ls -alh\n\tdrwxr-xr-x 2 root root 4.0K 4月   9 10:41 0.10.0\n\t-rw-r--r-- 1 root root  308 4月   9 10:41 maven-metadata-local.xml\n```\n\n## Linux 命令行方式: 搭建，编译，测试，运行calss文件，打包，运行jar包，安装，部署， 清理 命令.\n\n1. 搭建\n\n```shell\n\tmvn archetype:generate  -DinteractiveMode=false -DgroupId=com.HCI -DartifactId=HCI -Dpackage=com.HCI\n```\n\n查看\n\n```xml\n\t<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n\t  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n\t  <modelVersion>4.0.0</modelVersion>\n\t  <groupId>com.HCI</groupId>\n\t  <artifactId>HCI</artifactId>\n\t  <packaging>jar</packaging>\n\t  <version>1.0-SNAPSHOT</version>\n\t  <name>HCI</name>\n\t  <url>http://maven.apache.org</url>\n\t  <dependencies>\n\t    <dependency>\n\t      <groupId>junit</groupId>\n\t      <artifactId>junit</artifactId>\n\t      <version>3.8.1</version>\n\t      <scope>test</scope>\n\t    </dependency>\n\t  </dependencies>\n\t</project>\n```\n\n2. 编译源代码\n\n```shell\n\tmvn compile\n\tmvn compile -Dcheckstyle.skip=true： 不需要进行代码style检查进行编译\n```\nReference Link: https://checkstyle.org/cmdline.html\n\n ![](compile.JPG)\n\n3. 测试编译过的代码\n\n```shell\n\tmvn test\n```\n\n4. 执行编译过的代码\n\n```shell\n\t// App换成自己的执行入口函数(文件)名称.\n\tmvn exec:java -Dexec.mainClass=\"com.HCI.App\"\n```\n\n5. 生成构件包（一般为 jar 包或 war 包）\n\n```shell\n\tmvn package\n```\n![](jar.JPG)\n\n6. 拷贝 jar包\n\n```shell\n\tjava -cp target/HCI-1.0-SNAPSHOT.jar com.HCI.App\n```\n\n7. 将构件包安装到本地仓库\n\n```shell\n\tmvn install\n\tmvn install -DskipTests： 跳过测试, 构建包到本地./target目录\n\tjar xf search-image-by-image-rest-0.0.1-SNAPSHOT.jar: 解压jar包, 查看jar包内容\n```\n\n8. 将构件包部署到远程仓库\n\n```shell\n\tmvn deploy\n```\n\n9. 清空输出目录（即 target 目录）\n\n```shell\n\tmvn clean：\n```\n\n创建Maven项目如HCI后，执行 Maven 其它命令需要注意的是：必须在 Maven 项目的根目录处执行，也就是当前目录下一定存在一个名为 pom.xml 的文件\n\n如进入/home/ai/IdeaProjects/microservice/user-thrift-service目录再执行mvn install......命令\n\n\n## 遇到的问题\n\n### from thrift.Thrift import TType, TMessageType, TFrozenDict, TException, TApplicationException ImportError: cannot import name TFrozenDict\n解决方法是: 某些包没有关联上，装包时加上[hive]后缀\n\n```shell\n\t$ pip install pyhive[hive]\n```\n\n","categories":["technologies","maven"]},{"title":"Ubuntu18.04 安装 Mysql8.0","url":"/2021/03/13/technologies/maven/ubuntu18.04安装Mysql8.0/","content":"\n# Mysql 安装:\n\n### 1. 下载deb包\n```\n\thttps://dev.mysql.com/downloads/repo/apt/\n```\n\n<!-- more -->\n\n### 2. 跟新dpkg\n\n```shell\n\t$ dpkg -i mysql-apt-config_0.8.15-1_all.deb\n\t$ apt update\n```\n\n### 3. 安装mysql8\n\n```shell\n\t$ apt install mysql-server\n\t输入密码123456\n\t最后加密方式选择Legacy(5.x)\n```\n\n# Mysql 卸载:\n\n```\n搜索的一种卸载方式:\n首先在终端中查看MySQL的依赖项：dpkg --list|grep mysql\n卸载： sudo apt-get remove mysql-common\n卸载：sudo apt-get autoremove --purge mysql-server-5.7\n清除残留数据：dpkg -l|grep ^rc|awk ‘{print$2}’|sudo xargs dpkg -P\n再次查看MySQL的剩余依赖项：dpkg --list|grep mysql\n继续删除剩余依赖项，如：sudo apt-get autoremove --purge mysql-apt-config\n至此已经没有了MySQL的依赖项，彻底删除，Good Luck\n\n另外一种卸载方式:\nsudo apt-get autoremove --purge mysql-server \nsudo apt-get remove mysql-common\nsudo rm -rf /etc/mysql/ \nsudo rm -rf  /var/lib/mysql\n​​#清理残留数据\ndpkg -l |grep ^rc|awk '{print $2}' |sudo xargs dpkg -P  \nsudo apt autoremove\nsudo apt autoclean\n\n最终是用dpkg --list|grep mysql命令查看没有任何mysql信息输出即可\n```\n\n# Mysql登录\n\n### 第一种命令行方式:\n\n``` shell\nmysql -uroot -p123456\n```\n### 第二种mysql-workbench\n\n```shell\napt update\napt install mysql-workbench\nmysql-workbench\t\t// 可以通过键入 mysql-workbench或单击 MySQL Workbench 图标 (Activities -> MySQL Workbench) 从命令行启动它。\n```\n\n### 第三种Navigat 工具方式：\n - Navicat是可以管理多种数据库Mysql, redis, MongoDB等等的软件，收费\n * 连接名:localhost\n * 主机: 127.0.0.1\t\t// 用localhost 会报错 2002 - Can't connect to local MySQL server through socket '/var/lib/mysql/mysql.sock'(2 \"No such file or directory\")\n * 端口: 3306\t\t\t// mysql安装后默认服务端口是3306， 可通过命令 \"netstat -tap | grep mysql\" 查看\n * 用户名: root\n * 密码: 123456\n \n# 重启Mysql server\n\n```shell\nsystemctl restart mysql\t// Ubuntu18.04重启mysql会出错, 目前没解决，只是卸载mysql重装，最好别重启mysql服务\n```\n","categories":["technologies","maven"]},{"title":"ECDHE-RSA-AES256-GCM-SHA384 解释","url":"/2021/03/13/technologies/security/ECDHE-RSA-AES256-GCM-SHA384解释/","content":"\n## ECDHE-RSA-AES256-GCM-SHA384 解释\n\n“密钥交换算法 + 签名算法 + 对称加密算法 + 摘要算法”\n\n“握手时使用 ECDHE 算法进行密钥交换，用 RSA 签名和身份认证，握手后的通信使用 AES 对称算法，密钥长度 256 位，分组模式是 GCM，摘要算法 SHA384 用于消息认证和产生随机数。”\n\n<!-- more -->\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"algorithm security","url":"/2021/03/13/technologies/security/algorithm_security/","content":"\n# **Certificates Encryption Algorithm**\n\n## **RSA非对称加密算法**\nRSA(Rivest-Shamir-Adleman)\n> RSA是1977年由罗纳德·李维斯特（Ron Rivest）、阿迪·萨莫尔（Adi Shamir）和伦纳德·阿德曼（Leonard Adleman）一起提出的。当时他们三人都在麻省理工学院工作。RSA就是他们三人姓氏开头字母拼在一起组成的  \n\n非对称加密，公钥加密，私钥解密，反之亦然。由于需要大数的乘幂求模等算法，运行速度慢，不易于硬件实现。\n\n通常私钥长度有512bit，1024bit，2048bit，4096bit，长度越长，越安全，但是生成密钥越慢，加解密也越耗时。\n\n既然是加密，那肯定是不希望别人知道我的消息，所以只有我才能解密，所以可得出公钥负责加密，私钥负责解密；\n\n同理，既然是签名，那肯定是不希望有人冒充我发消息，只有我才能发布这个签名，所以可得出私钥负责签名，公钥负责验证。\n\n> 非对称加密算法实现机密信息交换的基本过程是：甲方生成一对密钥并将其中的一把作为公用密钥向其它方公开(用CA private key签名得到甲方证书)；用CA的public key解密得到该公用密钥的乙方使用该密钥对机密信息进行加密后再发送给甲方；甲方再用自己保存的另一把专用密钥对加密后的信息进行解密来获取信息.\n\n## **AES对称加密算法**\nAES,高级加密标准（Advanced Encryption Standard，缩写：AES）  \n\n对称加密，密钥最长只有256个bit，执行速度快，易于硬件实现。由于是对称加密，密钥需要在传输前通讯双方获知。\n\n基于以上特点，通常使用RSA来首先传输AES的密钥给对方，然后再使用AES来进行加密通讯。\n\nAES128和AES256主要区别是密钥长度不同（分别是128bits,256bits)、加密处理轮数不同（分别是10轮，14轮），后者强度高于前者。当前AES是较为安全的公认的对称加密算法。  \n\n现代密码学分为对称加密与非对称加密（公钥加密），代表算法分别有DES(现在发展为3DES）、AES与RSA等。非对称加密算法的资源消耗大于对称加密。一般是进行混合加密处理，例如使用RSA进行密钥分发、协商，使用AES进行业务数据的加解密。  \n\n# Signature Algorithm\n\n## **什么是SHA算法**\n> SHA的全称是“Secure Hash Algorithm”，中文翻译为：安全哈希算法，是由美国NSA和NIST两个组织共同发布的一系列密码散列函数，经历了SHA-0，SHA-1，SHA-2，SHA-3系列发展.\n\n### 什么是SHA-256算法\n> SHA256算法属于SHA-2系列，SHA-256对于任意长度的消息，都会产生一个256bit长的哈希值，称作消息摘要. 这个摘要相当于是个长度为32个字节的数组，通常用一个长度为64的十六进制字符串来表示.\n\n### 把消息转换为位字符串\nSHA-256算法是按照位作为输入，所以进行计算前必须把原始消息（比如字符串、文件等）转换成位字符串。\n比如，对字符串“abc”产生消息摘要，‘a’=97 ‘b’=98 ‘c’=99，先转换成24位的字符串：\n\n```\n\t 01100001 01100010 01100011\n```\n### 对转换得到的位字符串进行补位操作\n\n### 消息扩展、分组处理\n\n### 使用的常量和函数\n\n### 计算消息摘要\n\n### SHA-256安全性分析\nHash函数的安全性很大程度上取决于抗强碰撞的能力，即攻击者找出两个消息M和Mt，M≠Mt，使得H(M)=HMt  ,因此，评价一个Hash函数的安全性，就是看攻击者在现有的条件下，是否可以找到该函数的一对碰撞。目前已有的对Hash函数攻击的方法包括生日攻击、彩虹表攻击、差分攻击等。\n\n * 生日攻击：生日攻击是一种可用于攻击任何类型函数Hash函数的攻击方法。从攻击原理上看，它没有利用Hash函数的结构和任何代数弱性质，只依赖与Hash值的长度。因此，**抵御生日攻击最有效的方法是Hash值必须有足够的长度**。\n * 差分攻击：差分攻击是目前破译迭代Hash函数最有效的手法之一，其基本方法是利用明文的输入差值对输出差值的影响，运用差分的高概率的继承或者消除来产生最终的相同输出。\n\n用于消息唯一性和数据完整性验证的Hash函数，其安全性依赖于函数本身的属性和对抗碰撞的抵抗。Hash函数的算法结构特点和Hash值的长度是决定函数碰撞性的而主要因素，Hash值越长，越能抵御生日攻击。**SHA-256有256比特Hash值，MD5和SHA-1分别有128和160比特的Hash值。**因此，SHA-256比MD5和SHA-1能抵抗生日攻击。通过对Chabaud-Joux攻击SHA-256的分析，找到了SHA-256的一个部分碰撞，其复杂度为2^66，但无法找到SHA-256的一个整体碰撞，因此SHA-256算法也能抵御现有的差分攻击。由此可见，在抵御生日攻击和抵御已知差分攻击方面，SHA-256算法比现在广泛使用的MD5和SHA-1等更具安全性。\n\n### 比特币为什么选择SHA-256算法\n> SHA-256属于SHA-2系列，像之前的SHA0，SHA1都被证明是可以破解的，目前SHA2以及SHA3尚未被证实可以破解，至少目前来说是最安全的算法之一.\n> 未来即使使用量子计算机挖比特币，也无非是速度更快一点，比特币有难度调整机制，可以通过调整难度来对抗量子计算机，还可以通过升级算法到SHA-3系列来增加挖矿的难度.\n> 中本聪在设计比特币时，之所以选择SHA256，就是看中了SHA256的安全性，只要输入的数据有微小的区别，算出的结果就有天壤之别.\n\n# **AES-GCM加密算法**\n\n# 查看系统支持的密码列表\n\n**列出当前系统所支持的密码套件列表**\n\n```shell\n\t$ openssl ciphers -V 'ALL:COMPLEMENTOFALL'\n```\n**测试某个服务器是否支持特定的密码套件：**\n\n```shell\n\t$ openssl s_client -cipher \"ECDHE-RSA-AES128-SHA\" -connect www.qq.com:443 -tls1_1\n```\n参数说明: \n\n * -cipher 参数表示本次连接支持的密码套件\n * -connect 表示连接服务器的 443 端口\n * -tls1_1 表示客户端最高支持的 TLS/SSL 版本是 TLS v1.1\n\nTLS deployment, viw version etc.[Security/Server Side TLS](https://wiki.mozilla.org/Security/Server_Side_TLS)\n\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"cfssl","url":"/2021/03/13/technologies/security/cfssl/","content":"\n## Kubernetes 证书\n\n| 组件 | 使用的证书 |\n| :------: | :------: |\n| etcd | ca.pem, server.pem, server-key.pem |\n| kube-apiserver | ca.pem, server.pem, server-key.pem |\n| kubelet | ca.pem, ca-key.pem |\n| kube-proxy | ca.pem, kube-proxy.pem, kube-proxy-key.pem |\n| kubectl | ca.pem, admin.pem, admin-key.pem |\n\n## cfssl\n\ncfssl用来生成证书比openssl要简单直观些\n\n\n## cfssl安装\n安装cfssl相关的三个工具: \n\n```shell\n\t// 生成证书\n\t$ wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64\n\t// 用于将json文本导入生成证书\n\t$ wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64\n\t// 查看证书相关信息\n\t$ wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64\n\t$ chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 cfssl-certinfo_linux-amd64\n\t$ mv cfssl_linux-amd64 /usr/local/bin/cfssl\n\t$ mv cfssljson_linux-amd64 /usr/local/bin/cfssljson\n\t$ mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo\n\t$ cfssl --help\n```\n\n## cfssl 生成证书\n生成证书模板\n\n``` xml\n\t$ cfssl print-defaults config > config.json\n\t{\n\t    \"signing\": {\n\t        \"default\": {\n\t            \"expiry\": \"168h\"\n\t        },\n\t        \"profiles\": {\n\t            \"www\": {\n\t                \"expiry\": \"8760h\",\n\t                \"usages\": [\n\t                    \"signing\",\n\t                    \"key encipherment\",\n\t                    \"server auth\"\n\t                ]\n\t            },\n\t            \"client\": {\n\t                \"expiry\": \"8760h\",\n\t                \"usages\": [\n\t                    \"signing\",\n\t                    \"key encipherment\",\n\t                    \"client auth\"\n\t                ]\n\t            }\n\t        }\n\t    }\n\t}\n```\n\n生成证书请求模板\n\n```xml\n\t$ cfssl print-defaults csr > csr.json\n\t{\n\t    \"CN\": \"example.net\",\n\t    \"hosts\": [\n\t        \"example.net\",\n\t        \"www.example.net\"\n\t    ],\n\t    \"key\": {\n\t        \"algo\": \"ecdsa\",\n\t        \"size\": 256\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"US\",\n\t            \"L\": \"CA\",\n\t            \"ST\": \"San Francisco\"\n\t        }\n\t    ]\n\t}\n```\n\n## **k8s生成组件证书实例**\n所有k8s证书,配置,安装包都放到/opt/kubernetes/目录下\n\n```shell\n\tmkdir -p /opt/kubernetes/{ssl,cfg,bin}\n\tls /opt/kubernetes/\n\tbin/  cfg/  ssl/\n```\n\n### 1. 生成ca-key.pem私匙和ca.pem证书\n\n```xml\n\tcat > ca-config.json << EOF\n\t{\n\t    \"signing\": {\n\t        \"default\": {\n\t            \"expiry\": \"87600h\"\n\t        },\n\t        \"profiles\": {\n\t            \"kubernetes\": {\n\t                \"expiry\": \"87600h\",\n\t                \"usages\": [\n\t                    \"signing\",\n\t                    \"key encipherment\",\n\t                    \"server auth\"\n\t                ]\n\t            }\n\t        }\n\t    }\n\t}\n\tEOF\n\n\tcat > ca-csr.json << EOF\n\t{\n\t    \"CN\": \"kubernetes\",\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\t\t\t// 注意是algo而不是also\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\t\t\t// 哪个国家的可以随便写\n\t            \"L\": \"Beijing\",\t\t// 地点随便写\n\t            \"ST\": \"Beijing\",\t// 地点随便写\n\t            \"O\": \"k8s\",\t\t\t// 用户组, 固定的， 不能随便写\n\t            \"OU\": \"System\"\t\t// 用户, 固定的，不能随便写\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n\n生成ca.pem和ca-key.pem\n\n```shell\n\t// -bare ca 表示生成以ca开头的证书和key\n\tcfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\tls ca*\n\tca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem\n```\n\n### 2. 生成server端证书和key, 用于k8s的etcd和kube-apiserver\n\n```xml\n\tcat > server-csr.json << EOF\n\t{\n\t    \"CN\": \"kubernetes\",\n\t    \"hosts\": [\t\t\t// 包含了哪些机器IP和域名需要此server端证书\n\t        \"127.0.0.1\",\n\t        \"10.239.140.133\",\t\t// 要使用改证书的etcd或其他服务部署所在节点IP地址或域名\n\t        \"10.239.131.206\",\n\t        \"10.239.141.145\",\n\t        \"10.239.141.194\",\n\t        \"kubernetes.default\",\n\t        \"kubernetes.default.svc\",\n\t        \"kubernetes.default.svc.cluster\",\n\t        \"kubernetes.default.svc.cluster.local\"\n\t    ],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"k8s\",\t\t\t//和下面一起代表了用户和组去请求集群\n\t            \"OU\": \"System\"\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n\n生成server端证书\n\n```shell\n\t// -bare server 表示生成以server开头的证书和key\n\tcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server\n\tls server*\n\tserver.csr  server-csr.json  server-key.pem  server.pem\n```\n\n### 3. 生成admin证书, 集群管理员通过证书访问集群\n\n```xml\n\tcat > admin-csr.json << EOF\n\t{\n\t    \"CN\": \"admin\",\n\t    \"hosts\": [],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"system:masters\",\t// 用户组, 不要改动, 否则会认证失败\n\t            \"OU\": \"System\"\t\t\t// 用户\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n\n生成管理员证书和key\n\n```shell\n\t// -bare admin 表示生成以admin开头的证书和key\n\t$ cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin\n\t$ ls admin*\n\tadmin.csr  admin-csr.json  admin-key.pem  admin.pem\n```\n\n### 4. 生成kube-proxy证书\n工作节点通过kube-proxy组件访问api-server生成一些网络策略, 必须得有权限, 生成一个证书, 让kube-proxy携带证书去访问集群.\n\n```xml\n\tcat > kube-proxy-csr.json << EOF\n\t{\n\t    \"CN\": \"system:kube-proxy\",\t\t//固定,不能改变\n\t    \"hosts\": [],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"k8s\",\n\t            \"OU\": \"System\"\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n生成kube-proxy证书和key\n\n```shell\n\tcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy\n\tls kube-proxy*\n\tkube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem\n```\n\n### 5. 只保留*.pem, 删除其它文件\n\n```shell\n\tls | grep -v pem | xargs -i rm {}\n\tls\n\tadmin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  server-key.pem  server.pem\n```\n\n## **关闭防火墙或开发端口**\n### 关闭防火墙\n\n```shell\n\tsetenforce 0\n\tsystemctl stop firewalld.service\n\tsysctl net.bridge.bridge-nf-call-iptables=1\n```\n\n### 如果使用firewalld作为防火墙，则需要开放端口\n\n```shell\n\tfirewall-cmd --zone=public --add-port=2379/tcp --permanent\n\tfirewall-cmd --zone=public --add-port=2380/tcp --permanent\n\tfirewall-cmd --reload\n\tfirewall-cmd --list-all\n```\n\n## **etcd安装**\netcd是由coreos公司开发在GitHub上开源的存储键值对的数据库\n\n### etcd 下载\n本次测试安装etcd的3.2.12版本\nhttps://github.com/etcd-io/etcd/releases/tag/v3.2.12\n下载解压并移动到指定目录\n\n```shell\n\twget -L https://github.com/etcd-io/etcd/releases/download/v3.2.12/etcd-v3.2.12-linux-amd64.tar.gz\n\ttar -zxvf etcd-v3.2.12-linux-amd64.tar.gz\n\tls etcd-v3.2.12-linux-amd64\n\tDocumentation/  etcd  etcdctl  README-etcdctl.md  README.md  READMEv2-etcdctl.md\n\t// 将解压后文件中的可执行文件etct和etcdctl移动到/opt/kubernetes/bin/目录下\n\tmv etcd-v3.2.12-linux-amd64/etcd /opt/kubernetes/bin/\n\tmv etcd-v3.2.12-linux-amd64/etcdctl /opt/kubernetes/bin/\n```\n\n### 创建ETCD的配置文件\n\n```shell\n\tcat > /opt/kubernetes/cfg/etcd << EOF\n\t#[Member]\n\tETCD_NAME=\"etcd01\"\n\tETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n\tETCD_LISTEN_PEER_URLS=\"https://10.239.140.133:2380\"\n\tETCD_LISTEN_CLIENT_URLS=\"https://10.239.133:2379\"\n\t\n\t#[Clustering]\n\tETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.239.140.133:2380\"\n\tETCD_ADVERTISE_CLIENT_URLS=\"https://10.239.140.133:2379\"\n\tETCD_INITIAL_CLUSTER=\"etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380\"\n\tETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\n\tETCD_INITIAL_CLUSTER_STATE=\"new\"\n\tEOF\n```\n\n### 使用systemd来管理etcd服务\n\n```xml\n\tcat > /usr/lib/systemd/system/etcd.service << EOF\n\t[Unit]\t\t\t\t\t//systemd依赖的一些服务, 网络服务启动之后再启动此服务\n\tDescription=Etcd Server\n\tAfter=network.target\n\tAfter=network-online.target\n\tWants=network-online.target\n\t\n\t[Service]\n\tType=notify\n\tWorkingDirectory=/var/lib/etcd/\t\t\t\t\t//看网上配置有这个参数, 自己配置过程中没有加入\n\tEnvironmentFile=-/opt/kubernetes/cfg/etcd\t\t//指定服务启动配置文件\n\tExecStart=/opt/kubernetes/bin/etcd \\\t\t\t//服务启动选项\n\t--name=${ETCD_NAME} \\\n\t--data-dir=${ETCD_DATA_DIR} \\\n\t--listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \\\n\t--listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \\\n\t--advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \\\n\t--initial-advertise-peer-urls=${ETCD_INITIAL_ADVERTISE_PEER_URLS} \\\n\t--initial-cluster=${ETCD_INITIAL_CLUSTER} \\\n\t--initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \\\n\t--initial-cluster-state=new \\\n\t--cert-file=/opt/kubernetes/ssl/server.pem \\\t\t// 指定数字证书\n\t--key-file=/opt/kubernetes/ssl/server-key.pem \\\n\t--peer-cert-file=/opt/kubernetes/ssl/server.pem \\\n\t--peer-key-file=/opt/kubernetes/ssl/server-key.pem \\\n\t--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\\n\t--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n\tRestart=on-failure\n\tLimitNOFILE=65536\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\tEOF\n```\n\n启动etcd服务\n\n```shell\n\t// 启动etcd服务发现卡住, 且过一会会报个错误, 原因是另外request另外两台etcd服务2380得不到响应\n\t// 待另外至少一台etcd服务启动后再回来查看etcd服务状态就正常了\n\tsystemctl daemon-reload\n\tsystemctl start etcd\n\tsystemctl enable etcd\n\t// 完整的启动etcd服务命令其实就是运行以下命令:\n\t/opt/kubernetes/bin/etcd --name=\"etcd01\" --data-dir=\"/var/lib/etcd/default.etcd\" --listen-peer-urls=\"https://10.239.140.133:2380\" --listen-client-urls=\"https://10.239.140.133:2379,http://127.0.0.1:2379\" --advertise-client-urls=\"https://10.239.140.133:2379\" --initial-advertise-peer-urls=\"https://10.239.140.133:2380\" --initial-cluster=\"etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380\" --initial-cluster-token=\"etcd-cluster\" --initial-cluster-state=\"new\" --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n```\n\n查看服务\n\n```shell\n\t$ ps -ef | grep etcd \n\troot     23726     1  5 21:41 ?        00:00:00 /opt/kubernetes/bin/etcd --name=etcd01 --data-dir=/var/lib/etcd/default.etcd --listen-peer-urls=https://10.239.140.133:2380 --listen-client-urls=https://10.239.140.133:2379,http://127.0.0.1:2379 --advertise-client-urls=https://10.239.140.133:2379 --initial-advertise-peer-urls=https://10.239.140.133:2380 --initial-cluster=etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380 --initial-cluster-token=etcd-cluster --initial-cluster-state=new --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n\troot     23744 14957  0 21:41 pts/1    00:00:00 grep --color=auto etcd\n\t// 查看etcd服务状态和日志发现etcd一直尝试request其它两台机器, 状态不正常, 在其它两台也部署etcd后就可以了\n\t$ systemctl status etcd\n\t$ tail /var/log/messages\n```\n\n### etcd拷贝到其它机器\n\n```shell\n\t$ scp -r /opt/kubernetes root@10.239.131.206:/opt\n\t$ scp /usr/lib/systemd/system/etcd.service  root@10.239.131.206:/usr/lib/systemd/system/etcd.service\n\t// 只需要修改其它机器上/opt/kubernetes/cfg/etcd文件里的ETCD_NAME和其它参数IP地址然后就可以直接运行命令启动etcd服务.\n\t// 并且启动后不会卡住\n\t$ systemctl start etcd\n\t$ systemctl enable etcd\n```\n\n### 测试etcd集群状态是否正常\n\n```shell\n\t/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --endpoints=\"https://10.239.140.133:2379,https://10.239.131.206:2379,https://10.239.141.145:2379\" cluster-health\n\tmember 723f8ab932b4c3f6 is healthy: got healthy result from https://10.239.141.145:2379\n\tmember 8af8f5fa5f0b0b39 is healthy: got healthy result from https://10.239.131.206:2379\n\tmember 91dd39fb14e3de97 is healthy: got healthy result from https://10.239.140.133:2379\n\tcluster is healthy\n```\n\n## **遇到的问题**\n查看Centos7系统日子\n\n```shell\n\t$ cat /var/log/messages\n\t$ systemctl status etcd.service\n```\n出现以下错误:\n```\n\tetcd.service: main process exited, code=exited, status=2/INVALIDARGUMENT\n```\n很明显是运行etcd命令时候的参数错误, 改对参数就可以了.\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"openssl","url":"/2021/03/13/technologies/security/openssl/","content":"\n## openssl版本\n```\nopenssl version\nopenssl version -a\n```\n\n<!-- more -->\n\n## 支持的cipher\n```\nopenssl ciphers -v\nopenssl ciphers -V 'ALL:COMPLEMENTOFALL'\n```\n\n## 查看key\n\n```\nopenssl genrsa -out root-key.pem 3072\nopenssl rsa -in root-key.pem -text -noout\n```\n\n## 生成csr配置文件\n\nroot-ca.conf\n```\n[ req ]\nencrypt_key = no\nprompt = no\nutf8 = yes\ndefault_md = sha384\ndefault_bits = 3072\nreq_extensions = req_ext\nx509_extensions = req_ext\ndistinguished_name = req_dn\n[ req_ext ]\nsubjectKeyIdentifier = hash\nbasicConstraints = critical, CA:true\nkeyUsage = critical, digitalSignature, nonRepudiation, keyEncipherment, keyCertSign\n[ req_dn ]\nO = Istio\nCN = Root CA\n```\n\n## 查看csr\n\n```\nopenssl req -new -sha384 -key root-key.pem -config root-ca.conf -out root-cert.csr\nopenssl req -in root-cert.csr -noout -text\n```\n\n## 查看证书\n\n```\nopenssl x509 -req -sha384 -days 3650 -signkey root-key.pem \\\n        -extensions req_ext -extfile root-ca.conf \\\n        -in root-cert.csr -out root-cert.pem\n\nopenssl x509 -in root-cert.pem -text -noout\n```\n\n \n## 测试某个服务器是否支持特定的密码套件\n执行如下命令会提示 **`CONNECTED`** 说明服务器支持此密码套件\n```\n$ openssl s_client -connect hci-node01:30007 -cipher ECDHE-RSA-AES256-GCM-SHA384\nCONNECTED(00000003)\ndepth=2 O = Istio, CN = Root CA\nverify error:num=19:self signed certificate in certificate chain\nverify return:1\ndepth=2 O = Istio, CN = Root CA\nverify return:1\ndepth=1 O = Istio, CN = Intermediate CA, L = cluster1\nverify return:1\ndepth=0\nverify return:1\nDONE\n---\n......\n```\n\n## 获取证书\n```\n$ openssl s_client -showcerts -connect hci-node01:30007 > httpbin-proxy-cert.txt\n```\n\n## 验证证书\nverify命令对证书的有效性进行验证，verify 指令会沿着证书链一直向上验证，直到一个自签名的CA.\n\n```\nopenssl verify -CAfile <(cat Intermediate.pem RootCert.pem) UserCert.pem\n```\n\n**`语法`**\n```\nopenssl verify[-CApath directory] [-CAfile file] [-purpose purpose] [-policy arg] [-ignore_critical] [-crl_check] [-crl_check_all] [-policy_check] [-explicit_policy] [-inhibit_any] [-inhibit_map] [-x509_strict] [-extended_crl] [-use_deltas] [-policy_print] [-untrusted file] [-help] [-issuer_checks] [-verbose] [-] [certificates]\n```\n\n * -CAfile  filename    指定CA的证书文件，PEM格式，这个文件里可能不只包含一个证书。如果需要对证书链进行验证，指定的文件中应包含所有的证书。加入顶级CA证书文件名为0.pem，一级CA证书文件为1.pem，二级证书文件为2.pem，待验证的证书文件是eve.pem，那么需要先将0.pem，1.pem证书文件的内容包含到2.pem中。证书文件都是文本文件，简单地使用cat命令就可以进行连接.\n\n * -CApath directory     指定CA证书所在的目录，这个目录下可能存在证书链中的多个证书文件。为了对这个目录下的证书进行检索，证书文件的命名需要遵循xxxxxxxx.0，其中xxxxxxxx是openssl x509 -hash -in 证书， 的输出值，8个字母或数字。“.0”是要有的.\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"jwt jwks","url":"/2021/03/13/technologies/security/jwt_jwks/","content":"\nreferencd:\nhttps://docs.aws.amazon.com/zh_cn/cognito/latest/developerguide/amazon-cognito-user-pools-using-tokens-verifying-a-jwt.html\nhttps://blog.unosquare.com/why-and-how-to-improve-jwt-security-with-jwks-key-rotation-in-java\n\n<!-- more -->\n\n## jwt\n\n一个 JSON Web 令牌 (JWT) 包含三个部分：\n\n1. 标头\n\n2. 负载\n\n3. 签名\n\n```\n11111111111.22222222222.33333333333\n```\n这些部分编码为base64url字符串,并用点 **`(.)`** 字符分隔。如果您的JWT不符合此结构, 则视为无效, 不接受.  \n\n## jwks\n这是一个样本 jwks.json 文件:\n```\n{\n\t\"keys\": [{\n\t\t\"kid\": \"1234example=\",\n\t\t\"alg\": \"RS256\",\n\t\t\"kty\": \"RSA\",\n\t\t\"e\": \"AQAB\",\n\t\t\"n\": \"1234567890\",\n\t\t\"use\": \"sig\"\n\t}, {\n\t\t\"kid\": \"5678example=\",\n\t\t\"alg\": \"RS256\",\n\t\t\"kty\": \"RSA\",\n\t\t\"e\": \"AQAB\",\n\t\t\"n\": \"987654321\",\n\t\t\"use\": \"sig\"\n\t}]\n}\n```\n\n**1. 密钥ID kid(Key ID)**\n的 kid 是一个提示,指示用于保护令牌的JSONWeb签名(JWS)的密钥。\n\n**2. 算法alg(encryption algorithm)**\n的 alg 标头参数表示用于保护ID令牌的密码算法。用户池使用 RS256 加密算法，这是一种采用 SHA-256 的 RSA 签名。有关 RSA 的更多信息，请参阅 RSA 密码术。\n\n更多算法参考[specification](https://tools.ietf.org/html/rfc7518#section-3.3)\n\n| \"alg\" Param Value | Digital Signature Algorithm |\n| :-----: | :-----: |\n| RS256   | RSASSA-PKCS1-v1_5 using SHA-256 |\n| RS384   | RSASSA-PKCS1-v1_5 using SHA-384 |\n| RS512   | RSASSA-PKCS1-v1_5 using SHA-512 |\n\n**3. 密钥类型kty(Key Type)**\nkty 参数标识与密钥结合使用的加密算法系列，例如，在本示例中为“RSA”。\n\n**4. RSA指数(e)**\n e 参数包含RSA公钥的指数值。它表示为采用 Base64urlUInt 编码的值。\n e Parameter is used to define the RSA public exponent.\n\n**5. RSA模量(n)**\n n 参数包含RSA公钥的模量值。它表示为采用 Base64urlUInt 编码的值.  \n n Parameter is used to define the modulus for both the public and private keys. Its length, usually expressed in bits, is the key length.\n\n**6. 使用use(Public Key Use)**\n的 use 参数描述公钥的预期用途。对于本示例, use 价值 sig 表示签名。\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"Token jwt","url":"/2021/03/13/technologies/security/token_jwt/","content":"\n# Reference\n * [csdn blog](https://www.cnblogs.com/MyCode1990/p/13096423.html)\n * [简书](https://www.jianshu.com/p/4941a269a9d8)\n\n# **Json Web Token(JWT)简介**\nJWT属于Token验证方式的一种方法.\n> JSON Web Token（JWT）是一个开放标准（RFC 7519），它定义了一种紧凑的、自包含的方式，用于作为JSON对象在各方之间安全地传输信息。此信息可以验证和信任，因为它是数字签名的。JWTs可以使用密钥（使用HMAC算法）或使用RSA或ECDSA的公钥/私钥对进行签名.\n\n# **传统服务端验证客户端身份的方法**\nHTTP 是一种没有状态的协议，也就是它并不知道是谁是访问应用。这里我们把用户看成是客户端，客户端使用用户名还有密码通过了身份验证，不过下回这个客户端再发送请求时候，还得再验证一下。\n\n解决的方法就是，当用户请求登录的时候，如果没有问题，我们在服务端生成一条记录，这个记录里可以说明一下登录的用户是谁，然后把这条记录的 ID 号发送给客户端，客户端收到以后把这个 ID 号存储在 Cookie 里，下次这个用户再向服务端发送请求的时候，可以带着这个 Cookie ，这样服务端会验证一个这个 Cookie 里的信息，看看能不能在服务端这里找到对应的记录，如果可以，说明用户已经通过了身份验证，就把用户请求的数据返回给客户端。\n\n上面说的就是 Session，我们需要在服务端存储为登录的用户生成的 Session ，这些 Session 可能会存储在内存，磁盘，或者数据库里。我们可能需要在服务端定期的去清理过期的 Session 。\n\n# **基于 Token 的身份验证方法**\nToken是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。  \nToken是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。  \n使用基于 Token 的身份验证方法，在服务端不需要存储用户的登录记录。大概的流程是这样的：\n 1. 客户端使用用户名跟密码请求登录\n 2. 服务端收到请求，去验证用户名与密码\n 3. 验证成功后，服务端会签发一个 Token，再把这个 Token 发送给客户端\n 4. 客户端收到 Token 以后可以把它存储起来，比如放在 Cookie 里或者 Local Storage 里\n 5. 客户端每次向服务端请求资源的时候需要带着服务端签发的 Token\n 6. 服务端收到请求，然后去验证客户端请求里面带着的 Token，如果验证成功，就向客户端返回请求的数据\n\n# **为什么要使用JSON Web Token**\n * Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮.\n * JSON比XML不那么冗长，当它被编码时，它的大小也更小，使得JWT比SAML更紧凑。这使得JWT成为在HTML和HTTP环境中传递的一个很好的选择。\n * 安全方面，使用HMAC算法，SWT只能由共享密钥对称签名。但是，JWT和SAML令牌可以使用X.509证书形式的公钥/私钥对进行签名。与签名JSON的简单性相比，使用XML数字签名来签名XML而不引入隐藏的安全漏洞是非常困难的。\n * JSON解析器在大多数编程语言中都很常见，因为它们直接映射到对象。相反，XML没有自然的文档到对象的映射。这使得使用JWT比使用SAML断言更容易。\n * 在使用方面，JWT是在互联网上使用的。这突出了JSON Web令牌在多个平台（尤其是移动平台）上客户端处理的方便性。\n * 比较编码JWT和编码SAML的长度.\n\n# **JWT(Json Web Token 一种Token验证方法)**\n\n实施 Token 验证的方法挺多的，还有一些标准方法，比如 JWT，读作：jot ，表示：JSON Web Tokens 。JWT 标准的 Token 有三个部分：\n\n * header（头部）\n * payload（荷载,数据）\n * signature（签名）\n中间用点分隔开，并且都会使用 Base64 编码，所以真正的 Token 看起来像这样：\n\n```\n\teyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJuaW5naGFvLm5ldCIsImV4cCI6IjE0Mzg5NTU0NDUiLCJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlfQ.SwyHTEx_RQppr97g4J5lKXtabJecpejuef8AqKYMAJc\n```\n\n## **Header 头部**\n * 令牌的类型\n * 正在使用的签名算法(HMAC, SHA256, RSA等)。\n\n每个 JWT token 里面都有一个 header，也就是头部数据。里面包含了使用的算法，这个 JWT 是不是带签名的或者加密的。主要就是说明一下怎么处理这个 JWT token 。  \n头部里包含的东西可能会根据 JWT 的类型有所变化，比如一个加密的 JWT 里面要包含使用的加密的算法。唯一在头部里面要包含的是 alg 这个属性，如果是加密的 JWT，这个属性的值就是使用的签名或者解密用的算法。如果是未加密的 JWT，这个属性的值要设置成 none。  \n示例：\n\n```js\n\t{\n\t  \"typ\": \"JWT\",\n\t  \"alg\": \"HS256\"\n\t}\n```\n意思是这个 JWT 用的算法是 HS256。上面的内容得用 base64url 的形式编码一下，所以就变成这样：\n\n```\n\teyJhbGciOiJIUzI1NiJ9\n```\n## **Payload 有效负载**\n有效负载包含了\"声明(claims)\", 有三种类型的claims：\n * registered claims 已注册的 (不是强制的，而是推荐,iss（发行者）、exp（到期时间）、sub（主题）、aud（受众）和其他)\n * public claims 公开的 (可以由使用JWT的用户随意定义, 为了避免冲突，应该在[IANA JSON Web令牌注册表](https://www.iana.org/assignments/jwt/jwt.xhtml)中定义它们，或者将它们定义为包含防冲突命名空间的URI)\n * private claims 私有的\n\nPayload 里面是 Token 的具体内容，这些内容里面有一些是标准字段，你也可以添加其它需要的内容。下面是标准字段：\n * iss：Issuer，发行者\n * sub：Subject，主题\n * aud：Audience，观众\n * exp：Expiration time，过期时间\n * nbf：Not before\n * iat：Issued at，发行时间\n * jti：JWT ID\n\n比如下面这个 Payload ，用到了 iss 发行人，还有 exp 过期时间这两个标准字段。另外还有两个自定义的字段，一个是 name ，还有一个是 admin 。\n\n```js\n\t{\n\t  \"sub\": \"1234567890\",\n\t  \"name\": \"John Doe\",\n\t  \"iss\": \"ninghao.net\",\n\t  \"exp\": \"1438955445\",\n\t  \"admin\": true\n\t}\n```\n使用 base64url 编码以后就变成了这个样子：\n\n```\n\t eyJpc3MiOiJuaW5naGFvLm5ldCIsImV4cCI6IjE0Mzg5NTU0NDUiLCJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlfQ\n```\n**请注意，对于已签名的令牌，此信息虽然受保护不受篡改，但任何人都可以读取。除非经过加密，否则不要将机密信息放在JWT的有效负载或头部**.\n\n## **Signature 签名**\n要创建签名部分，您必须已经有了 经过编码的**头部**、经过编码的**负载**、一个**秘钥**、在头部中指定的**算法**，这样就可以进行签名了.  \nSignature这部分内容有三个部分，先是用 Base64 编码的 header.payload ，再用加密算法加密一下，加密的时候要放进去一个 Secret ，这个相当于是一个密码，这个密码秘密地存储在服务端。  \n\n * header\n * payload\n * secret\n\n```\n\tconst encodedString = base64UrlEncode(header) + \".\" + base64UrlEncode(payload); \n\tHMACSHA256(encodedString, 'secret');\n```\n处理完成以后看起来像这样：\n\n```\n\t SwyHTEx_RQppr97g4J5lKXtabJecpejuef8AqKYMAJc\n```\n最后这个在服务端生成并且要发送给客户端的 Token 看起来像这样：\n\n```\n\t eyJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJuaW5naGFvLm5ldCIsImV4cCI6IjE0Mzg5NTU0NDUiLCJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlfQ.SwyHTEx_RQppr97g4J5lKXtabJecpejuef8AqKYMAJc\n```\n输出的内容是三个由点分隔的Base64 URL字符串。它可以在HTML和HTTP环境中轻松传递，它比XML的标准（如SAML）更加紧凑.  \n客户端收到这个 Token 以后把它存储下来，下回向服务端发送请求的时候就带着这个 Token 。服务端收到这个 Token ，然后进行验证，通过以后就会返回给客户端想要的资源。\n\n## **签名的作用:**  \n(1) 签名用于验证消息在传输过程中没有被更改。\n(2) 使用私钥签名的令牌，还可以验证JWT的发送者是它所说的发送者。\n\n# **签发 Json Web Token(JWT)**\n * JWT 对 “信息” 进行签名，产生一个令牌。\n * 签名的令牌可以验证其中包含的内容的完整性（防篡改）。\n * 也可对“信息”加密，加密的令牌则对其他方隐藏这些内容。\n * 当令牌使用公钥/私钥对签名时，签名还证明只有持有私钥的一方才是签名方。可以非对称加密方式证明了\n\n\n## **HS256 算法签发Json Web Token(JWT)**\n这种算法需要一个密钥（密码）.  \n在项目里随便添加一个 .js 文件，比如 index.js，在文件里添加下面这些代码：  \n\n``` js\n\tconst jwt = require('jsonwebtoken')\n\t// Token 数据\n\tconst payload = {\n\tname: 'wanghao',\n\tadmin: true\n\t}\n\t// 密钥\n\tconst secret = 'ILOVENINGHAO'\n\t// 签发 Token\n\tconst token = jwt.sign(payload, secret, { expiresIn: '1day' })\n\t// 输出签发的 Token\n\tconsole.log(token)\n```\n非常简单，就是用了刚刚为项目安装的 jsonwebtoken 里面提供的 jwt.sign 功能，去签发一个 token。这个 sign 方法需要三个参数：\n * playload：签发的 token 里面要包含的一些数据。\n * secret：签发 token 用的密钥，在验证 token 的时候同样需要用到这个密钥。\n * options：一些其它的选项。\n\n在命令行下面，用 node 命令，执行一下项目里的 index.js 这个文件（node index.js），会输出应用签发的 token：\n\n```\n\teyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlLCJpYXQiOjE1MjkwMzM5MDYsImV4cCI6MTUyOTEyMDMwNn0.DctA2QlUCrM6wLWkIO78wBVN0NLpjoIq4T5B_2WJ-PU\n```\n上面的 Token 内容并没有加密，所以如果用一些 JWT 解码功能，可以看到 Token 里面包含的内容，内容由三个部分组成，像这样：\n\n```\n\t// header\n\t{\n\t  \"alg\": \"HS256\", \n\t  \"typ\": \"JWT\"\n\t}\n\t// payload\n\t{\n\t  \"admin\": true, \n\t  \"iat\": 1529033906, \n\t  \"name\": \"wanghao\", \n\t  \"exp\": 1529120306\n\t}\n\t// signature\n\tDctA2QlUCrM6wLWkIO78wBVN0NLpjoIq4T5B_2WJ-PU\n```\n\n> 假设用户通过了某种身份验证，你就可以使用上面的签发 Token 的功能为用户签发一个 Token。一般在客户端那里会把它保存在 Cookie 或 LocalStorage 里面。\n> 用户下次向我们的应用请求受保护的资源的时候，可以在请求里带着我们给它签发的这个 Token，后端应用收到请求，检查签名，如果验证通过确定这个 Token 是我们自己签发的，那就可以为用户响应回他需要的资源。\n\n## **RS256 算法签发Json Web Token(JWT)**\n默认签发还有验证 Token 的时候用的是 HS256 算法，这种算法需要一个密钥（密码）。我们还可以使用 RS256 算法签发与验证 JWT。这种方法可以让我们分离开签发与验证，签发时需要用一个密钥，验证时使用公钥，也就是有公钥的地方只能做验证，但不能签发 JWT。\n\n在项目下面创建一个新的目录，里面可以存储即将生成的密钥与公钥文件。\n\n```shell\n\t$ cd ~/desktop/jwt-demo\n\t$ mkdir config\n\t$ cd config\n```\n\n**密钥:**\n先生成一个密钥文件：\n\n```shell\n\t ssh-keygen -t rsa -b 2048 -f private.key\n```\n**公钥:**\n基于上面生成的密钥，再去创建一个对应的公钥：\n\n```shell\n\t openssl rsa -in private.key -pubout -outform PEM -out public.key\n```\n**签发 JWT（RS256 算法）**\n用 RS256 算法签发 JWT 的时候，需要从文件系统上读取创建的密钥文件里的内容。\n\n```js\n\t const fs = require('fs')\n\t // 获取签发 JWT 时需要用的密钥\n\t const privateKey = fs.readFileSync('./config/private.key')\n\t 签发仍然使用 jwt.sign 方法，只不过在选项参数里特别说明一下使用的算法是 RS256：\n\t // 签发 Token\n\t const tokenRS256 = jwt.sign(payload, privateKey, { algorithm: 'RS256' })\n\t // 输出签发的 Token\n\t console.log('RS256 算法：', tokenRS256)\n```\n\n# **验证Json Web Token(JWT)**\n\n## **HS256 算法验证Json Web TOken(JWT)**\n需要一个签发token时候用的密钥（密码）来验证,这种算法需要一个密钥（密码）.  \n验证 JWT 的用效性，确定一下用户的 JWT 是我们自己签发的，首先要得到用户的这个 JWT Token，然后用 jwt.verify 这个方法去做一下验证。这个方法是 Node.js 的 jsonwebtoken 这个包里提供的，在其它的应用框架或者系统里，你可能会找到类似的方法来验证 JWT。  \n打开项目的 index.js 文件，里面添加几行代码：\n\n``` js\n\t// 验证 Token\n\tjwt.verify(token, 'bad secret', (error, decoded) => {\n\t  if (error) {\n\t    console.log(error.message)\n\t    return\n\t  }\n\t  console.log(decoded)\n\t})\n```\n\n把要验证的 Token 数据，还有签发这个 Token 的时候用的那个密钥告诉 verify 这个方法，在一个回调里面有两个参数，error 表示错误，decoded 是解码之后的 Token 数据。\n\n执行：\n\n```shell\n\t$ node ~/desktop/jwt-demo/index.js\n```\n输出：\n\n```\n\teyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlLCJpYXQiOjE1MjkwMzQ3MzMsImV4cCI6MTUyOTEyMTEzM30.swXojmu7VimFu3BoIgAxxpmm2J05dvD0HT3yu10vuqU\n\tinvalid signature\n```\n注意输出了一个 invalid signature ，表示 Token 里的签名不对，这是因为我们组长 verify 方法提供的密钥并不是签发 Token 的时候用的那个密钥。这样修改一下：\n\n```shell\n\tjwt.verify(token, secret, (error, decoded) => { ...\n```\n再次运行，会输出类似的数据：\n\n```\n\teyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoid2FuZ2hhbyIsImFkbWluIjp0cnVlLCJpYXQiOjE1MjkwMzUzODYsImV4cCI6MTUyOTEyMTc4Nn0.mkNrt4TfcfmP22xd3C_GQn8qnUmlB39dKT9SpIBTBGI\n\t{ name: 'wanghao', admin: true, iat: 1529035386, exp: 1529121786 }\n```\n\n## **RS256 算法验证Json Web Token(JWT)**\n验证 JWT（RS256 算法）\n验证使用 RS256 算法签发的 JWT，需要在文件系统上读取公钥文件里的内容。然后用 jwt 的 verify 方法去做验证。\n\n```js\n\t// 获取验证 JWT 时需要用的公钥\n\tconst publicKey = fs.readFileSync('./config/public.key')\n\t\n\t// 验证 Token\n\tjwt.verify(tokenRS256, publicKey, (error, decoded) => {\n\t  if (error) {\n\t    console.log(error.message)\n\t    return\n\t  }\n\t  console.log(decoded)\n\t})\n```\n\n# **使用场景**\n![](token_scene1.png)\n\n1. 应用程序或客户端，向授权服务器请求授权。\n2. 当授权被通过时，授权服务器将向应用程序返回一个访问令牌token。\n3. 应用程序使用访问令牌访问受保护的资源。\n**请注意，使用签名的令牌，令牌中包含的所有信息都将公开给用户或其他方（虽然他们无法更改它，但可以阅读）。这意味着您不应将机密信息放入令牌中.**\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"Ceph 02 deployment on Kubernetes","url":"/2021/03/13/storage/ceph/ceph_02_deployment/","content":"\nThis document describes the steps to enable Ceph cluster in Kubernetes(k8s)  \n\n## **Prerequisites**\n1. A running Kubernetes environmen.  \n\n\n## **Setup steps**\n1. The first step is to deploy the Rook operator. Check that you are using the example yaml files that correspond to your release of Rook.  \n\n```shell\n\t$ git clone --single-branch --branch release-1.3 https://github.com/rook/rook.git\n\t$ cd cluster/examples/kubernetes/ceph\n\t$ kubectl create -f common.yaml\n\t$ kubectl create -f operator.yaml\n\t\n\tVerify the rook-ceph-operator is in the Running state before proceeding\n\t$ kubectl -n rook-ceph get pod\n```\n\n2. Now that the Rook operator is running we can create the Ceph cluster. For the cluster to survive reboots, make sure you set the dataDirHostPath property that is valid for your hosts.  \n\n```shell\n\t$ kubectl create -f cluster.yaml\n```\n\n3. Use kubectl to list pods in the rook-ceph namespace. You should be able to see the following pods once they are all running. The number of osd pods will depend on the number of nodes in the cluster and the number of devices configured. If you did not modify the cluster.yaml above, it is expected that one OSD will be created per node. The CSI, rook-ceph-agent, and rook-discover pods are also optional depending on your settings.  \n\n```shell\n\t$ kubectl -n rook-ceph get pod\n\t  NAME                                                 READY   STATUS      RESTARTS   AGE\n\t  csi-cephfsplugin-provisioner-d77bb49c6-n5tgs         5/5     Running     0          140s\n\t  csi-cephfsplugin-provisioner-d77bb49c6-v9rvn         5/5     Running     0          140s\n\t  csi-cephfsplugin-rthrp                               3/3     Running     0          140s\n\t  csi-rbdplugin-hbsm7                                  3/3     Running     0          140s\n\t  csi-rbdplugin-provisioner-5b5cd64fd-nvk6c            6/6     Running     0          140s\n\t  csi-rbdplugin-provisioner-5b5cd64fd-q7bxl            6/6     Running     0          140s\n\t  rook-ceph-agent-4zkg8                                1/1     Running     0          140s\n\t  rook-ceph-crashcollector-minikube-5b57b7c5d4-hfldl   1/1     Running     0          105s\n\t  rook-ceph-mgr-a-64cd7cdf54-j8b5p                     1/1     Running     0          77s\n\t  rook-ceph-mon-a-694bb7987d-fp9w7                     1/1     Running     0          105s\n\t  rook-ceph-mon-b-856fdd5cb9-5h2qk                     1/1     Running     0          94s\n\t  rook-ceph-mon-c-57545897fc-j576h                     1/1     Running     0          85s\n\t  rook-ceph-operator-85f5b946bd-s8grz                  1/1     Running     0          92m\n\t  rook-ceph-osd-0-6bb747b6c5-lnvb6                     1/1     Running     0          23s\n\t  rook-ceph-osd-1-7f67f9646d-44p7v                     1/1     Running     0          24s\n\t  rook-ceph-osd-2-6cd4b776ff-v4d68                     1/1     Running     0          25s\n\t  rook-ceph-osd-prepare-node1-vx2rz                    0/2     Completed   0          60s\n\t  rook-ceph-osd-prepare-node2-ab3fd                    0/2     Completed   0          60s\n\t  rook-ceph-osd-prepare-node3-w4xyz                    0/2     Completed   0          60s\n\t  rook-discover-dhkb8                                  1/1     Running     0          140s\n```\n\n4. To verify that the cluster is in a healthy state, connect to the Rook toolbox and run the ceph status command.  \n\n* All mons should be in quorum  \n* A mgr should be active  \n* At least one OSD should be active  \n* If the health is not HEALTH_OK, the warnings or errors should be investigated.  \n\nRunning the Toolbox in Kubernetes:  \n\n```shell\n\t$ kubectl create -f toolbox.yaml\n```\n\nWait for the toolbox pod to download its container and get to the running state:\n\n```shell\n\t$ kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\"\n```\n\nOnce the rook-ceph-tools pod is running, you can connect to it with:\n\n```shell\n\t$ kubectl -n rook-ceph exec -it $(kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath='{.items[0].metadata.name}') bash\n```\n\nExample:\n\n```shell\n\t$ ceph status\n\t  cluster:\n\t    id:     a0452c76-30d9-4c1a-a948-5d8405f19a7c\n\t    health: HEALTH_OK\n\t\n\t  services:\n\t    mon: 3 daemons, quorum a,b,c (age 3m)\n\t    mgr: a(active, since 2m)\n\t    osd: 3 osds: 3 up (since 1m), 3 in (since 1m)\n\t...\n```\n\nWhen you are done with the toolbox, you can remove the deployment, but if you want continue to doing other test experiments below, please keep the toolbox:\n\n```shell\n\t$ kubectl -n rook-ceph delete deployment rook-ceph-tools\n```\n\n## **Object Store**\n### 1. Create an Object Store\n\n```shell\n\t$ cd cluster/examples/kubernetes/ceph\n\t// Create the object store\n\t$ kubectl create -f object.yaml\n\t// To confirm the object store is configured, wait for the rgw pod to start\n\t$ kubectl -n rook-ceph get pod -l app=rook-ceph-rgw\n```\n\n### 2. Define a storage class that will allow object clients to create a bucket.\n\n```shell\n\t// Define the storage class that will allow object clients to create a bucket\n\t$ kubectl create -f storageclass-bucket-delete.yaml\n```\n\n### 3.1 Create a Bucket\nAn Object Bucket Claim (OBC) is custom resource which requests a bucket (new or existing) and is described by a Custom Resource Definition (CRD).\n\n```shell\n\t// Create an Object Bucket Claim (OBC) \n\t$ kubectl create -f object-bucket-claim-delete.yaml\n```\n\nWhen the OBC is created, the Rook-Ceph bucket provisioner will create a new bucket.A `secret` and `ConfigMap` are created with the same name as the OBC and in the same namespace. The secret contains credentials used by the application pod to access the bucket. The ConfigMap contains bucket endpoint information and is also consumed by the pod. \n\n```shell\n\t// Check the created bucket name, \"ceph-bkt-5124df41-7939-4aa6-b989-2bff0aa38deb\" as shown below.\n\t$ kubectl describe obc ceph-delete-bucket\n\t  ......\n\t  Spec:\n\t    Object Bucket Name:    obc-default-ceph-delete-bucket\n\t    Bucket Name:           ceph-bkt-5124df41-7939-4aa6-b989-2bff0aa38deb\n\t    Generate Bucket Name:  ceph-bkt\n\t    Storage Class Name:    rook-ceph-delete-bucket\n\t  Status:\n\t    Phase:  Bound\t\t// [Notice]: The Status must be Bound, if `Pending`, please wait or delete and recreate the obc.\n\t  Events:   <none>\n```\nAn Object Bucket (OB) is a custom resource automatically generated when a bucket is provisioned. It is a global resource, typically not visible to non-admin users, and contains information specific to the bucket. It is described by an OB CRD.\n\n```shell\n\t// Check the OB\n\t$ kubectl get ob\n\t  NAME                             AGE\n\t  obc-default-ceph-delete-bucket   5m16s\n```\n\n### 3.2 Create another Bucket\nChange the ObjectBucketClaim metadata.name and create another bucket.\n\n```shell\n\t$ vim object-bucket-claim-delete.yaml\n\t  apiVersion: objectbucket.io/v1alpha1\n\t  kind: ObjectBucketClaim\n\t  metadata:\n\t    name: ceph-bucket-new\t\t# Change bucket name\n\t  spec:\n\t    generateBucketName: ceph-bkt\n\t    storageClassName: rook-ceph-bucket\n\t$ kubectl create -f object-bucket-claim-delete.yaml\n```\n\n**Client Connections**\nThe following commands extract key pieces of information from the secret and configmap:\n\n```shell\n\t// config-map, secret, OBC will part of default if no specific name space mentioned\n\t$ export AWS_HOST=$(kubectl -n default get cm ceph-delete-bucket -o yaml | grep BUCKET_HOST | awk '{print $2}' | awk 'NR==1')\n\t$ export AWS_ACCESS_KEY_ID=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_ACCESS_KEY_ID | awk '{print $2}' | awk 'NR==1'| base64 --decode)\n\t$ export AWS_SECRET_ACCESS_KEY=$(kubectl -n default get secret ceph-delete-bucket -o yaml | grep AWS_SECRET_ACCESS_KEY | awk '{print $2}' | awk 'NR==1'| base64 --decode)\n```\n\n### 4. Consume the Object Storage\n**Enter into the toolbox**\n\n```shell\n\tkubectl exec $(kubectl get po -l app=rook-ceph-tools -n rook-ceph -o jsonpath='{.items[0].metadata.name}') -it -n rook-ceph -- bash\n```\n\nConnected to the Rook toolbox Pod and then set the four environment variables for use by your client(ie. inside the toolbox).\n\n```shell\n\t$ export AWS_HOST=<host>\n\t$ export AWS_ENDPOINT=<endpoint>\n\t$ export AWS_ACCESS_KEY_ID=<accessKey>\n\t$ export AWS_SECRET_ACCESS_KEY=<secretKey>\n```\n\nEndpoint: The endpoint where the rgw service is listening. Run ```kubectl -n rook-ceph get svc rook-ceph-rgw-my-store```, then combine the clusterIP and the port.\n\n**Install s3cmd**\n\n```shell\n\t$ echo proxy=http://Proxy-Name:913 >> /etc/yum.conf\n\t$ yum --assumeyes install s3cmd\n```\n**PUT or GET an object**\nUpload a file to the newly created bucket\n\n```shell\n\t$ echo \"Hello Rook\" > /tmp/rookObj\n\t$ s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://<-Your-Bucket-Name-> // As follow.\n\t// $ s3cmd put /tmp/rookObj --no-ssl --host=${AWS_HOST} --host-bucket=  s3://ceph-bkt-5124df41-7939-4aa6-b989-2bff0aa38deb\n\t  upload: '/tmp/rookObj' -> 's3://ceph-bkt-5124df41-7939-4aa6-b989-2bff0aa38deb/rookObj'  [1 of 1]\n\t   11 of 11   100% in    0s   190.11 B/s  done\n```\n\nDownload and verify the file from the bucket\n\n```shell\n\t$ s3cmd get s3://<-Your-Bucket-Name->/rookObj /tmp/rookObj-download --no-ssl --host=${AWS_HOST} --host-bucket=\n\t  download: 's3://ceph-bkt-5124df41-7939-4aa6-b989-2bff0aa38deb/rookObj' -> '/tmp/rookObj-download'  [1 of 1]\n\t   11 of 11   100% in    0s   254.34 B/s  done\n\t$ cat /tmp/rookObj-download\n```\nList the buckets\n\n```shell\n\t$ s3cmd ls --no-ssl --host=${AWS_HOST} --host-bucket=\n```\n\n## Teardown\n1. First you will need to clean up the resources created on top of the Rook cluster.  \n\n```shell\n\t$ kubectl delete -n rook-ceph cephblockpool replicapool\n\t$ kubectl delete storageclass rook-ceph-block\n\t$ kubectl delete -f csi/cephfs/kube-registry.yaml\n\t$ kubectl delete storageclass csi-cephfs\n```\n2. Delete the CephCluster CRD\n\n```shell\n\t$ kubectl -n rook-ceph delete cephcluster rook-ceph\n```\n3. Delete the Operator and related Resources\n\n```shell\n\t$ kubectl delete -f operator.yaml\n\t$ kubectl delete -f common.yaml\n```\n4. 删掉未删除的crd, 如果以前创建过Object bucket的话.\n\n```shell\n\t$ kubectl -n rook-ceph patch crd objectbuckets.objectbucket.io --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\n\t$ kubectl -n rook-ceph patch crd objectbucketclaims.objectbucket.io --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\n```\n5. /var/lib/rook: Path on each host in the cluster where configuration is cached by the ceph mons and osds. so need to clean the files in the path.\n\n```shell\n\t$ rm -rf /var/lib/rook/\n```\nAdditional: If there are ceph related files in the \"/var/lib/kubelet/plugins/\" and \"/var/lib/kubelet/plugins_registry/\" path, delete them.\n\n```shell\n\t$ rm -rf /var/lib/kubelet/plugins/*\n\t$ rm -rf /var/lib/kubelet/plugins_registry/*\n```\n6. Delete the data on hosts\n\n```shell\n\t#!/usr/bin/env bash\n\tDISK=\"/dev/sdb\"\n\t# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)\n\t# You will have to run this step for all disks.\n\tsgdisk --zap-all $DISK\n\tdd if=/dev/zero of=\"$DISK\" bs=1M count=100 oflag=direct,dsync\n\t\n\t# These steps only have to be run once on each node\n\t# If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.\n\tls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %\n\t# ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter)\n\trm -rf /dev/ceph-*\n```\n## FAQ\nIf the cluster resource still exists even though you have executed the delete command earlier, see the command below remove the finalizer.\n\n```shell\n\t$ kubectl -n rook-ceph patch crd cephclusters.ceph.rook.io --type merge -p '{\"metadata\":{\"finalizers\": [null]}}'\n```\n\n## 遇到的问题\n删除其它机器pvc绑定的/var/lib/kubelet/plugins/*出错\n\n```shell\n\t$ rm -rf /var/lib/kubelet/plugins/*\n\trm: cannot remove ‘/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-d6ea6990-2a0b-4f4b-9838-3a971424732d/globalmount’: Device or resource busy\n\trm: cannot remove ‘/var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-a08fed0b-b16d-4a74-a052-7936d6fb8340/globalmount’: Device or resource busy\n```\n解决方法:\n\n```shell\n\t$ umount /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-d6ea6990-2a0b-4f4b-9838-3a971424732d/globalmount\n\t$ umount /var/lib/kubelet/plugins/kubernetes.io/csi/pv/pvc-a08fed0b-b16d-4a74-a052-7936d6fb8340/globalmount\n```\n再进行删除即可\n\n","tags":["storage"],"categories":["storage","ceph"]},{"title":"MinIO 04 client SDK","url":"/2021/03/13/storage/minio/minIO_04_client_SDK/","content":"\n## Reference\n\n参考下载并生成AWS s3 SDK\n[Setting Up the AWS SDK for C++](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/setup.html)\n\n[source code](https://github.com/aws/aws-sdk-cpp)\n[SDK_usage_guide.md](https://github.com/aws/aws-sdk-cpp/blob/master/Docs/SDK_usage_guide.md)\n[Using CMake Exports with the AWS SDK for C++](https://aws.amazon.com/blogs/developer/using-cmake-exports-with-the-aws-sdk-for-c/)\n[AWS SDK for C++ Developer Guide](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/welcome.html)\n[Creating, Listing, and Deleting Buckets](https://docs.aws.amazon.com/sdk-for-cpp/v1/developer-guide/examples-s3-buckets.html)\n[AWS Code Sample Catalog](https://docs.aws.amazon.com/code-samples/latest/catalog/welcome.html)\n[aws-doc-sdk-examples](https://github.com/awsdocs/aws-doc-sdk-examples/tree/master/cpp/example_code/s3)\n\n## **Samples**\n\n### **c++ SDK**\n\ngcc/g++ 4.9+\n\n```shell\n\t$ g++ -v\n\t......\n\tgcc version 8.3.1 20190311 (Red Hat 8.3.1-3) (GCC)\n```\n\n安装cmake\n\n```shell\n\t$ cmake --version\n\tcmake version 3.17.0\n\tCMake suite maintained and supported by Kitware (kitware.com/cmake).\n```\n安装依赖库openssl & curl\n\n```shell\n\t$ yum install openssl-devel libcurl-devel\n```\n升级curl\n\n```shell\n\t$ rpm -Uvh https://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/l/libmetalink-0.1.3-1.el7.x86_64.rpm\n\t$ yum --showduplicates list curl --disablerepo=\"*\" --enablerepo=\"city*\"\n\t$ curl.x86_64                                            7.72.0-2.0.cf.rhel7                                             @city-fan.org\n\t......\n\tAvailable Packages\n\tcurl.x86_64                                            7.72.0-2.0.cf.rhel7                                             city-fan.org\n```\n\n#### Download aws sdk\n\n```shell\n\t$ wget https://github.com/aws/aws-sdk-cpp/archive/1.8.55.tar.gz\n\t$ tar -zxvf 1.8.55.tar.gz\n\t$ mkdir build_dir\n\t$ cd build_dir\n\t(编译全部, 会耗费一个小时左右时间, 推荐使用以下只编译S3 service)$ cmake /home/***/aws-sdk-cpp-1.8.55/ -DCMAKE_BUILD_TYPE=Debug\n\t$ cmake /home/***/aws-sdk-cpp-1.8.55/ -D CMAKE_BUILD_TYPE=[Debug | Release] -D BUILD_ONLY=\"s3\"\n\tmake\n\tmake install // 安装头文件等到系统/usr/local/include目录\n```\n\n#### 开发SDK\n拷贝aws sdk的头文件和动态库到项目目录\n\n```shell\n\t// 拷贝整个aws头文件到项目目录\n\tcp /usr/local/include/aws ***/cpp/include/\n\t\n\t// 拷贝aws两个动态库到项目目录\n\tcp /usr/local/lib64/libaws-cpp-sdk-core.so ***/cpp/libs/\n\tcp /usr/local/lib64/libaws-cpp-sdk-s3.so ***/cpp/libs/\n```\n\n查看项目目录\n\n```shell\n\t$ ls cpp\n\tCMakeLists.txt  include/  libs/  main/  src/\n\n\t$ tree cpp\n\tcpp\n\t├── CMakeLists.txt\n\t├── include\n\t│   ├── aws\n\t│   │   ├── core\n\t│   │   │   ├── Aws.h\n\t│   │   │   └── ....h\n\t│   │   ├── s3\n\t│   │   │   ├── model\n\t│   │   │   │   ├── Bucket.h\n\t│   │   │   │   ├── CreateBucketRequest.h\n\t│   │   │   │   └── ....h\n\t│   │   │   ├── S3Client.h\n\t│   │   │   └── ....h\n\t│   │   └── utils\n\t│   │       ├── StringUtils.h\n\t│   │       ├── UUID.h\n\t│   │       └── ....h\n\t│   └── minio_client_sdk.h\n\t├── libs\n\t│   ├── libaws-cpp-sdk-core.so\n\t│   ├── libaws-cpp-sdk-s3.so\n\t│   └── libminio_client_SDK.so\n\t├── main\n\t│   ├── CMakeLists.txt\n\t│   └── main.cpp\n\t└── src\n\t    ├── CMakeLists.txt\n\t    └── minio_client_sdk.cpp\n```\n\n#### **cpp/CMakeLists.txt**\n\n```\n\tCMAKE_MINIMUM_REQUIRED(VERSION 3.14)\n\t\n\tproject(minio_SDK)\n\t\n\t#头文件查找优先级高于系统默认目录/usr/include和/usr/local/include\n\tINCLUDE_DIRECTORIES(${PROJECT_SOURCE_DIR}/include)\n\t\n\tADD_SUBDIRECTORY(${PROJECT_SOURCE_DIR}/src)\n\t\n\tadd_subdirectory(${PROJECT_SOURCE_DIR}/main)\n\t\n\t#MESSAGE(STATUS ${SRC_LIST})\n```\n\n#### **cpp/main**\n\n**cpp/main/CMakeLists.txt**\n\n```\n\t#头文件搜索目录,PROJECT_SOURCE_DIR为cmake预定义变量，项目的顶层目录\n\tinclude_directories(${PROJECT_SOURCE_DIR}/include)\n\t\n\t#链接库搜索目录\n\t#它相当于g++命令的-L选项的作用，也相当于环境变量中增加LD_LIBRARY_PATH的路径的作用\n\tlink_directories(${PROJECT_SOURCE_DIR}/libs)\n\t\n\t#重新定义目标二进制可执行文件的存放位置\n\tset(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/build/bin)\n\t\n\tadd_executable(main main.cpp) #生成.cpp文件的可执行文件\n\t\n\t#指定多个链接库\n\ttarget_link_libraries(main minio_client_SDK aws-cpp-sdk-core aws-cpp-sdk-s3)\n```\n\n**cpp/main/main.cpp**\n\n```cpp\n\t#include <iostream>\n\t#include <aws/s3/S3Client.h>\n\t#include <minio_client_sdk.h>\n\tusing namespace std;\n\t\n\tint main(int argc, char *argv[])\n\t{\n\t    string minio_server_address = \"127.0.0.1:30007\";\n\t    string access_key = \"hceminio\";\n\t    string access_secret = \"hceminio123\";\n\t    Aws::SDKOptions m_options;\n\t    S3Client *m_client = access_minio_server(minio_server_address, m_options, access_key, access_secret);\n\t\n\t    string uuid = random_UUID();\n\t    cout << \"UUID: \" << uuid << endl;\n\t    delete_Bucket(m_client, \"bucket04\");\n\t    list_Buckets(m_client);\n\t    create_Bucket(m_client, \"bucket04\");\n\t\n\t    list_Objects(m_client, \"bucket01\");\n\t    delete_Object(m_client, \"bucket01\", \"pic_object1\");\n\t    download_Object(m_client, \"bucket01\", \"clean.sh\", \"t1.txt\");\n\t    upload_Object(m_client, \"bucket01\", \"pic_object4\", \"tt.txt\");\n\t\n\t    close_minio_server(m_client, m_options);\n\t\n\t    return 0;\n\t}\n```\n\n#### **cpp/src**\n**cpp/src/CMakeLists.txt**\n\n```\n\t#指定头文件搜索目录，若顶层CMakeLists.txt也指定了文件搜索目录，则该处可以省略\n\t#头文件查找优先级高于系统默认目录/usr/include和/usr/local/include\n\tinclude_directories(${PROJECT_SOURCE_DIR}/include)\n\t\n\tFILE(GLOB SRC_LIST \"${PROJECT_SOURCE_DIR}/src/*.cpp\")\n\t\n\t#重新定义目标链接库文件的存放位置\n\tSET(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/libs)\n\t\n\t#将SRC_LIST变量中的所有.cpp文件编译生成库名为add的静态库\n\t#SHARED为生成动态库, STATIC为生成静态库.\n\tADD_LIBRARY(minio_client_SDK SHARED ${SRC_LIST})\n```\n\n**cpp/src/minio_client_sdk.cpp**\n\n```cpp\n\t#define MINIO_CLIENT_SDK\n\t#include <iostream>\n\t#include <fstream>\n\t\n\t#include <aws/s3/S3Client.h>\n\t#include <aws/core/Aws.h>\n\t#include <aws/core/auth/AWSCredentialsProvider.h>\n\t#include <aws/core/utils/StringUtils.h>\n\t#include <aws/core/utils/UUID.h>\n\t\n\tusing namespace std;\n\tusing namespace Aws::S3;\n\tusing namespace Aws::S3::Model;\n\t\n\t#include <aws/s3/model/Bucket.h>\n\t#include <aws/s3/model/CreateBucketRequest.h>\n\t#include <aws/s3/model/DeleteBucketRequest.h>\n\t#include <aws/s3/model/DeleteObjectRequest.h>\n\t#include <aws/s3/model/GetObjectRequest.h>\n\t#include <aws/s3/model/ListObjectsRequest.h>\n\t#include <aws/s3/model/Object.h>\n\t#include <aws/s3/model/PutObjectRequest.h>\n\t\n\tS3Client *access_minio_server(string minio_server_address, Aws::SDKOptions m_options, string access_key, string access_secret)\n\t{\n\t    S3Client *m_client = {NULL};\n\t\n\t    Aws::InitAPI(m_options);\n\t    Aws::Client::ClientConfiguration cfg;\n\t    cfg.endpointOverride = minio_server_address.c_str();\n\t    cfg.scheme = Aws::Http::Scheme::HTTP;\n\t    cfg.verifySSL = false;\n\t    m_client = new S3Client(Aws::Auth::AWSCredentials(access_key.c_str(), access_secret.c_str()), cfg, Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, false, Aws::S3::US_EAST_1_REGIONAL_ENDPOINT_OPTION::NOT_SET);\n\t    return m_client;\n\t}\n\t\n\tbool close_minio_server(S3Client *m_client, Aws::SDKOptions m_options)\n\t{\n\t    if (m_client != nullptr)\n\t    {\n\t        delete m_client;\n\t        m_client = NULL;\n\t    }\n\t    Aws::ShutdownAPI(m_options);\n\t\n\t    return true;\n\t}\n\t\n\tstring random_UUID()\n\t{\n\t    Aws::String random_uuid = Aws::Utils::UUID::RandomUUID();\n\t    Aws::String uuid = Aws::Utils::StringUtils::ToLower(random_uuid.c_str());\n\t    return uuid.c_str();\n\t}\n\t\n\tbool list_Buckets(S3Client *m_client)\n\t{\n\t    Aws::S3::Model::ListBucketsOutcome outcome = m_client->ListBuckets();\n\t    if (outcome.IsSuccess())\n\t    {\n\t        std::cout << \"Bucket names:\" << std::endl\n\t                  << std::endl;\n\t        Aws::Vector<Aws::S3::Model::Bucket> buckets = outcome.GetResult().GetBuckets();\n\t        for (Aws::S3::Model::Bucket &bucket : buckets)\n\t        {\n\t            std::cout << bucket.GetName() << std::endl;\n\t        }\n\t        return true;\n\t    }\n\t    else\n\t    {\n\t        std::cout << \"Error: ListBuckets: \" << outcome.GetError().GetMessage() << std::endl;\n\t\n\t        return false;\n\t    }\n\t}\n\t\n\tbool create_Bucket(S3Client *m_client, string bucketName)\n\t{\n\t    Aws::S3::Model::CreateBucketRequest request;\n\t    request.SetBucket(bucketName.c_str());\n\t    Aws::S3::Model::CreateBucketOutcome outcome = m_client->CreateBucket(request);\n\t    if (!outcome.IsSuccess())\n\t    {\n\t        auto err = outcome.GetError();\n\t        std::cout << \"Error: CreateBucket: \" << err.GetExceptionName() << \": \" << err.GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t    return true;\n\t}\n\t\n\tbool delete_Bucket(S3Client *m_client, string bucketName)\n\t{\n\t    Aws::S3::Model::DeleteBucketRequest request;\n\t    request.SetBucket(bucketName.c_str());\n\t    Aws::S3::Model::DeleteBucketOutcome outcome = m_client->DeleteBucket(request);\n\t    if (!outcome.IsSuccess())\n\t    {\n\t        auto err = outcome.GetError();\n\t        std::cout << \"Error: DeleteBucket: \" << err.GetExceptionName() << \": \" << err.GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t    return true;\n\t}\n\t\n\tbool list_Objects(S3Client *m_client, string bucketName)\n\t{\n\t    Aws::S3::Model::ListObjectsRequest request;\n\t    request.WithBucket(bucketName.c_str());\n\t    auto outcome = m_client->ListObjects(request);\n\t    if (outcome.IsSuccess())\n\t    {\n\t        std::cout << \"Objects in bucket '\" << bucketName << \"':\" << std::endl\n\t                  << std::endl;\n\t        Aws::Vector<Aws::S3::Model::Object> objects = outcome.GetResult().GetContents();\n\t        for (Aws::S3::Model::Object &object : objects)\n\t        {\n\t            std::cout << object.GetKey() << std::endl;\n\t        }\n\t        return true;\n\t    }\n\t    else\n\t    {\n\t        std::cout << \"Error: ListObjects: \" << outcome.GetError().GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t}\n\t\n\tbool download_Object(S3Client *m_client, string bucketName, string objectKey, string pathKey)\n\t{\n\t    Aws::S3::Model::GetObjectRequest object_request;\n\t    object_request.WithBucket(bucketName.c_str()).WithKey(objectKey.c_str());\n\t    auto get_object_outcome = m_client->GetObject(object_request);\n\t    if (get_object_outcome.IsSuccess())\n\t    {\n\t        Aws::OFStream local_file;\n\t        local_file.open(pathKey.c_str(), std::ios::out | std::ios::binary);\n\t        local_file << get_object_outcome.GetResult().GetBody().rdbuf();\n\t        std::cout << \"Done!\" << std::endl;\n\t        return true;\n\t    }\n\t    else\n\t    {\n\t        std::cout << \"GetObject error: \" << get_object_outcome.GetError().GetExceptionName() << \" \" << get_object_outcome.GetError().GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t}\n\t\n\tbool upload_Object(S3Client *m_client, string bucketName, string objectKey, string pathKey)\n\t{\n\t    PutObjectRequest putObjectRequest;\n\t    putObjectRequest.WithBucket(bucketName.c_str()).WithKey(objectKey.c_str());\n\t    auto input_data = Aws::MakeShared<Aws::FStream>(\"PutObjectInputStream\", pathKey.c_str(), std::ios_base::in | std::ios_base::binary);\n\t    putObjectRequest.SetBody(input_data);\n\t    auto putObjectResult = m_client->PutObject(putObjectRequest);\n\t    if (putObjectResult.IsSuccess())\n\t    {\n\t        std::cout << \"Done!\" << std::endl;\n\t        return true;\n\t    }\n\t    else\n\t    {\n\t        std::cout << \"PutObject error: \" << putObjectResult.GetError().GetExceptionName() << \" \" << putObjectResult.GetError().GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t}\n\t\n\tbool delete_Object(S3Client *m_client, string bucketName, string objectKey)\n\t{\n\t    Aws::S3::Model::DeleteObjectRequest request;\n\t    request.WithKey(objectKey.c_str()).WithBucket(bucketName.c_str());\n\t    Aws::S3::Model::DeleteObjectOutcome outcome = m_client->DeleteObject(request);\n\t    if (!outcome.IsSuccess())\n\t    {\n\t        auto err = outcome.GetError();\n\t        std::cout << \"Error: DeleteObject: \" << err.GetExceptionName() << \": \" << err.GetMessage() << std::endl;\n\t        return false;\n\t    }\n\t    else\n\t    {\n\t        return true;\n\t    }\n\t}\n```\n\n#### **cpp/include**\n**cpp/include/minio_client_sdk.h**\n\n```cpp\n\t#include <iostream>\n\t#include <aws/core/Aws.h>\n\t#include <aws/s3/S3Client.h>\n\t\n\tusing namespace std;\n\tusing namespace Aws::S3;\n\t\n\tS3Client *access_minio_server(string minio_server_address, Aws::SDKOptions m_options, string access_key, string access_secret);\n\tbool close_minio_server(S3Client *m_client, Aws::SDKOptions m_options);\n\t\n\tstring random_UUID();\n\tbool list_Buckets(S3Client *m_client);\n\tbool create_Bucket(S3Client *m_client, string bucketName);\n\tbool delete_Bucket(S3Client *m_client, string bucketName);\n\t\n\tbool list_Objects(S3Client *m_client, string bucketName);\n\tbool download_Object(S3Client *m_client, string bucketName, string objectKey, string pathKey);\n\tbool upload_Object(S3Client *m_client, string bucketName, string objectKey, string pathKey);\n\tbool delete_Object(S3Client *m_client, string bucketName, string objectKey);\n```\n\n#### **编译执行**\n\n```shell\n\t$ cd cpp\n\t$ mkdir build\n\t$ cd build\n\t$ cmake ..\n\t$ make\n\t$ ./bin/main\n```\n\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"regenerateTestingCerts.sh","url":"/2021/03/13/technologies/docker/regenerateTestingCerts/","content":"\n## regenerateTestingCerts.sh脚本内容\n\n<!-- more -->\n\n```\n#!/usr/bin/env bash\n\n# Script to be used for generating testing certs only for notary-server and notary-signer\n# Will also create a root-ca and intermediate-ca, deleting those keys when finished\n\nOPENSSLCNF=\nfor path in /etc/pki/tls/openssl.cnf /etc/openssl/openssl.cnf /etc/ssl/openssl.cnf /usr/local/etc/openssl/openssl.cnf; do\n    if [[ -e ${path} ]]; then\n        OPENSSLCNF=${path}\n    fi\ndone\nif [[ -z ${OPENSSLCNF} ]]; then\n    printf \"Could not find openssl.cnf\"\n    exit 1\nfi\n\n# First generates root-ca\nopenssl genrsa -out \"root-ca.key\" 4096\nopenssl req -new -key \"root-ca.key\" -out \"root-ca.csr\" -sha512 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Notary Testing CA'\n\ncat > \"root-ca.cnf\" <<EOL\n[root_ca]\nbasicConstraints = critical,CA:TRUE,pathlen:1\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 3650 -in \"root-ca.csr\" -signkey \"root-ca.key\" -sha512 \\\n        -out \"root-ca.crt\" -extfile \"root-ca.cnf\" -extensions root_ca\ncp \"root-ca.crt\" \"../cmd/notary/root-ca.crt\"\n\nrm \"root-ca.cnf\" \"root-ca.csr\"\n\n# Then generate intermediate-ca\nopenssl genrsa -out \"intermediate-ca.key\" 4096\nopenssl req -new -key \"intermediate-ca.key\" -out \"intermediate-ca.csr\" -sha512 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=Notary Intermediate Testing CA'\n\ncat > \"intermediate-ca.cnf\" <<EOL\n[intermediate_ca]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:TRUE,pathlen:0\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, nonRepudiation, cRLSign, keyCertSign\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 3650 -in \"intermediate-ca.csr\" -sha512 \\\n        -CA \"root-ca.crt\" -CAkey \"root-ca.key\"  -CAcreateserial \\\n        -out \"intermediate-ca.crt\" -extfile \"intermediate-ca.cnf\" -extensions intermediate_ca\n\nrm \"intermediate-ca.cnf\" \"intermediate-ca.csr\"\nrm \"root-ca.key\" \"root-ca.srl\"\n\n# Then generate notary-server\nopenssl genrsa -out \"notary-server.key\" 3072\n# Use the existing notary-server key\nopenssl req -new -key \"notary-server.key\" -out \"notary-server.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=notary-server'\n\ncat > \"notary-server.cnf\" <<EOL\n[notary_server]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:notary-server, DNS:notaryserver, DNS:localhost, IP:10.239.140.65, IP:127.0.0.1\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 750 -in \"notary-server.csr\" -sha384 \\\n        -CA \"intermediate-ca.crt\" -CAkey \"intermediate-ca.key\"  -CAcreateserial \\\n        -out \"notary-server.crt\" -extfile \"notary-server.cnf\" -extensions notary_server\n# append the intermediate cert to this one to make it a proper bundle\ncat \"intermediate-ca.crt\" >> \"notary-server.crt\"\n\nrm \"notary-server.cnf\" \"notary-server.csr\"\n\n# Then generate notary-signer\nopenssl genrsa -out \"notary-signer.key\" 3072\n# Use the existing notary-signer key\nopenssl req -new -key \"notary-signer.key\" -out \"notary-signer.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=notary-signer'\n\ncat > \"notary-signer.cnf\" <<EOL\n[notary_signer]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:notary-signer, DNS:notarysigner, DNS:localhost, IP:127.0.0.1\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 750 -in \"notary-signer.csr\" -sha384 \\\n        -CA \"intermediate-ca.crt\" -CAkey \"intermediate-ca.key\"  -CAcreateserial \\\n        -out \"notary-signer.crt\" -extfile \"notary-signer.cnf\" -extensions notary_signer\n# append the intermediate cert to this one to make it a proper bundle\ncat \"intermediate-ca.crt\" >> \"notary-signer.crt\"\n\nrm \"notary-signer.cnf\" \"notary-signer.csr\"\n\n# Then generate notary-escrow\nopenssl genrsa -out \"notary-escrow.key\" 3072\n# Use the existing notary-escrow key\nopenssl req -new -key \"notary-escrow.key\" -out \"notary-escrow.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=notary-escrow'\n\ncat > \"notary-escrow.cnf\" <<EOL\n[notary_escrow]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:notary-escrow, DNS:notaryescrow, DNS:localhost, IP:127.0.0.1\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 750 -in \"notary-escrow.csr\" -sha384 \\\n        -CA \"intermediate-ca.crt\" -CAkey \"intermediate-ca.key\"  -CAcreateserial \\\n        -out \"notary-escrow.crt\" -extfile \"notary-escrow.cnf\" -extensions notary_escrow\n# append the intermediate cert to this one to make it a proper bundle\ncat \"intermediate-ca.crt\" >> \"notary-escrow.crt\"\n\nrm \"notary-escrow.cnf\" \"notary-escrow.csr\"\n\n\n# Then generate secure.example.com\n# Use the existing secure.example.com key\nopenssl req -new -key \"secure.example.com.key\" -out \"secure.example.com.csr\" -sha384 \\\n        -subj '/C=US/ST=CA/L=San Francisco/O=Docker/CN=secure.example.com'\n\ncat > \"secure.example.com.cnf\" <<EOL\n[secure.example.com]\nauthorityKeyIdentifier=keyid,issuer\nbasicConstraints = critical,CA:FALSE\nextendedKeyUsage=serverAuth,clientAuth\nkeyUsage = critical, digitalSignature, keyEncipherment\nsubjectAltName = DNS:secure.example.com, DNS:localhost, IP:127.0.0.1\nsubjectKeyIdentifier=hash\nEOL\n\nopenssl x509 -req -days 750 -in \"secure.example.com.csr\" -sha384 \\\n        -CA \"intermediate-ca.crt\" -CAkey \"intermediate-ca.key\"  -CAcreateserial \\\n        -out \"secure.example.com.crt\" -extfile \"secure.example.com.cnf\" -extensions secure.example.com\nrm \"secure.example.com.cnf\" \"secure.example.com.csr\"\nrm \"intermediate-ca.key\" \"intermediate-ca.srl\"\n\n\n# generate self-signed_docker.com-notary.crt and self-signed_secure.example.com\nfor selfsigned in self-signed_docker.com-notary self-signed_secure.example.com; do\n        subj='/O=Docker/CN=docker.com\\/notary'\n        if [[ \"${selfsigned}\" =~ .*example.com ]]; then\n                subj='/O=secure.example.com/CN=secure.example.com'\n        fi\n\n        openssl ecparam -name prime256v1 -genkey -out \"${selfsigned}.key\"\n        openssl req -new -key \"${selfsigned}.key\" -out \"${selfsigned}.csr\" -sha256 -subj \"${subj}\"\n        cat > \"${selfsigned}.cnf\" <<EOL\n[selfsigned]\nbasicConstraints = critical,CA:FALSE\nkeyUsage = critical, digitalSignature, keyEncipherment\nextendedKeyUsage=codeSigning\nsubjectKeyIdentifier=hash\nEOL\n\n        openssl x509 -req -days 750 -in \"${selfsigned}.csr\" -signkey \"${selfsigned}.key\" \\\n                -out \"${selfsigned}.crt\" -extfile \"${selfsigned}.cnf\" -extensions selfsigned\n\n        rm \"${selfsigned}.cnf\" \"${selfsigned}.csr\" \"${selfsigned}.key\"\ndone\n\n# Postgresql keys for testing server/client auth\n\ncommand -v cfssljson  >/dev/null 2>&1 || { \n    echo >&2 \"Installing cfssl tools\"; go get -u github.com/cloudflare/cfssl/cmd/...;\n}\n\n# Create a dir to store keys generated temporarily\nmkdir cfssl\ncd cfssl\n\n# Generate CA and certificates\n\necho '{\"CN\": \"Test Notary CA\",\"key\":{\"algo\":\"rsa\",\"size\":2048}}' | cfssl gencert -initca - | cfssljson -bare ca -\n\necho '{\"signing\":{\"default\":{\"expiry\":\"43800h\"},\"profiles\":{\"server\":{\"expiry\":\"43800h\", \"usages\":[\"signing\",\"key encipherment\",\"server auth\"]},\"client\":{\"expiry\":\"43800h\", \"usages\":[\"signing\",\"key encipherment\",\"client auth\"]}}}}' > ca-config.json\n\necho '{\"CN\":\"database\",\"hosts\":[\"postgresql\",\"mysql\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' > server.json\n\n# Generate server cert and private key\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server server.json | cfssljson -bare server\n\n# Generate client certificate (notary server)\necho '{\"CN\":\"server\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' > notary-server.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client notary-server.json | cfssljson -bare notary-server\n\n# Generate client certificate (notary notary-signer)\necho '{\"CN\":\"signer\",\"hosts\":[\"\"],\"key\":{\"algo\":\"rsa\",\"size\":2048}}' > notary-signer.json\n\ncfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client notary-signer.json | cfssljson -bare notary-signer\n\n# Copy keys over to ../fixtures/database/[...] and ../notarysql/postgresql-initdb.d/[...]\ncp ca.pem ../database/\ncp notary-signer.pem ../database/\ncp notary-signer-key.pem ../database/\ncp notary-server.pem ../database\ncp notary-server-key.pem ../database/\n\ncp ca.pem ../../notarysql/postgresql-initdb.d/root.crt\ncp server.pem ../../notarysql/postgresql-initdb.d/server.crt\ncp server-key.pem ../../notarysql/postgresql-initdb.d/server.key\n\n# remove the working dir\ncd ..\nrm -rf cfssl\n\n```\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"Dockerfile introduction","url":"/2021/03/13/technologies/docker/dockerfile/","content":"\nReference Link：\n(官网): https://docs.docker.com/develop/develop-images/dockerfile_best-practices/\n\n## Dockerfile brief introduction\n> 在Docker中创建镜像最常用的方式，就是使用Dockerfile。Dockerfile是一个Docker镜像的描述文件，我们可以理解成火箭发射的A、B、C、D…的步骤。Dockerfile其内部包含了一条条的指令，每一条指令构建一层，因此每一条指令的内容，就是描述该层应当如何构建。\n![](01.png)\n![](02.png)\n\n### From\n指明构建的新镜像是来自于哪个基础镜像\n\n```\n\tFrom ubuntu:18.04\n```\n\n### MAINTAINER\n指明镜像维护着及其联系方式（一般是邮箱地址），例如：\n\n```\n\tMAINTAINER Edison Zhou <edisonchou@hotmail.com>\n```\n不过，MAINTAINER并不推荐使用，更推荐使用LABEL来指定镜像作者，例如：\n\n```\n\tLABEL maintainer=\"edisonzhou.cn\"\n```\n### RUN\n构建镜像时运行的Shell命令，例如：\n\n```\n\tRUN [\"yum\", \"install\", \"httpd\"]\n\tRUN yum install httpd\n```\n\n### ADD\n拷贝文件或目录到镜像中，例如：\n\n```\n\tADD <src>...<dest>\n\tADD html.tar.gz /var/www/html\n\tADD https://xxx.com/html.tar.gz /var/www/html\n\tPS：如果是URL或压缩包，会自动下载或自动解压。\n```\n\n### COPY\n原路径：可以是多个，甚至可以是通配符  \n目标路径：可以是容器内的绝对路径，也可以是相对于工作目录的相对路径(工作目录可以用 WORKDIR 指令来指定，不需要事先创建，会自动创建)  \n拷贝文件或目录到镜像中，用法同ADD，只是不支持自动下载和解压，例如：  \n\n```\n\tCOPY ./start.sh /start.sh\n\t# 拷贝start.sh和src下的所有源文件到image的/home/workfile目录\n\tCOPY ./start.sh ./src/*.cpp /home/workfile\n\t# 利用 通配符 进行复制\n\tCOPY hom* /mydir/\n\tCOPY hom?.txt /mydir/\n\t\n\t# 拷贝文件夹, 必须在image目录中定义folder name\n\tCOPY folder /home/folder\n```\nnote : COPY 会将原文件的各种数据都保留，比如 读、写、执行权限，可以通过 --chown=<user>:<group> 选项来改变文件的所属用户及所属组。  \nAlthough `ADD` and `COPY` are functionally similar, generally speaking, `COPY` is preferred. That’s because it’s more transparent than `ADD`.  \n\n### WORKDIR\n为RUN、CMD、ENTRYPOINT以及COPY和AND设置工作目录，例如：\n\n```\n\tWORKDIR /home/zhan\n```\n\n### VOLUME\n指定容器挂载点到宿主机自动生成的目录或其他容器，例如：\n\n```\n\tVOLUME [\"/var/lib/mysql\"]\n\tPS：一般不会在Dockerfile中用到，更常见的还是在docker run的时候指定-v数据卷。\n```\n\n### EXPOSE\n声明容器运行的服务端口，例如：\n\n```\n\tEXPOSE 80 443\n```\n\n### CMD\n启动容器时执行的Shell命令，`会被docker run命令行指定的参数所覆盖`, 例如：\n\n```\n\tCMD [\"-C\", \"/start.sh\"] \n\tCMD [\"/usr/sbin/sshd\", \"-D\"] \n\tCMD /usr/sbin/sshd -D\n```\n\n### ENTRYPOINT\n启动容器时执行的Shell命令，同CMD类似，只是由ENTRYPOINT启动的程序不会被docker run命令行指定的参数所覆盖，而且，这些命令行参数会被当作参数传递给ENTRYPOINT指定指定的程序，例如：\n\n```\n\tENTRYPOINT [\"/bin/bash\", \"-C\", \"/start.sh\"]\n\tENTRYPOINT /bin/bash -C '/start.sh'\n\tPS：Dockerfile文件中也可以存在多个ENTRYPOINT指令，但仅有最后一个会生效。\n```\n### ENV\n设置环境内环境变量，例如：\n```\n\tENV MYSQL_ROOT_PASSWORD 123456\n\tENV JAVA_HOME /usr/local/jdk1.8.0_45\n\tENV http_proxy http://proxy:913\n```\n### USER\n为RUN、CMD和ENTRYPOINT执行Shell命令指定运行用户，例如：\n```\n\tUSER <user>[:<usergroup>]\n\tUSER <UID>[:<UID>]\n\tUSER edisonzhou\n```\n### HEALTHCHECK\n告诉Docker如何测试容器以检查它是否仍在工作，即健康检查，例如：\n```\n\tHEALTHCHECK --interval=5m --timeout=3s --retries=3 \\\n\t\tCMD curl -f http:/localhost/ || exit 1\n\t其中，一些选项的说明：\n\t--interval=DURATION (default: 30s)：每隔多长时间探测一次，默认30秒\n\t-- timeout= DURATION (default: 30s)：服务响应超时时长，默认30秒\n\t--start-period= DURATION (default: 0s)：服务启动多久后开始探测，默认0秒\n\t--retries=N (default: 3)：认为检测失败几次为宕机，默认3次\n\t一些返回值的说明：\n\t0：容器成功是健康的，随时可以使用\n\t1：不健康的容器无法正常工作\n\t2：保留不使用此退出代码\n```\n### ARG\n在构建镜像时，指定一些参数，例如：\n```\n\tFROM centos:6\n\tARG user # ARG user=root\n\tUSER $user\n\t这时，我们在docker build时可以带上自定义参数user了，如下所示：\n\tdocker build --build-arg user=edisonzhou Dockerfile .\n```\n\n## 配置proxy\n### 第一种: 配置proxy环境变量, 有时候需要加http和https两个proxy, 而下面两个还没找出怎么加两个proxy.\n```\n\tENV http_proxy=http://child-prc.intel.com:913\n\tENV https_proxy=http://child-prc.intel.com:913\n\tENV HTTP_PROXY=http://child-prc.intel.com:913\n\tENV HTTPS_PROXY=http://child-prc.intel.com:913\n```\n\n### 第二种: 在Dockerfile中命令行加上proxy\n\n```shell\n\tRUN pip install Flask-JSON==0.3.3 --proxy=http://child-prc.intel.com:913\n```\n### (推荐)第三种: 在构建image命令行上加proxy, 这样不会写道image里,给其他人用时候不会暴漏自己公司的proxy\n\n```shell\n\tdocker build --pull -t \"productpage:${VERSION}\" -t \"productpage:latest\" --build-arg HTTP_PROXY=http://child-prc.intel.com:913 --build-arg HTTPS_PROXY=http://child-prc.intel.com:913 .\n```\n\n然后在Dockerfile里面你应该添加\n\n```\n\tARG HTTP_PROXY\n\tARG HTTPS_PROXY\n```\n\n## Use cases\n### Case1: 编译执行在一个image里\n\n```shell\n\t$ cd /home/zhan\n\n\t$ touch test.c \n```\n\ntest.c添加如下内容\n\n``` c\n\t#include <stdio.h>\n\t\n\tint main(int argc, char* argv[])\n\t{\n\t\tprintf(\"Hello World!\\n\");\n\t\treturn 0;\n\t}\n```\n\n``` shell\n\t$ touch Dockerfile \n```\n\nDockerfile添加如下内容\n\n```\n\tFrom ubuntu:18.04\n\tLABEL maintainer=\"tester.com\"\n\tRUN mkdir -p /home/zhan\n\tWORKDIR /home/zhan\n\tCOPY test.c .\n\tENV http_proxy http://proxy:913\n\tRUN apt-get update\n\tRUN apt-get install gcc -y\n\tRUN gcc -g test.c -o test.o\n\t#CMD [\"/bin/bash\"]\n\tCMD [\"./test.o\"]\n```\n\n构建image\n\n``` shell\n\t$ docker build -t dockerfile-01:1.0 .\n\t$ docker images\n\tREPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE\n\tdockerfile-01                        1.0                 cd9a95fccaa9        45 minutes ago      207MB\n\tubuntu                               18.04               4e5021d210f6        6 weeks ago         64.2MB\n```\n\n * 第一种执行应用方式: 启动容器直接运行可执行文件\n\n``` shell\n\t$ docker run --rm dockerfile-01\n\tHello World!\n```\n\n * 第二种执行应用方式: 启动容器，进入容器，再手动执行可执行文件\n\n``` shell\n\t$ docker run -itd --name test01 dockerfile-01 /bin/bash\n\t$ docker exec -it test01 /bin/bash\n\troot@bfb87ec4b27a:/home/zhan# ls\n\ttest.c  test.o\n\troot@bfb87ec4b27a:/home/zhan#\n```\n\n进入容器可以看到直接到/home/zhan目录，有test.c和test.o两个文件\n\n### Case2: 编译执行在两个image里\n因为编译生成的image非常大，因此把编译生成的文件再copy到另一个image就减小了体积\nDockfile内容如下\n\n``` shell\n\tFrom ubuntu:18.04 as builder # 基于ubuntu:18.04创建第一个image\n\tLABEL maintainer=\"tester.com\"\n\tRUN mkdir -p /home/zhan\n\tWORKDIR /home/zhan\n\tCOPY test.c .\n\tENV http_proxy http://proxy:913\n\tRUN apt-get update\n\tRUN apt-get install gcc -y\n\tRUN gcc -g test.c -o test.o\n\t\n\tFROM ubuntu:18.04 # 基于ubuntu:18.04再创建第二个image\n\tWORKDIR /root/\n\t# 从上面的 image \"builder\" 拷贝编译好的可执行文件/home/zhan/test.o文件到当前image的/root/目录\n\tCOPY --from=builder /home/zhan/test.o .\n\tCMD [\"./test.o\"]\n```\n\n构建image\n\n``` shell\n\t$ docker build -t dockerfile-01:2.0 .\n\t$ docker images\n\tREPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE\n\tdockerfile-01                        2.0                 f29fa900630d        5 seconds ago       63.3MB\n\tdockerfile-01                        1.0                 cd9a95fccaa9        45 minutes ago      207MB\n\tubuntu                               18.04               4e5021d210f6        6 weeks ago         63.2MB\n```\n\n可以看到新生成的两个image， 一个image为<none>:<none> 大小为207MB, 另一个image为dockerfile-01:2.0 大小为64.2MB\n以后就可以用dockerfile-01:2.0作为项目中的应用docker容器\n\n* 第一种执行应用方式: 启动容器直接运行可执行文件\n\n``` shell\n\t$ docker run --rm dockerfile-01:2.0\n\tHello World!\n```\n\n* 第二种执行应用方式: 启动容器，进入容器，再手动执行可执行文件\n\n``` shell\n\t$ docker run -itd --name test02 dockerfile-01:2.0 /bin/bash\n\tbaef91591402ac368816b8baf9251859db9aa9e378845d88009ad8dffdcb8028\n\t$ docker exec -it test02 /bin/bash\n\troot@baef91591402:~# ls\n\ttest.o\n\troot@baef91591402:~#\n```\n\n进入容器可以看到直接进入/root目录，只有可执行文件test.o\n\n\n### Additional\n\n找一个目录新建两个文件app.js和Dockerfile\n``` shell\n\troot@alpha:/home/zhan/images_test# ls\n\tapp.js  Dockerfile\n```\napp.js\n\n``` js\n\tconst http = require('http');\n\tconst os = require('os');\n\tconsole.log(\"Kubia server starting ...\");\n\t\n\tvar handler = function(request, response) {\n\t\tconsole.log(\"Received request from\" + request.connection.remoteAddress);\n\t\tresponse.writeHead(200);\n\t\tresponse.end(\"You've hit \" + os.hostname() + \"\\n\");\n\t};\n\t\n\tvar www = http.createServer(handler);\n\twww.listen(8080);\n```\n\nDockerfile\n\n```\n\tFROM node:7\n\tADD app.js /app.js\n\tENTRYPOINT [\"node\",\"app.js\"]\n```\n\n运行如下命令\n\n``` shell\n\t$ docker build -t kubia .\n\t$ docker images\n\t$ docker run --name kubia-container -p 8081:8080 -d kubia:latest\n\t$ curl 10.239.140.186:8081 或者 curl http://10.239.1n40.186:8081 (linux添加proxy如child-prc.intel.com:913后用这行命令)\n\tYou've hit fef0dd414fe4\n\t// 浏览器输入http://localhost:8081也能返回同样的container ID, \n\t// 如果机器没有添加proxy代理用 $ curl localhost:8081 就可以\n\n\t$ docker exec -it kubia-container1 bash\n\t$ cat /etc/hostname\n\tfef0dd414fe4 // 发现container的hostname和container ID是一样的十六进制数\n\n\t$ ps aux  // 查看容器内运行的进程\n\tUSER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\n\troot         1  0.0  0.3 813600 26092 ?        Ssl  06:58   0:00 node app.js\n\troot        11  0.1  0.0  20244  3232 pts/0    Ss   07:24   0:00 bash\n\troot        17  0.0  0.0  17500  2048 pts/0    R+   07:26   0:00 ps aux\n\troot@fef0dd414fe4:/#\n\t在宿主主机上运行这个命令 $ ps aux | grep app.js\n\troot     12028  0.0  0.0  21532  1044 pts/1    S+   15:26   0:00 grep --color=auto app.js\n\troot     30631  0.0  0.3 813600 26092 ?        Ssl  14:58   0:00 node app.js\n\t发现进程的ID在容器中与主机上不同, 容器使用独立的PID Linux命名空间并且有着独立的系列号，完全独立于进程树.\n\t正如拥有独立的进程树一样，每个容器也拥有独立的文件系统.\n\t容器内的应用不仅拥有独立的文件系统，还有进程、用户、主机名和网络接口.\n```\n\n## Dockerfile逆向\n\n``` shell\n\t$ docker history dockerfile-01:1.0\n\tIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n\t01c4f6c6dc0b        3 hours ago         /bin/sh -c #(nop)  CMD [\"./test.o\"]             0B\n\t9237cc69b6b9        3 hours ago         /bin/sh -c gcc -g test.c -o test.o              10.8kB\n\t4e775aeb3a93        3 hours ago         /bin/sh -c apt-get install gcc -y               124MB\n\t4524eb24d841        3 hours ago         /bin/sh -c apt-get update                       34.2MB\n\t91c7861b5cb3        3 hours ago         /bin/sh -c #(nop)  ENV http_proxy=http://chi…   0B\n\t1ca351444c36        3 hours ago         /bin/sh -c #(nop) COPY file:48688e4f8a2a718c…   102B\n\t82e8f67a2e3f        3 hours ago         /bin/sh -c #(nop) WORKDIR /home/zhan            0B\n\t21555537a646        3 hours ago         /bin/sh -c mkdir -p /home/zhan                  0B\n\t14b43e5758aa        3 hours ago         /bin/sh -c #(nop)  LABEL maintainer=tester.c…   0B\n\t56def654ec22        6 weeks ago         /bin/sh -c #(nop)  CMD [\"/bin/bash\"]            0B\n\t<missing>           6 weeks ago         /bin/sh -c mkdir -p /run/systemd && echo 'do…   7B\n\t<missing>           6 weeks ago         /bin/sh -c [ -z \"$(apt-get indextargets)\" ]     0B\n\t<missing>           6 weeks ago         /bin/sh -c set -xe   && echo '#!/bin/sh' > /…   745B\n\t<missing>           6 weeks ago         /bin/sh -c #(nop) ADD file:4974bb5483c392fb5…   63.2MB\n```\n\n## 遇到的问题\n### docker proxy\ndocker pull ubuntu等系统镜像后登陆上去，配置公司proxy后无法连接网络，apt-get update无法执行\n解决方案: 修改主机上的文件 /etc/docker/daemon.json 内容如下, 添加主机IP:\n\n```\n\t{ \"insecure-registries\": [\"10.239.140.186\"] }\n```\n\n之后重启docker\n\n``` shell\n\t$ systemctl daemon-reload\n\t$ systemctl restart docker.service\n```\n再次登陆容器配置好proxy, apt-get update就可以执行了\n\n### 删除所有<none>镜像\n\n一条命令ko\n\n``` shell\n\tdocker rmi `docker images | grep  \"<none>\" | awk '{print $3}'`\n```\n如果确定所有none镜像确实没用，直接加个-f强制删除，谨慎\n\n``` shell\n\tdocker rmi -f `docker images | grep  \"<none>\" | awk '{print $3}'`\n```\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"安装 docker compose","url":"/2021/03/13/technologies/docker/安装docker-compose/","content":"\nReference: https://docs.docker.com/compose/install/\n\n<!-- more -->\n\n## 下载docker-compose\n\nRun this command to download the current stable release of Docker Compose:\n```\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.28.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n```\n> To install a different version of Compose, substitute 1.28.2 with the version of Compose you want to use.\n\n## 复制到环境变量\nApply executable permissions to the binary:\n```\nsudo chmod +x /usr/local/bin/docker-compose\n```\n**`Note:`** If the command docker-compose fails after installation, check your path. You can also create a symbolic link to /usr/bin or any other directory in your path.\nFor example:\n```\nsudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n```\n\n## 命令自动补充\n\nOptionally, install [command completion](https://docs.docker.com/compose/completion/) for the **`bash`** and **`zsh`** shell.\n\nTest the installation.\n```\n$ docker-compose --version\ndocker-compose version 1.28.2, build 1110ad01\n```\n\n\n","tags":["docker"],"categories":["technologies","docker"]},{"title":"MinIO 03 client","url":"/2021/03/13/storage/minio/minIO_03_client.md/","content":"\nOfficial web site: [https://docs.min.io/docs/minio-client-quickstart-guide.html](https://docs.min.io/docs/minio-client-quickstart-guide.html)\n下面介绍的是使用MinIO自导的client： mc\n\n## **Download mc tool**\nGNU/Linux\n\n```shell\n\t$ wget https://dl.min.io/client/mc/release/linux-amd64/mc\n\t$ chmod +x mc\n\t$ ./mc --help\n\t$ ./mc --version\n```\n\nMicrosoft Windows\n\n```shell\n\t下载:https://dl.min.io/client/mc/release/windows-amd64/mc.exe\n\tmc.exe --help\n```\n\nMinIO Client (mc) provides a modern alternative to UNIX commands like ls, cat, cp, mirror, diff etc. It supports filesystems and Amazon S3 compatible cloud storage service (AWS Signature v2 and v4).\n\n```\n\talias       set, remove and list aliases in configuration file\n\tls          list buckets and objects\n\tmb          make a bucket\n\trb          remove a bucket\n\tcp          copy objects\n\tmirror      synchronize object(s) to a remote site\n\tcat         display object contents\n\thead        display first 'n' lines of an object\n\tpipe        stream STDIN to an object\n\tshare       generate URL for temporary access to an object\n\tfind        search for objects\n\tsql         run sql queries on objects\n\tstat        show object metadata\n\tmv          move objects\n\ttree        list buckets and objects in a tree format\n\tdu          summarize disk usage recursively\n\tretention   set retention for object(s)\n\tlegalhold   set legal hold for object(s)\n\tdiff        list differences in object name, size, and date between two buckets\n\trm          remove objects\n\tversioning  manage bucket versioning\n\tlock        manage default bucket object lock configuration\n\tilm         manage bucket lifecycle\n\tencrypt     manage bucket encryption config\n\tevent       manage object notifications\n\twatch       listen for object notification events\n\tpolicy      manage anonymous access to buckets and objects\n\ttag         manage tags for bucket(s) and object(s)\n\tadmin       manage MinIO servers\n\tupdate      update mc to latest release\n```\n\n## 添加minio server\n\n```shell\n\t$ set +o history\t\t// 关闭history记录\n\t$ mc config host add <MinIOName> http://127.0.0.1:30007 <username> <password> --api \"s3v4\"\n\t// 部署tls后需要将http换成https\n\t$ mc config host add hce-minio https://127.0.0.1:30007 <username> <password> --api \"s3v4\"\n\t$ set -o history\t\t// 打开history记录\n```\nmc 工具config配置文件默认存放位置为: `/root/.mc/`\nBy providing the `--api s3v4` directly and it shouldn't call the validation call path.\n\n\n## **Note**\n当MinIO server配置TLS时候，使用`mc`的命令中需要添加 `--insecure`, 如:\n\n```shell\n\t$ ./mc ls myminio --insecure\n```\n\n## Command List\n**查看，添加，移除minio host**\n\n```shell\n\t// 查看 host\n\t$ mc config host list\n\t// 添加 host, server没有配置TLS\n\t$ mc config host add myminio http://127.0.0.1:30007 minio minio123 --api S3v4\n\t// 添加 host, server配置TLS, http变为https\n\t$ mc config host add myminio https://127.0.0.1:30007 minio minio123 --api S3v4\n\t// 移除 host\n\t$ mc config host remove myminio\n\n\t再次查看host\n\t./mc config host list | grep myminio -A 5\n\tmyminio\n\t  URL       : http://127.0.0.1:30007\n\t  AccessKey : hceminio\n\t  SecretKey : hceminio123\n\t  API       : S3v4\n\t  Path      : auto\n```\n\n**查看bucket, object**\n\n```shell\n\t//查看所有bucket\n\t$ mc ls myminio --insecure //server没有配置TLS可以不加`--insecure`\n\t//查看bucket下的object\n\t$ mc ls myminio/my-bucket01/\n```\n\n**创建bucket**\n\n```shell\n\t$ mc mb myminio/my-bucket03 --insecure\n```\n**添加,删除文件(object)**\n\n```shell\n\t// 添加文件(objcet)\n\t$ mc cp minioinstance.yaml myminio/my-bucket03 --insecure\n\t// 删除文件\n\t$ mc rm myminio/my-bucket03/minioinstance.yaml --insecure\n```\n\n**查看bucket使用量**\n\n```shell\n\t$ mc du myminio\n```\n\n\n\n\n\n\n\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"kubernetes 准入控制","url":"/2021/03/13/microService/kubernetes/kubernetes_准入控制/","content":"\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"kubernetes upgrade","url":"/2021/03/13/microService/kubernetes/kubernetes_upgrade/","content":"\nReference: \n\n最新版本:  https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\n中文版本:  https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\n1.19版本: https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/\n\n<!-- more -->\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"Metrics-Server, Kubernetes-Dashboard","url":"/2021/03/13/microService/kubernetes/metrics-server,kubernetes-dashboard/","content":"\n## **Metrics Server**\nMetrics Server用于提供核心指标(Core Metrics), 包括 Node, Pod 的 CPU 和 内存 使用指标.  \n\n对其他自定义指标(Custom Metrics)的监控则由 Prometheus 等组件来完成.  \n\n\n## **Metrics server部署**\nReference Link: \n(官网)https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/\n(aws)https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n\n下载yaml文件\n\n\twget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.4.1/components.yaml\n添加 `--kubelet-insecure-tls` to the components.yaml\nReference Link: https://github.com/kubernetes-sigs/metrics-server/issues/131#issuecomment-418405881\n\n```xml\n\tvim components\n\t129     spec:\n\t130       containers:\n\t131       - args:\n\t132         - --cert-dir=/tmp\n\t133         - --secure-port=4443\n\t134         - --kubelet-insecure-tls\n\t135         - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n\t136         - --kubelet-use-node-status-port\n\t137         image: k8s.gcr.io/metrics-server/metrics-server:v0.4.1\n\t138         imagePullPolicy: IfNotPresent\n```\n\n部署\n\n```shell\n\tkubectl apply -f components.yaml\n\tkubectl get deployment metrics-server -n kube-system\n```\n\n### **查看系统资源利用情况**\n\n```shell\n\t$ kubectl top nodes\n\tNAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\n\tlaboratory    239m         1%     8778Mi          27%\n\tmaster-node   1492m        12%    12721Mi         40%\n\tnode01        940m         31%    2154Mi          27%\n\n\n\t$ kubectl top pod -n kube-system\n\tNAME                                  CPU(cores)   MEMORY(bytes)\n\tcoredns-f9fd979d6-p9dkp               9m           16Mi\n\tcoredns-f9fd979d6-rkk8r               6m           12Mi\n\tetcd-laboratory                       46m          153Mi\n\tetcd-master-node                      66m          151Mi\n\tetcd-node01                           118m         154Mi\n\tkube-apiserver-laboratory             58m          343Mi\n\tkube-apiserver-master-node            79m          327Mi\n\tkube-apiserver-node01                 110m         376Mi\n\tkube-controller-manager-laboratory    25m          48Mi\n\tkube-controller-manager-master-node   3m           20Mi\n\tkube-controller-manager-node01        4m           27Mi\n\tkube-proxy-6pp74                      1m           21Mi\n\tkube-proxy-7m2b4                      1m           17Mi\n\tkube-proxy-x7fxv                      1m           19Mi\n\tkube-scheduler-laboratory             5m           19Mi\n\tkube-scheduler-master-node            5m           21Mi\n\tkube-scheduler-node01                 5m           20Mi\n\tkube-vip-laboratory                   6m           7Mi\n\tkube-vip-master-node                  8m           6Mi\n\tkube-vip-node01                       47m          10Mi\n\tmetrics-server-bc4467d77-plcxk        4m           19Mi\n\ttiller-deploy-6b8b4b4bbc-4255j        1m           8Mi\n\tweave-net-97jn5                       7m           68Mi\n\tweave-net-hwl2m                       3m           95Mi\n\tweave-net-pvh76                       2m           52Mi\n```\n\n## **Kubernetes Dashboard部署**\n\n![](01.JPG)\n\nReference Link\n(官网)https://github.com/kubernetes/dashboard\n(aws)https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html\n(csdn)https://blog.csdn.net/networken/article/details/85607593\n\n```shell\n\tkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml\n\tkubectl get po -n kubernetes-dashboard\n```\n\n### **User Guide**\n\nAccessing Dashboard: https://github.com/kubernetes/dashboard/blob/master/docs/user/accessing-dashboard/README.md#kubectl-port-forward\n总共有三种方式, 介绍一种简单的, 其它的Node Port， kube proxy等方式请参考上面链接.\n\n```shell\n\tkubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8080:443\n```\n\n### **创建user**\n(官网)https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md\n\n```xml\n\tcat > dashboard-adminuser.yaml << EOF\n\tapiVersion: v1\n\tkind: ServiceAccount\n\tmetadata:\n\t  name: admin-user\n\t  namespace: kubernetes-dashboard\n\t\n\t---\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tkind: ClusterRoleBinding\n\tmetadata:\n\t  name: admin-user\n\troleRef:\n\t  apiGroup: rbac.authorization.k8s.io\n\t  kind: ClusterRole\n\t  name: cluster-admin\n\tsubjects:\n\t- kind: ServiceAccount\n\t  name: admin-user\n\t  namespace: kubernetes-dashboard  \n\tEOF\n```\n\n部署 user\n\n```shell\n\tkubectl apply -f dashboard-adminuser.yaml\n```\n\n查看admin-user账户的token\n\n```shell\n\tkubectl -n kubernetes-dashboard describe secret $(kubectl -n kubernetes-dashboard get secret | grep admin-user | awk '{print $1}')\n```\n**说明：** 上面创建了一个叫admin-user的服务账号，并放在kubernetes-dashboard 命名空间下，并将cluster-admin角色绑定到admin-user账户，这样admin-user账户就有了管理员的权限。默认情况下，kubeadm创建集群时已经创建了cluster-admin角色，我们直接绑定即可。  \n### **浏览器登陆查看**\n\n```shell\n\t// 一个终端打开浏览器如: firefox\n\tfirefox\n\t\n\t// 终端上输入https://localhost:8080\n\t// 输入上面的admin-user的token登陆即可查看\n```\n\n## **FAQ遇到的问题**\n\n### **问题1:部署完metrics-server后运行kubectl get nodes出错**\n\n```shell\n\t$ kubectl get nodes\n\tThe connection to the server 10.239.140.201:6443 was refused - did you specify the right host or port?\n```\n解决方案:\n1. 确定在components.yam里添加**`- --kubelet-insecure-tls`**\n2. 在公司中下载东西需要添加proxy, 部署完之后, 确定在**`/etc/kubernetes/manifests/kube-apiserver.yaml`**里注释掉proxy相关信息\n\n\n```xml\n\tvim /etc/kubernetes/manifests/kube-apiserver.yaml\n\t41     ......\n\t42     env:\n\t43     - name: NO_PROXY\n\t44       value: 127.0.0.1,10.239.140.201,10.239.140.137,10.239.131.210,10.239.140.50,master-node,laboratory,node01,k8s-vip\n\t45     #- name: http_proxy\n\t46     #  value: http://child-prc.intel.com:913\n\t47     #- name: HTTPS_PROXY\n\t48     #  value: http://child-prc.intel.com:913\n\t49     #- name: https_proxy\n\t50     #  value: http://child-prc.intel.com:913\n\t51     #- name: HTTP_PROXY\n\t52     #  value: http://child-prc.intel.com:913\n\t53     ......\n```\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"k8s重新启动pod等资源","url":"/2021/03/13/microService/kubernetes/k8s重新启动pod等资源/","content":"\n## k8s重新启动deployment或daemon\n\n<!-- more -->\n\n```\n$ kubectl rollout -h\nManage the rollout of a resource.\n\n Valid resource types include:\n\n  *  deployments\n  *  daemonsets\n  *  statefulsets\n\nExamples:\n  # Rollback to the previous deployment\n  kubectl rollout undo deployment/abc\n\n  # Check the rollout status of a daemonset\n  kubectl rollout status daemonset/foo\n\nAvailable Commands:\n  history     View rollout history\n  pause       Mark the provided resource as paused\n  restart     Restart a resource\n  resume      Resume a paused resource\n  status      Show the status of the rollout\n  undo        Undo a previous rollout\n\nUsage:\n  kubectl rollout SUBCOMMAND [options]\n\nUse \"kubectl <command> --help\" for more information about a given command.\nUse \"kubectl options\" for a list of global command-line options (applies to all commands).\n```\n\n## 实例\n\n```\nkubectl rollout restart deploy/<POD-NAME> -n <NAMESPACE>  // deploy是deployment资源的简称\n```\n\n\n## k8s重新启动pod\n```\nkubectl get pod {podname} -n {namespace} -o yaml | kubectl replace --force -f -\n```\n\n","tags":["k8s"],"categories":["microService","kubernetes"]},{"title":"1.services analysis","url":"/2021/03/13/microService/rest_api/services_analysis/","content":"![](architecture.png)\n# API Gateway\n\n# Zookeeper 服务的注册和发现\n\n# 服务API\n * REST\n * Thrift\n * Dubbo - 基于kv的存储来进行服务的发布和订阅\n\n## REST API\n![](REST_FUL_API.png)  \n * REST，即Representational State Transfer的缩写,中文是\"表现层状态转化\"。\n通俗来讲就是：资源在网络中以某种表现形式进行状态转移。(再通俗来说，就是通过HTTP请求服务器上的某资源，使该资源copy了一份到服务请求方那去了(get动作)。\nRepresentational：某种表现形式，比如用JSON，XML，JPEG等；HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对\"表现形式\"的描述。\nState Transfer：状态变化。通过HTTP动词（GET,POST,DELETE,DETC）实现。\n互联网通信协议HTTP协议，是一个无状态协议。**这意味着，所有的状态都保存在服务器端。\n**因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生\"状态转化\"（State Transfer）。\nHTTP协议里面，四个表示操作方式的动词：GET、POST、PUT、DELETE。\n它们分别对应四种基本操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源。\n\n * REST是由谁提出来的:\nRoy Thomes Fielding在他2000年的博士论文中提出REST架构模式，他是HTTP协议(v1.0和v1.1)的主要设计者、Apache服务器作者之一、Apache基金会第一任主席。\n\n * 什么是REST ful API\n基于REST构建的API就是Restful风格。\n\n * 为什么产生了这种架构模式\n传统的那种JSP前后端耦合在一起的网页模式我们称之为“上古时期”网页，这种模式弊端很多。\n近年来，随着移动技术的发展，各种移动端设备层出不穷，RESTful可以通过一套统一的接口为 Web，iOS和Android提供服务。\n另外对于广大平台来说，比如新浪微博开放平台，微信公共平台等，它们不需要有显式的前端，只需要一套提供服务的接口，于是RESTful更是它们最好的选择。\n\n * 如何设计规范的REST ful API接口\n  - [RESETful API 设计规范](https://godruoyi.com/posts/the-resetful-api-design-specification)\n  -  [RESTful API 最佳实践](http://www.ruanyifeng.com/blog/2018/10/restful-api-best-practices.html)\n\n## 单点登陆系统 慕课网有免费课程\n访问单点登陆系统，校验拿到的token或者称为tickets(票据)是不是正确的, 然后使用这个东西去换取用户的具体信息, 再存储到当前的服务里面\n\n\n\n","tags":["kubernetes"],"categories":["microService","REST-API"]},{"title":"39 | 谈谈Service与Ingress","url":"/2021/03/13/reference/k8s/39.谈谈Service与Ingress/","content":"\n![](1.jpg)\n\n## 谈谈Service与Ingress\n\n在上一篇文章中，我为你详细讲解了将 Service 暴露给外界的三种方法。其中有一个叫作 LoadBalancer 类型的 Service，它会为你在 Cloud Provider（比如：Google Cloud 或者 OpenStack）里创建一个与该 Service 对应的负载均衡服务。\n<!-- more -->\n但是，相信你也应该能感受到，由于每个 Service 都要有一个负载均衡服务，所以这个做法实际上既浪费成本又高。作为用户，我其实更希望看到 Kubernetes 为我内置一个全局的负载均衡器。然后，通过我访问的 URL，把请求转发给不同的后端 Service。\n\n**这种全局的、为了代理不同后端 Service 而设置的负载均衡服务，就是 Kubernetes 里的 Ingress 服务。**\n\n所以，Ingress 的功能其实很容易理解：**所谓 Ingress，就是 Service 的“Service”。**\n\n举个例子，假如我现在有这样一个站点：https://cafe.example.com。其中，https://cafe.example.com/coffee，对应的是“咖啡点餐系统”。而，https://cafe.example.com/tea，对应的则是“茶水点餐系统”。这两个系统，分别由名叫 coffee 和 tea 这样两个 Deployment 来提供服务。\n\n那么现在，`我如何能使用 Kubernetes 的 Ingress 来创建一个统一的负载均衡器，从而实现当用户访问不同的域名时，能够访问到不同的 Deployment 呢？`\n\n上述功能，在 Kubernetes 里就需要通过 Ingress 对象来描述，如下所示：\n\n\tapiVersion: extensions/v1beta1\n\tkind: Ingress\n\tmetadata:\n\t  name: cafe-ingress\n\tspec:\n\t  tls:\n\t  - hosts:\n\t    - cafe.example.com\n\t    secretName: cafe-secret\n\t  rules:\n\t  - host: cafe.example.com\n\t    http:\n\t      paths:\n\t      - path: /tea\n\t        backend:\n\t          serviceName: tea-svc\n\t          servicePort: 80\n\t      - path: /coffee\n\t        backend:\n\t          serviceName: coffee-svc\n\t          servicePort: 80\n在上面这个名叫 cafe-ingress.yaml 文件中，最值得我们关注的，是 rules 字段。在 Kubernetes 里，这个字段叫作：**IngressRule。**\n\nIngressRule 的 Key，就叫做：host。它必须是一个标准的域名格式（Fully Qualified Domain Name）的字符串，而不能是 IP 地址。\n\n> 备注：Fully Qualified Domain Name 的具体格式，可以参考[RFC 3986标准](https://tools.ietf.org/html/rfc3986)。\n\n而 host 字段定义的值，就是这个 Ingress 的入口。这也就意味着，当用户访问 cafe.example.com 的时候，实际上访问到的是这个 Ingress 对象。这样，Kubernetes 就能使用 IngressRule 来对你的请求进行下一步转发。\n\n而接下来 IngressRule 规则的定义，则依赖于 path 字段。你可以简单地理解为，这里的每一个 path 都对应一个后端 Service。所以在我们的例子里，我定义了两个 path，它们分别对应 coffee 和 tea 这两个 Deployment 的 Service（即：coffee-svc 和 tea-svc）。\n\n**通过上面的讲解，不难看到，所谓 Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。**\n\n一个 Ingress 对象的主要内容，实际上就是一个“反向代理”服务（比如：Nginx）的配置文件的描述。而这个代理服务对应的转发规则，就是 IngressRule。\n\n这就是为什么在每条 IngressRule 里，需要有一个 host 字段来作为这条 IngressRule 的入口，然后还需要有一系列 path 字段来声明具体的转发策略。这其实跟 Nginx、HAproxy 等项目的配置文件的写法是一致的。\n\n而有了 Ingress 这样一个统一的抽象，Kubernetes 的用户就无需关心 Ingress 的具体细节了。\n\n在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。\n\n然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。\n\n接下来，`我就以最常用的 Nginx Ingress Controller 为例，在我们前面用 kubeadm 部署的 Bare-metal 环境中，和你实践一下 Ingress 机制的使用过程`。\n\n部署 Nginx Ingress Controller 的方法非常简单，如下所示：\n\n\t$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml\n其中，在[mandatory.yaml](https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml)这个文件里，正是 Nginx 官方为你维护的 Ingress Controller 的定义。我们来看一下它的内容：\n\n\tkind: ConfigMap\n\tapiVersion: v1\n\tmetadata:\n\t  name: nginx-configuration\n\t  namespace: ingress-nginx\n\t  labels:\n\t    app.kubernetes.io/name: ingress-nginx\n\t    app.kubernetes.io/part-of: ingress-nginx\n\t---\n\tapiVersion: extensions/v1beta1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-ingress-controller\n\t  namespace: ingress-nginx\n\t  labels:\n\t    app.kubernetes.io/name: ingress-nginx\n\t    app.kubernetes.io/part-of: ingress-nginx\n\tspec:\n\t  replicas: 1\n\t  selector:\n\t    matchLabels:\n\t      app.kubernetes.io/name: ingress-nginx\n\t      app.kubernetes.io/part-of: ingress-nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app.kubernetes.io/name: ingress-nginx\n\t        app.kubernetes.io/part-of: ingress-nginx\n\t      annotations:\n\t        ...\n\t    spec:\n\t      serviceAccountName: nginx-ingress-serviceaccount\n\t      containers:\n\t        - name: nginx-ingress-controller\n\t          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0\n\t          args:\n\t            - /nginx-ingress-controller\n\t            - --configmap=$(POD_NAMESPACE)/nginx-configuration\n\t            - --publish-service=$(POD_NAMESPACE)/ingress-nginx\n\t            - --annotations-prefix=nginx.ingress.kubernetes.io\n\t          securityContext:\n\t            capabilities:\n\t              drop:\n\t                - ALL\n\t              add:\n\t                - NET_BIND_SERVICE\n\t            # www-data -> 33\n\t            runAsUser: 33\n\t          env:\n\t            - name: POD_NAME\n\t              valueFrom:\n\t                fieldRef:\n\t                  fieldPath: metadata.name\n\t            - name: POD_NAMESPACE\n\t            - name: http\n\t              valueFrom:\n\t                fieldRef:\n\t                  fieldPath: metadata.namespace\n\t          ports:\n\t            - name: http\n\t              containerPort: 80\n\t            - name: https\n\t              containerPort: 443\n可以看到，在上述 YAML 文件中，我们定义了一个使用 nginx-ingress-controller 镜像的 Pod。需要注意的是，这个 Pod 的启动命令需要使用该 Pod 所在的 Namespace 作为参数。而这个信息，当然是通过 Downward API 拿到的，即：Pod 的 env 字段里的定义（env.valueFrom.fieldRef.fieldPath）。\n\n而这个 Pod 本身，就是一个监听 Ingress 对象以及它所代理的后端 Service 变化的控制器。\n\n当一个新的 Ingress 对象由用户创建后，nginx-ingress-controller 就会根据 Ingress 对象里定义的内容，生成一份对应的 Nginx 配置文件（/etc/nginx/nginx.conf），并使用这个配置文件启动一个 Nginx 服务。\n\n而一旦 Ingress 对象被更新，nginx-ingress-controller 就会更新这个配置文件。需要注意的是，如果这里只是被代理的 Service 对象被更新，nginx-ingress-controller 所管理的 Nginx 服务是不需要重新加载（reload）的。这当然是因为 nginx-ingress-controller 通过[Nginx Lua](https://github.com/openresty/lua-nginx-module)方案实现了 Nginx Upstream 的动态配置。\n\n此外，nginx-ingress-controller 还允许你通过 Kubernetes 的 ConfigMap 对象来对上述 Nginx 配置文件进行定制。这个 ConfigMap 的名字，需要以参数的方式传递给 nginx-ingress-controller。而你在这个 ConfigMap 里添加的字段，将会被合并到最后生成的 Nginx 配置文件当中。\n\n**可以看到，一个 Nginx Ingress Controller 为你提供的服务，其实是一个可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。**\n\n当然，为了让用户能够用到这个 Nginx，我们就需要创建一个 Service 来把 Nginx Ingress Controller 管理的 Nginx 服务暴露出去，如下所示：\n\n\t$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/provider/baremetal/service-nodeport.yaml\n由于我们使用的是 Bare-metal 环境，所以 service-nodeport.yaml 文件里的内容，就是一个 NodePort 类型的 Service，如下所示：\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: ingress-nginx\n\t  namespace: ingress-nginx\n\t  labels:\n\t    app.kubernetes.io/name: ingress-nginx\n\t    app.kubernetes.io/part-of: ingress-nginx\n\tspec:\n\t  type: NodePort\n\t  ports:\n\t    - name: http\n\t      port: 80\n\t      targetPort: 80\n\t      protocol: TCP\n\t    - name: https\n\t      port: 443\n\t      targetPort: 443\n\t      protocol: TCP\n\t  selector:\n\t    app.kubernetes.io/name: ingress-nginx\n\t    app.kubernetes.io/part-of: ingress-nginx\n可以看到，这个 Service 的唯一工作，就是将所有携带 ingress-nginx 标签的 Pod 的 80 和 433 端口暴露出去。\n\n而如果你是公有云上的环境，你需要创建的就是 LoadBalancer 类型的 Service 了。\n\n**上述操作完成后，你一定要记录下这个 Service 的访问入口，即：宿主机的地址和 NodePort 的端口，**如下所示：\n\n\t$ kubectl get svc -n ingress-nginx\n\tNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\n\tingress-nginx   NodePort   10.105.72.96   <none>        80:30044/TCP,443:31453/TCP   3h\n为了后面方便使用，我会把上述访问入口设置为环境变量：\n\n\t$ IC_IP=10.168.0.2 # 任意一台宿主机的地址\n\t$ IC_HTTPS_PORT=31453 # NodePort端口\n`在 Ingress Controller 和它所需要的 Service 部署完成后，我们就可以使用它了。`\n\n> 备注：这个“咖啡厅”Ingress 的所有示例文件，都在这里。\n\n首先，我们要在集群里部署我们的应用 Pod 和它们对应的 Service，如下所示：\n\n\t$ kubectl create -f cafe.yaml\n然后，我们需要创建 Ingress 所需的 SSL 证书（tls.crt）和密钥（tls.key），这些信息都是通过 Secret 对象定义好的，如下所示：\n\n\t$ kubectl create -f cafe-secret.yaml\n这一步完成后，我们就可以创建在本篇文章一开始定义的 Ingress 对象了，如下所示：\n\n\t$ kubectl create -f cafe-ingress.yaml\n这时候，我们就可以查看一下这个 Ingress 对象的信息，如下所示：\n\n\t$ kubectl get ingress\n\tNAME           HOSTS              ADDRESS   PORTS     AGE\n\tcafe-ingress   cafe.example.com             80, 443   2h\n\t$ kubectl describe ingress cafe-ingress\n\tName:             cafe-ingress\n\tNamespace:        default\n\tAddress:          \n\tDefault backend:  default-http-backend:80 (<none>)\n\tTLS:\n\t  cafe-secret terminates cafe.example.com\n\tRules:\n\t  Host              Path  Backends\n\t  ----              ----  --------\n\t  cafe.example.com  \n\t                    /tea      tea-svc:80 (<none>)\n\t                    /coffee   coffee-svc:80 (<none>)\n\tAnnotations:\n\tEvents:\n\t  Type    Reason  Age   From                      Message\n\t  ----    ------  ----  ----                      -------\n\t  Normal  CREATE  4m    nginx-ingress-controller  Ingress default/cafe-ingress\n可以看到，这个 Ingress 对象最核心的部分，正是 Rules 字段。其中，我们定义的 Host 是cafe.example.com，它有两条转发规则（Path），分别转发给 tea-svc 和 coffee-svc。\n\n> 当然，在 Ingress 的 YAML 文件里，你还可以定义多个 Host，比如restaurant.example.com、movie.example.com等等，来为更多的域名提供负载均衡服务。\n\n接下来，我们就可以通过访问这个 Ingress 的地址和端口，访问到我们前面部署的应用了，比如，当我们访问https://cafe.example.com:443/coffee时，应该是 coffee 这个 Deployment 负责响应我的请求。我们可以来尝试一下：\n\n\t$ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/coffee --insecureServer address: 10.244.1.56:80\n\tServer name: coffee-7dbb5795f6-vglbv\n\tDate: 03/Nov/2018:03:55:32 +0000\n\tURI: /coffee\n\tRequest ID: e487e672673195c573147134167cf898\n我们可以看到，访问这个 URL 得到的返回信息是：Server name: coffee-7dbb5795f6-vglbv。这正是 coffee 这个 Deployment 的名字。\n\n而当我访问https://cafe.example.com:433/tea的时候，则应该是 tea 这个 Deployment 负责响应我的请求（Server name: tea-7d57856c44-lwbnp），如下所示：\n\n\t$ curl --resolve cafe.example.com:$IC_HTTPS_PORT:$IC_IP https://cafe.example.com:$IC_HTTPS_PORT/tea --insecure\n\tServer address: 10.244.1.58:80\n\tServer name: tea-7d57856c44-lwbnp\n\tDate: 03/Nov/2018:03:55:52 +0000\n\tURI: /tea\n\tRequest ID: 32191f7ea07cb6bb44a1f43b8299415c\n可以看到，Nginx Ingress Controller 为我们创建的 Nginx 负载均衡器，已经成功地将请求转发给了对应的后端 Service。\n\n以上，就是 Kubernetes 里 Ingress 的设计思想和使用方法了。\n\n不过，你可能会有一个疑问，**如果我的请求没有匹配到任何一条 IngressRule，那么会发生什么呢？**\n\n首先，既然 Nginx Ingress Controller 是用 Nginx 实现的，那么它当然会为你返回一个 Nginx 的 404 页面。\n\n不过，Ingress Controller 也允许你通过 Pod 启动命令里的–default-backend-service 参数，设置一条默认规则，比如：–default-backend-service=nginx-default-backend。\n\n\n这样，任何匹配失败的请求，就都会被转发到这个名叫 nginx-default-backend 的 Service。所以，你就可以通过部署一个专门的 Pod，来为用户返回自定义的 404 页面了。\n\n**总结**\n\n在这篇文章里，我为你详细讲解了 Ingress 这个概念在 Kubernetes 里到底是怎么一回事儿。正如我在文章里所描述的，Ingress 实际上就是 Kubernetes 对“反向代理”的抽象。\n\n目前，Ingress 只能工作在七层，而 Service 只能工作在四层。所以当你想要在 Kubernetes 里为应用进行 TLS 配置等 HTTP 相关的操作时，都必须通过 Ingress 来进行。\n\n当然，正如同很多负载均衡项目可以同时提供七层和四层代理一样，将来 Ingress 的进化中，也会加入四层代理的能力。这样，一个比较完善的“反向代理”机制就比较成熟了。\n\n而 Kubernetes 提出 Ingress 概念的原因其实也非常容易理解，有了 Ingress 这个抽象，用户就可以根据自己的需求来自由选择 Ingress Controller。比如，如果你的应用对代理服务的中断非常敏感，那么你就应该考虑选择类似于 Traefik 这样支持“热加载”的 Ingress Controller 实现。\n\n更重要的是，一旦你对社区里现有的 Ingress 方案感到不满意，或者你已经有了自己的负载均衡方案时，你只需要做很少的编程工作，就可以实现一个自己的 Ingress Controller。\n\n在实际的生产环境中，Ingress 带来的灵活度和自由度，对于使用容器的用户来说，其实是非常有意义的。要知道，当年在 Cloud Foundry 项目里，不知道有多少人为了给 Gorouter 组件配置一个 TLS 而伤透了脑筋。\n\n\n**思考题**\n\n如果我的需求是，当访问www.mysite.com和 forums.mysite.com时，分别访问到不同的 Service（比如：site-svc 和 forums-svc）。那么，这个 Ingress 该如何定义呢？请你描述出 YAML 文件中的 rules 字段。","categories":["reference","k8s"]},{"title":"40 | Kubernetes的资源模型与资源管理","url":"/2021/03/13/reference/k8s/40.Kubernetes的资源模型与资源管理/","content":"\n![](1.jpg)\n\n## Kubernetes的资源模型与资源管理\n\n作为一个容器集群编排与管理项目，Kubernetes 为用户提供的基础设施能力，不仅包括了我在前面为你讲述的应用定义和描述的部分，还包括了对应用的资源管理和调度的处理。那么，从今天这篇文章开始，我就来为你详细讲解一下后面这部分内容。\n<!-- more -->\n`而作为 Kubernetes 的资源管理与调度部分的基础，我们要从它的资源模型开始说起。`\n\n我在前面的文章中已经提到过，在 Kubernetes 里，Pod 是最小的原子调度单位。这也就意味着，所有跟调度和资源管理相关的属性都应该是属于 Pod 对象的字段。而这其中最重要的部分，就是 Pod 的 CPU 和内存配置，如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: frontend\n\tspec:\n\t  containers:\n\t  - name: db\n\t    image: mysql\n\t    env:\n\t    - name: MYSQL_ROOT_PASSWORD\n\t      value: \"password\"\n\t    resources:\n\t      requests:\n\t        memory: \"64Mi\"\n\t        cpu: \"250m\"\n\t      limits:\n\t        memory: \"128Mi\"\n\t        cpu: \"500m\"\n\t  - name: wp\n\t    image: wordpress\n\t    resources:\n\t      requests:\n\t        memory: \"64Mi\"\n\t        cpu: \"250m\"\n\t      limits:\n\t        memory: \"128Mi\"\n\t        cpu: \"500m\"\n\n> 备注：关于哪些属性属于 Pod 对象，而哪些属性属于 Container，你可以在回顾一下第 14 篇文章《深入解析 Pod 对象（一）：基本概念》中的相关内容。\n\n在 Kubernetes 中，像 CPU 这样的资源被称作“可压缩资源”（compressible resources）。它的典型特点是，当可压缩资源不足时，Pod 只会“饥饿”，但不会退出。\n\n而像内存这样的资源，则被称作“不可压缩资源（incompressible resources）。当不可压缩资源不足时，Pod 就会因为 OOM（Out-Of-Memory）被内核杀掉。\n\n而由于 Pod 可以由多个 Container 组成，所以 CPU 和内存资源的限额，是要配置在每个 Container 的定义上的。这样，Pod 整体的资源配置，就由这些 Container 的配置值累加得到。\n\n其中，Kubernetes 里为 CPU 设置的单位是“CPU 的个数”。比如，cpu=1 指的就是，这个 Pod 的 CPU 限额是 1 个 CPU。当然，具体“1 个 CPU”在宿主机上如何解释，是 1 个 CPU 核心，还是 1 个 vCPU，还是 1 个 CPU 的超线程（Hyperthread），完全取决于宿主机的 CPU 实现方式。Kubernetes 只负责保证 Pod 能够使用到“1 个 CPU”的计算能力。\n\n此外，Kubernetes 允许你将 CPU 限额设置为分数，比如在我们的例子里，CPU limits 的值就是 500m。所谓 500m，指的就是 500 millicpu，也就是 0.5 个 CPU 的意思。这样，这个 Pod 就会被分配到 1 个 CPU 一半的计算能力。\n\n当然，**你也可以直接把这个配置写成 cpu=0.5。但在实际使用时，我还是推荐你使用 500m 的写法，毕竟这才是 Kubernetes 内部通用的 CPU 表示方式。**\n\n\n而对于内存资源来说，它的单位自然就是 bytes。Kubernetes 支持你使用 Ei、Pi、Ti、Gi、Mi、Ki（或者 E、P、T、G、M、K）的方式来作为 bytes 的值。比如，在我们的例子里，Memory requests 的值就是 64MiB (2 的 26 次方 bytes) 。这里要注意区分 MiB（mebibyte）和 MB（megabyte）的区别。\n\n> 备注：1Mi=1024*1024；1M=1000*1000\n\n此外，不难看到，**Kubernetes 里 Pod 的 CPU 和内存资源，实际上还要分为 limits 和 requests 两种情况，**如下所示：\n\n\tspec.containers[].resources.limits.cpu\n\tspec.containers[].resources.limits.memory\n\tspec.containers[].resources.requests.cpu\n\tspec.containers[].resources.requests.memory\n这两者的区别其实非常简单：在调度的时候，kube-scheduler 只会按照 requests 的值进行计算。而在真正设置 Cgroups 限制的时候，kubelet 则会按照 limits 的值来进行设置。\n\n更确切地说，当你指定了 requests.cpu=250m 之后，相当于将 Cgroups 的 cpu.shares 的值设置为 (250/1000)*1024。而当你没有设置 requests.cpu 的时候，cpu.shares 默认则是 1024。这样，Kubernetes 就通过 cpu.shares 完成了对 CPU 时间的按比例分配。\n\n而如果你指定了 limits.cpu=500m 之后，则相当于将 Cgroups 的 cpu.cfs_quota_us 的值设置为 (500/1000)*100ms，而 cpu.cfs_period_us 的值始终是 100ms。这样，Kubernetes 就为你设置了这个容器只能用到 CPU 的 50%。\n\n而对于内存来说，当你指定了 limits.memory=128Mi 之后，相当于将 Cgroups 的 memory.limit_in_bytes 设置为 128 * 1024 * 1024。而需要注意的是，在调度的时候，调度器只会使用 requests.memory=64Mi 来进行判断。\n\n**Kubernetes 这种对 CPU 和内存资源限额的设计，实际上参考了 Borg 论文中对“动态资源边界”的定义，**既：容器化作业在提交时所设置的资源边界，并不一定是调度系统所必须严格遵守的，这是因为在实际场景中，大多数作业使用到的资源其实远小于它所请求的资源限额。\n\n基于这种假设，Borg 在作业被提交后，会主动减小它的资源限额配置，以便容纳更多的作业、提升资源利用率。而当作业资源使用量增加到一定阈值时，Borg 会通过“快速恢复”过程，还原作业原始的资源限额，防止出现异常情况。\n\n而 Kubernetes 的 requests+limits 的做法，其实就是上述思路的一个简化版：用户在提交 Pod 时，可以声明一个相对较小的 requests 值供调度器使用，而 Kubernetes 真正设置给容器 Cgroups 的，则是相对较大的 limits 值。不难看到，这跟 Borg 的思路相通的。\n\n`在理解了 Kubernetes 资源模型的设计之后，我再来和你谈谈 Kubernetes 里的 QoS 模型。`在 Kubernetes 中，不同的 requests 和 limits 的设置方式，其实会将这个 Pod 划分到不同的 QoS 级别当中。\n\n**当 Pod 里的每一个 Container 都同时设置了 requests 和 limits，并且 requests 和 limits 值相等的时候，这个 Pod 就属于 Guaranteed 类别，**如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: qos-demo\n\t  namespace: qos-example\n\tspec:\n\t  containers:\n\t  - name: qos-demo-ctr\n\t    image: nginx\n\t    resources:\n\t      limits:\n\t        memory: \"200Mi\"\n\t        cpu: \"700m\"\n\t      requests:\n\t        memory: \"200Mi\"\n\t        cpu: \"700m\"\n当这个 Pod 创建之后，它的 qosClass 字段就会被 Kubernetes 自动设置为 Guaranteed。需要注意的是，当 Pod 仅设置了 limits 没有设置 requests 的时候，Kubernetes 会自动为它设置与 limits 相同的 requests 值，所以，这也属于 Guaranteed 情况。\n\n**而当 Pod 不满足 Guaranteed 的条件，但至少有一个 Container 设置了 requests。那么这个 Pod 就会被划分到 Burstable 类别。**比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: qos-demo-2\n\t  namespace: qos-example\n\tspec:\n\t  containers:\n\t  - name: qos-demo-2-ctr\n\t    image: nginx\n\t    resources:\n\t      limits\n\t        memory: \"200Mi\"\n\t      requests:\n\t        memory: \"100Mi\"\n\n**而如果一个 Pod 既没有设置 requests，也没有设置 limits，那么它的 QoS 类别就是 BestEffort。**比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: qos-demo-3\n\t  namespace: qos-example\n\tspec:\n\t  containers:\n\t  - name: qos-demo-3-ctr\n\t    image: nginx\n那么，Kubernetes 为 Pod 设置这样三种 QoS 类别，具体有什么作用呢？\n\n实际上，**QoS 划分的主要应用场景，是当宿主机资源紧张的时候，kubelet 对 Pod 进行 Eviction（即资源回收）时需要用到的。**\n\n具体地说，当 Kubernetes 所管理的宿主机上不可压缩资源短缺时，就有可能触发 Eviction。比如，可用内存（memory.available）、可用的宿主机磁盘空间（nodefs.available），以及容器运行时镜像存储空间（imagefs.available）等等。\n\n目前，Kubernetes 为你设置的 Eviction 的默认阈值如下所示：\n\n\tmemory.available<100Mi\n\tnodefs.available<10%\n\tnodefs.inodesFree<5%\n\timagefs.available<15%\n当然，上述各个触发条件在 kubelet 里都是可配置的。比如下面这个例子：\n\n\tkubelet --eviction-hard=imagefs.available<10%,memory.available<500Mi,nodefs.available<5%,nodefs.inodesFree<5% --eviction-soft=imagefs.available<30%,nodefs.available<10% --eviction-soft-grace-period=imagefs.available=2m,nodefs.available=2m --eviction-max-pod-grace-period=600\n在这个配置中，你可以看到 **Eviction 在 Kubernetes 里其实分为 Soft 和 Hard 两种模式。**\n\n其中，Soft Eviction 允许你为 Eviction 过程设置一段“优雅时间”，比如上面例子里的 imagefs.available=2m，就意味着当 imagefs 不足的阈值达到 2 分钟之后，kubelet 才会开始 Eviction 的过程。\n\n而 Hard Eviction 模式下，Eviction 过程就会在阈值达到之后立刻开始。\n\n> Kubernetes 计算 Eviction 阈值的数据来源，主要依赖于从 Cgroups 读取到的值，以及使用 cAdvisor 监控到的数据。\n\n当宿主机的 Eviction 阈值达到后，就会进入 MemoryPressure 或者 DiskPressure 状态，从而避免新的 Pod 被调度到这台宿主机上。\n\n而当 Eviction 发生的时候，kubelet 具体会挑选哪些 Pod 进行删除操作，就需要参考这些 Pod 的 QoS 类别了。\n\n * 首当其冲的，自然是 BestEffort 类别的 Pod。\n\n * 其次，是属于 Burstable 类别、并且发生“饥饿”的资源使用量已经超出了 requests 的 Pod。\n\n * 最后，才是 Guaranteed 类别。并且，Kubernetes 会保证只有当 Guaranteed 类别的 Pod 的资源使用量超过了其 limits 的限制，或者宿主机本身正处于 Memory Pressure 状态时，Guaranteed 的 Pod 才可能被选中进行 Eviction 操作。\n\n当然，对于同 QoS 类别的 Pod 来说，Kubernetes 还会根据 Pod 的优先级来进行进一步地排序和选择。\n\n在理解了 Kubernetes 里的 QoS 类别的设计之后，我再来为你讲解一下`Kubernetes 里一个非常有用的特性：cpuset 的设置。`\n\n我们知道，在使用容器的时候，你可以通过设置 cpuset 把容器绑定到某个 CPU 的核上，而不是像 cpushare 那样共享 CPU 的计算能力。\n\n这种情况下，由于操作系统在 CPU 之间进行上下文切换的次数大大减少，容器里应用的性能会得到大幅提升。事实上，**cpuset 方式，是生产环境里部署在线应用类型的 Pod 时，非常常用的一种方式。**\n\n\n可是，这样的需求在 Kubernetes 里又该如何实现呢？\n\n其实非常简单。\n\n * 首先，你的 Pod 必须是 Guaranteed 的 QoS 类型；\n\n * 然后，你只需要将 Pod 的 CPU 资源的 requests 和 limits 设置为同一个相等的整数值即可。\n\n比如下面这个例子：\n\n\n\tspec:\n\t  containers:\n\t  - name: nginx\n\t    image: nginx\n\t    resources:\n\t      limits:\n\t        memory: \"200Mi\"\n\t        cpu: \"2\"\n\t      requests:\n\t        memory: \"200Mi\"\n\t        cpu: \"2\"\n这时候，该 Pod 就会被绑定在 2 个独占的 CPU 核上。当然，具体是哪两个 CPU 核，是由 kubelet 为你分配的。\n\n以上，就是 Kubernetes 的资源模型和 QoS 类别相关的主要内容。\n\n**总结**\n\n在本篇文章中，我先为你详细讲解了 Kubernetes 里对资源的定义方式和资源模型的设计。然后，我为你讲述了 Kubernetes 里对 Pod 进行 Eviction 的具体策略和实践方式。\n\n正是基于上述讲述，在实际的使用中，我强烈建议你将 DaemonSet 的 Pod 都设置为 Guaranteed 的 QoS 类型。否则，一旦 DaemonSet 的 Pod 被回收，它又会立即在原宿主机上被重建出来，这就使得前面资源回收的动作，完全没有意义了。\n\n**思考题**\n\n为什么宿主机进入 MemoryPressure 或者 DiskPressure 状态后，新的 Pod 就不会被调度到这台宿主机上呢？","categories":["reference","k8s"]},{"title":"41 | 十字路口上的Kubernetes默认调度器","url":"/2021/03/13/reference/k8s/41.十字路口上的Kubernetes默认调度器/","content":"\n![](1.jpg)\n\n## 十字路口上的Kubernetes默认调度器\n\n在上一篇文章中，我主要为你介绍了 Kubernetes 里关于资源模型和资源管理的设计方法。而在今天这篇文章中，我就来为你介绍一下 Kubernetes 的默认调度器（default scheduler）。\n<!-- more -->\n**在 Kubernetes 项目中，默认调度器的主要职责，就是为一个新创建出来的 Pod，寻找一个最合适的节点（Node）。**\n\n而这里“最合适”的含义，包括两层：\n\n从集群所有的节点中，根据调度算法挑选出所有可以运行该 Pod 的节点；\n\n从第一步的结果中，再根据调度算法挑选一个最符合条件的节点作为最终结果。\n\n所以在具体的调度流程中，默认调度器会首先调用一组叫作 Predicate 的调度算法，来检查每个 Node。然后，再调用一组叫作 Priority 的调度算法，来给上一步得到的结果里的每个 Node 打分。最终的调度结果，就是得分最高的那个 Node。\n\n而我在前面的文章中曾经介绍过，调度器对一个 Pod 调度成功，实际上就是将它的 spec.nodeName 字段填上调度结果的节点名字。\n\n> 备注：这里你可以再回顾下第 14 篇文章《深入解析 Pod 对象（一）：基本概念》中的相关内容。\n\n在 Kubernetes 中，上述调度机制的工作原理，可以用如下所示的一幅示意图来表示。\n\n![](2.jpg)\n\n可以看到，`Kubernetes 的调度器的核心，实际上就是两个相互独立的控制循环。`\n\n其中，**第一个控制循环，我们可以称之为 Informer Path**。它的主要目的，是启动一系列 Informer，用来监听（Watch）Etcd 中 Pod、Node、Service 等与调度相关的 API 对象的变化。比如，当一个待调度 Pod（即：它的 nodeName 字段是空的）被创建出来之后，调度器就会通过 Pod Informer 的 Handler，将这个待调度 Pod 添加进调度队列。\n\n在默认情况下，Kubernetes 的调度队列是一个 PriorityQueue（优先级队列），并且当某些集群信息发生变化的时候，调度器还会对调度队列里的内容进行一些特殊操作。这里的设计，主要是出于调度优先级和抢占的考虑，我会在后面的文章中再详细介绍这部分内容。\n\n此外，Kubernetes 的默认调度器还要负责对调度器缓存（即：scheduler cache）进行更新。事实上，Kubernetes 调度部分进行性能优化的一个最根本原则，就是尽最大可能将集群信息 Cache 化，以便从根本上提高 Predicate 和 Priority 调度算法的执行效率。\n\n而**第二个控制循环，是调度器负责 Pod 调度的主循环，我们可以称之为 Scheduling Path。**\n\nScheduling Path 的主要逻辑，就是不断地从调度队列里出队一个 Pod。然后，调用 Predicates 算法进行“过滤”。这一步“过滤”得到的一组 Node，就是所有可以运行这个 Pod 的宿主机列表。当然，Predicates 算法需要的 Node 信息，都是从 Scheduler Cache 里直接拿到的，这是调度器保证算法执行效率的主要手段之一。\n\n接下来，调度器就会再调用 Priorities 算法为上述列表里的 Node 打分，分数从 0 到 10。得分最高的 Node，就会作为这次调度的结果。\n\n调度算法执行完成后，调度器就需要将 Pod 对象的 nodeName 字段的值，修改为上述 Node 的名字。**这个步骤在 Kubernetes 里面被称作 Bind。**\n\n但是，为了不在关键调度路径里远程访问 APIServer，Kubernetes 的默认调度器在 Bind 阶段，只会更新 Scheduler Cache 里的 Pod 和 Node 的信息。**这种基于“乐观”假设的 API 对象更新方式，在 Kubernetes 里被称作 Assume。**\n\nAssume 之后，调度器才会创建一个 Goroutine 来异步地向 APIServer 发起更新 Pod 的请求，来真正完成 Bind 操作。如果这次异步的 Bind 过程失败了，其实也没有太大关系，等 Scheduler Cache 同步之后一切就会恢复正常。\n\n当然，正是由于上述 Kubernetes 调度器的“乐观”绑定的设计，当一个新的 Pod 完成调度需要在某个节点上运行起来之前，该节点上的 kubelet 还会通过一个叫作 Admit 的操作来再次验证该 Pod 是否确实能够运行在该节点上。这一步 Admit 操作，实际上就是把一组叫作 GeneralPredicates 的、最基本的调度算法，比如：“资源是否可用”“端口是否冲突”等再执行一遍，作为 kubelet 端的二次确认。\n\n> 备注：关于 Kubernetes 默认调度器的调度算法，我会在下一篇文章里为你讲解。\n\n**除了上述的“Cache 化”和“乐观绑定”，Kubernetes 默认调度器还有一个重要的设计，那就是“无锁化”。**\n\n在 Scheduling Path 上，调度器会启动多个 Goroutine 以节点为粒度并发执行 Predicates 算法，从而提高这一阶段的执行效率。而与之类似的，Priorities 算法也会以 MapReduce 的方式并行计算然后再进行汇总。而在这些所有需要并发的路径上，调度器会避免设置任何全局的竞争资源，从而免去了使用锁进行同步带来的巨大的性能损耗。\n\n所以，在这种思想的指导下，如果你再去查看一下前面的调度器原理图，你就会发现，Kubernetes 调度器只有对调度队列和 Scheduler Cache 进行操作时，才需要加锁。而这两部分操作，都不在 Scheduling Path 的算法执行路径上。\n\n当然，Kubernetes 调度器的上述设计思想，也是在集群规模不断增长的演进过程中逐步实现的。尤其是** “Cache 化”，这个变化其实是最近几年 Kubernetes 调度器性能得以提升的一个关键演化。**\n\n不过，随着 Kubernetes 项目发展到今天，它的默认调度器也已经来到了一个关键的十字路口。事实上，Kubernetes 现今发展的主旋律，是整个开源项目的“民主化”。也就是说，Kubernetes 下一步发展的方向，是组件的轻量化、接口化和插件化。所以，我们才有了 CRI、CNI、CSI、CRD、Aggregated APIServer、Initializer、Device Plugin 等各个层级的可扩展能力。可是，默认调度器，却成了 Kubernetes 项目里最后一个没有对外暴露出良好定义过的、可扩展接口的组件。\n\n当然，这是有一定的历史原因的。在过去几年，Kubernetes 发展的重点，都是以功能性需求的实现和完善为核心。在这个过程中，它的很多决策，还是以优先服务公有云的需求为主，而性能和规模则居于相对次要的位置。\n\n而现在，随着 Kubernetes 项目逐步趋于稳定，越来越多的用户开始把 Kubernetes 用在规模更大、业务更加复杂的私有集群当中。很多以前的 Mesos 用户，也开始尝试使用 Kubernetes 来替代其原有架构。`在这些场景下，对默认调度器进行扩展和重新实现，就成了社区对 Kubernetes 项目最主要的一个诉求。`\n\n所以，Kubernetes 的默认调度器，是目前这个项目里为数不多的、正在经历大量重构的核心组件之一。这些正在进行的重构的目的，一方面是将默认调度器里大量的“技术债”清理干净；另一方面，就是为默认调度器的可扩展性设计进行铺垫。\n\n而 Kubernetes 默认调度器的可扩展性设计，可以用如下所示的一幅示意图来描述：\n\n![](3.jpg)\n\n可以看到，默认调度器的可扩展机制，在 Kubernetes 里面叫作 Scheduler Framework。顾名思义，这个设计的主要目的，就是在调度器生命周期的各个关键点上，为用户暴露出可以进行扩展和实现的接口，从而实现由用户自定义调度器的能力。\n\n上图中，每一个绿色的箭头都是一个可以插入自定义逻辑的接口。比如，上面的 Queue 部分，就意味着你可以在这一部分提供一个自己的调度队列的实现，从而控制每个 Pod 开始被调度（出队）的时机。\n\n而 Predicates 部分，则意味着你可以提供自己的过滤算法实现，根据自己的需求，来决定选择哪些机器。\n\n**需要注意的是，上述这些可插拔式逻辑，都是标准的 Go 语言插件机制（Go plugin 机制），**也就是说，你需要在编译的时候选择把哪些插件编译进去。\n\n有了上述设计之后，扩展和自定义 Kubernetes 的默认调度器就变成了一件非常容易实现的事情。这也意味着默认调度器在后面的发展过程中，必然不会在现在的实现上再添加太多的功能，反而还会对现在的实现进行精简，最终成为 Scheduler Framework 的一个最小实现。而调度领域更多的创新和工程工作，就可以交给整个社区来完成了。这个思路，是完全符合我在前面提到的 Kubernetes 的“民主化”设计的。\n\n不过，这样的 Scheduler Framework 也有一个不小的问题，那就是一旦这些插入点的接口设计不合理，就会导致整个生态没办法很好地把这个插件机制使用起来。而与此同时，这些接口本身的变更又是一个费时费力的过程，一旦把控不好，就很可能会把社区推向另一个极端，即：Scheduler Framework 没法实际落地，大家只好都再次 fork kube-scheduler。\n\n**总结**\n\n在本篇文章中，我为你详细讲解了 Kubernetes 里默认调度器的设计与实现，分析了它现在正在经历的重构，以及未来的走向。\n\n不难看到，在 Kubernetes 的整体架构中，kube-scheduler 的责任虽然重大，但其实它却是在社区里最少受到关注的组件之一。这里的原因也很简单，调度这个事情，在不同的公司和团队里的实际需求一定是大相径庭的，上游社区不可能提供一个大而全的方案出来。所以，将默认调度器进一步做轻做薄，并且插件化，才是 kube-scheduler 正确的演进方向。\n\n**思考题**\n\n请问，Kubernetes 默认调度器与 Mesos 的“两级”调度器，有什么异同呢？\n\n","categories":["reference","k8s"]},{"title":"43 | Kubernetes默认调度器的优先级与抢占机制","url":"/2021/03/13/reference/k8s/43.Kubernetes默认调度器的优先级与抢占机制/","content":"\n![](1.jpg)\n\n## Kubernetes默认调度器的优先级与抢占机制\n\n在上一篇文章中，我为你详细讲解了 Kubernetes 默认调度器的主要调度算法的工作原理。在本篇文章中，我再来为你讲解一下 Kubernetes 调度器里的另一个重要机制，即：优先级（Priority ）和抢占（Preemption）机制。\n<!-- more -->\n`首先需要明确的是，优先级和抢占机制，解决的是 Pod 调度失败时该怎么办的问题。`\n\n正常情况下，当一个 Pod 调度失败后，它就会被暂时“搁置”起来，直到 Pod 被更新，或者集群状态发生变化，调度器才会对这个 Pod 进行重新调度。\n\n但在有时候，我们希望的是这样一个场景。当一个高优先级的 Pod 调度失败后，该 Pod 并不会被“搁置”，而是会“挤走”某个 Node 上的一些低优先级的 Pod 。这样就可以保证这个高优先级 Pod 的调度成功。这个特性，其实也是一直以来就存在于 Borg 以及 Mesos 等项目里的一个基本功能。\n\n而在 Kubernetes 里，优先级和抢占机制是在 1.10 版本后才逐步可用的。要使用这个机制，你首先需要在 Kubernetes 里提交一个 PriorityClass 的定义，如下所示：\n\n\tapiVersion: scheduling.k8s.io/v1beta1\n\tkind: PriorityClass\n\tmetadata:\n\t  name: high-priority\n\tvalue: 1000000\n\tglobalDefault: false\n\tdescription: \"This priority class should be used for high priority service pods only.\"\n上面这个 YAML 文件，定义的是一个名叫 high-priority 的 PriorityClass，其中 value 的值是 1000000 （一百万）。\n\n**Kubernetes 规定，优先级是一个 32 bit 的整数，最大值不超过 1000000000（10 亿，1 billion），并且值越大代表优先级越高。**而超出 10 亿的值，其实是被 Kubernetes 保留下来分配给系统 Pod 使用的。显然，这样做的目的，就是保证系统 Pod 不会被用户抢占掉。\n\n而一旦上述 YAML 文件里的 globalDefault 被设置为 true 的话，那就意味着这个 PriorityClass 的值会成为系统的默认值。而如果这个值是 false，就表示我们只希望声明使用该 PriorityClass 的 Pod 拥有值为 1000000 的优先级，而对于没有声明 PriorityClass 的 Pod 来说，它们的优先级就是 0。\n\n在创建了 PriorityClass 对象之后，Pod 就可以声明使用它了，如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: nginx\n\t  labels:\n\t    env: test\n\tspec:\n\t  containers:\n\t  - name: nginx\n\t    image: nginx\n\t    imagePullPolicy: IfNotPresent\n\t  priorityClassName: high-priority\n可以看到，这个 Pod 通过 priorityClassName 字段，声明了要使用名叫 high-priority 的 PriorityClass。当这个 Pod 被提交给 Kubernetes 之后，Kubernetes 的 PriorityAdmissionController 就会自动将这个 Pod 的 spec.priority 字段设置为 1000000。\n\n而我在前面的文章中曾为你介绍过，调度器里维护着一个调度队列。所以，当 Pod 拥有了优先级之后，高优先级的 Pod 就可能会比低优先级的 Pod 提前出队，从而尽早完成调度过程。这个过程，就是“优先级”这个概念在 Kubernetes 里的主要体现。\n\n> 备注：这里，你可以再回顾一下第 41 篇文章《十字路口上的 Kubernetes 默认调度器》中的相关内容。\n\n而当一个高优先级的 Pod 调度失败的时候，调度器的抢占能力就会被触发。这时，调度器就会试图从当前集群里寻找一个节点，使得当这个节点上的一个或者多个低优先级 Pod 被删除后，待调度的高优先级 Pod 就可以被调度到这个节点上。**这个过程，就是“抢占”这个概念在 Kubernetes 里的主要体现。**\n\n为了方便叙述，我接下来会把待调度的高优先级 Pod 称为“抢占者”（Preemptor）。\n\n当上述抢占过程发生时，抢占者并不会立刻被调度到被抢占的 Node 上。事实上，调度器只会将抢占者的 spec.nominatedNodeName 字段，设置为被抢占的 Node 的名字。然后，抢占者会重新进入下一个调度周期，然后在新的调度周期里来决定是不是要运行在被抢占的节点上。这当然也就意味着，即使在下一个调度周期，调度器也不会保证抢占者一定会运行在被抢占的节点上。\n\n这样设计的一个重要原因是，调度器只会通过标准的 DELETE API 来删除被抢占的 Pod，所以，这些 Pod 必然是有一定的“优雅退出”时间（默认是 30s）的。而在这段时间里，其他的节点也是有可能变成可调度的，或者直接有新的节点被添加到这个集群中来。所以，鉴于优雅退出期间，集群的可调度性可能会发生的变化，**把抢占者交给下一个调度周期再处理，是一个非常合理的选择。**\n\n而在抢占者等待被调度的过程中，如果有其他更高优先级的 Pod 也要抢占同一个节点，那么调度器就会清空原抢占者的 spec.nominatedNodeName 字段，从而允许更高优先级的抢占者执行抢占，并且，这也就使得原抢占者本身，也有机会去重新抢占其他节点。这些，都是设置 nominatedNodeName 字段的主要目的。\n\n那么，`Kubernetes 调度器里的抢占机制，又是如何设计的呢？`\n\n接下来，我就为你详细讲述一下这其中的原理。\n\n我在前面已经提到过，抢占发生的原因，一定是一个高优先级的 Pod 调度失败。这一次，我们还是称这个 Pod 为“抢占者”，称被抢占的 Pod 为“牺牲者”（victims）。\n\n而 Kubernetes 调度器实现抢占算法的一个最重要的设计，就是在调度队列的实现里，使用了两个不同的队列。\n\n**第一个队列，叫作 activeQ。**凡是在 activeQ 里的 Pod，都是下一个调度周期需要调度的对象。所以，当你在 Kubernetes 集群里新创建一个 Pod 的时候，调度器会将这个 Pod 入队到 activeQ 里面。而我在前面提到过的、调度器不断从队列里出队（Pop）一个 Pod 进行调度，实际上都是从 activeQ 里出队的。\n\n**第二个队列，叫作 unschedulableQ，**专门用来存放调度失败的 Pod。\n\n而这里的一个关键点就在于，当一个 unschedulableQ 里的 Pod 被更新之后，调度器会自动把这个 Pod 移动到 activeQ 里，从而给这些调度失败的 Pod “重新做人”的机会。\n\n现在，回到我们的抢占者调度失败这个时间点上来。\n\n调度失败之后，抢占者就会被放进 unschedulableQ 里面。\n\n然后，这次失败事件就会触发**调度器为抢占者寻找牺牲者的流程。**\n\n**第一步**，调度器会检查这次失败事件的原因，来确认抢占是不是可以帮助抢占者找到一个新节点。这是因为有很多 Predicates 的失败是不能通过抢占来解决的。比如，PodFitsHost 算法（负责的是，检查 Pod 的 nodeSelector 与 Node 的名字是否匹配），这种情况下，除非 Node 的名字发生变化，否则你即使删除再多的 Pod，抢占者也不可能调度成功。\n\n**第二步**，如果确定抢占可以发生，那么调度器就会把自己缓存的所有节点信息复制一份，然后使用这个副本来模拟抢占过程。\n\n这里的抢占过程很容易理解。调度器会检查缓存副本里的每一个节点，然后从该节点上最低优先级的 Pod 开始，逐一“删除”这些 Pod。而每删除一个低优先级 Pod，调度器都会检查一下抢占者是否能够运行在该 Node 上。一旦可以运行，调度器就记录下这个 Node 的名字和被删除 Pod 的列表，这就是一次抢占过程的结果了。\n\n当遍历完所有的节点之后，调度器会在上述模拟产生的所有抢占结果里做一个选择，找出最佳结果。而这一步的**判断原则，就是尽量减少抢占对整个系统的影响**。比如，需要抢占的 Pod 越少越好，需要抢占的 Pod 的优先级越低越好，等等。\n\n\n在得到了最佳的抢占结果之后，这个结果里的 Node，就是即将被抢占的 Node；被删除的 Pod 列表，就是牺牲者。所以接下来，**调度器就可以真正开始抢占的操作了**，这个过程，可以分为三步。\n\n**第一步**，调度器会检查牺牲者列表，清理这些 Pod 所携带的 nominatedNodeName 字段。\n\n**第二步**，调度器会把抢占者的 nominatedNodeName，设置为被抢占的 Node 的名字。\n\n**第三步**，调度器会开启一个 Goroutine，同步地删除牺牲者。\n\n而第二步对抢占者 Pod 的更新操作，就会触发到我前面提到的“重新做人”的流程，从而让抢占者在下一个调度周期重新进入调度流程。\n\n所以**接下来，调度器就会通过正常的调度流程把抢占者调度成功**。这也是为什么，我前面会说调度器并不保证抢占的结果：在这个正常的调度流程里，是一切皆有可能的。\n\n不过，对于任意一个待调度 Pod 来说，因为有上述抢占者的存在，它的调度过程，其实是有一些特殊情况需要特殊处理的。\n\n具体来说，在为某一对 Pod 和 Node 执行 Predicates 算法的时候，如果待检查的 Node 是一个即将被抢占的节点，即：调度队列里有 nominatedNodeName 字段值是该 Node 名字的 Pod 存在（可以称之为：“潜在的抢占者”）。那么，**调度器就会对这个 Node ，将同样的 Predicates 算法运行两遍。**\n\n**第一遍**， 调度器会假设上述“潜在的抢占者”已经运行在这个节点上，然后执行 Predicates 算法；\n\n**第二遍**， 调度器会正常执行 Predicates 算法，即：不考虑任何“潜在的抢占者”。\n\n而只有这两遍 Predicates 算法都能通过时，这个 Pod 和 Node 才会被认为是可以绑定（bind）的。\n\n不难想到，这里需要执行第一遍 Predicates 算法的原因，是由于 InterPodAntiAffinity 规则的存在。\n\n由于 InterPodAntiAffinity 规则关心待考察节点上所有 Pod 之间的互斥关系，所以我们在执行调度算法时必须考虑，如果抢占者已经存在于待考察 Node 上时，待调度 Pod 还能不能调度成功。\n\n当然，这也就意味着，我们在这一步只需要考虑那些优先级等于或者大于待调度 Pod 的抢占者。毕竟对于其他较低优先级 Pod 来说，待调度 Pod 总是可以通过抢占运行在待考察 Node 上。\n\n而我们需要执行第二遍 Predicates 算法的原因，则是因为“潜在的抢占者”最后不一定会运行在待考察的 Node 上。关于这一点，我在前面已经讲解过了：Kubernetes 调度器并不保证抢占者一定会运行在当初选定的被抢占的 Node 上。\n\n以上，就是 Kubernetes 默认调度器里优先级和抢占机制的实现原理了。\n\n**总结**\n\n在本篇文章中，我为你详细讲述了 Kubernetes 里关于 Pod 的优先级和抢占机制的设计与实现。\n\n这个特性在 v1.11 之后已经是 Beta 了，意味着比较稳定了。所以，我建议你在 Kubernetes 集群中开启这两个特性，以便实现更高的资源使用率。\n\n**思考题**\n\n当整个集群发生可能会影响调度结果的变化（比如，添加或者更新 Node，添加和更新 PV、Service 等）时，调度器会执行一个被称为 MoveAllToActiveQueue 的操作，把所调度失败的 Pod 从 unscheduelableQ 移动到 activeQ 里面。请问这是为什么？\n\n一个相似的问题是，当一个已经调度成功的 Pod 被更新时，调度器则会将 unschedulableQ 里所有跟这个 Pod 有 Affinity/Anti-affinity 关系的 Pod，移动到 activeQ 里面。请问这又是为什么呢？\n","categories":["reference","k8s"]},{"title":"44 | Kubernetes GPU管理与Device Plugin机制","url":"/2021/03/13/reference/k8s/44.Kubernetes GPU管理与Device Plugin机制/","content":"\n![](1.jpg)\n\n## Kubernetes GPU管理与Device Plugin机制\n\n2016 年，随着 AlphaGo 的走红和 TensorFlow 项目的异军突起，一场名为 AI 的技术革命迅速从学术界蔓延到了工业界，所谓的 AI 元年，就此拉开帷幕。\n<!-- more -->\n当然，机器学习或者说人工智能，并不是什么新鲜的概念。而这次热潮的背后，云计算服务的普及与成熟，以及算力的巨大提升，其实正是将人工智能从象牙塔带到工业界的一个重要推手。\n\n而与之相对应的，从 2016 年开始，Kubernetes 社区就不断收到来自不同渠道的大量诉求，希望能够在 Kubernetes 集群上运行 TensorFlow 等机器学习框架所创建的训练（Training）和服务（Serving）任务。而这些诉求中，除了前面我为你讲解过的 Job、Operator 等离线作业管理需要用到的编排概念之外，还有一个亟待实现的功能，就是对 GPU 等硬件加速设备管理的支持。\n\n不过， 正如同 TensorFlow 之于 Google 的战略意义一样，**GPU 支持对于 Kubernetes 项目来说，其实也有着超过技术本身的考虑**。所以，尽管在硬件加速器这个领域里，Kubernetes 上游有着不少来自 NVIDIA 和 Intel 等芯片厂商的工程师，但这个特性本身，却从一开始就是以 Google Cloud 的需求为主导来推进的。\n\n而对于云的用户来说，在 GPU 的支持上，他们最基本的诉求其实非常简单：我只要在 Pod 的 YAML 里面，声明某容器需要的 GPU 个数，那么 Kubernetes 为我创建的容器里就应该出现对应的 GPU 设备，以及它对应的驱动目录。\n\n以 NVIDIA 的 GPU 设备为例，上面的需求就意味着当用户的容器被创建之后，这个容器里必须出现如下两部分设备和目录：\n\n1. GPU 设备，比如 /dev/nvidia0；\n\n2. GPU 驱动目录，比如 /usr/local/nvidia/*。\n\n其中，GPU 设备路径，正是该容器启动时的 Devices 参数；而驱动目录，则是该容器启动时的 Volume 参数。所以，在 Kubernetes 的 GPU 支持的实现里，kubelet 实际上就是将上述两部分内容，设置在了创建该容器的 CRI （Container Runtime Interface）参数里面。这样，等到该容器启动之后，对应的容器里就会出现 GPU 设备和驱动的路径了。\n\n不过，Kubernetes 在 Pod 的 API 对象里，并没有为 GPU 专门设置一个资源类型字段，而是使用了一种叫作 Extended Resource（ER）的特殊字段来负责传递 GPU 的信息。比如下面这个例子：\n\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: cuda-vector-add\n\tspec:\n\t  restartPolicy: OnFailure\n\t  containers:\n\t    - name: cuda-vector-add\n\t      image: \"k8s.gcr.io/cuda-vector-add:v0.1\"\n\t      resources:\n\t        limits:\n\t          nvidia.com/gpu: 1\n可以看到，在上述 Pod 的 limits 字段里，这个资源的名称是nvidia.com/gpu，它的值是 1。也就是说，这个 Pod 声明了自己要使用一个 NVIDIA 类型的 GPU。\n\n而在 kube-scheduler 里面，它其实并不关心这个字段的具体含义，只会在计算的时候，一律将调度器里保存的该类型资源的可用量，直接减去 Pod 声明的数值即可。所以说，Extended Resource，其实是 Kubernetes 为用户设置的一种对自定义资源的支持。\n\n当然，为了能够让调度器知道这个自定义类型的资源在每台宿主机上的可用量，宿主机节点本身，就必须能够向 API Server 汇报该类型资源的可用数量。在 Kubernetes 里，各种类型的资源可用量，其实是 Node 对象 Status 字段的内容，比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Node\n\tmetadata:\n\t  name: node-1\n\t...\n\tStatus:\n\t  Capacity:\n\t   cpu:  2\n\t   memory:  2049008Ki\n而为了能够在上述 Status 字段里添加自定义资源的数据，你就必须使用 PATCH API 来对该 Node 对象进行更新，加上你的自定义资源的数量。这个 PATCH 操作，可以简单地使用 curl 命令来发起，如下所示：\n\n\t# 启动 Kubernetes 的客户端 proxy，这样你就可以直接使用 curl 来跟 Kubernetes  的API Server 进行交互了\n\t$ kubectl proxy\n\t# 执行 PACTH 操作\n\t$ curl --header \"Content-Type: application/json-patch+json\" \\\n\t--request PATCH \\\n\t--data '[{\"op\": \"add\", \"path\": \"/status/capacity/nvidia.com/gpu\", \"value\": \"1\"}]' \\\n\thttp://localhost:8001/api/v1/nodes/<your-node-name>/status\n\tPATCH 操作完成后，你就可以看到 Node 的 Status 变成了如下所示的内容：\n\tapiVersion: v1\n\tkind: Node\n\t...\n\tStatus:\n\t  Capacity:\n\t   cpu:  2\n\t   memory:  2049008Ki\n\t   nvidia.com/gpu: 1\n这样在调度器里，它就能够在缓存里记录下 node-1 上的nvidia.com/gpu类型的资源的数量是 1。\n\n当然，在 Kubernetes 的 GPU 支持方案里，你并不需要真正去做上述关于 Extended Resource 的这些操作。在 Kubernetes 中，对所有硬件加速设备进行管理的功能，都是由一种叫作 Device Plugin 的插件来负责的。这其中，当然也就包括了对该硬件的 Extended Resource 进行汇报的逻辑。\n\nKubernetes 的 Device Plugin 机制，我可以用如下所示的一幅示意图来和你解释清楚。\n\n![](2.jpg)\n\n我们先从这幅示意图的右侧开始看起。\n\n首先，对于每一种硬件设备，都需要有它所对应的 Device Plugin 进行管理，这些 Device Plugin，都通过 gRPC 的方式，同 kubelet 连接起来。以 NVIDIA GPU 为例，它对应的插件叫作[NVIDIA GPU device plugin](https://github.com/NVIDIA/k8s-device-plugin)。\n\n这个 Device Plugin 会通过一个叫作 ListAndWatch 的 API，定期向 kubelet 汇报该 Node 上 GPU 的列表。比如，在我们的例子里，一共有三个 GPU（GPU0、GPU1 和 GPU2）。这样，kubelet 在拿到这个列表之后，就可以直接在它向 APIServer 发送的心跳里，以 Extended Resource 的方式，加上这些 GPU 的数量，比如nvidia.com/gpu=3。所以说，用户在这里是不需要关心 GPU 信息向上的汇报流程的。\n\n需要注意的是，ListAndWatch 向上汇报的信息，只有本机上 GPU 的 ID 列表，而不会有任何关于 GPU 设备本身的信息。而且 kubelet 在向 API Server 汇报的时候，只会汇报该 GPU 对应的 Extended Resource 的数量。当然，kubelet 本身，会将这个 GPU 的 ID 列表保存在自己的内存里，并通过 ListAndWatch API 定时更新。\n\n而当一个 Pod 想要使用一个 GPU 的时候，它只需要像我在本文一开始给出的例子一样，在 Pod 的 limits 字段声明nvidia.com/gpu: 1。那么接下来，Kubernetes 的调度器就会从它的缓存里，寻找 GPU 数量满足条件的 Node，然后将缓存里的 GPU 数量减 1，完成 Pod 与 Node 的绑定。\n\n这个调度成功后的 Pod 信息，自然就会被对应的 kubelet 拿来进行容器操作。而当 kubelet 发现这个 Pod 的容器请求一个 GPU 的时候，kubelet 就会从自己持有的 GPU 列表里，为这个容器分配一个 GPU。此时，kubelet 就会向本机的 Device Plugin 发起一个 Allocate() 请求。这个请求携带的参数，正是即将分配给该容器的设备 ID 列表。\n\n当 Device Plugin 收到 Allocate 请求之后，它就会根据 kubelet 传递过来的设备 ID，从 Device Plugin 里找到这些设备对应的设备路径和驱动目录。当然，这些信息，正是 Device Plugin 周期性的从本机查询到的。比如，在 NVIDIA Device Plugin 的实现里，它会定期访问 nvidia-docker 插件，从而获取到本机的 GPU 信息。\n\n而被分配 GPU 对应的设备路径和驱动目录信息被返回给 kubelet 之后，kubelet 就完成了为一个容器分配 GPU 的操作。接下来，kubelet 会把这些信息追加在创建该容器所对应的 CRI 请求当中。这样，当这个 CRI 请求发给 Docker 之后，Docker 为你创建出来的容器里，就会出现这个 GPU 设备，并把它所需要的驱动目录挂载进去。\n\n至此，Kubernetes 为一个 Pod 分配一个 GPU 的流程就完成了。\n\n对于其他类型硬件来说，要想在 Kubernetes 所管理的容器里使用这些硬件的话，也需要遵循上述 Device Plugin 的流程来实现如下所示的 Allocate 和 ListAndWatch API：\n\n\t  service DevicePlugin {\n\t        // ListAndWatch returns a stream of List of Devices\n\t        // Whenever a Device state change or a Device disappears, ListAndWatch\n\t        // returns the new list\n\t        rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}\n\t        // Allocate is called during container creation so that the Device\n\t        // Plugin can run device specific operations and instruct Kubelet\n\t        // of the steps to make the Device available in the container\n\t        rpc Allocate(AllocateRequest) returns (AllocateResponse) {}\n\t  }\n目前，Kubernetes 社区里已经实现了很多硬件插件，比如FPGA、SRIOV、RDMA等等。感兴趣的话，你可以点击这些链接来查看这些 Device Plugin 的实现。\n\n**总结**\n\n在本篇文章中，我为你详细讲述了 Kubernetes 对 GPU 的管理方式，以及它所需要使用的 Device Plugin 机制。\n\n需要指出的是，Device Plugin 的设计，长期以来都是以 Google Cloud 的用户需求为主导的，所以，它的整套工作机制和流程上，实际上跟学术界和工业界的真实场景还有着不小的差异。\n\n这里最大的问题在于，GPU 等硬件设备的调度工作，实际上是由 kubelet 完成的。即，kubelet 会负责从它所持有的硬件设备列表中，为容器挑选一个硬件设备，然后调用 Device Plugin 的 Allocate API 来完成这个分配操作。可以看到，在整条链路中，调度器扮演的角色，仅仅是为 Pod 寻找到可用的、支持这种硬件设备的节点而已。\n\n这就使得，Kubernetes 里对硬件设备的管理，只能处理“设备个数”这唯一一种情况。一旦你的设备是异构的、不能简单地用“数目”去描述具体使用需求的时候，比如，“我的 Pod 想要运行在计算能力最强的那个 GPU 上”，Device Plugin 就完全不能处理了。\n\n更不用说，在很多场景下，我们其实希望在调度器进行调度的时候，就可以根据整个集群里的某种硬件设备的全局分布，做出一个最佳的调度选择。\n\n此外，上述 Device Plugin 的设计，也使得 Kubernetes 里，缺乏一种能够对 Device 进行描述的 API 对象。这就使得如果你的硬件设备本身的属性比较复杂，并且 Pod 也关心这些硬件的属性的话，那么 Device Plugin 也是完全没有办法支持的。\n\n更为棘手的是，在 Device Plugin 的设计和实现中，Google 的工程师们一直不太愿意为 Allocate 和 ListAndWatch API 添加可扩展性的参数。这就使得，当你确实需要处理一些比较复杂的硬件设备使用需求时，是没有办法通过扩展 Device Plugin 的 API 来实现的。\n\n针对这些问题，RedHat 在社区里曾经大力推进过 ResourceClass的设计，试图将硬件设备的管理功能上浮到 API 层和调度层。但是，由于各方势力的反对，这个提议最后不了了之了。\n\n所以说，目前 Kubernetes 本身的 Device Plugin 的设计，实际上能覆盖的场景是非常单一的，属于“可用”但是“不好用”的状态。并且， Device Plugin 的 API 本身的可扩展性也不是很好。这也就解释了为什么像 NVIDIA 这样的硬件厂商，实际上并没有完全基于上游的 Kubernetes 代码来实现自己的 GPU 解决方案，而是做了一定的改动，也就是 fork。这，实属不得已而为之。\n\n**思考题**\n\n请你结合自己的需求谈一谈，你希望如何对当前的 Device Plugin 进行改进呢？或者说，你觉得当前的设计已经完全够用了吗\n\n\n","categories":["reference","k8s"]},{"title":"42 | Kubernetes默认调度器调度策略解析","url":"/2021/03/13/reference/k8s/42.Kubernetes默认调度器调度策略解析/","content":"\n![](1.jpg)\n\n## Kubernetes默认调度器调度策略解析\n\n在上一篇文章中，我主要为你讲解了 Kubernetes 默认调度器的设计原理和架构。在今天这篇文章中，我们就专注在调度过程中 Predicates 和 Priorities 这两个调度策略主要发生作用的阶段。\n<!-- more -->\n`首先，我们一起看看 Predicates。`\n\n**Predicates 在调度过程中的作用，可以理解为 Filter，**即：它按照调度策略，从当前集群的所有节点中，“过滤”出一系列符合条件的节点。这些节点，都是可以运行待调度 Pod 的宿主机。\n\n而在 Kubernetes 中，默认的调度策略有如下三种。\n\n**第一种类型，叫作 GeneralPredicates。**\n\n顾名思义，这一组过滤规则，负责的是最基础的调度策略。比如，PodFitsResources 计算的就是宿主机的 CPU 和内存资源等是否够用。\n\n当然，我在前面已经提到过，PodFitsResources 检查的只是 Pod 的 requests 字段。需要注意的是，Kubernetes 的调度器并没有为 GPU 等硬件资源定义具体的资源类型，而是统一用一种名叫 Extended Resource 的、Key-Value 格式的扩展字段来描述的。比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: extended-resource-demo\n\tspec:\n\t  containers:\n\t  - name: extended-resource-demo-ctr\n\t    image: nginx\n\t    resources:\n\t      requests:\n\t        alpha.kubernetes.io/nvidia-gpu: 2\n\t      limits:\n\t        alpha.kubernetes.io/nvidia-gpu: 2\n可以看到，我们这个 Pod 通过alpha.kubernetes.io/nvidia-gpu=2这样的定义方式，声明使用了两个 NVIDIA 类型的 GPU。\n\n而在 PodFitsResources 里面，调度器其实并不知道这个字段 Key 的含义是 GPU，而是直接使用后面的 Value 进行计算。当然，在 Node 的 Capacity 字段里，你也得相应地加上这台宿主机上 GPU 的总数，比如：alpha.kubernetes.io/nvidia-gpu=4。这些流程，我在后面讲解 Device Plugin 的时候会详细介绍。\n\n而 PodFitsHost 检查的是，宿主机的名字是否跟 Pod 的 spec.nodeName 一致。\n\nPodFitsHostPorts 检查的是，Pod 申请的宿主机端口（spec.nodePort）是不是跟已经被使用的端口有冲突。\n\nPodMatchNodeSelector 检查的是，Pod 的 nodeSelector 或者 nodeAffinity 指定的节点，是否与待考察节点匹配，等等。\n\n可以看到，像上面这样一组 GeneralPredicates，正是 Kubernetes 考察一个 Pod 能不能运行在一个 Node 上最基本的过滤条件。所以，GeneralPredicates 也会被其他组件（比如 kubelet）直接调用。\n\n我在上一篇文章中已经提到过，kubelet 在启动 Pod 前，会执行一个 Admit 操作来进行二次确认。这里二次确认的规则，就是执行一遍 GeneralPredicates。\n\n**第二种类型，是与 Volume 相关的过滤规则。**\n\n这一组过滤规则，负责的是跟容器持久化 Volume 相关的调度策略。\n\n其中，NoDiskConflict 检查的条件，是多个 Pod 声明挂载的持久化 Volume 是否有冲突。比如，AWS EBS 类型的 Volume，是不允许被两个 Pod 同时使用的。所以，当一个名叫 A 的 EBS Volume 已经被挂载在了某个节点上时，另一个同样声明使用这个 A Volume 的 Pod，就不能被调度到这个节点上了。\n\n而 MaxPDVolumeCountPredicate 检查的条件，则是一个节点上某种类型的持久化 Volume 是不是已经超过了一定数目，如果是的话，那么声明使用该类型持久化 Volume 的 Pod 就不能再调度到这个节点了。\n\n而 VolumeZonePredicate，则是检查持久化 Volume 的 Zone（高可用域）标签，是否与待考察节点的 Zone 标签相匹配。\n\n此外，这里还有一个叫作 VolumeBindingPredicate 的规则。它负责检查的，是该 Pod 对应的 PV 的 nodeAffinity 字段，是否跟某个节点的标签相匹配。\n\n在前面的第 29 篇文章《PV、PVC 体系是不是多此一举？从本地持久化卷谈起》中，我曾经为你讲解过，Local Persistent Volume（本地持久化卷），必须使用 nodeAffinity 来跟某个具体的节点绑定。这其实也就意味着，在 Predicates 阶段，Kubernetes 就必须能够根据 Pod 的 Volume 属性来进行调度。\n\n此外，如果该 Pod 的 PVC 还没有跟具体的 PV 绑定的话，调度器还要负责检查所有待绑定 PV，当有可用的 PV 存在并且该 PV 的 nodeAffinity 与待考察节点一致时，这条规则才会返回“成功”。比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: PersistentVolume\n\tmetadata:\n\t  name: example-local-pv\n\tspec:\n\t  capacity:\n\t    storage: 500Gi\n\t  accessModes:\n\t  - ReadWriteOnce\n\t  persistentVolumeReclaimPolicy: Retain\n\t  storageClassName: local-storage\n\t  local:\n\t    path: /mnt/disks/vol1\n\t  nodeAffinity:\n\t    required:\n\t      nodeSelectorTerms:\n\t      - matchExpressions:\n\t        - key: kubernetes.io/hostname\n\t          operator: In\n\t          values:\n\t          - my-node\n可以看到，这个 PV 对应的持久化目录，只会出现在名叫 my-node 的宿主机上。所以，任何一个通过 PVC 使用这个 PV 的 Pod，都必须被调度到 my-node 上才可以正常工作。VolumeBindingPredicate，正是调度器里完成这个决策的位置。\n\n**第三种类型，是宿主机相关的过滤规则。**\n\n这一组规则，主要考察待调度 Pod 是否满足 Node 本身的某些条件。\n\n比如，PodToleratesNodeTaints，负责检查的就是我们前面经常用到的 Node 的“污点”机制。只有当 Pod 的 Toleration 字段与 Node 的 Taint 字段能够匹配的时候，这个 Pod 才能被调度到该节点上。\n\n> 备注：这里，你也可以再回顾下第 21 篇文章《容器化守护进程的意义：DaemonSet》中的相关内容。\n\n而 NodeMemoryPressurePredicate，检查的是当前节点的内存是不是已经不够充足，如果是的话，那么待调度 Pod 就不能被调度到该节点上。\n\n**第四种类型，是 Pod 相关的过滤规则。**\n\n这一组规则，跟 GeneralPredicates 大多数是重合的。而比较特殊的，是 PodAffinityPredicate。这个规则的作用，是检查待调度 Pod 与 Node 上的已有 Pod 之间的亲密（affinity）和反亲密（anti-affinity）关系。比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: with-pod-antiaffinity\n\tspec:\n\t  affinity:\n\t    podAntiAffinity: \n\t      requiredDuringSchedulingIgnoredDuringExecution: \n\t      - weight: 100  \n\t        podAffinityTerm:\n\t          labelSelector:\n\t            matchExpressions:\n\t            - key: security \n\t              operator: In \n\t              values:\n\t              - S2\n\t          topologyKey: kubernetes.io/hostname\n\t  containers:\n\t  - name: with-pod-affinity\n\t    image: docker.io/ocpqe/hello-pod\n这个例子里的 podAntiAffinity 规则，就指定了这个 Pod 不希望跟任何携带了 security=S2 标签的 Pod 存在于同一个 Node 上。需要注意的是，PodAffinityPredicate 是有作用域的，比如上面这条规则，就仅对携带了 Key 是kubernetes.io/hostname标签的 Node 有效。这正是 topologyKey 这个关键词的作用。\n\n而与 podAntiAffinity 相反的，就是 podAffinity，比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: with-pod-affinity\n\tspec:\n\t  affinity:\n\t    podAffinity: \n\t      requiredDuringSchedulingIgnoredDuringExecution: \n\t      - labelSelector:\n\t          matchExpressions:\n\t          - key: security \n\t            operator: In \n\t            values:\n\t            - S1 \n\t        topologyKey: failure-domain.beta.kubernetes.io/zone\n\t  containers:\n\t  - name: with-pod-affinity\n\t    image: docker.io/ocpqe/hello-pod\n这个例子里的 Pod，就只会被调度到已经有携带了 security=S1 标签的 Pod 运行的 Node 上。而这条规则的作用域，则是所有携带 Key 是failure-domain.beta.kubernetes.io/zone标签的 Node。\n\n此外，上面这两个例子里的 requiredDuringSchedulingIgnoredDuringExecution 字段的含义是：这条规则必须在 Pod 调度时进行检查（requiredDuringScheduling）；但是如果是已经在运行的 Pod 发生变化，比如 Label 被修改，造成了该 Pod 不再适合运行在这个 Node 上的时候，Kubernetes 不会进行主动修正（IgnoredDuringExecution）。\n\n上面这四种类型的 Predicates，就构成了调度器确定一个 Node 可以运行待调度 Pod 的基本策略。\n\n**在具体执行的时候， 当开始调度一个 Pod 时，Kubernetes 调度器会同时启动 16 个 Goroutine，来并发地为集群里的所有 Node 计算 Predicates，最后返回可以运行这个 Pod 的宿主机列表。**\n\n需要注意的是，在为每个 Node 执行 Predicates 时，调度器会按照固定的顺序来进行检查。这个顺序，是按照 Predicates 本身的含义来确定的。比如，宿主机相关的 Predicates 会被放在相对靠前的位置进行检查。要不然的话，在一台资源已经严重不足的宿主机上，上来就开始计算 PodAffinityPredicate，是没有实际意义的。\n\n`接下来，我们再来看一下 Priorities。`\n\n在 Predicates 阶段完成了节点的“过滤”之后，Priorities 阶段的工作就是为这些节点打分。这里打分的范围是 0-10 分，得分最高的节点就是最后被 Pod 绑定的最佳节点。\n\nPriorities 里最常用到的一个打分规则，是 LeastRequestedPriority。它的计算方法，可以简单地总结为如下所示的公式：\n\n\tscore = (cpu((capacity-sum(requested))10/capacity) + memory((capacity-sum(requested))10/capacity))/2\n可以看到，这个算法实际上就是在选择空闲资源（CPU 和 Memory）最多的宿主机。\n\n而与 LeastRequestedPriority 一起发挥作用的，还有 BalancedResourceAllocation。它的计算公式如下所示：\n\n\tscore = 10 - variance(cpuFraction,memoryFraction,volumeFraction)*10\n其中，每种资源的 Fraction 的定义是 ：Pod 请求的资源 / 节点上的可用资源。而 variance 算法的作用，则是计算每两种资源 Fraction 之间的“距离”。而最后选择的，则是资源 Fraction 差距最小的节点。\n\n所以说，BalancedResourceAllocation 选择的，其实是调度完成后，所有节点里各种资源分配最均衡的那个节点，从而避免一个节点上 CPU 被大量分配、而 Memory 大量剩余的情况。\n\n此外，还有 NodeAffinityPriority、TaintTolerationPriority 和 InterPodAffinityPriority 这三种 Priority。顾名思义，它们与前面的 PodMatchNodeSelector、PodToleratesNodeTaints 和 PodAffinityPredicate 这三个 Predicate 的含义和计算方法是类似的。但是作为 Priority，一个 Node 满足上述规则的字段数目越多，它的得分就会越高。\n\n在默认 Priorities 里，还有一个叫作 ImageLocalityPriority 的策略。它是在 Kubernetes v1.12 里新开启的调度规则，即：如果待调度 Pod 需要使用的镜像很大，并且已经存在于某些 Node 上，那么这些 Node 的得分就会比较高。\n\n当然，为了避免这个算法引发调度堆叠，调度器在计算得分的时候还会根据镜像的分布进行优化，即：如果大镜像分布的节点数目很少，那么这些节点的权重就会被调低，从而“对冲”掉引起调度堆叠的风险。\n\n以上，就是 Kubernetes 调度器的 Predicates 和 Priorities 里默认调度规则的主要工作原理了。\n\n`在实际的执行过程中，调度器里关于集群和 Pod 的信息都已经缓存化，所以这些算法的执行过程还是比较快的。`\n\n此外，对于比较复杂的调度算法来说，比如 PodAffinityPredicate，它们在计算的时候不只关注待调度 Pod 和待考察 Node，还需要关注整个集群的信息，比如，遍历所有节点，读取它们的 Labels。这时候，Kubernetes 调度器会在为每个待调度 Pod 执行该调度算法之前，先将算法需要的集群信息初步计算一遍，然后缓存起来。这样，在真正执行该算法的时候，调度器只需要读取缓存信息进行计算即可，从而避免了为每个 Node 计算 Predicates 的时候反复获取和计算整个集群的信息。\n\n**总结**\n\n在本篇文章中，我为你讲述了 Kubernetes 默认调度器里的主要调度算法。\n\n需要注意的是，除了本篇讲述的这些规则，Kubernetes 调度器里其实还有一些默认不会开启的策略。你可以通过为 kube-scheduler 指定一个配置文件或者创建一个 ConfigMap ，来配置哪些规则需要开启、哪些规则需要关闭。并且，你可以通过为 Priorities 设置权重，来控制调度器的调度行为。\n\n**思考题**\n\n请问，如何能够让 Kubernetes 的调度器尽可能地将 Pod 分布在不同机器上，避免“堆叠”呢？请简单描述下你的算法。\n","categories":["reference","k8s"]},{"title":"46 | 解读 CRI 与 容器运行时","url":"/2021/03/13/reference/k8s/46.解读 CRI 与 容器运行时/","content":"\n![](1.jpg)\n\n## 解读 CRI 与 容器运行时\n\n在上一篇文章中，我为你详细讲解了 kubelet 的工作原理和 CRI 的来龙去脉。在今天这篇文章中，我们就来进一步地、更深入地了解一下 CRI 的设计与工作原理。\n<!-- more -->\n首先，我们先来简要回顾一下有了 CRI 之后，Kubernetes 的架构图，如下所示。\n\n![](2.png)\n\n在上一篇文章中我也提到了，CRI 机制能够发挥作用的核心，就在于每一种容器项目现在都可以自己实现一个 CRI shim，自行对 CRI 请求进行处理。这样，Kubernetes 就有了一个统一的容器抽象层，使得下层容器运行时可以自由地对接进入 Kubernetes 当中。\n\n所以说，这里的 CRI shim，就是容器项目的维护者们自由发挥的“场地”了。而除了 dockershim 之外，其他容器运行时的 CRI shim，都是需要额外部署在宿主机上的。\n\n举个例子。CNCF 里的 containerd 项目，就可以提供一个典型的 CRI shim 的能力，即：将 Kubernetes 发出的 CRI 请求，转换成对 containerd 的调用，然后创建出 runC 容器。而 runC 项目，才是负责执行我们前面讲解过的设置容器 Namespace、Cgroups 和 chroot 等基础操作的组件。所以，这几层的组合关系，可以用如下所示的示意图来描述。\n\n![](3.png)\n\n**而作为一个 CRI shim，containerd 对 CRI 的具体实现，又是怎样的呢？**\n\n我们先来看一下 CRI 这个接口的定义。下面这幅示意图，就展示了 CRI 里主要的待实现接口。\n\n![](4.png)\n\n具体地说，**我们可以把 CRI 分为两组：**\n\n * 第一组，是 RuntimeService。它提供的接口，主要是跟容器相关的操作。比如，创建和启动容器、删除容器、执行 exec 命令等等。\n\n * 而第二组，则是 ImageService。它提供的接口，主要是容器镜像相关的操作，比如拉取镜像、删除镜像等等。\n\n关于容器镜像的操作比较简单，所以我们就暂且略过。接下来，我主要为你讲解一下 RuntimeService 部分。\n\n**在这一部分，CRI 设计的一个重要原则，就是确保这个接口本身，只关注容器，不关注 Pod。**这样做的原因，也很容易理解。\n\n**第一，**Pod 是 Kubernetes 的编排概念，而不是容器运行时的概念。所以，我们就不能假设所有下层容器项目，都能够暴露出可以直接映射为 Pod 的 API。\n\n**第二，**如果 CRI 里引入了关于 Pod 的概念，那么接下来只要 Pod API 对象的字段发生变化，那么 CRI 就很有可能需要变更。而在 Kubernetes 开发的前期，Pod 对象的变化还是比较频繁的，但对于 CRI 这样的标准接口来说，这个变更频率就有点麻烦了。\n\n所以，在 CRI 的设计里，并没有一个直接创建 Pod 或者启动 Pod 的接口。\n\n不过，相信你也已经注意到了，CRI 里还是有一组叫作 RunPodSandbox 的接口的。\n\n这个 PodSandbox，对应的并不是 Kubernetes 里的 Pod API 对象，而只是抽取了 Pod 里的一部分与容器运行时相关的字段，比如 HostName、DnsConfig、CgroupParent 等。所以说，PodSandbox 这个接口描述的，其实是 Kubernetes 将 Pod 这个概念映射到容器运行时层面所需要的字段，或者说是一个 Pod 对象子集。\n\n而作为具体的容器项目，你就需要自己决定如何使用这些字段来实现一个 Kubernetes 期望的 Pod 模型。这里的原理，可以用如下所示的示意图来表示清楚。\n\n![](5.png)\n\n比如，当我们执行 kubectl run 创建了一个名叫 foo 的、包括了 A、B 两个容器的 Pod 之后。这个 Pod 的信息最后来到 kubelet，kubelet 就会按照图中所示的顺序来调用 CRI 接口。\n\n在具体的 CRI shim 中，这些接口的实现是可以完全不同的。比如，如果是 Docker 项目，dockershim 就会创建出一个名叫 foo 的 Infra 容器（pause 容器），用来“hold”住整个 Pod 的 Network Namespace。\n\n而如果是基于虚拟化技术的容器，比如 Kata Containers 项目，它的 CRI 实现就会直接创建出一个轻量级虚拟机来充当 Pod。\n\n此外，需要注意的是，在 RunPodSandbox 这个接口的实现中，你还需要调用 networkPlugin.SetUpPod(…) 来为这个 Sandbox 设置网络。这个 SetUpPod(…) 方法，实际上就在执行 CNI 插件里的 add(…) 方法，也就是我在前面为你讲解过的 CNI 插件为 Pod 创建网络，并且把 Infra 容器加入到网络中的操作。\n\n\n> 备注：这里，你可以再回顾下第 34 篇文章《Kubernetes 网络模型与 CNI 网络插件》中的相关内容。\n\n接下来，kubelet 继续调用 CreateContainer 和 StartContainer 接口来创建和启动容器 A、B。对应到 dockershim 里，就是直接启动 A，B 两个 Docker 容器。所以最后，宿主机上会出现三个 Docker 容器组成这一个 Pod。\n\n而如果是 Kata Containers 的话，CreateContainer 和 StartContainer 接口的实现，就只会在前面创建的轻量级虚拟机里创建两个 A、B 容器对应的 Mount Namespace。所以，最后在宿主机上，只会有一个叫作 foo 的轻量级虚拟机在运行。关于像 Kata Containers 或者 gVisor 这种所谓的安全容器项目，我会在下一篇文章中为你详细介绍。\n\n除了上述对容器生命周期的实现之外，CRI shim 还有一个重要的工作，就是如何实现 exec、logs 等接口。这些接口跟前面的操作有一个很大的不同，就是这些 gRPC 接口调用期间，kubelet 需要跟容器项目维护一个长连接来传输数据。这种 API，我们就称之为 Streaming API。\n\nCRI shim 里对 Streaming API 的实现，依赖于一套独立的 Streaming Server 机制。这一部分原理，可以用如下所示的示意图来为你描述。\n\n![](6.png)\n\n可以看到，当我们对一个容器执行 kubectl exec 命令的时候，这个请求首先交给 API Server，然后 API Server 就会调用 kubelet 的 Exec API。\n\n这时，kubelet 就会调用 CRI 的 Exec 接口，而负责响应这个接口的，自然就是具体的 CRI shim。\n\n但在这一步，CRI shim 并不会直接去调用后端的容器项目（比如 Docker ）来进行处理，而只会返回一个 URL 给 kubelet。这个 URL，就是该 CRI shim 对应的 Streaming Server 的地址和端口。\n\n而 kubelet 在拿到这个 URL 之后，就会把它以 Redirect 的方式返回给 API Server。所以这时候，API Server 就会通过重定向来向 Streaming Server 发起真正的 /exec 请求，与它建立长连接。\n\n当然，这个 Streaming Server 本身，是需要通过使用 SIG-Node 为你维护的 Streaming API 库来实现的。并且，Streaming Server 会在 CRI shim 启动时就一起启动。此外，Stream Server 这一部分具体怎么实现，完全可以由 CRI shim 的维护者自行决定。比如，对于 Docker 项目来说，dockershim 就是直接调用 Docker 的 Exec API 来作为实现的。\n\n以上，就是 CRI 的设计以及具体的工作原理了。\n\n**总结**\n\n在本篇文章中，我为你详细解读了 CRI 的设计和具体工作原理，并为你梳理了实现 CRI 接口的核心流程。\n\n从这些讲解中不难看出，CRI 这个接口的设计，实际上还是比较宽松的。这就意味着，作为容器项目的维护者，我在实现 CRI 的具体接口时，往往拥有着很高的自由度，这个自由度不仅包括了容器的生命周期管理，也包括了如何将 Pod 映射成为我自己的实现，还包括了如何调用 CNI 插件来为 Pod 设置网络的过程。\n\n所以说，当你对容器这一层有特殊的需求时，我一定优先建议你考虑实现一个自己的 CRI shim ，而不是修改 kubelet 甚至容器项目的代码。这样通过插件的方式定制 Kubernetes 的做法，也是整个 Kubernetes 社区最鼓励和推崇的一个最佳实践。这也正是为什么像 Kata Containers、gVisor 甚至虚拟机这样的“非典型”容器，都可以无缝接入到 Kubernetes 项目里的重要原因。\n\n**思考题**\n\n请你思考一下，我前面讲解过的 Device Plugin 为容器分配的 GPU 信息，是通过 CRI 的哪个接口传递给 dockershim，最后交给 Docker API 的呢？\n","categories":["reference","k8s"]},{"title":"45 | 幕后英雄：SIG-Node与CRI","url":"/2021/03/13/reference/k8s/45.幕后英雄：SIG-Node与CRI/","content":"\n![](1.jpg)\n\n## 幕后英雄：SIG-Node与CRI\n\n在前面的文章中，我为你详细讲解了关于 Kubernetes 调度和资源管理相关的内容。实际上，在调度这一步完成后，Kubernetes 就需要负责将这个调度成功的 Pod，在宿主机上创建出来，并把它所定义的各个容器启动起来。这些，都是 kubelet 这个核心组件的主要功能。\n<!-- more -->\n在接下来三篇文章中，我就深入到 kubelet 里面，为你详细剖析一下 Kubernetes 对容器运行时的管理能力。\n\n在 Kubernetes 社区里，与 kubelet 以及容器运行时管理相关的内容，都属于 SIG-Node 的范畴。如果你经常参与社区的话，你可能会觉得，相比于其他每天都热闹非凡的 SIG 小组，SIG-Node 是 Kubernetes 里相对沉寂也不太发声的一个小组，小组里的成员也很少在外面公开宣讲。\n\n不过，正如我前面所介绍的，SIG-Node 以及 kubelet，其实是 Kubernetes 整套体系里非常核心的一个部分。 毕竟，它们才是 Kubernetes 这样一个容器编排与管理系统，跟容器打交道的主要“场所”。\n\n而 kubelet 这个组件本身，也是 Kubernetes 里面第二个不可被替代的组件（第一个不可被替代的组件当然是 kube-apiserver）。也就是说，**无论如何，我都不太建议你对 kubelet 的代码进行大量的改动。保持 kubelet 跟上游基本一致的重要性，就跟保持 kube-apiserver 跟上游一致是一个道理。**\n\n当然， kubelet 本身，也是按照“控制器”模式来工作的。它实际的工作原理，可以用如下所示的一幅示意图来表示清楚。\n\n![](2.png)\n\n可以看到，kubelet 的工作核心，就是一个控制循环，即：SyncLoop（图中的大圆圈）。而驱动这个控制循环运行的事件，包括四种：\n\n1. Pod 更新事件；\n\n2. Pod 生命周期变化；\n\n3. kubelet 本身设置的执行周期；\n\n4. 定时的清理事件。\n\n所以，跟其他控制器类似，kubelet 启动的时候，要做的第一件事情，就是设置 Listers，也就是注册它所关心的各种事件的 Informer。这些 Informer，就是 SyncLoop 需要处理的数据的来源。\n\n此外，kubelet 还负责维护着很多很多其他的子控制循环（也就是图中的小圆圈）。这些控制循环的名字，一般被称作某某 Manager，比如 Volume Manager、Image Manager、Node Status Manager 等等。\n\n不难想到，这些控制循环的责任，就是通过控制器模式，完成 kubelet 的某项具体职责。比如 Node Status Manager，就负责响应 Node 的状态变化，然后将 Node 的状态收集起来，并通过 Heartbeat 的方式上报给 APIServer。再比如 CPU Manager，就负责维护该 Node 的 CPU 核的信息，以便在 Pod 通过 cpuset 的方式请求 CPU 核的时候，能够正确地管理 CPU 核的使用量和可用量。\n\n那么这个 **SyncLoop，又是如何根据 Pod 对象的变化，来进行容器操作的呢？**\n\n实际上，kubelet 也是通过 Watch 机制，监听了与自己相关的 Pod 对象的变化。当然，这个 Watch 的过滤条件是该 Pod 的 nodeName 字段与自己相同。kubelet 会把这些 Pod 的信息缓存在自己的内存里。\n\n而当一个 Pod 完成调度、与一个 Node 绑定起来之后， 这个 Pod 的变化就会触发 kubelet 在控制循环里注册的 Handler，也就是上图中的 HandlePods 部分。此时，通过检查该 Pod 在 kubelet 内存里的状态，kubelet 就能够判断出这是一个新调度过来的 Pod，从而触发 Handler 里 ADD 事件对应的处理逻辑。\n\n在具体的处理过程当中，kubelet 会启动一个名叫 Pod Update Worker 的、单独的 Goroutine 来完成对 Pod 的处理工作。\n\n比如，如果是 ADD 事件的话，kubelet 就会为这个新的 Pod 生成对应的 Pod Status，检查 Pod 所声明使用的 Volume 是不是已经准备好。然后，调用下层的容器运行时（比如 Docker），开始创建这个 Pod 所定义的容器。\n\n而如果是 UPDATE 事件的话，kubelet 就会根据 Pod 对象具体的变更情况，调用下层容器运行时进行容器的重建工作。\n\n\n在这里需要注意的是，**kubelet 调用下层容器运行时的执行过程，并不会直接调用 Docker 的 API，而是通过一组叫作 CRI（Container Runtime Interface，容器运行时接口）的 gRPC 接口来间接执行的。**\n\nKubernetes 项目之所以要在 kubelet 中引入这样一层单独的抽象，当然是为了对 Kubernetes 屏蔽下层容器运行时的差异。实际上，对于 1.6 版本之前的 Kubernetes 来说，它就是直接调用 Docker 的 API 来创建和管理容器的。\n\n但是，正如我在本专栏开始介绍容器背景的时候提到过的，Docker 项目风靡全球后不久，CoreOS 公司就推出了 rkt 项目来与 Docker 正面竞争。在这种背景下，Kubernetes 项目的默认容器运行时，自然也就成了两家公司角逐的重要战场。\n\n毋庸置疑，Docker 项目必然是 Kubernetes 项目最依赖的容器运行时。但凭借与 Google 公司非同一般的关系，CoreOS 公司还是在 2016 年成功地将对 rkt 容器的支持，直接添加进了 kubelet 的主干代码里。\n\n不过，这个“赶鸭子上架”的举动，并没有为 rkt 项目带来更多的用户，反而给 kubelet 的维护人员，带来了巨大的负担。\n\n不难想象，在这种情况下， **kubelet 任何一次重要功能的更新，都不得不考虑 Docker 和 rkt 这两种容器运行时的处理场景，然后分别更新 Docker 和 rkt 两部分代码。**\n\n更让人为难的是，由于 rkt 项目实在太小众，kubelet 团队所有与 rkt 相关的代码修改，都必须依赖于 CoreOS 的员工才能做到。这不仅拖慢了 kubelet 的开发周期，也给项目的稳定性带来了巨大的隐患。\n\n与此同时，在 2016 年，Kata Containers 项目的前身 runV 项目也开始逐渐成熟，这种基于虚拟化技术的强隔离容器，与 Kubernetes 和 Linux 容器项目之间具有良好的互补关系。所以，**在 Kubernetes 上游，对虚拟化容器的支持很快就被提上了日程。**\n\n不过，虽然虚拟化容器运行时有各种优点，但它与 Linux 容器截然不同的实现方式，使得它跟 Kubernetes 的集成工作，比 rkt 要复杂得多。如果此时，再把对 runV 支持的代码也一起添加到 kubelet 当中，那么接下来 kubelet 的维护工作就可以说完全没办法正常进行了。\n\n所以，在 2016 年，SIG-Node 决定开始动手解决上述问题。而解决办法也很容易想到，那就是把 kubelet 对容器的操作，统一地抽象成一个接口。这样，kubelet 就只需要跟这个接口打交道了。而作为具体的容器项目，比如 Docker、 rkt、runV，它们就只需要自己提供一个该接口的实现，然后对 kubelet 暴露出 gRPC 服务即可。\n\n这一层统一的容器操作接口，就是 CRI 了。我会在下一篇文章中，为你详细讲解 CRI 的设计与具体的实现原理。\n\n而在有了 CRI 之后，Kubernetes 以及 kubelet 本身的架构，就可以用如下所示的一幅示意图来描述。\n\n![](3.png)\n\n可以看到，当 Kubernetes 通过编排能力创建了一个 Pod 之后，调度器会为这个 Pod 选择一个具体的节点来运行。这时候，kubelet 当然就会通过前面讲解过的 SyncLoop 来判断需要执行的具体操作，比如创建一个 Pod。那么此时，kubelet 实际上就会调用一个叫作 GenericRuntime 的通用组件来发起创建 Pod 的 CRI 请求。\n\n那么，**这个 CRI 请求，又该由谁来响应呢？**\n\n如果你使用的容器项目是 Docker 的话，那么负责响应这个请求的就是一个叫作 dockershim 的组件。它会把 CRI 请求里的内容拿出来，然后组装成 Docker API 请求发给 Docker Daemon。\n\n需要注意的是，在 Kubernetes 目前的实现里，dockershim 依然是 kubelet 代码的一部分。当然，在将来，dockershim 肯定会被从 kubelet 里移出来，甚至直接被废弃掉。\n\n而更普遍的场景，就是你需要在每台宿主机上单独安装一个负责响应 CRI 的组件，这个组件，一般被称作 CRI shim。顾名思义，CRI shim 的工作，就是扮演 kubelet 与容器项目之间的“垫片”（shim）。所以它的作用非常单一，那就是实现 CRI 规定的每个接口，然后把具体的 CRI 请求“翻译”成对后端容器项目的请求或者操作。\n\n**总结**\n\n在本篇文章中，我首先为你介绍了 SIG-Node 的职责，以及 kubelet 这个组件的工作原理。\n\n接下来，我为你重点讲解了 kubelet 究竟是如何将 Kubernetes 对应用的定义，一步步转换成最终对 Docker 或者其他容器项目的 API 请求的。\n\n不难看到，在这个过程中，kubelet 的 SyncLoop 和 CRI 的设计，是其中最重要的两个关键点。也正是基于以上设计，SyncLoop 本身就要求这个控制循环是绝对不可以被阻塞的。所以，凡是在 kubelet 里有可能会耗费大量时间的操作，比如准备 Pod 的 Volume、拉取镜像等，SyncLoop 都会开启单独的 Goroutine 来进行操作。\n\n**思考题**\n\n请问，在你的项目中，你是如何部署 kubelet 这个组件的？为什么要这么做呢？","categories":["reference","k8s"]},{"title":"50 | 让日志无处可逃：容器日志收集与管理","url":"/2021/03/13/reference/k8s/50.让日志无处可逃：容器日志收集与管理/","content":"\n![](1.jpg)\n\n## 让日志无处可逃：容器日志收集与管理\n\n在前面的文章中，我为你详细讲解了 Kubernetes 的核心监控体系和自定义监控体系的设计与实现思路。而在本篇文章里，我就来为你详细介绍一下 Kubernetes 里关于容器日志的处理方式。\n<!-- more -->\n首先需要明确的是，Kubernetes 里面对容器日志的处理方式，都叫作 cluster-level-logging，即：这个日志处理系统，与容器、Pod 以及 Node 的生命周期都是完全无关的。这种设计当然是为了保证，无论是容器挂了、Pod 被删除，甚至节点宕机的时候，应用的日志依然可以被正常获取到。\n\n而对于一个容器来说，当应用把日志输出到 stdout 和 stderr 之后，容器项目在默认情况下就会把这些日志输出到宿主机上的一个 JSON 文件里。这样，你通过 kubectl logs 命令就可以看到这些容器的日志了。\n\n上述机制，就是我们今天要讲解的容器日志收集的基础假设。而如果你的应用是把文件输出到其他地方，比如直接输出到了容器里的某个文件里，或者输出到了远程存储里，那就属于特殊情况了。当然，我在文章里也会对这些特殊情况的处理方法进行讲述。\n\n而 Kubernetes 本身，实际上是不会为你做容器日志收集工作的，所以为了实现上述 cluster-level-logging，你需要在部署集群的时候，提前对具体的日志方案进行规划。而 Kubernetes 项目本身，主要为你推荐了三种日志方案。\n\n**第一种，在 Node 上部署 logging agent，将日志文件转发到后端存储里保存起来。**这个方案的架构图如下所示。\n\n![](2.jpg)\n\n不难看到，这里的核心就在于 logging agent ，它一般都会以 DaemonSet 的方式运行在节点上，然后将宿主机上的容器日志目录挂载进去，最后由 logging-agent 把日志转发出去。\n\n举个例子，我们可以通过 Fluentd 项目作为宿主机上的 logging-agent，然后把日志转发到远端的 ElasticSearch 里保存起来供将来进行检索。具体的操作过程，你可以通过阅读[这篇文档](https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/)来了解。另外，在很多 Kubernetes 的部署里，会自动为你启用 [logrotate](https://linux.die.net/man/8/logrotate)，在日志文件超过 10MB 的时候自动对日志文件进行 rotate 操作。\n\n可以看到，在 Node 上部署 logging agent 最大的优点，在于一个节点只需要部署一个 agent，并且不会对应用和 Pod 有任何侵入性。所以，这个方案，在社区里是最常用的一种。\n\n但是也不难看到，这种方案的不足之处就在于，它要求应用输出的日志，都必须是直接输出到容器的 stdout 和 stderr 里。\n\n所以，**Kubernetes 容器日志方案的第二种，就是对这种特殊情况的一个处理，即：当容器的日志只能输出到某些文件里的时候，我们可以通过一个 sidecar 容器把这些日志文件重新输出到 sidecar 的 stdout 和 stderr 上，这样就能够继续使用第一种方案了。**这个方案的具体工作原理，如下所示。\n\n![](3.jpg)\n\n比如，现在我的应用 Pod 只有一个容器，它会把日志输出到容器里的 /var/log/1.log 和 2.log 这两个文件里。这个 Pod 的 YAML 文件如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: counter\n\tspec:\n\t  containers:\n\t  - name: count\n\t    image: busybox\n\t    args:\n\t    - /bin/sh\n\t    - -c\n\t    - >\n\t      i=0;\n\t      while true;\n\t      do\n\t        echo \"$i: $(date)\" >> /var/log/1.log;\n\t        echo \"$(date) INFO $i\" >> /var/log/2.log;\n\t        i=$((i+1));\n\t        sleep 1;\n\t      done\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t  volumes:\n\t  - name: varlog\n\t    emptyDir: {}\n在这种情况下，你用 kubectl logs 命令是看不到应用的任何日志的。而且我们前面讲解的、最常用的方案一，也是没办法使用的。\n\n那么这个时候，我们就可以为这个 Pod 添加两个 sidecar 容器，分别将上述两个日志文件里的内容重新以 stdout 和 stderr 的方式输出出来，这个 YAML 文件的写法如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: counter\n\tspec:\n\t  containers:\n\t  - name: count\n\t    image: busybox\n\t    args:\n\t    - /bin/sh\n\t    - -c\n\t    - >\n\t      i=0;\n\t      while true;\n\t      do\n\t        echo \"$i: $(date)\" >> /var/log/1.log;\n\t        echo \"$(date) INFO $i\" >> /var/log/2.log;\n\t        i=$((i+1));\n\t        sleep 1;\n\t      done\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t  - name: count-log-1\n\t    image: busybox\n\t    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/1.log']\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t  - name: count-log-2\n\t    image: busybox\n\t    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/2.log']\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t  volumes:\n\t  - name: varlog\n\t    emptyDir: {}\n这时候，你就可以通过 kubectl logs 命令查看这两个 sidecar 容器的日志，间接看到应用的日志内容了，如下所示：\n\n\t$ kubectl logs counter count-log-1\n\t0: Mon Jan 1 00:00:00 UTC 2001\n\t1: Mon Jan 1 00:00:01 UTC 2001\n\t2: Mon Jan 1 00:00:02 UTC 2001\n\t...\n\t$ kubectl logs counter count-log-2\n\tMon Jan 1 00:00:00 UTC 2001 INFO 0\n\tMon Jan 1 00:00:01 UTC 2001 INFO 1\n\tMon Jan 1 00:00:02 UTC 2001 INFO 2\n\t...\n由于 sidecar 跟主容器之间是共享 Volume 的，所以这里的 sidecar 方案的额外性能损耗并不高，也就是多占用一点 CPU 和内存罢了。\n\n但需要注意的是，这时候，宿主机上实际上会存在两份相同的日志文件：一份是应用自己写入的；另一份则是 sidecar 的 stdout 和 stderr 对应的 JSON 文件。这对磁盘是很大的浪费。所以说，除非万不得已或者应用容器完全不可能被修改，否则我还是建议你直接使用方案一，或者直接使用下面的第三种方案。\n\n**第三种方案，就是通过一个 sidecar 容器，直接把应用的日志文件发送到远程存储里面去。**也就是相当于把方案一里的 logging agent，放在了应用 Pod 里。这种方案的架构如下所示：\n\n![](4.jpg)\n\n在这种方案里，你的应用还可以直接把日志输出到固定的文件里而不是 stdout，你的 logging-agent 还可以使用 fluentd，后端存储还可以是 ElasticSearch。只不过， fluentd 的输入源，变成了应用的日志文件。一般来说，我们会把 fluentd 的输入源配置保存在一个 ConfigMap 里，如下所示：\n\n\tapiVersion: v1\n\tkind: ConfigMap\n\tmetadata:\n\t  name: fluentd-config\n\tdata:\n\t  fluentd.conf: |\n\t    <source>\n\t      type tail\n\t      format none\n\t      path /var/log/1.log\n\t      pos_file /var/log/1.log.pos\n\t      tag count.format1\n\t    </source>\n\t    \n\t    <source>\n\t      type tail\n\t      format none\n\t      path /var/log/2.log\n\t      pos_file /var/log/2.log.pos\n\t      tag count.format2\n\t    </source>\n\t    \n\t    <match **>\n\t      type google_cloud\n\t    </match>\n然后，我们在应用 Pod 的定义里，就可以声明一个 Fluentd 容器作为 sidecar，专门负责将应用生成的 1.log 和 2.log 转发到 ElasticSearch 当中。这个配置，如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: counter\n\tspec:\n\t  containers:\n\t  - name: count\n\t    image: busybox\n\t    args:\n\t    - /bin/sh\n\t    - -c\n\t    - >\n\t      i=0;\n\t      while true;\n\t      do\n\t        echo \"$i: $(date)\" >> /var/log/1.log;\n\t        echo \"$(date) INFO $i\" >> /var/log/2.log;\n\t        i=$((i+1));\n\t        sleep 1;\n\t      done\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t  - name: count-agent\n\t    image: k8s.gcr.io/fluentd-gcp:1.30\n\t    env:\n\t    - name: FLUENTD_ARGS\n\t      value: -c /etc/fluentd-config/fluentd.conf\n\t    volumeMounts:\n\t    - name: varlog\n\t      mountPath: /var/log\n\t    - name: config-volume\n\t      mountPath: /etc/fluentd-config\n\t  volumes:\n\t  - name: varlog\n\t    emptyDir: {}\n\t  - name: config-volume\n\t    configMap:\n\t      name: fluentd-config\n可以看到，这个 Fluentd 容器使用的输入源，就是通过引用我们前面编写的 ConfigMap 来指定的。这里我用到了 Projected Volume 来把 ConfigMap 挂载到 Pod 里。如果你对这个用法不熟悉的话，可以再回顾下第 15 篇文章《 深入解析 Pod 对象（二）：使用进阶》中的相关内容。\n\n需要注意的是，这种方案虽然部署简单，并且对宿主机非常友好，但是这个 sidecar 容器很可能会消耗较多的资源，甚至拖垮应用容器。并且，由于日志还是没有输出到 stdout 上，所以你通过 kubectl logs 是看不到任何日志输出的。\n\n以上，就是 Kubernetes 项目对容器应用日志进行管理最常用的三种手段了。\n\n**总结**\n\n在本篇文章中，我为你详细讲解了 Kubernetes 项目对容器应用日志的收集方式。综合对比以上三种方案，我比较建议你将应用日志输出到 stdout 和 stderr，然后通过在宿主机上部署 logging-agent 的方式来集中处理日志。\n\n这种方案不仅管理简单，kubectl logs 也可以用，而且可靠性高，并且宿主机本身，很可能就自带了 rsyslogd 等非常成熟的日志收集组件来供你使用。\n\n除此之外，还有一种方式就是在编写应用的时候，就直接指定好日志的存储后端，如下所示：\n\n![](5.jpg)\n\n在这种方案下，Kubernetes 就完全不必操心容器日志的收集了，这对于本身已经有完善的日志处理系统的公司来说，是一个非常好的选择。\n\n最后需要指出的是，无论是哪种方案，你都必须要及时将这些日志文件从宿主机上清理掉，或者给日志目录专门挂载一些容量巨大的远程盘。否则，一旦主磁盘分区被打满，整个系统就可能会陷入崩溃状态，这是非常麻烦的。\n\n**思考题**\n\n请问，当日志量很大的时候，直接将日志输出到容器 stdout 和 stderr 上，有没有什么隐患呢？有没有解决办法呢？\n\n你还有哪些容器收集的方案，是否可以分享一下？\n\n","categories":["reference","k8s"]},{"title":"48 | Prometheus、Metrics Server与Kubernetes监控体系","url":"/2021/03/13/reference/k8s/48.Prometheus、Metrics Server与Kubernetes监控体系/","content":"\n![](1.jpg)\n\n## Prometheus、Metrics Server与Kubernetes监控体系\n\n通过前面的文章，我已经和你分享过了 Kubernetes 的核心架构，编排概念，以及具体的设计与实现。接下来，我会用 3 篇文章，为你介绍 Kubernetes 监控相关的一些核心技术。\n<!-- more -->\n首先需要明确指出的是，Kubernetes 项目的监控体系曾经非常繁杂，在社区中也有很多方案。但这套体系发展到今天，已经完全演变成了以 Prometheus 项目为核心的一套统一的方案。\n\n在这里，可能有一些同学对 Prometheus 项目还太不熟悉。所以，我先来简单为你介绍一下这个项目。\n\n实际上，Prometheus 项目是当年 CNCF 基金会起家时的“第二把交椅”。而这个项目发展到今天，已经全面接管了 Kubernetes 项目的整套监控体系。\n\n比较有意思的是，Prometheus 项目与 Kubernetes 项目一样，也来自于 Google 的 Borg 体系，它的原型系统，叫作 BorgMon，是一个几乎与 Borg 同时诞生的内部监控系统。而 Prometheus 项目的发起原因也跟 Kubernetes 很类似，都是希望通过对用户更友好的方式，将 Google 内部系统的设计理念，传递给用户和开发者。\n\n作为一个监控系统，Prometheus 项目的作用和工作方式，其实可以用如下所示的一张官方示意图来解释。\n\n![](2.png)\n\n可以看到，Prometheus 项目工作的核心，是使用 Pull （抓取）的方式去搜集被监控对象的 Metrics 数据（监控指标数据），然后，再把这些数据保存在一个 TSDB （时间序列数据库，比如 OpenTSDB、InfluxDB 等）当中，以便后续可以按照时间进行检索。\n\n有了这套核心监控机制， Prometheus 剩下的组件就是用来配合这套机制的运行。比如 Pushgateway，可以允许被监控对象以 Push 的方式向 Prometheus 推送 Metrics 数据。而 Alertmanager，则可以根据 Metrics 信息灵活地设置报警。当然， Prometheus 最受用户欢迎的功能，还是通过 Grafana 对外暴露出的、可以灵活配置的监控数据可视化界面。\n\n有了 Prometheus 之后，我们就可以按照 Metrics 数据的来源，来对 Kubernetes 的监控体系做一个汇总了。\n\n**第一种 Metrics，是宿主机的监控数据**。这部分数据的提供，需要借助一个由 Prometheus 维护的[Node Exporter](https://github.com/prometheus/node_exporter) 工具。一般来说，Node Exporter 会以 DaemonSet 的方式运行在宿主机上。其实，所谓的 Exporter，就是代替被监控对象来对 Prometheus 暴露出可以被“抓取”的 Metrics 信息的一个辅助进程。\n\n而 Node Exporter 可以暴露给 Prometheus 采集的 Metrics 数据， 也不单单是节点的负载（Load）、CPU 、内存、磁盘以及网络这样的常规信息，它的 Metrics 指标可以说是“包罗万象”，你可以查看[这个列表](https://github.com/prometheus/node_exporter#enabled-by-default)来感受一下。\n\n**第二种 Metrics，是来自于 Kubernetes 的 API Server、kubelet 等组件的 /metrics API。**除了常规的 CPU、内存的信息外，这部分信息还主要包括了各个组件的核心监控指标。比如，对于 API Server 来说，它就会在 /metrics API 里，暴露出各个 Controller 的工作队列（Work Queue）的长度、请求的 QPS 和延迟数据等等。这些信息，是检查 Kubernetes 本身工作情况的主要依据。\n\n**第三种 Metrics，是 Kubernetes 相关的监控数据。**这部分数据，一般叫作 Kubernetes 核心监控数据（core metrics）。这其中包括了 Pod、Node、容器、Service 等主要 Kubernetes 核心概念的 Metrics。\n\n其中，容器相关的 Metrics 主要来自于 kubelet 内置的 cAdvisor 服务。在 kubelet 启动后，cAdvisor 服务也随之启动，而它能够提供的信息，可以细化到每一个容器的 CPU 、文件系统、内存、网络等资源的使用情况。\n\n需要注意的是，这里提到的 Kubernetes 核心监控数据，其实使用的是 Kubernetes 的一个非常重要的扩展能力，叫作 Metrics Server。\n\nMetrics Server 在 Kubernetes 社区的定位，其实是用来取代 Heapster 这个项目的。在 Kubernetes 项目发展的初期，Heapster 是用户获取 Kubernetes 监控数据（比如 Pod 和 Node 的资源使用情况） 的主要渠道。而后面提出来的 Metrics Server，则把这些信息，通过标准的 Kubernetes API 暴露了出来。这样，Metrics 信息就跟 Heapster 完成了解耦，允许 Heapster 项目慢慢退出舞台。\n\n而有了 Metrics Server 之后，用户就可以通过标准的 Kubernetes API 来访问到这些监控数据了。比如，下面这个 URL：\n\n\thttp://127.0.0.1:8001/apis/metrics.k8s.io/v1beta1/namespaces/<namespace-name>/pods/<pod-name>\n当你访问这个 Metrics API 时，它就会为你返回一个 Pod 的监控数据，而这些数据，其实是从 kubelet 的 Summary API （即 <kubelet_ip>:<kubelet_port>/stats/summary）采集而来的。Summary API 返回的信息，既包括了 cAdVisor 的监控数据，也包括了 kubelet 本身汇总的信息。\n\n需要指出的是， Metrics Server 并不是 kube-apiserver 的一部分，而是通过 Aggregator 这种插件机制，在独立部署的情况下同 kube-apiserver 一起统一对外服务的。\n\n这里，Aggregator APIServer 的工作原理，可以用如下所示的一幅示意图来表示清楚：\n\n![](3.png)\n\n> 备注：图片出处https://blog.jetstack.io/blog/resource-and-custom-metrics-hpa-v2/\n\n可以看到，当 Kubernetes 的 API Server 开启了 Aggregator 模式之后，你再访问 apis/metrics.k8s.io/v1beta1 的时候，实际上访问到的是一个叫作 kube-aggregator 的代理。而 kube-apiserver，正是这个代理的一个后端；而 Metrics Server，则是另一个后端。\n\n而且，在这个机制下，你还可以添加更多的后端给这个 ** kube-aggregator。所以 kube-aggregator 其实就是一个根据 URL 选择具体的 API 后端的代理服务器。**通过这种方式，我们就可以很方便地扩展 Kubernetes 的 API 了。\n\n而 Aggregator 模式的开启也非常简单：\n\n * 如果你是使用 kubeadm 或者[官方的 kube-up.sh](https://github.com/kubernetes/kubernetes/blob/master/cluster/kube-up.sh) 脚本部署 Kubernetes 集群的话，Aggregator 模式就是默认开启的；\n\n * 如果是手动 DIY 搭建的话，你就需要在 kube-apiserver 的启动参数里加上如下所示的配置：\n\n\n\t--requestheader-client-ca-file=<path to aggregator CA cert>\n\t--requestheader-allowed-names=front-proxy-client\n\t--requestheader-extra-headers-prefix=X-Remote-Extra-\n\t--requestheader-group-headers=X-Remote-Group\n\t--requestheader-username-headers=X-Remote-User\n\t--proxy-client-cert-file=<path to aggregator proxy cert>\n\t--proxy-client-key-file=<path to aggregator proxy key>\n而这些配置的作用，主要就是为 Aggregator 这一层设置对应的 Key 和 Cert 文件。而这些文件的生成，就需要你自己手动完成了，具体流程请参考这篇[官方文档](https://github.com/kubernetes-sigs/apiserver-builder-alpha/blob/master/docs/concepts/auth.md)。\n\nAggregator 功能开启之后，你只需要将 Metrics Server 的 YAML 文件部署起来，如下所示：\n\n\t$ git clone https://github.com/kubernetes-incubator/metrics-server\n\t$ cd metrics-server\n\t$ kubectl create -f deploy/1.8+/\n接下来，你就会看到 metrics.k8s.io 这个 API 出现在了你的 Kubernetes API 列表当中。\n\n在理解了 Prometheus 关心的三种监控数据源，以及 Kubernetes 的核心 Metrics 之后，作为用户，你其实要做的就是将 Prometheus Operator 在 Kubernetes 集群里部署起来。然后，按照本篇文章一开始介绍的架构，把上述 Metrics 源配置起来，让 Prometheus 自己去进行采集即可。\n\n在后续的文章中，我会为你进一步剖析 Kubernetes 监控体系以及自定义 Metrics （自定义监控指标）的具体技术点。\n\n**总结**\n\n在本篇文章中，我主要为你介绍了 Kubernetes 当前监控体系的设计，介绍了 Prometheus 项目在这套体系中的地位，讲解了以 Prometheus 为核心的监控系统的架构设计。\n\n然后，我为你详细地解读了 Kubernetes 核心监控数据的来源，即：Metrics Server 的具体工作原理，以及 Aggregator APIServer 的设计思路。\n\n通过以上讲述，我希望你能够对 Kubernetes 的监控体系形成一个整体的认知，体会到 Kubernetes 社区在监控这个事情上，全面以 Prometheus 项目为核心进行建设的大方向。\n\n最后，在具体的监控指标规划上，我建议你**遵循业界通用的 USE 原则和 RED 原则。**\n\n其中，USE 原则指的是，按照如下三个维度来规划资源监控指标：\n\n1. 利用率（Utilization），资源被有效利用起来提供服务的平均时间占比；\n\n2. 饱和度（Saturation），资源拥挤的程度，比如工作队列的长度；\n\n3. 错误率（Errors），错误的数量。\n\n\n而 RED 原则指的是，按照如下三个维度来规划服务监控指标：\n\n1. 每秒请求数量（Rate）；\n\n2. 每秒错误数量（Errors）；\n\n3. 服务响应时间（Duration）。\n\n\n不难发现， USE 原则主要关注的是“资源”，比如节点和容器的资源使用情况，而 RED 原则主要关注的是“服务”，比如 kube-apiserver 或者某个应用的工作情况。这两种指标，在我今天为你讲解的 Kubernetes + Prometheus 组成的监控体系中，都是可以完全覆盖到的。\n\n**思考题**\n\n在监控体系中，对于数据的采集，其实既有 Prometheus 这种 Pull 模式，也有 Push 模式。请问，你如何看待这两种模式的异同和优缺点呢？\n","categories":["reference","k8s"]},{"title":"49 | Custom Metrics 让Auto Scaling不再 食之无味","url":"/2021/03/13/reference/k8s/49.Custom Metrics让Auto Scaling不再“食之无味”/","content":"\n![](1.jpg)\n\n## Custom Metrics让Auto Scaling不再食之无味\n\n在上一篇文章中，我为你详细讲述了 Kubernetes 里的核心监控体系的架构。不难看到，Prometheus 项目在其中占据了最为核心的位置。\n<!-- more -->\n实际上，借助上述监控体系，Kubernetes 就可以为你提供一种非常有用的能力，那就是 Custom Metrics，自定义监控指标。\n\n在过去的很多 PaaS 项目中，其实都有一种叫作 Auto Scaling，即自动水平扩展的功能。只不过，这个功能往往只能依据某种指定的资源类型执行水平扩展，比如 CPU 或者 Memory 的使用值。\n\n而在真实的场景中，用户需要进行 Auto Scaling 的依据往往是自定义的监控指标。比如，某个应用的等待队列的长度，或者某种应用相关资源的使用情况。这些复杂多变的需求，在传统 PaaS 项目和其他容器编排项目里，几乎是不可能轻松支持的。\n\n而凭借强大的 API 扩展机制，Custom Metrics 已经成为了 Kubernetes 的一项标准能力。并且，Kubernetes 的自动扩展器组件 Horizontal Pod Autoscaler （HPA）， 也可以直接使用 Custom Metrics 来执行用户指定的扩展策略，这里的整个过程都是非常灵活和可定制的。\n\n不难想到，Kubernetes 里的 Custom Metrics 机制，也是借助 Aggregator APIServer 扩展机制来实现的。这里的具体原理是，当你把 Custom Metrics APIServer 启动之后，Kubernetes 里就会出现一个叫作custom.metrics.k8s.io的 API。而当你访问这个 URL 时，Aggregator 就会把你的请求转发给 Custom Metrics APIServer 。\n\n而 Custom Metrics APIServer 的实现，其实就是一个 Prometheus 项目的 Adaptor。\n\n比如，现在我们要实现一个根据指定 Pod 收到的 HTTP 请求数量来进行 Auto Scaling 的 Custom Metrics，这个 Metrics 就可以通过访问如下所示的自定义监控 URL 获取到：\n\nhttps://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/pods/sample-metrics-app/http_requests \n\n这里的工作原理是，当你访问这个 URL 的时候，Custom Metrics APIServer 就会去 Prometheus 里查询名叫 sample-metrics-app 这个 Pod 的 http_requests 指标的值，然后按照固定的格式返回给访问者。\n\n当然，http_requests 指标的值，就需要由 Prometheus 按照我在上一篇文章中讲到的核心监控体系，从目标 Pod 上采集。\n\n这里具体的做法有很多种，最普遍的做法，就是让 Pod 里的应用本身暴露出一个 /metrics API，然后在这个 API 里返回自己收到的 HTTP 的请求的数量。所以说，接下来 HPA 只需要定时访问前面提到的自定义监控 URL，然后根据这些值计算是否要执行 Scaling 即可。\n\n接下来，我通过一个具体的实例，来为你讲解一下 Custom Metrics 具体的使用方式。这个实例的 GitHub 库[在这里](https://github.com/resouer/kubeadm-workshop)，你可以点击链接查看。在这个例子中，我依然会假设你的集群是 kubeadm 部署出来的，所以 Aggregator 功能已经默认开启了。\n\n> 备注：我们这里使用的实例，fork 自 Lucas 在上高中时做的一系列 Kubernetes 指南。\n\n**首先**，我们当然是先部署 Prometheus 项目。这一步，我当然会使用 Prometheus Operator 来完成，如下所示：\n\n\t$ kubectl apply -f demos/monitoring/prometheus-operator.yaml\n\tclusterrole \"prometheus-operator\" created\n\tserviceaccount \"prometheus-operator\" created\n\tclusterrolebinding \"prometheus-operator\" created\n\tdeployment \"prometheus-operator\" created\n\t$ kubectl apply -f demos/monitoring/sample-prometheus-instance.yaml\n\tclusterrole \"prometheus\" created\n\tserviceaccount \"prometheus\" created\n\tclusterrolebinding \"prometheus\" created\n\tprometheus \"sample-metrics-prom\" created\n\tservice \"sample-metrics-prom\" created\n**第二步**，我们需要把 Custom Metrics APIServer 部署起来，如下所示：\n\n\t$ kubectl apply -f demos/monitoring/custom-metrics.yaml\n\tnamespace \"custom-metrics\" created\n\tserviceaccount \"custom-metrics-apiserver\" created\n\tclusterrolebinding \"custom-metrics:system:auth-delegator\" created\n\trolebinding \"custom-metrics-auth-reader\" created\n\tclusterrole \"custom-metrics-read\" created\n\tclusterrolebinding \"custom-metrics-read\" created\n\tdeployment \"custom-metrics-apiserver\" created\n\tservice \"api\" created\n\tapiservice \"v1beta1.custom-metrics.metrics.k8s.io\" created\n\tclusterrole \"custom-metrics-server-resources\" created\n\tclusterrolebinding \"hpa-controller-custom-metrics\" created\n**第三步**，我们需要为 Custom Metrics APIServer 创建对应的 ClusterRoleBinding，以便能够使用 curl 来直接访问 Custom Metrics 的 API：\n\n\t$ kubectl create clusterrolebinding allowall-cm --clusterrole custom-metrics-server-resources --user system:anonymous\n\tclusterrolebinding \"allowall-cm\" created\n**第四步**，我们就可以把待监控的应用和 HPA 部署起来了，如下所示：\n\n\t$ kubectl apply -f demos/monitoring/sample-metrics-app.yaml\n\tdeployment \"sample-metrics-app\" created\n\tservice \"sample-metrics-app\" created\n\tservicemonitor \"sample-metrics-app\" created\n\thorizontalpodautoscaler \"sample-metrics-app-hpa\" created\n\tingress \"sample-metrics-app\" created\n这里，我们需要关注一下 HPA 的配置，如下所示：\n\n\tkind: HorizontalPodAutoscaler\n\tapiVersion: autoscaling/v2beta1\n\tmetadata:\n\t  name: sample-metrics-app-hpa\n\tspec:\n\t  scaleTargetRef:\n\t    apiVersion: apps/v1\n\t    kind: Deployment\n\t    name: sample-metrics-app\n\t  minReplicas: 2\n\t  maxReplicas: 10\n\t  metrics:\n\t  - type: Object\n\t    object:\n\t      target:\n\t        kind: Service\n\t        name: sample-metrics-app\n\t      metricName: http_requests\n\t      targetValue: 100\n可以看到，**HPA 的配置，就是你设置 Auto Scaling 规则的地方。**\n\n比如，scaleTargetRef 字段，就指定了被监控的对象是名叫 sample-metrics-app 的 Deployment，也就是我们上面部署的被监控应用。并且，它最小的实例数目是 2，最大是 10。\n\n在 metrics 字段，我们指定了这个 HPA 进行 Scale 的依据，是名叫 http_requests 的 Metrics。而获取这个 Metrics 的途径，则是访问名叫 sample-metrics-app 的 Service。\n\n有了这些字段里的定义， HPA 就可以向如下所示的 URL 发起请求来获取 Custom Metrics 的值了：\n\n\thttps://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests\n需要注意的是，上述这个 URL 对应的被监控对象，是我们的应用对应的 Service。这跟本文一开始举例用到的 Pod 对应的 Custom Metrics URL 是不一样的。当然，**对于一个多实例应用来说，通过 Service 来采集 Pod 的 Custom Metrics 其实才是合理的做法。**\n\n这时候，我们可以通过一个名叫 hey 的测试工具来为我们的应用增加一些访问压力，具体做法如下所示：\n\n\t$ # Install hey\n\t$ docker run -it -v /usr/local/bin:/go/bin golang:1.8 go get github.com/rakyll/hey\n\t$ export APP_ENDPOINT=$(kubectl get svc sample-metrics-app -o template --template {{.spec.clusterIP}}); echo ${APP_ENDPOINT}\n\t$ hey -n 50000 -c 1000 http://${APP_ENDPOINT}\n与此同时，如果你去访问应用 Service 的 Custom Metircs URL，就会看到这个 URL 已经可以为你返回应用收到的 HTTP 请求数量了，如下所示：\n\n\t$ curl -sSLk https://<apiserver_ip>/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests\n\t{\n\t  \"kind\": \"MetricValueList\",\n\t  \"apiVersion\": \"custom-metrics.metrics.k8s.io/v1beta1\",\n\t  \"metadata\": {\n\t    \"selfLink\": \"/apis/custom-metrics.metrics.k8s.io/v1beta1/namespaces/default/services/sample-metrics-app/http_requests\"\n\t  },\n\t  \"items\": [\n\t    {\n\t      \"describedObject\": {\n\t        \"kind\": \"Service\",\n\t        \"name\": \"sample-metrics-app\",\n\t        \"apiVersion\": \"/__internal\"\n\t      },\n\t      \"metricName\": \"http_requests\",\n\t      \"timestamp\": \"2018-11-30T20:56:34Z\",\n\t      \"value\": \"501484m\"\n\t    }\n\t  ]\n\t}\n**这里需要注意的是，Custom Metrics API 为你返回的 Value 的格式。**\n\n在为被监控应用编写 /metrics API 的返回值时，我们其实比较容易计算的，是该 Pod 收到的 HTTP request 的总数。所以，我们这个[应用的代码](https://github.com/resouer/kubeadm-workshop/blob/master/images/autoscaling/server.js)其实是如下所示的样子：\n\n\t  if (request.url == \"/metrics\") {\n\t    response.end(\"# HELP http_requests_total The amount of requests served by the server in total\\n# TYPE http_requests_total counter\\nhttp_requests_total \" + totalrequests + \"\\n\");\n\t    return;\n\t  }\n可以看到，我们的应用在 /metrics 对应的 HTTP response 里返回的，其实是 http_requests_total 的值。这，也就是 Prometheus 收集到的值。\n\n而 Custom Metrics APIServer 在收到对 http_requests 指标的访问请求之后，它会从 Prometheus 里查询 http_requests_total 的值，然后把它折算成一个以时间为单位的请求率，最后把这个结果作为 http_requests 指标对应的值返回回去。\n\n所以说，我们在对前面的 Custom Metircs URL 进行访问时，会看到值是 501484m，这里的格式，其实就是 milli-requests，相当于是在过去两分钟内，每秒有 501 个请求。这样，应用的开发者就无需关心如何计算每秒的请求个数了。而这样的“请求率”的格式，是可以直接被 HPA 拿来使用的。\n\n这时候，如果你同时查看 Pod 的个数的话，就会看到 HPA 开始增加 Pod 的数目了。\n\n不过，在这里你可能会有一个疑问，Prometheus 项目，又是如何知道采集哪些 Pod 的 /metrics API 作为监控指标的来源呢。\n\n实际上，如果仔细观察一下我们前面创建应用的输出，你会看到有一个类型是 ServiceMonitor 的对象也被创建了出来。它的 YAML 文件如下所示：\n\n\tapiVersion: monitoring.coreos.com/v1\n\tkind: ServiceMonitor\n\tmetadata:\n\t  name: sample-metrics-app\n\t  labels:\n\t    service-monitor: sample-metrics-app\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: sample-metrics-app\n\t  endpoints:\n\t  - port: web\n这个 ServiceMonitor 对象，正是 Prometheus Operator 项目用来指定被监控 Pod 的一个配置文件。可以看到，我其实是通过 Label Selector 为 Prometheus 来指定被监控应用的。\n\n**总结**\n\n在本篇文章中，我为你详细讲解了 Kubernetes 里对自定义监控指标，即 Custom Metrics 的设计与实现机制。这套机制的可扩展性非常强，也终于使得 Auto Scaling 在 Kubernetes 里面不再是一个“食之无味”的鸡肋功能了。\n\n另外可以看到，Kubernetes 的 Aggregator APIServer，是一个非常行之有效的 API 扩展机制。而且，Kubernetes 社区已经为你提供了一套叫作 KubeBuilder 的工具库，帮助你生成一个 API Server 的完整代码框架，你只需要在里面添加自定义 API，以及对应的业务逻辑即可。\n\n**思考题**\n\n在你的业务场景中，你希望使用什么样的指标作为 Custom Metrics ，以便对 Pod 进行 Auto Scaling 呢？怎么获取到这个指标呢？\n","categories":["reference","k8s"]},{"title":"47 | 绝不仅仅是安全：Kata Containers 与 gVisor","url":"/2021/03/13/reference/k8s/47.绝不仅仅是安全：Kata Containers与gVisor/","content":"\n![](1.jpg)\n\n## 绝不仅仅是安全：Kata Containers 与 gVisor\n\n在上一篇文章中，我为你详细地讲解了 kubelet 和 CRI 的设计和具体的工作原理。而在讲解 CRI 的诞生背景时，我也提到过，这其中的一个重要推动力，就是基于虚拟化或者独立内核的安全容器项目的逐渐成熟。\n<!-- more -->\n使用虚拟化技术来做一个像 Docker 一样的容器项目，并不是一个新鲜的主意。早在 Docker 项目发布之后，Google 公司就开源了一个实验性的项目，叫作 novm。这，可以算是试图使用常规的虚拟化技术来运行 Docker 镜像的第一次尝试。不过，novm 在开源后不久，就被放弃了，这对于 Google 公司来说或许不算是什么新鲜事，但是 novm 的昙花一现，还是激发出了很多内核开发者的灵感。\n\n所以在 2015 年，几乎在同一个星期，Intel OTC （Open Source Technology Center） 和国内的 HyperHQ 团队同时开源了两个基于虚拟化技术的容器实现，分别叫做 Intel Clear Container 和 runV 项目。\n\n而在 2017 年，借着 Kubernetes 的东风，这两个相似的容器运行时项目在中立基金会的撮合下最终合并，就成了现在大家耳熟能详的 Kata Containers 项目。 由于 Kata Containers 的本质就是一个精简后的轻量级虚拟机，所以它的特点，就是“像虚拟机一样安全，像容器一样敏捷”。\n\n而在 2018 年，Google 公司则发布了一个名叫 gVisor 的项目。gVisor 项目给容器进程配置一个用 Go 语言实现的、运行在用户态的、极小的“独立内核”。这个内核对容器进程暴露 Linux 内核 ABI，扮演着“Guest Kernel”的角色，从而达到了将容器和宿主机隔离开的目的。\n\n不难看到，无论是 Kata Containers，还是 gVisor，它们实现安全容器的方法其实是殊途同归的。这两种容器实现的本质，都是给进程分配了一个独立的操作系统内核，从而避免了让容器共享宿主机的内核。这样，容器进程能够看到的攻击面，就从整个宿主机内核变成了一个极小的、独立的、以容器为单位的内核，从而有效解决了容器进程发生“逃逸”或者夺取整个宿主机的控制权的问题。这个原理，可以用如下所示的示意图来表示清楚。\n\n![](2.png)\n\n而它们的区别在于，Kata Containers 使用的是传统的虚拟化技术，通过虚拟硬件模拟出了一台“小虚拟机”，然后在这个小虚拟机里安装了一个裁剪后的 Linux 内核来实现强隔离。\n\n而 gVisor 的做法则更加激进，Google 的工程师直接用 Go 语言“模拟”出了一个运行在用户态的操作系统内核，然后通过这个模拟的内核来代替容器进程向宿主机发起有限的、可控的系统调用。\n\n接下来，我就来为你详细解读一下 KataContainers 和 gVisor 具体的设计原理。\n\n**首先，我们来看 KataContainers。**它的工作原理可以用如下所示的示意图来描述。\n\n![](3.png)\n\n我们前面说过，Kata Containers 的本质，就是一个轻量化虚拟机。所以当你启动一个 Kata Containers 之后，你其实就会看到一个正常的虚拟机在运行。这也就意味着，一个标准的虚拟机管理程序（Virtual Machine Manager, VMM）是运行 Kata Containers 必备的一个组件。在我们上面图中，使用的 VMM 就是 Qemu。\n\n而使用了虚拟机作为进程的隔离环境之后，Kata Containers 原生就带有了 Pod 的概念。即：这个 Kata Containers 启动的虚拟机，就是一个 Pod；而用户定义的容器，就是运行在这个轻量级虚拟机里的进程。在具体实现上，Kata Containers 的虚拟机里会有一个特殊的 Init 进程负责管理虚拟机里面的用户容器，并且只为这些容器开启 Mount Namespace。所以，这些用户容器之间，原生就是共享 Network 以及其他 Namespace 的。\n\n此外，为了跟上层编排框架比如 Kubernetes 进行对接，Kata Containers 项目会启动一系列跟用户容器对应的 shim 进程，来负责操作这些用户容器的生命周期。当然，这些操作，实际上还是要靠虚拟机里的 Init 进程来帮你做到。\n\n而在具体的架构上，Kata Containers 的实现方式同一个正常的虚拟机其实也非常类似。这里的原理，可以用如下所示的一幅示意图来表示。\n\n![](4.jpg)\n\n可以看到，当 Kata Containers 运行起来之后，虚拟机里的用户进程（容器），实际上只能看到虚拟机里的、被裁减过的 Guest Kernel，以及通过 Hypervisor 虚拟出来的硬件设备。\n\n而为了能够对这个虚拟机的 I/O 性能进行优化，Kata Containers 也会通过 vhost 技术（比如：vhost-user）来实现 Guest 与 Host 之间的高效的网络通信，并且使用 PCI Passthrough （PCI 穿透）技术来让 Guest 里的进程直接访问到宿主机上的物理设备。这些架构设计与实现，其实跟常规虚拟机的优化手段是基本一致的。\n\n相比之下，gVisor 的设计其实要更加“激进”一些。它的原理，可以用如下所示的示意图来表示清楚。\n\n![](5.png)\n\ngVisor 工作的核心，在于它为应用进程、也就是用户容器，启动了一个名叫 Sentry 的进程。 而 Sentry 进程的主要职责，就是提供一个传统的操作系统内核的能力，即：运行用户程序，执行系统调用。所以说，Sentry 并不是使用 Go 语言重新实现了一个完整的 Linux 内核，而只是一个对应用进程“冒充”内核的系统组件。\n\n在这种设计思想下，我们就不难理解，Sentry 其实需要自己实现一个完整的 Linux 内核网络栈，以便处理应用进程的通信请求。然后，把封装好的二层帧直接发送给 Kubernetes 设置的 Pod 的 Network Namespace 即可。\n\n此外，Sentry 对于 Volume 的操作，则需要通过 9p 协议交给一个叫做 Gofer 的代理进程来完成。Gofer 会代替应用进程直接操作宿主机上的文件，并依靠 seccomp 机制将自己的能力限制在最小集，从而防止恶意应用进程通过 Gofer 来从容器中“逃逸”出去。\n\n而在具体的实现上，gVisor 的 Sentry 进程，其实还分为两种不同的实现方式。这里的工作原理，可以用下面的示意图来描述清楚。\n\n![](6.png)\n\n**第一种实现方式**，是使用 Ptrace 机制来拦截用户应用的系统调用（System Call），然后把这些系统调用交给 Sentry 来进行处理。\n\n这个过程，对于应用进程来说，是完全透明的。而 Sentry 接下来，则会扮演操作系统的角色，在用户态执行用户程序，然后仅在需要的时候，才向宿主机发起 Sentry 自己所需要执行的系统调用。这，就是 gVisor 对用户应用进程进行强隔离的主要手段。不过， Ptrace 进行系统调用拦截的性能实在是太差，仅能供 Demo 时使用。\n\n而**第二种实现方式，**则更加具有普适性。它的工作原理如下图所示。\n\n![](7.png)\n\n在这种实现里，Sentry 会使用 KVM 来进行系统调用的拦截，这个性能比 Ptrace 就要好很多了。\n\n当然，为了能够做到这一点，Sentry 进程就必须扮演一个 Guest Kernel 的角色，负责执行用户程序，发起系统调用。而这些系统调用被 KVM 拦截下来，还是继续交给 Sentry 进行处理。只不过在这时候，Sentry 就切换成了一个普通的宿主机进程的角色，来向宿主机发起它所需要的系统调用。\n\n可以看到，**在这种实现里，Sentry 并不会真的像虚拟机那样去虚拟出硬件设备、安装 Guest 操作系统。它只是借助 KVM 进行系统调用的拦截，以及处理地址空间切换等细节。**\n\n值得一提的是，在 Google 内部，他们也是使用的第二种基于 Hypervisor 的 gVisor 实现。只不过 Google 内部有自己研发的 Hypervisor，所以要比 KVM 实现的性能还要好。\n\n通过以上的讲述，相信你对 Kata Containers 和 gVisor 的实现原理，已经有一个感性的认识了。需要指出的是，到目前为止，gVisor 的实现依然不是非常完善，有很多 Linux 系统调用它还不支持；有很多应用，在 gVisor 里还没办法运行起来。 此外，gVisor 也暂时没有实现一个 Pod 多个容器的支持。当然，在后面的发展中，这些工程问题一定会逐渐解决掉的。\n\n另外，你可能还听说过 AWS 在 2018 年末发布的一个叫做 Firecracker 的安全容器项目。这个项目的核心，其实是一个用 Rust 语言重新编写的 VMM（即：虚拟机管理器）。这就意味着， Firecracker 和 Kata Containers 的本质原理，其实是一样的。只不过， Kata Containers 默认使用的 VMM 是 Qemu，而 Firecracker，则使用自己编写的 VMM。所以，理论上，Kata Containers 也可以使用 Firecracker 运行起来。\n\n**总结**\n\n在本篇文章中，我为你详细地介绍了拥有独立内核的安全容器项目，对比了 KataContainers 和 gVisor 的设计与实现细节。\n\n在性能上，KataContainers 和 KVM 实现的 gVisor 基本不分伯仲，在启动速度和占用资源上，基于用户态内核的 gVisor 还略胜一筹。但是，对于系统调用密集的应用，比如重 I/O 或者重网络的应用，gVisor 就会因为需要频繁拦截系统调用而出现性能急剧下降的情况。此外，gVisor 由于要自己使用 Sentry 去模拟一个 Linux 内核，所以它能支持的系统调用是有限的，只是 Linux 系统调用的一个子集。\n\n不过，gVisor 虽然现在没有任何优势，但是这种通过在用户态运行一个操作系统内核，来为应用进程提供强隔离的思路，的确是未来安全容器进一步演化的一个非常有前途的方向。\n\n值得一提的是，Kata Containers 团队在 gVisor 之前，就已经 Demo 了一个名叫 Linuxd 的项目。这个项目，使用了 User Mode Linux (UML) 技术，在用户态运行起了一个真正的 Linux Kernel 来为应用进程提供强隔离，从而避免了重新实现 Linux Kernel 带来的各种麻烦。\n\n有兴趣的话，你可以[在这里查看](https://lc32018.sched.com/event/ER8x/run-linux-kernel-as-a-daemon-lai-jiangshan-hypersh)这个演讲。我相信，这个方向，应该才是安全容器进化的未来。这比 Unikernels 这种根本不适合实际场景中使用的思路，要靠谱得多。\n\n> 本篇图片出处均引自 [Kata Containers 的官方对比资料](https://object-storage-ca-ymq-1.vexxhost.net/swift/v1/6e4619c416ff4bd19e1c087f27a43eea/www-assets-prod/presentation-media/kata-containers-and-gvisor-a-quantitave-comparison.pdf)。\n\n**思考题**\n\n安全容器的意义，绝不仅仅止于安全。你可以想象一下这样一个场景：比如，你的宿主机的 Linux 内核版本是 3.6，但是应用却必须要求 Linux 内核版本是 4.0。这时候，你就可以把这个应用运行在一个 KataContainers 里。那么请问，你觉得使用 gVisor 是否也能提供这种能力呢？原因是什么呢？\n","categories":["reference","k8s"]},{"title":"51 | 谈谈Kubernetes开源社区和未来走向","url":"/2021/03/13/reference/k8s/51.谈谈Kubernetes开源社区和未来走向/","content":"\n![](1.jpg)\n\n## 谈谈Kubernetes开源社区和未来走向\n\n在前面的文章中，我已经为你详细讲解了容器与 Kubernetes 项目的所有核心技术点。在今天这最后一篇文章里，我就跟你谈一谈 Kubernetes 开源社区以及 CNCF 相关的一些话题。\n<!-- more -->\n我们知道 Kubernetes 这个项目是托管在 CNCF 基金会下面的。但是，我在专栏最前面讲解容器与 Kubernetes 的发展历史的时候就已经提到过，CNCF 跟 Kubernetes 的关系，并不是传统意义上的基金会与托管项目的关系，CNCF 实际上扮演的，是 Kubernetes 项目的 Marketing 的角色。\n\n这就好比，本来 Kubernetes 项目应该是由 Google 公司一家维护、运营和推广的。但是为了表示中立，并且吸引更多的贡献者加入，Kubernetes 项目从一开始就选择了由基金会托管的模式。而这里的关键在于，这个基金会本身，就是 Kubernetes 背后的“大佬们”一手创建出来的，然后以中立的方式，对 Kubernetes 项目进行运营和 Marketing。\n\n通过这种方式，Kubernetes 项目既避免了因为 Google 公司在开源社区里的“不良作风”和非中立角色被竞争对手口诛笔伐，又可以站在开源基金会的制高点上团结社区里所有跟容器相关的力量。而随后 CNCF 基金会的迅速发展和壮大，也印证了这个思路其实是非常正确和有先见之明的。\n\n不过，在 Kubernetes 和 Prometheus 这两个 CNCF 的一号和二号项目相继毕业之后，现在 CNCF 社区的更多职能，就是扮演一个传统的开源基金会的角色，吸纳会员，帮助项目孵化和运转。\n\n只不过，由于 Kubernetes 项目的巨大成功，CNCF 在云计算领域已经取得了极高的声誉和认可度，也填补了以往 Linux 基金会在这一领域的空白。所以说，你可以认为现在的 CNCF，就是云计算领域里的 Apache ，而它的作用跟当年大数据领域里 Apache 基金会的作用是一样的。\n\n\n不过，需要指出的是，**对于开源项目和开源社区的运作来说，第三方基金会从来就不是一个必要条件。**事实上，这个世界上绝大多数成功的开源项目和社区，都来自于一个聪明的想法或者一帮杰出的黑客。在这些项目的发展过程中，一个独立的、第三方基金会的作用，更多是在该项目发展到一定程度后主动进行商业运作的一部分。开源项目与基金会间的这一层关系，希望你不要本末倒置了。\n\n另外，需要指出的是，CNCF 基金会仅仅负责成员项目的 Marketing， 而绝不会、也没有能力直接影响具体项目的发展历程。无论是任何一家成员公司或者是 CNCF 的 TOC（Technical Oversight Committee，技术监督委员会），都没有对 Kubernetes 项目“指手画脚”的权利，除非这位 TOC 本人就是 Kubernetes 项目里的关键人物。\n\n所以说，真正能够影响 Kubernetes 项目发展的，当然还是 Kubernetes 社区本身。可能你会好奇，**Kubernetes 社区本身的运作方式，又是怎样的呢？**\n\n通常情况下，一个基金会下面托管的项目，都需要遵循基金会本身的管理机制，比如统一的 CI 系统、Code Review 流程、管理方式等等。\n\n但是，在我们这个社区的实际情况，是先有的 Kubernetes，然后才有的 CNCF，并且 CNCF 基金会还是 Kubernetes “一手带大”的。所以，在项目治理这个事情上，Kubernetes 项目早就自成体系，并且发展得非常完善了。而基金会里的其他项目一般各自为阵，CNCF 不会对项目本身的治理方法提出过多的要求。\n\n而说到 **Kubernetes 项目的治理方式，其实还是比较贴近 Google 风格的，即：重视代码，重视社区的民主性。**\n\n首先，Kubernetes 项目是一个没有“Maintainer”的项目。这一点非常有意思，Kubernetes 项目里曾经短时间内存在过 Maintainer 这个角色，但是很快就被废弃了。取而代之的，则是 approver+reviewer 机制。这里具体的原理，是在 Kubernetes 的每一个目录下，你都可以添加一个 OWNERS 文件，然后在文件里写入这样的字段：\n\n\tapprovers:\n\t- caesarxuchao\n\treviewers:\n\t- lavalamp\n\tlabels:\n\t- sig/api-machinery\n\t- area/apiserver\n比如，上面这个例子里，approver 的 GitHub ID 就是 caesarxuchao （Xu Chao），reviewer 就是 lavalamp。这就意味着，任何人提交的 Pull Request（PR，代码修改请求），只要修改了这个目录下的文件，那么就必须要经过 lavalamp 的 Code Review，然后再经过 caesarxuchao 的 Approve 才可以被合并。当然，在这个文件里，caesarxuchao 的权力是最大的，它可以既做 Code Review，也做最后的 Approve。但， lavalamp 是不能进行 Approve 的。\n\n当然，无论是 Code Review 通过，还是 Approve，这些维护者只需要在 PR 下面 Comment /lgtm 和 /approve，Kubernetes 项目的机器人（k8s-ci-robot）就会自动给该 PR 加上 lgtm 和 approve 标签，然后进入 Kubernetes 项目 CI 系统的合并队列，最后被合并。此外，如果你要对这个项目加标签，或者把它 Assign 给其他人，也都可以通过 Comment 的方式来进行。\n\n可以看到，在上述整个过程中，代码维护者不需要对 Kubernetes 项目拥有写权限，就可以完成代码审核、合并等所有流程。这当然得益于 Kubernetes 社区完善的机器人机制，这也是 GitHub 最吸引人的特性之一。\n\n顺便说一句，很多人问我，**GitHub 比 GitLab 或者其他代码托管平台强在哪里？**实际上， GitHub 庞大的 API 和插件生态，才是这个产品最具吸引力的地方。\n\n当然，当你想要将你的想法以代码的形式提交给 Kubernetes 项目时，除非你的改动是 bugfix 或者很简单的改动，否则，你直接提交一个 PR 上去，是大概率不会被 Approve 的。**这里的流程，**一定要按照我下面的讲解来进行：\n\n1. 在 Kubernetes 主库里创建 Issue，详细地描述你希望解决的问题、方案，以及开发计划。而如果社区里已经有相关的 Issue 存在，那你就必须要在这里把它们引用过来。而如果社区里已经存在相同的 Issue 了，你就需要确认一下，是不是应该直接转到原有 issue 上进行讨论。\n\n2. 给 Issue 加上与它相关的 SIG 的标签。比如，你可以直接 Comment /sig node，那么这个 Issue 就会被加上 sig-node 的标签，这样 SIG-Node 的成员就会特别留意这个 Issue。\n\n3. 收集社区对这个 Issue 的信息，回复 Comment，与 SIG 成员达成一致。必要的时候，你还需要参加 SIG 的周会，更好地阐述你的想法和计划。\n\n4. 在与 SIG 的大多数成员达成一致后，你就可以开始进行详细的设计了。\n\n5. 如果设计比较复杂的话，你还需要在 Kubernetes 的设计提议目录（在 Kubernetes Community 库里）下提交一个 PR，把你的设计文档加进去。这时候，所有关心这个设计的社区成员，都会来对你的设计进行讨论。不过最后，在整个 Kubernetes 社区只有很少一部分成员才有权限来 Review 和 Approve 你的设计文档。他们当然也被定义在了这个目录下面的 OWNERS 文件里，如下所示：\n\n\n\treviewers:\n\t  - brendandburns\n\t  - dchen1107\n\t  - jbeda\n\t  - lavalamp\n\t  - smarterclayton\n\t  - thockin\n\t  - wojtek-t\n\t  - bgrant0607\n\tapprovers:\n\t  - brendandburns\n\t  - dchen1107\n\t  - jbeda\n\t  - lavalamp\n\t  - smarterclayton\n\t  - thockin\n\t  - wojtek-t\n\t  - bgrant0607\n\tlabels:\n\t  - kind/design\n这几位成员，就可以称为社区里的“大佬”了。不过我在这里要提醒你的是，“大佬”并不一定代表水平高，所以你还是要擦亮眼睛。此外，Kubernetes 项目的几位创始成员，被称作 Elders（元老），分别是 jbeda、bgrant0607、brendandburns、dchen1107 和 thockin。你可以查看一下这个列表与上述“大佬”名单有什么不同。\n\n1. 上述 Design Proposal 被合并后，你就可以开始按照设计文档的内容编写代码了。这个流程，才是正常大家所熟知的编写代码、提交 PR、通过 CI 测试、进行 Code Review，然后等待合并的流程。\n\n2. 如果你的 feature 是需要要在 Kubernetes 的正式 Release 里发布上线的，那么你还需要在[Kubernetes Enhancements](https://github.com/kubernetes/enhancements/tree/master/keps)这个库里面提交一个 KEP（即 Kubernetes Enhancement Proposal）。这个 KEP 的主要内容，是详细地描述你的编码计划、测试计划、发布计划，以及向后兼容计划等软件工程相关的信息，供全社区进行监督和指导。\n\n以上内容，就是 Kubernetes 社区运作的主要方式了。\n\n**总结**\n\n在本篇文章里，我为你详细讲述了 CNCF 和 Kubernetes 社区的关系，以及 Kubernetes 社区的运作方式，希望能够帮助你更好地理解这个社区的特点和它的先进之处。\n\n除此之外，你可能还听说过 Kubernetes 社区里有一个叫作 Kubernetes Steering Committee 的组织。这个组织，其实也是属于[Kubernetes Community 库](https://github.com/kubernetes/community)的一部分。这个组织成员的主要职能，是对 Kubernetes 项目治理的流程进行约束和规范，但通常并不会直接干涉 Kubernetes 具体的设计和代码实现。\n\n其实，到目前为止，Kubernetes 社区最大的一个优点，就是把“搞政治”的人和“搞技术”的人分得比较清楚。相信你也不难理解，这两种角色在一个活跃的开源社区里其实都是需要的，但是，如果这两部分人发生了大量的重合，那对于一个开源社区来说，恐怕就是个灾难了。\n\n**思考题**\n\n你能说出 Kubernetes 社区同 OpenStack 社区相比的不同点吗？你觉得这两个社区各有哪些优缺点呢？\n","categories":["reference","k8s"]},{"title":"53 | 特别放送 | 2019 年，容器技术生态会发生些什么？","url":"/2021/03/13/reference/k8s/53.特别放送2019年，容器技术生态会发生些什么/","content":"\n![](1.jpg)\n\n## 特别放送 | 2019 年，容器技术生态会发生些什么？\n\n虽然“深入剖析 Kubernetes”专栏已经更新结束了，但我仍在挂念着每一个订阅专栏的“你”，也希望能多和你分享一些我的观点和看法，希望对你有所帮助。今天我和你分享的主题是：2019 年，容器技术生态会发生些什么。\n<!-- more -->\n**1. Kubernetes 项目被采纳度将持续增长**\n\n作为“云原生”（Cloud Native）理念落地的核心，Kubernetes 项目已经成为了构建容器化平台体系的默认选择。但是，不同于一个只能生产资源的集群管理工具，**Kubernetes 项目最大的价值，乃在于它从一开始就提倡的声明式 API 和以此为基础“控制器”模式。**\n\n在这个体系的指导下， Kubernetes 项目保证了在自身突飞猛进的发展过程中 API 层的相对稳定和一定的向后兼容能力，这是作为一个平台级项目被用户广泛接受和认可的重要前提。\n\n**更重要的是，Kubernetes 项目为使用者提供了宝贵的 API 可扩展能力和良好的 API 编程范式，催生出了一个完全基于 Kubernetes API 构建出来的上层应用服务生态。可以说，正是这个生态的逐步完善与日趋成熟，才确立了 Kubernetes 项目如今在云平台领域牢不可破的领导地位，**也间接宣告了其他竞品方案的边缘化。\n\n与此同时，上述事实标准的确立，也使得“正确和合理地使用了 Kubernetes 的能力”，在某种意义上成为了评判上层应用服务框架（比如 PaaS 和 Serverless ）的一个重要依据：这不仅包括了对框架本身复杂性和易用性的考量，也包括了对框架可扩展性和演进趋势的预期与判断。\n\n不过，相比于国外公有云上以 Kubernetes 为基础的容器化作业的高占比，国内公有云市场对容器的采纳程度目前仍然处于比较初步的水平，直接贩卖虚拟机及其关联 IaaS 层能力依然是国内绝大多数公有云提供商的主要业务形态。\n\n所以，不同于国外市场容器技术增长逐步趋于稳定、Kubernetes 公有云服务已经开始支撑头部互联网客户的情况，**Kubernetes 以及容器技术在国内云计算市场里依然具有巨大的增长空间和强劲的发展势头。**\n\n不难预测，Kubernetes 项目在国内公有云上的逐渐铺开，会逐渐成为接下来几年国内公有云市场上的一个重要趋势。而无论是国内外，大量 Kubernetes 项目相关岗位的涌现，正是验证这个趋势与变化的一个最直接的征兆。\n\n**2. “Serverless 化”与“多样性”将成为上层应用服务生态的两大关键词**\n\n当云上的平台层被 Kubernetes 项目逐步统一之后，过去长期纠结在应用编排、调度与资源管理上裹足不前的 PaaS 项目得到了生产力的全面释放，进而在云平台层之上催生出了一个日趋多样化的应用服务生态。\n\n事实上，这个生态的本质与 2014 年之前的 PaaS 生态没有太大不同。只不过，当原本 PaaS 项目的平台层功能（编排、调度、资源管理等）被剥离了出来之后，PaaS 终于可以专注于应用服务和发布流程管理这两个最核心的功能，开始向更轻、更薄、更以应用为中心的方向进行演进。而在这个过程中， Serverless 自然开始成为了主流话题。\n\n这里需要指出的是，Serverless 从 2014 年 AWS 发布 Lambda 时专门用来指代函数计算（或者说 FaaS）发展到今天，已经被扩展成了包括大多数 PaaS 功能在内的一个泛指术语，即：Serverless = FaaS + BaaS。\n\n而究其本质，**“高可扩展性”、“工作流驱动”和“按使用计费”，**可以认为是 Serverless 最主要的三个特征。这也是为什么我们会说今天大家所谈论的 Serverless，其实是经典 PaaS 演进到今天的一种“极端”形态。\n\n伴随着 Serverless 概念本身的“横向发展”，我们不难预料到，2019 年之后云端的应用服务生态，一定会趋于多样化，进而覆盖到更多场景下的应用服务管理需求。**并且，无论是 Function、传统应用、容器、存储服务、网络服务，都会开始尝试以不同的方式和形态嵌入到“高可扩展性”、“工作流驱动”和“按使用计费”这三个特征当中。**\n\n当然，这种变化趋势的原因也不言而喻：Serverless 三个特征背后所体现的，**乃是云端应用开发过程向“用户友好”和“低心智负担”方向演进的最直接途径。而这种“简单、经济、可信赖”的朴实诉求，正是云计算诞生的最初期许和永恒的发展方向。**\n\n而在这种上层应用服务能力向 Serverless 迁移的演进过程中，不断被优化的 Auto-scaling 能力和细粒度的资源隔离技术，将会成为确保 Serverless 能为用户带来价值的最有力保障。\n\n**3. 看得见、摸得着、能落地的“云原生”**\n\n自从 CNCF 社区迅速崛起以来，“云原生”三个字就成了各大云厂商竞相角逐的一个关键词。不过，相比于 Kubernetes 项目和容器技术实实在在的发展和落地过程，云原生（Cloud Native）的概念却长期以来“曲高和寡”，让人很难说出个所以然来。\n\n其实，“云原生”的本质，不是简单对 Kubernetes 生态体系的一个指代。**“云原生” 刻画出的，是一个使用户能低心智负担的、敏捷的，以可扩展、可复制的方式，最大化利用“云”的能力、发挥“云”的价值的一条最佳路径。**\n\n而这其中，**“不可变基础设施”** 是“云原生”的实践基础（这也是容器技术的核心价值）；而 Kubernetes、Prometheus、Envoy 等** CNCF 核心项目，则可以认为是这个路径落地的最佳实践。**这套理论体系的发展过程，与 CNCF 基金会创立的初衷和云原生生态的发展历程是完全一致的。\n\n也正是伴随着这样的发展过程，云原生对于它的使用者的意义，在 2019 年之后已经变得非常清晰：**是否采用云原生技术体系，实际上已经成为了一个关系到是不是要最大化“云”的价值、是不是要在“云”上赢取最广泛用户群体的一个关键取舍。**这涉及到的，是关系到整个组织的发展、招聘、产品形态等一系列核心问题，而绝非一个单纯的技术决定。\n\n明白了这一层道理，在 2019 年，我们已经不难看到，国内最顶尖的技术公司们，都已经开始在云原生技术框架下发起了实实在在的技术体系升级与落地的“战役”。显然，大家都已经注意到，**相比于纠结于“云原生到底是什么”这样意识形态话题，抓紧时间和机遇将 Kubernetes 及其周边核心技术生态在组织中生长起来，并借此机会完成自身基础技术体系的转型与升级，才是这些体量庞大的技术巨人赶上这次云计算浪潮的不二法宝。**\n\n在这个背景下，所谓“云原生”体系在这些公司的落地，只是这个激动人心的技术革命背后的一个附加值而已。\n\n而在“云原生”这个关键词的含义不断清晰的过程中，我们一定要再次强调：**云原生不等于 CNCF，更不等于 Kubernetes。**云原生固然源自于 Kubernetes 技术生态和理念，**但也必然是一个超越 CNCF 和 Kubernetes 存在的一个全集。**它被创立的目的和始终在坚持探索的方向，**是使用户能够最大化利用“云”的能力、发挥“云”的价值，而不是在此过程中构建一个又一个不可复制、不可扩展的“巨型烟囱”。**\n\n所以说，云原生这个词语的准确定义，是围绕着 Kubernetes 技术生态为核心的，但也一定是一个伴随着 CNCF 社区和 Kubernetes 项目不断演进而日趋完善的一个动态过程。而更为重要的是，**在这次以“云”为关键词的技术革命当中，我们每一个人都有可能成为“云原生”的一个重要的定义者。**","categories":["reference","k8s"]},{"title":"54 | 特别放送 | 基于 Kubernetes 的云原生应用管理，到底应该怎么做？","url":"/2021/03/13/reference/k8s/54.特别放送.基于 Kubernetes 的云原生应用管理，到底应该怎么做/","content":"\n![](1.jpg)\n\n## 特别放送 | 基于 Kubernetes 的云原生应用管理，到底应该怎么做？\n\n虽然《深入剖析 Kubernetes》专栏已经完结了一段时间了，但是在留言中，很多同学依然在不时地推敲与消化专栏里的知识和案例。对此我非常开心，同时也看到大家在实践 Kubernetes 的过程中存在的很多问题。所以在接下来的一段时间里，我会以 Kubernetes 最为重要的一个主线能力作为专题，对专栏内容从广度和深度两个方向上进行一系列延伸与拓展。希望这些内容，能够帮助你在探索这个全世界最受欢迎的开源生态的过程中，更加深刻地理解到 Kubernetes 项目的内涵与本质。\n<!-- more -->\n随着 Kubernetes 项目的日趋成熟与稳定，越来越多的人都在问我这样一个问题：现在的 Kubernetes 项目里，最有价值的部分到底是哪些呢？\n\n为了回答这个问题，我们不妨一起回到第 13 篇文章《为什么我们需要 Pod？》中，来看一下几个非常典型的用户提问。\n\n> 用户一：关于升级 War 和 Tomcat 那块，也是先修改 yaml，然后 Kubenertes 执行升级命令，pod 会重新启动，生产也是按照这种方式吗？所以这种情况下，如果只是升级个 War 包，或者加一个新的 War 包，Tomcat 也要重新启动？这就不是完全松耦合了？\n\n> 用户二：WAR 包的例子并没有解决频发打包的问题吧? WAR 包变动后, geektime/sample:v2 包仍然需要重新打包。这和东西一股脑装在 tomcat 中后, 重新打 tomcat 并没有差太多吧?\n\n> 用户三：关于部署 war 包和 tomcat，在升级 war 的时候，先修改 yaml，然后 Kubernetes 会重启整个 pod，然后按照定义好的容器启动顺序流程走下去？正常生产是按照这种方式进行升级的吗？\n\n在《为什么我们需要 Pod？》这篇文章中，为了讲解 Pod 里容器间关系（即：容器设计模式）的典型场景，我举了一个“WAR 包与 Web 服务器解耦”的例子。在这个例子中，我既没有让你通过 Volume 的方式将 WAR 包挂载到 Tomcat 容器中，也没有建议你把 WAR 包和 Tomcat 打包在一个镜像里，而是用了一个 InitContainer 将 WAR 包“注入”给了 Tomcat 容器。\n\n不过，不同用户面对的场景不同，对问题的思考角度也不一样。所以在这一节里，大家提出了很多不同维度的问题。这些问题总结起来，其实无外乎有两个疑惑：\n\n如果 WAR 包更新了，那不是也得重新制作 WAR 包容器的镜像么？这和重新打 Tomcat 镜像有很大区别吗？\n\n当用户通过 YAML 文件将 WAR 包镜像更新后，整个 Pod 不会重建么？Tomcat 需要重启么？\n\n这里的两个问题，实际上都聚焦在了这样一个对于 Kubernetes 项目至关重要的核心问题之上：**基于 Kubernetes 的应用管理，到底应该怎么做？**\n\n比如，对于第一个问题，在不同规模、不同架构的组织中，可能有着不同的看法。一般来说，如果组织的规模不大、发布和迭代次数不多的话，将 WAR 包（应用代码）的发布流程和 Tomcat （Web 服务器）的发布流程解耦，实际上很难有较强的体感。在这些团队中，Tomcat 本身很可能就是开发人员自己负责管理的，甚至被认为是应用的一部分，无需进行很明确的分离。\n\n而对于更多的组织来说，Tomcat 作为全公司通用的 Web 服务器，往往有一个专门的小团队兼职甚至全职负责维护。这不仅包括了版本管理、统一升级和安全补丁等工作，还会包括全公司通用的性能优化甚至定制化内容。\n\n在这种场景下，WAR 包的发布流水线（制作 WAR 包镜像的流水线），和 Tomcat 的发布流水线（制作 Tomcat 镜像的流水线）其实是通过两个完全独立的团队在负责维护，彼此之间可能都不知晓。\n\n这时候，在 Pod 的定义中直接将两个容器解耦，相比于每次发布前都必须先将两个镜像“融合”成一个镜像然后再发布，就要自动化得多了。这个原因是显而易见的：开发人员不需要额外维护一个“重新打包”应用的脚本、甚至手动地去做这个步骤了。\n\n**这正是上述设计模式带来的第一个好处：自动化。**\n\n\n当然，正如另外一些用户指出的那样，这个“解耦”的工作，貌似也可以通过把 WAR 包以 Volume 的方式挂载进 Tomcat 容器来完成，对吧？\n\n然而，相比于 Volume 挂载的方式，通过在 Pod 定义中解耦上述两个容器，其实还会带来**另一个更重要的好处，叫作：自描述。**\n\n为了解释这个好处，我们不妨来重新看一下这个 Pod 的定义：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: javaweb-2\n\tspec:\n\t  initContainers:\n\t  - image: geektime/sample:v2\n\t    name: war\n\t    command: [\"cp\", \"/sample.war\", \"/app\"]\n\t    volumeMounts:\n\t    - mountPath: /app\n\t      name: app-volume\n\t  containers:\n\t  - image: geektime/tomcat:7.0\n\t    name: tomcat\n\t    command: [\"sh\",\"-c\",\"/root/apache-tomcat-7.0.42-v2/bin/start.sh\"]\n\t    volumeMounts:\n\t    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps\n\t      name: app-volume\n\t    ports:\n\t    - containerPort: 8080\n\t      hostPort: 8001 \n\t  volumes:\n\t  - name: app-volume\n\t    emptyDir: {}\n现在，我来问你这样一个问题：**这个 Pod 里，应用的版本是多少？Tomcat 的版本又是多少？**\n\n相信你一眼就能看出来：应用版本是 v2，Tomcat 的版本是 7.0.42-v2。\n\n**没错！所以我们说，一个良好编写的 Pod 的 YAML 文件应该是“自描述”的，它直接描述了这个应用本身的所有信息。**\n\n但是，如果我们改用 Volume 挂载的方式来解耦 WAR 包和 Tomcat 服务器，这个 Pod 的 YAML 文件会变成什么样子呢？如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: javaweb-2\n\tspec:\n\t  containers:\n\t  - image: geektime/tomcat:7.0\n\t    name: tomcat\n\t    command: [\"sh\",\"-c\",\"/root/apache-tomcat-7.0.42-v2/bin/start.sh\"]\n\t    volumeMounts:\n\t    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps\n\t      name: app-volume\n\t    ports:\n\t    - containerPort: 8080\n\t      hostPort: 8001 \n\t  volumes:\n\t  - name: app-volume\n\t    flexVolume:\n\t      driver: \"alicloud/disk\"\n\t      fsType: \"ext4\"\n\t      options:\n\t        volumeId: \"d-bp1j17ifxfasvts3tf40\"\n在上面这个例子中，我们就通过了一个名叫“app-volume”的数据卷（Volume），来为我们的 Tomcat 容器提供 WAR 包文件。需要注意的是，这个 Volume 必须是持久化类型的数据卷（比如本例中的阿里云盘），绝不可以是 emptyDir 或者 hostPath 这种临时的宿主机目录，否则一旦 Pod 重调度你的 WAR 包就找不回来了。\n\n然而，如果这时候我再问你：这个 Pod 里，应用的版本是多少？Tomcat 的版本又是多少？\n\n这时候，你可能要傻眼了：在这个 Pod YAML 文件里，根本看不到应用的版本啊，它是通过 Volume 挂载而来的！\n\n**也就是说，这个 YAML 文件再也没有“自描述”的能力了。**\n\n更为棘手的是，在这样的一个系统中，你肯定是不可能手工地往这个云盘里拷贝 WAR 包的。所以，上面这个 Pod 要想真正工作起来，你还必须在外部再维护一个系统，专门负责在云盘里拷贝指定版本的 WAR 包，或者直接在制作这个云盘的过程中把指定 WAR 包打进去。然而，无论怎么做，这个工作都是非常不舒服并且自动化程度极低的，我强烈不推荐。\n\n**要想 “Volume 挂载”的方式真正能工作，可行方法只有一种：那就是写一个专门的 Kubernetes Volume 插件（比如，Flexvolume 或者 CSI 插件）。**这个插件的特殊之处，在于它在执行完 “Mount 阶段”后，会自动执行一条从远端下载指定 WAR 包文件的命令，从而将 WAR 包正确放置在这个 Volume 里。这个 WAR 包文件的名字和路径，可以通过 Volume 的自定义参数传递，比如：\n\n\t...\n\tvolumes:\n\t  - name: app-volume\n\t    flexVolume:\n\t      driver: \"geektime/war-vol\"\n\t      fsType: \"ext4\"\n\t      options:\n\t        downloadURL: \"https://github.com/geektime/sample/releases/download/v2/sample.war\"\n在这个例子中， 我就定义了 app-volume 的类型是 geektime/war-vol，在挂载的时候，它会自动从 downloadURL 指定的地址下载指定的  WAR 包，问题解决。\n\n可以看到，这个 YAML  文件也是“自描述”的：因为你可以通过 downloadURL 等参数知道这个应用到底是什么版本。看到这里，你是不是已经感受到 “Volume 挂载的方式” 实际上一点都不简单呢？\n\n在明白了我们在 Pod 定义中解耦 WAR 包容器和 Tomcat 容器能够得到的两个好处之后，第一个问题也就回答得差不多了。这个问题的本质，其实是一个关于“ Kubernetes 应用究竟应该如何描述”的问题。\n\n**而这里的原则，最重要的就是“自描述”。**\n\n我们之前已经反复讲解过，Kubernetes 项目最强大的能力，就是“声明式”的应用定义方式。这个“声明式”背后的设计思想，是在 YAML 文件（Kubernetes API  对象）中描述应用的“终态”。然后 Kubernetes 负责通过“控制器模式”不断地将整个系统的实际状态向这个“终态”逼近并且达成一致。\n\n而“声明式”最大的好处是什么呢？\n\n**“声明式”带来最大的好处，其实正是“自动化”。**作为一个 Kubernetes 用户，你只需要在 YAML 里描述清楚这个应用长什么样子，那么剩下的所有事情，就都可以放心地交给 Kubernetes 自动完成了：它会通过控制器模式确保这个系统里的应用状态，最终并且始终跟你在 YAML 文件里的描述完全一致。\n\n这种**“把简单交给用户，把复杂留给自己”**的精神，正是一个“声明式”项目的精髓所在了。\n\n这也就意味着，如果你的 YAML 文件不是“自描述”的，那么 Kubernetes 就不能“完全”理解你的应用的“终态”到底是什么样子的，它也就没办法把所有的“复杂”都留给自己。这不，你就得自己去写一个额外 Volume 插件去了。\n\n回到之前用户提到的第二个问题：当通过 YAML 文件将 WAR 包镜像更新后，整个 Pod 不会重建么？Tomcat 需要重启么？\n\n实际上，当一个  Pod 里的容器镜像被更新后，kubelet 本身就能够判断究竟是哪个容器需要更新，而不会“无脑”地重建整个 Pod。当然，你的 Tomcat 需要配置好 reloadable=“true”，这样就不需要重启 Tomcat 服务器了，这是一个非常常见的做法。\n\n但是，**这里还有一个细节需要注意。**即使 kubelet 本身能够“智能”地单独重建被更新的容器，但如果你的 Pod 是用  Deployment 管理的话，它会按照自己的发布策略（RolloutStrategy） 来通过重建的方式更新 Pod。\n\n这时候，如果这个 Pod 被重新调度到其他机器上，那么 kubelet “单独重建被更新的容器”的能力就没办法发挥作用了。所以说，要让这个案例中的“解耦”能力发挥到最佳程度，你还需要一个“原地升级”的功能，即：允许 Kubernetes 在原地进行 Pod 的更新，避免重调度带来的各种麻烦。\n\n原地升级能力，在 Kubernetes 的默认控制器中都是不支持的。但，这是社区开源控制器项目  https://github.com/openkruise/kruise 的重要功能之一，如果你感兴趣的话可以研究一下。\n\n**总结**\n\n说到这里，再让我们回头看一下文章最开始大家提出的共性问题：现在的 Kubernetes 项目里，最有价值的部分到底是哪些？这个项目的本质到底在哪部分呢？\n\n实际上，通过深入地讲解 “Tomcat 与 WAR 包解耦”这个案例，你可以看到 Kubernetes 的“声明式 API”“容器设计模式”“控制器原理”，以及 kubelet 的工作机制等很多核心知识点，实际上是可以通过一条主线贯穿起来的。这条主线，从“应用如何描述”开始，到“容器如何运行”结束。\n\n**这条主线，正是 Kubernetes 项目中最具价值的那个部分，即：云原生应用管理（Cloud Native Application Management）。**它是一条连接 Kubernetes 项目绝大多数核心特性的关键线索，也是 Kubernetes 项目乃至整个云原生社区这五年来飞速发展背后唯一不变的精髓。\n\n","categories":["reference","k8s"]},{"title":"55 | 结束语 | Kubernetes：赢开发者赢天下","url":"/2021/03/13/reference/k8s/55.结束语Kubernetes：赢开发者赢天下/","content":"\n![](1.jpg)\n\n## 结束语 | Kubernetes：赢开发者赢天下\n\n在本专栏一开始，我用了大量的笔墨和篇幅和你探讨了这样一个话题：Kubernetes 为什么会赢？\n<!-- more -->\n而在当时的讨论中，我为你下了这样一个结论：Kubernetes 项目之所以能赢，最重要的原因在于它争取到了云计算生态里的绝大多数开发者。不过，相信在那个时候，你可能会对这个结论有所疑惑：大家不都说 Kubernetes 是一个运维工具么？怎么就和开发者搭上了关系呢？\n\n事实上，Kubernetes 项目发展到今天，已经成为了云计算领域中平台层当仁不让的事实标准。但这样的生态地位，并不是一个运维工具或者 Devops 项目所能达成的。这里的原因也很容易理解：Kubernetes 项目的成功，是成千上万云计算平台上的开发者用脚投票的结果。而在学习完本专栏之后，相信你也应该能够明白，云计算平台上的开发者们所关心的，并不是调度，也不是资源管理，更不是网络或者存储，他们关心的只有一件事，那就是 Kubernetes 的 API。\n\n这也是为什么，在 Kubernetes 这个项目里，只要是跟 API 相关的事情，那就都是大事儿；只要是想要在这个社区构建影响力的人或者组织，就一定会在 API 层面展开角逐。这一层 “API 为王”的思路，早已经深入到了 Kubernetes 里每一个 API 对象的每一个字段的设计过程当中。\n\n所以说，Kubernetes 项目的本质其实只有一个，那就是“控制器模式”。这个思想，不仅仅是 Kubernetes 项目里每一个组件的“设计模板”，也是 Kubernetes 项目能够将开发者们紧紧团结到自己身边的重要原因。作为一个云计算平台的用户，能够用一个 YAML 文件表达我开发的应用的最终运行状态，并且自动地对我的应用进行运维和管理。这种信赖关系，就是连接 Kubernetes 项目和开发者们最重要的纽带。更重要的是，当这个 API 趋向于足够稳定和完善的时候，越来越多的开发者会自动汇集到这个 API 上来，依托它所提供的能力构建出一个全新的生态。\n\n事实上，在云计算发展的历史上，像这样一个围绕一个 API 创建出一个“新世界”的例子，已经出现过了一次，这正是 AWS 和它庞大的开发者生态的故事。而这一次 Kubernetes 项目的巨大成功，其实就是 AWS 故事的另一个版本而已。只不过，相比于 AWS 作为基础设施层提供运维和资源抽象标准的故事，Kubernetes 生态终于把触角触碰到了应用开发者的边界，使得应用的开发者可以有能力去关心自己开发的应用的运行状态和运维方法，实现了经典 PaaS 项目很多年前就已经提出、但却始终没能达成的美好愿景。\n\n这也是为什么我在本专栏里一再强调，Kubernetes 项目里最重要的，是它的“容器设计模式”，是它的 API 对象，是它的 API 编程范式。这些，都是未来云计算时代的每一个开发者需要融会贯通、融化到自己开发基因里的关键所在。也只有这样，作为一个开发者，你才能够开发和构建出符合未来云计算形态的应用。而更重要的是，也只有这样，你才能够借助云计算的力量，让自己的应用真正产生价值。\n\n而通过本专栏的讲解，我希望你能够真正理解 Kubernetes API 背后的设计思想，能够领悟 Kubernetes 项目为了赢得开发者信赖的“煞费苦心”。更重要的是，当你带着这种“觉悟”再去理解和学习 Kubernetes 调度、网络、存储、资源管理、容器运行时的设计和实现方法时，才会真正触碰到这些机制隐藏在文档和代码背后的灵魂所在。\n\n所以说，**当你不太理解为什么要学习 Kubernetes 项目的时候，或者，你在学习 Kubernetes 项目感到困难的时候，不妨想象一下 Kubernetes 就是未来的 Linux 操作系统。**在这个云计算以前所未有的速度迅速普及的世界里，Kubernetes 项目很快就会像操作系统一样，成为每一个技术从业者必备的基础知识。而现在，你不仅牢牢把握住了这个项目的精髓，也就是声明式 API 和控制器模式；掌握了这个 API 独有的编程范式，即 Controller 和 Operator；还以此为基础详细地了解了这个项目每一个核心模块和功能的设计与实现方法。那么，对于这个未来云计算时代的操作系统，你还有什么好担心的呢？\n\n所以说，《深入剖析 Kubernetes 》专栏的结束，其实是你技术生涯全新的开始。我相信你一定能够带着这个“赢开发者赢天下”的启发，在云计算的海洋里继续乘风破浪、一往无前！\n","categories":["reference","k8s"]},{"title":"52 | 答疑：在问题中解决问题，在思考中产生思考","url":"/2021/03/13/reference/k8s/52.答疑：在问题中解决问题，在思考中产生思考/","content":"\n![](1.jpg)\n\n## 在问题中解决问题，在思考中产生思考\n\n**问题 1：**你是否知道如何修复容器中的 top 指令以及 /proc 文件系统中的信息呢？（提示：lxcfs）\n<!-- more -->\n其实，这个问题的答案在提示里其实已经给出了，即 lxcfs 方案。通过 lxcfs，你可以把宿主机的 /var/lib/lxcfs/proc 文件系统挂载到 Docker 容器的 /proc 目录下。使得容器中进程读取相应文件内容时，实际上会从容器对应的 Cgroups 中读取正确的资源限制。 从而得到正确的 top 命令的返回值。\n\n问题选自第 6 篇文章《白话容器基础（二）：隔离与限制》。\n\n**问题 2：**既然容器的 rootfs（比如，Ubuntu 镜像），是以只读方式挂载的，那么又如何在容器里修改 Ubuntu 镜像的内容呢？（提示：Copy-on-Write）\n\n这个问题的答案也同样出现在了提示里。\n\n简单地说，修改一个镜像里的文件的时候，联合文件系统首先会从上到下在各个层中查找有没有目标文件。如果找到，就把这个文件复制到可读写层进行修改。这个修改的结果会屏蔽掉下层的文件，这种方式就被称为 copy-on-write。\n\n问题选自第 7 篇文章《白话容器基础（三）：深入理解容器镜像》。\n\n**问题 3：**你在查看 Docker 容器的 Namespace 时，是否注意到有一个叫 cgroup 的 Namespace？它是 Linux 4.6 之后新增加的一个 Namespace，你知道它的作用吗？\n\nLinux 内核从 4.6 开始，支持了一个新的 Namespace 叫作：Cgroup Namespace。 我们知道，正常情况下，在一个容器里查看 /proc/$PID/cgroup，是会看到整个宿主机的 cgroup 信息的。而有了 Cgroup Namespace 后，每个容器里的进程都会有自己 Cgroup Namespace，从而获得一个属于自己的 Cgroups 文件目录视图。也就是说，Cgroups 文件系统也可以被 Namespace 隔离起来了。\n\n问题选自第 8 篇文章《 白话容器基础（四）：重新认识 Docker 容器》。\n\n**问题 4：**你能否说出，Kubernetes 使用的这个“控制器模式”，跟我们平常所说的“事件驱动”，有什么区别和联系吗？\n\n这里“控制器模式”和“事件驱动”最关键的区别在于：\n\n对于控制器来说，被监听对象的变化是一个持续的信号，比如变成 ADD 状态。只要这个状态没变化，那么此后无论任何时候控制器再去查询对象的状态，都应该是 ADD。\n\n而对于事件驱动来说，它只会在 ADD 事件发生的时候发出一个事件。如果控制器错过了这个事件，那么它就有可能再也没办法知道 ADD 这个事件的发生了。\n\n问题选自第 16 篇文章《编排其实很简单：谈谈“控制器”模型》。\n\n**问题 5：**在实际场景中，有一些分布式应用的集群是这么工作的：当一个新节点加入到集群时，或者老节点被迁移后重建时，这个节点可以从主节点或者其他从节点那里同步到自己所需要的数据。\n\n在这种情况下，你认为是否还有必要将这个节点 Pod 与它的 PV 进行一对一绑定呢？（提示：这个问题的答案根据不同的项目是不同的。关键在于，重建后的节点进行数据恢复和同步的时候，是不是一定需要原先它写在本地磁盘里的数据）\n\n这个问题的答案是不需要。\n\n像这种不依赖于 PV 保持存储状态或者不依赖于 DNS 名字保持拓扑状态的”非典型“应用的管理，都应该使用 Operator 来实现。\n\n问题选自第 19 篇文章《深入理解 StatefulSet（二）：存储状态》。\n\n**问题 6：**我在文中提到，在 Kubernetes v1.11 之前，DaemonSet 所管理的 Pod 的调度过程，实际上都是由 DaemonSet Controller 自己而不是由调度器完成的。你能说出这其中有哪些原因吗？\n\n这里的原因在于，默认调度器之前的功能不是很完善，比如，缺乏优先级和抢占机制。所以，它没办法保证 DaemonSet ，尤其是部署时候的系统级的、高优先级的 DaemonSet 一定会调度成功。这种情况下，就会影响到集群的部署了。\n\n问题选自第 21 篇文章《容器化守护进程的意义：DaemonSet》。\n\n**问题 7：**在 Operator 的实现过程中，我们再一次用到了 CRD。可是，你一定要明白，CRD 并不是万能的，它有很多场景不适用，还有性能瓶颈。你能列举出一些不适用 CRD 的场景么？你知道造成 CRD 性能瓶颈的原因主要在哪里么？\n\nCRD 目前不支持 protobuf，当 API Object 数量 >1K，或者单个对象 >1KB，或者高频请求时，CRD 的响应都会有问题。 所以，CRD 千万不能也不应该被当作数据库使用。\n\n其实像 Kubernetes ，或者说 Etcd 本身，最佳的使用场景就是作为配置管理的依赖。此外，如果业务需求不能用 CRD 进行建模的时候，比如，需要等待 API 最终返回，或者需要检查 API 的返回值，也是不能用 CRD 的。同时，当你需要完整的 APIServer 而不是只关心 API 对象的时候，请使用 API Aggregator。\n\n问题选自第 27 篇文章《聪明的微创新：Operator 工作原理解读》。\n\n**问题 8：**正是由于需要使用“延迟绑定”这个特性，Local Persistent Volume 目前还不能支持 Dynamic Provisioning。你是否能说出，为什么“延迟绑定”会跟 Dynamic Provisioning 有冲突呢？\n\n延迟绑定将 Volume Bind 的时机，推迟到了第一个使用该 Volume 的 Pod 到达调度器的时候。可是对于 Dynamic Provisioning 来说，它是要在管理 Volume 的控制循环里就为 PVC 创建 PV 然后绑定起来的，这个时间点跟 Pod 被调度的时间点是不相关的。\n\n问题选自第 29 篇文章《 PV、PVC 体系是不是多此一举？从本地持久化卷谈起》。\n\n**问题 9：**请你根据编写 FlexVolume 和 CSI 插件的流程，分析一下什么时候该使用 FlexVolume，什么时候应该使用 CSI？\n\n在文章中我其实已经提到，CSI 与 FlexVolume 的最大区别，在于 CSI 可以实现 Provision 阶段。所以说，对于不需要 Provision 的情况 ，比如你的远程存储服务总是事先准备好或者准备起来非常简单的情况下，就可以考虑使用 FlexVolume。但在生产环境下，我都会优先推荐 CSI 的方案。\n\n问题选自第 31 篇文章《容器存储实践：CSI 插件编写指南》。\n\n**问题 10：**Flannel 通过“隧道”机制，实现了容器之间三层网络（IP 地址）的连通性。但是，根据这个机制的工作原理，你认为 Flannel 能保证容器二层网络（MAC 地址）的连通性吗？为什么呢？\n\n不能保证，因为“隧道”机制只能保证被封装的 IP 包可以到达目的地。而只要网络插件能满足 Kubernetes 网络的三个假设，Kubernetes 并不关心你的网络插件的实现方式是把容器二层连通的，还是三层连通的。\n\n问题选自第 33 篇文章《深入解析容器跨主机网络》。\n\n**问题 11：**你能否能总结一下三层网络方案和“隧道模式”的异同，以及各自的优缺点？\n\n在第 35 篇文章的正文里，我已经为你讲解过，隧道模式最大的特点，在于需要通过某种方式比如 UDP 或者 VXLAN 来对原始的容器间通信的网络包进行封装，然后伪装成宿主机间的网络通信来完成容器跨主通信。这个过程中就不可避免地需要封包和解封包。这两个操作的性能损耗都是非常明显的。而三层网络方案则避免了这个过程，所以性能会得到很大的提升。\n\n不过，隧道模式的优点在于，它依赖的底层原理非常直白，内核里的实现也非常成熟和稳定。而三层网络方案，相对来说维护成本会比较高，容易碰到路由规则分发和设置出现问题的情况，并且当容器数量很多时，宿主机上的路由规则会非常复杂，难以 Debug。\n\n所以最终选择选择哪种方案，还是要看自己的具体需求。\n\n问题选自第 35 篇文章《解读 Kubernetes 三层网络方案》。\n\n**问题 12：**为什么宿主机进入 MemoryPressure 或者 DiskPressure 状态后，新的 Pod 就不会被调度到这台宿主机上呢？\n\n在 Kubernetes 里，实际上有一种叫作 Taint Nodes by Condition 的机制，即当\n\nNode 本身进入异常状态的时候，比如 Condition 变成了 DiskPressure。那么， Kubernetes 会通过 Controller 自动给 Node 加上对应的 Taint，从而阻止新的 Pod 调度到这台宿主机上。\n\n问题选自第 40 篇文章《Kubernetes 的资源模型与资源管理》。\n\n**问题 13：**Kubernetes 默认调度器与 Mesos 的“两级”调度器，有什么异同呢？\n\nMesos 的两级调度器的设计，是 Mesos 自己充当 0 层调度器（Layer 0），负责统一管理整个集群的资源情况，把可用资源以 Resource Offer 的方式暴露出去；而上层的大数据框架（比如 Spark）则充当 1 层调度器（Layer 1），它会负责根据 Layer 0 发来的 Resource Offer 来决定把任务调度到某个具体的节点上。这样做的好处是：\n\n第一，上层大数据框架本身往往自己已经实现了调度逻辑，这样它就可以很方便地接入到 Mesos 里面；\n\n第二，这样的设计，使得 Mesos 本身能够统一地对上层所有框架进行资源分配，资源利用率和调度效率就可以得到很好的保证了。\n\n相比之下，Kubernetes 的默认调度器实际上无论从功能还是性能上都要简单得多。这也是为什么把 Spark 这样本身就具有调度能力的框架接入到 Kubernetes 里还是比较困难的。\n\n问题选自第 41 篇文章《十字路口上的 Kubernetes 默认调度器》。\n\n**问题 14：**当整个集群发生可能会影响调度结果的变化（比如，添加或者更新 Node，添加和更新 PV、Service 等）时，调度器会执行一个被称为 MoveAllToActiveQueue 的操作，把所调度失败的 Pod 从 unscheduelableQ 移动到 activeQ 里面。请问这是为什么？\n\n一个相似的问题是，当一个已经调度成功的 Pod 被更新时，调度器则会将 unschedulableQ 里所有跟这个 Pod 有 Affinity/Anti-affinity 关系的 Pod，移动到 activeQ 里面。请问这又是为什么呢？\n\n其实，这两个问题的答案是一样的。\n\n在正常情况下，默认调度器在调度失败后，就会把该 Pod 放到 unschedulableQ 里。unschedulableQ 里的 Pod 是不会出现在下个调度周期里的。但是，当集群本身发生变化时，这个 Pod 就有可能再次变成可调度的了，所以这时候调度器要把它们移动到 activeQ 里面，这样它们就获得了下一次调度的机会。\n\n类似地，当原本已经调度成功的 Pod 被更新后，也有可能触发 unschedulableQ 里与它有 Affinity 或者 Anti-Affinity 关系的 Pod 变成可调度的，所以它也需要获得“重新做人”的机会。\n\n问题选自第 43 篇文章《Kubernetes 默认调度器的优先级与抢占机制》。\n\n**问题 15：**请你思考一下，我前面讲解过的 Device Plugin 为容器分配的 GPU 信息，是通过 CRI 的哪个接口传递给 dockershim，最后交给 Docker API 的呢？\n\n既然 GPU 是 Devices 信息，那当然是通过 CRI 的 CreateContainerRequest 接口。这个接口的参数 ContainerConfig 里就有容器 Devices 的描述。\n\n问题选自第 46 篇文章《解读 CRI 与 容器运行时》。\n\n**问题 16：**安全容器的意义，绝不仅仅止于安全。你可以想象一下这样一个场景：比如，你的宿主机的 Linux 内核版本是 3.6，但是应用却必须要求 Linux 内核版本是 4.0。这时候，你就可以把这个应用运行在一个 KataContainers 里。那么请问，你觉得使用 gVisor 是否也能提供这种能力呢？原因是什么呢？\n\n答案是不能。gVisor 的实现里并没有一个真正的 Linux Guest Kernel 在运行。所以它不能像 KataContainers 或者虚拟机那样，实现容器和宿主机不同 Kernel 甚至不同操作系统的需求。\n\n但还是要强调一下，以 gVisor 为代表的用户态 Kernel 方案是安全容器的未来，只是现在还不够完善。\n\n问题选自第 47 篇文章《绝不仅仅是安全：Kata Containers 与 gVisor》。\n\n**问题 17：**将日志直接输出到 stdout 和 stderr，有没有什么其他的隐患或者问题呢？如何进行处理呢？\n\n这样做有一个问题，就是日志都需要经过 Docker Daemon 的处理才会写到宿主机磁盘上，所以宿主机没办法以容器为单位进行日志文件的 Rotate。这时候，还是要考虑通过宿主机的 Agent 来对容器日志进行处理和收集的方案。\n\n问题选自第 50 篇文章《让日志无处可逃：容器日志收集与管理》。\n\n**问题 18：**你能说出 Kubernetes 社区同 OpenStack 社区相比的不同点吗？你觉得各有哪些优缺点呢？\n\nOpenStack 社区非常强调民主化，治理方式相对松散，这导致它在治理上没能把主线和旁线分开，政治和技术也没有隔离。这使得后期大量的低价值或者周边型的项目不断冲进 OpenStack 社区，大大降低了社区的含金量，并且分散了大量的社区精力在这些价值相对不高的项目上，从而拖慢并干扰了比如 Cinder、Neutron 等核心项目的演进步伐和方向，最终使得整个社区在容器的热潮下难以掉头，不可避免地走向了下滑的态势。\n\n相比之下，CNCF 基金会成功地帮助 Kubernetes 社区分流了低价值以及周边型项目的干扰，并且完全承接了 Marketing 的角色，使得 Kubernetes 社区在后面大量玩家涌入的时候，依然能够专注在主线的演进上。\n\nKubernetes 社区和 OpenStack 社区的这个区别，是非常关键的。\n\n问题选自第 51 篇文章《谈谈 Kubernetes 开源社区和未来走向》。\n","categories":["reference","k8s"]},{"title":"istio 05 virtual service","url":"/2021/03/13/microService/istio/istio_05_virtual_service/","content":"\n## **Classic traffic management model**\n1. 没有流量管理\n![](legacy_1.PNG)\n\n2. 有流量管理, 如nginx, 限制某个IP或IP网段访问, 也可以选择将客户端的请求路由到后台的任意一个web服务器上.\n![](legacy_2.PNG)\n\n## **Istio traffic**\nIstio traffic management 特指数据面的流量管理\n\nistio traffic management API Resource\n * Virtual services\n * Destination rules\n * Gateways\n * Service entries\n * Sidecars\n\n![](istio_traffic_1.PNG)\n**Sample .yaml resource files**\n\n### **scenario 1**\n![](scenario_1.PNG)\n1. jiuxi-client.yaml\n2. jiuxi-deployment.yaml\n3. jiuxi-svc.yaml\n4. jiuxi-vs.yaml\n\n\njiuxi-client.yaml\n\n```xml\n\t$ touch jiuxi-client.yaml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadataL:\n\t  name: client\n\tspec:\n\t  replicas: 1\n\t  selector:\n\t    matchLabels:\n\t      app: client\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: client\n\t    spec:\n\t      containers:\n\t      - name: busybox\n\t        image: busybox\n\t        imagePullPolicy: IfNotPresent\n\t        command: [ \"/bin/sh\", \"-c\", \"sleep 3600\" ]\n```\n\n```shell\n\t$ kubectl apply -f jiuxi-client.yaml\n```\n\njiuxi-deploy.yaml\n\n```xml\n\t$ touch jiuxi-deploy.yaml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: httpd\n\t  labels:\n\t    server: httpd\n\t    app: web\n\tspec:\n\t  replicas: 1\n\t  selector:\n\t    matchLabels:\n\t      server: httpd\n\t      app: web\n\t  template:\n\t    metadata:\n\t      name: httpd\n\t      labels:\n\t        server: httpd\n\t        app: web\n\t    spec:\n\t      containers:\n\t      - name: busybox\n\t        image: busybox\n\t        imagePullPolicy: IfNotPresent\n\t        command: [ \"/bin/sh\", \"-c\", \"echo 'hello httpd' > /var/www/index.html; httpd -f -p 8080 -h /var/www\" ]\n\t---\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: tomcat\n\t  labels:\n\t    server: tomcat\n\t    app: web\n\tspec:\n\t  replicas: 1\n\t  selector:\n\t    matchLabels:\n\t      server: tomcat\n\t      app: web\n\t  template:\n\t    metadata:\n\t      name: tomcat\n\t      labels:\n\t        server: tomcat\n\t        app: web\n\t    spec:\n\t      containers:\n\t      - name: tomcat\n\t        image: docker.io/kubeguide/tomcat-app:v1\n\t        imagePullPolicy: IfNotPresent\n```\n```shell\n\t$ kubectl apply -f jiuxi-deploy.yaml\n```\n\njiuxi-svc.yaml\n\n```xml\n\t$ touch jiuxi-svc.yaml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: tomcat-svc\n\tspec:\n\t  selector:\n\t    server: tomcat\n\t  ports:\n\t  - name http\n\t    port: 8080\n\t    targetPort: 8080\n\t    protocol: TCP\n\t---\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: httpd-svc\n\tspec:\n\t  selector:\n\t    server: httpd\n\t  ports:\n\t  - name http\n\t    port: 8080\n\t    targetPort: 8080\n\t    protocol: TCP\n```\n```shell\n\t$ kubectl apply -f jiuxi-svc.yaml\n```\n也可以写完上面三个yaml文件后直接执行下面命令一次全部部署\n\n```shell\n\t$ kubectl apply -f .\n```\n查看service最终有没有跟pod绑定:\n\n```shell\n\t$ kubectl get endpoints\n\t$ curl http://<ENDPOINTS-IP>:8080\n```\n登陆client pod\n\n```shell\n\t$ kubectl exec -it client-*** -- sh\n\t/ # wget -q -O - http://tomcat-svc:8080\t\t// wget --help\n\t/ # wget -q -O - http://httpd-svc:8080\t\t//-q: quiet静态访问,不进行下载文件; -O: 输出文件, '-'会输出到控制台\n\t/ # wget -q -O - http://<httpd-svc-clusterIP>:8080\t//效果跟上面一样\n```\n\n### **scenario 2**\n![](scenario_2.PNG)\n修改添加jiuxi-svc.yaml内容\n\n```xml\n\t$ touch jiuxi-svc.yaml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: tomcat-svc\n\tspec:\n\t  selector:\n\t    server: tomcat\n\t  ports:\n\t  - name http\n\t    port: 8080\n\t    targetPort: 8080\n\t    protocol: TCP\n\t---\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: httpd-svc\n\tspec:\n\t  selector:\n\t    server: httpd\n\t  ports:\n\t  - name http\n\t    port: 8080\n\t    targetPort: 8080\n\t    protocol: TCP\n\t---\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: web-svc\n\tspec:\n\t  selector:\n\t    app: web\t//将此Service与httpd和tomcat两个Pod相关联\n\t  ports:\n\t  - name http\n\t    port: 8080\n\t    targetPort: 8080\n\t    protocol: TCP\n```\n```shell\n\t$ kubectl apply -f jiuxi-svc.yaml\n```\n查看service最终有没有跟pod绑定,并多次执行curl 操作查看是否实现轮询:\n\n```shell\n\t$ kubectl get endpoints\n\t$ curl http://<web-svc-ENDPOINTS-IP>:8080\n```\n登陆client pod\n\n```shell\n\t$ kubectl exec -it client-*** -- sh\n\t//多次执行wget操作查看是否轮询访问httpd和tomcat服务\n\t/ # wget -q -O - http://web-svc:8080\n\t/ # wget -q -O - http://<web-svc-clusterIP>:8080\t//效果跟上面一样\n```\n\n### **scenario 3**\n让VirtualService规则生效的话，前提条件是所有的服务都被istio注入sidecar, 使这些服务处于istio的服务网格控制下.  \n在没有注入sidecar的istio控制之外来访问，VirtualService制定的服务规则不会生效.  \nEnvoy + VirtualService --> traffic routing  \nweb service(VirtualService,组件是envoy)充当[反向代理](https://www.jianshu.com/p/3b82ff3321b4)服务器.  \nVirtualService主要包含两部分: hosts field(hosts), routing rules(如http或tcp下的路由规则).  \n![](scenario_3.PNG)\n\njiuxi-vs.yaml\n\n```xml\n\t$ touch jiuxi-vs.yaml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: web-svc-vs\n\tspec:\t\t// hosts: 指在k8s上可寻址的目标资源如Service或istio网关Gateway, 如果流量要访问这个目标资源的话, 要遵循下面http的路由规则.\n\t  hosts:\t// 指Istio的VirtualService作用在K8s的哪个Service上, 这里是web-svc这个Service\n\t  - web-svc\t\t// 短域名方式, 因为是在同一个namespace下, 全域名写法(default命名空间): web-svc.default.svc.cluster.local\n\t  http:\t\t\t// 上面如果客户端和要访问的服务不在同一个namespace下的话必须用全域名\n\t  - route:\n\t    - destination:\n\t        host: tomcat-svc\t// 指的是k8s的Service\n\t      weight: 80\n\t    - destination:\n\t        host: httpd-svc\n\t      weight: 20\n```\n部署VirtualService并查看是否部署成功\n\n```shell\n\t$ kubectl apply -f jiuxi-vs.yaml\n\t$ kubectl get virtualservices.networking.istio.io\n\tNAME       GATEWAYS     HOSTS      AGE\n\tweb-svc-vs              [web-svc]  9s\n```\nVirtualService是istio资源，不能通过`kubectl get svc`查看, 也因此相对K8s的Service来说是虚拟服务:VirtualService  \n\n注入sidecar\n\n```shell\n\t$ istioctl kube-inject jiuxi-client.yaml | kubectl apply -f -\n\t$ istioctl kube-inject jiuxi-deploy.yaml | kubectl apply -f -\n```\n登陆client, 在istio服务网格内访问服务\n\n```shell\n\t$ kubectl exec -it client-*** -- sh\n\t/ # wget -q -O - http://web-svc:8080\t//多次执行查看效果\n\t/ # wget -q -O - http://<web-svc-clusterIP>:8080\t//效果跟上面一样\n```\n\n## **Elements of Routing Rules**\n更多istio路由规则请访问istio官网: Istio -> Reference -> Configuration -> Traffic Management -> VirtualService:\n[https://istio.io/docs/reference/config/networking/virtual-service/#HTTPMatchRequest](https://istio.io/docs/reference/config/networking/virtual-service/#HTTPMatchRequest)\n\n * 匹配条件(match condition)\n * 路由目的地(destination)\nVirtualService的优先级是yaml文件越往上的路由规则优先级越高.  \n实例：\n\n\n``` xml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: web-vs-svc\n\tspec:\n\t  hosts:\n\t  - \"web-svc\"\n\t  http:\n\t  - match:\t\t\t// 也可以没有match, 只有route, 表示所有http或tcp或其它流量都遵循同一套route规则.\n\t    - headers:\t\t// 下面是路由组合规则: 1.请求头key有exact=jiuxi\n\t        end-user:\n\t          exact: jiuxi\n\t      uri:\n\t        prefix: \"/ratings/v2/\"\t// 2.请求的uri的前缀等于\"/ratings/v2/\"\n\t      ignoreUriCase: true\t\t// 3.请求的uri忽略大小写\n\t    route:\n\t    - destination:\n\t        host: ratings.prod.svc.cluster.local\t// VirtualService与此ratings这个K8s Service资源在同一个namespace下的话可以写成: ratings\n\t  - route:\n\t    - destination:\n\t        host: tomcat-svc\n```\n复制上面的jiuxi-vs.yaml为jiuxi-vs-with-condition.yaml并修改内容如下:\n\n```shell\n\t$ touch jiuxi-vs.yaml\n```\n```xml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: web-svc-vs\n\tspec:\t\t// hosts: 指在k8s上可寻址的目标资源如Service或istio网关Gateway, 如果流量要访问这个目标资源的话, 要遵循下面http的路由规则.\n\t  hosts:\t// 指Istio的VirtualService作用在K8s的哪个Service上, 这里是web-svc这个Service\n\t  - web-svc\t\t// 短域名方式, 因为是在同一个namespace下, 全域名写法(default命名空间): web-svc.default.svc.cluster.local\n\t  http:\t\t\t// 上面如果客户端和要访问的服务不在同一个namespace下的话必须用全域名\n\t  - match:\n\t    - headers:\n\t        end-user:\n\t          exact: jiuxi\n\t    route:\n\t    - destination:\n\t        host: tomcat-svc\t// 指的是k8s的Service\n\t      weight: 80\n\t    - destination:\n\t        host: httpd-svc\n\t      weight: 20\n\t  - route:\n\t    - destination:\n\t        host: httpd-svc\n\t$ kubectl apply -f jiuxi-vs-with-condition.yaml\n```\n登陆client, 在istio服务网格内访问服务\n\n```shell\n\t$ kubectl exec -it client-*** -- sh\n\t/ # wget -q -O - http://web-svc:8080\t//多次执行查看效果, 只访问到httpd服务\n\t/ # wget -q -O - http://web-svc:8080 --header 'end-user: jiuxi'\t//多次执行发现多次访问到tomcat服务\n```\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio 06 network namespace","url":"/2021/03/13/microService/istio/istio_06_network_namespace/","content":"\n## 网卡, iptables, 路由表\n网卡相当于整个计算机的网关, 有唯一的MAC地址, 用来识别计算机并用来跟其它计算机通信, 网卡位于OSI七层模型的第二层链路层.\n\n查看iptables: `$ iptables -nvL`用来查看流量经过网卡进入计算机后的流向, 指定哪些端口服务.\n\n查看路由表: `$ route -n`, 定义网络流量最终可以通过哪个网络设备出去.\n\n## ping和traceroute命令\n操作网络OSI模型2层链路层网命令ping, traceroute\n\n```shell\n\t//ping\n\t$ ping -c 3 127.0.0.1\t//ping 3次 环回地址\n\t$ ping -c 3 127.33.99.11\t//环回地址不止是指127.0.0.1, 只要是以127开头的网段都可以.\n\n\t// traceroute\n\t$ yum install -y net-tools\n\t$ yum install -y traceroute\n\t$ traceroute 127.0.0.1\n```\n\n## ip命令查看管理网络命名空间(namespace)\n`ip 命令`是在`centos 7`引入进来的, 基本不用以前的ifconfig命令.  \n`$ docker exec -it container cmd...` docker 命令操作实际上是在linux的网络命名空间发展来的.  \n`$ kubectl exec -it pod -- cmd ...` kubernetes 命令操作实际上也是在linux的网络命名空间发展来的.  \n**linux网络命名空间是属于linux内核的一个功能,必须要内核支持, docker是在此基础上做了扩展和封装.**\n\n### **ip 基本命令**\n\n```shell\n\t$ ip netns add eden\t\t\t//添加Linux network namespace, 与宿主机的网络命名空间隔离\n\t$ ip netns ls\t\t\t\t//查看\n\t$ ip netns delete eden\t\t//删除 network namespace\n\t$ ip netns add greenland\n```\n进入新建的网络命名空间(与`docker exec -it container cmd...`非常类似).  \n\n```shell\n\t$ ip netns exec greenland bash\t\t// 进入网络命名空间, 但是用的User和主机提示符跟原来相同\n\t$ ip link\t\t\t//查看有什么设备\n\t$ ip addr\t\t\t//查看ip设备地址\n\t$ exit\n```\n重命名网络命名空间的User和主机提示符\n\n```shell\n\t$ ip netns exec greenland bash -rcfile <(echo \"PS1=\\\"greenland\\\"\")\t// 修改User和主机提示符为greenland\n\tgreenland> ip link\t\t\t\t\t// 1. 查看网络设备是处于\"state DOWN mode DEFAULT\"\n\tgreenland> ping -c 3 127.0.0.1\t\t// 2. 返回: connect: Network is unreachable\n\tgreenland> ip link set lo up\t\t// 3. 启用设备\n\tgreenland> ip link\t\t\t\t\t// 4. 再次查看网络设备处于\"up\"\n\tgreenland> ping -c 3 127.8.8.8\t\t// 5. 查看可以ping通了\n\n\tgreenland> ip addr\n\tgreenland> route -n\t\t\t\t\t// 查看路由表\n\tgreenland> iptables -nvL -t nat\t\t// 查看iptables\n\tgreenland> ifconfig\t\t\t\t\t// 返回没有内容因为设备都处于没有启动状态\n\tgreenland> exit\t\t\t\t\t\t// 退出\n```\n不进入网络命名空间而直接执行新建的网络命名空间里的命令\n\n```shell\n\t$ ip netns exec greenland ip addr\t\t// 去掉bash, 不进入网络命名空间\n```\n### **实例: 西门庆和潘金莲的私会**\n**类比西门庆(李瓶儿)和潘金莲(武大郎)在各自家里(network namespace)通过连接打造好的相关联放在各自家里固定位置(IP address)的一半梯子(跨network namespace)来互相访问**\n![](link_1.PNG)\n1. 创建西门庆和潘金莲的家(network namespace)\n\n\n\t$ ip netns add ximenqing-ns\n\t$ ip netns add panjinlian-ns\n\t$ ip netns ls\n2. 打造相关联的用来拼接梯子的两半梯子:\n\n\n\t//veth 从名字上来看是 Virtual ETHernet(虚拟以太网)的缩写，它的作用很简单，就是要把从一个 network namespace 发出的数据包转发到另一个 namespace。veth 设备是成对的，一个是 container 之中，另一个在 container 之外，即在真实机器上能看到的\n\t$ ip link add ximenqing type veth peer name panjinlian\n\t$ ip ls\n3. 将打造好的两半梯子各自放到西门庆和潘金莲家里, 在需要的时候再拿出来拼接, 梯子状态是down.  \n梯子放进西门庆和潘金莲家里后, 宿主机network namespace里的xinmenqing和panjinlian桥(网络设备)就消失了.\n\n\n\t$ ip link set ximenqing netns ximenqing-ns\t\t// 把ximenqing这个梯子(设备)塞到ximenqing家里(network namespace)\n\t$ ip netns exec ximenqing ip link\t//查看西门庆家中的设备\n\t$ ip link set panjinlian netns panjinlian-ns\t\t// 把ximenqing这个梯子(设备)塞到ximenqing家里(network namespace)\n\t$ ip netns exec panjinlian ip link\t//查看潘金莲家中的设备\n4. 设定好各自一半梯子连接时候把梯子都放到各自家里确定位置,连接时候才能定位到确切位置(设置好各自的IP地址)\n\n\n\t$ ip netns exec ximenqing-ns ip addr add dev ximenqing 192.168.188.96/24\t// 192网段采用24子网掩码\n\t$ ip netns exec ximenqing-ns ip addr\t\t//查看梯子是否已固定好(IP已设定)\n\t$ ip netns exec panjinlian-ns ip addr add dev panjinlian 192.168.188.69/24\n\t$ ip netns exec panjinlian-ns ip addr\n5. 梯子固定好位置后把ximenqing和panjinlian家的梯子都up起来, 只up一家没用.\n\n\n\t$ ip netns exec ximenqing-ns ip link set ximenqing up\n\t$ ip netns exec ximenqing-ns ip addr\t//可以查看到ximenqing这个网络设备状态是\"LOWERLAYERDOWN\"单方面起来了, 因为还需要把它相关联的panjinlian的那一半梯子也起来.  \n\t$ ip netns exec panjinlian-ns ip link set ximenqing up\n\t$ ip netns exec panjinlian-ns ip addr\n6. 西门庆通过梯子去找潘金莲, 或潘金莲通过梯子去找西门庆.\n\n\n\t$ ip netns exec ximenqing-ns ping -c 3 192.168.188.69\n\t$ ip netns exec panjinlian-ns ping -c 3 192.168.188.96\n\n\n### **实例: 王婆帮助西门庆和潘金莲私会**\n西门庆和潘金莲打造好的各自的一半梯子和王婆家里的桥对应的一半梯子相连, 西门庆和潘金莲通过王婆家相连进行私会.  \n![](link_2_1.PNG)\n![](link_2_2.PNG)\n1. 创建wangpo这家连接桥\n\n\n\t$ ip link add wangpo type bridge\t// 创建wangpo这个桥, 和panjinlian-ns和ximenqing-ns不一样\n\t$ ip netns ls\n2. 打造西门庆和王婆家的桥相关联的用来拼接的两半梯子:\n\n\n\t$ ip link add wp2xmq type veth peer name xmq2wp\n\t$ ip link\t\t// 查看创建好的西门庆到王婆家的一半梯子和王婆家到西门庆家的一半梯子\n\t$ ip link add pjl2wp type veth peer name wp2pjl\n\t$ ip link\n3. 将打造好的四半梯子，一个放到西门庆家，一个放到潘金莲家，两个连接到王婆家的桥上两端.\n\n\n\t$ ip link set xmq2wp netns ximenqing-ns\t\t// xiq2wp这半桥塞到西门庆家里\n\t$ ip netns exec ximenqing-ns ip link\t\t\t// 查看有没有塞进去\n\t$ ip link set pjl2wp netns panjinlian-ns\n\t$ ip netns exec panjinlian-ns ip link\n\t\n\t$ ip link set wp2xmq master wangpo\t\t\t// wp2xmq这半桥连接王婆这个桥, 还是可以在宿主机总network namespace看得到\n\t$ ip link set wp2pjl master wangpo\n\t$ ip link\n\t$ bridge link\t\t//查看王婆家刚添加的网络设备wp2xmq, wp2pjl\n4. 固定(设立ip address)西门庆家的梯子, 而不需要固定连接王婆家的那一半梯子\n\n\n\t$ ip netns exec ximenqing-ns ip addr add dev xmq2wp 192.168.188.96/24\n\t$ ip netns exec ximenqing-ns ip addr\n\t$ ip netns exec panjinlian-ns ip addr add dev pjl2wp 192.168.188.69/24\n\t$ ip netns exec panjinlian-ns ip addr\n5. 把西门庆, 潘金莲的梯子都up起来, 也要up王婆家的桥\n\n\n\t$ ip netns exec ximenqing-ns ip link set xmq2wp up\t//up 塞到西门庆家里的一半梯子\n\t$ ip netns exec ximenqing-ns ip link\n\t$ ip netns exec panjinlian-ns ip link set pjl2wp up\t//up 塞到潘金莲家里的一半梯子\n\t$ ip netns exec panjinlian-ns ip link\n\t\n\t$ ip link set wangpo up\t\t//up 王婆家的桥\n\t$ ip link set wp2xmq up \t//up王婆家的桥连接西门庆家的梯子\n\t$ ip link set wp2pjl up\t\t//up王婆家的桥连接潘金莲家的梯子\n\t$ bridge link\t\t\t\t//查看连接王婆家桥的梯子是否up\n6. 王婆起到桥接作用(虚拟网桥), 西门庆和潘金莲通过王婆家桥进行私会.\n\n\n\t$ ip netns ximenqing-ns ping -c 3 192.168.188.69\n\t$ ip netns panjinlian-ns ping -c 3 192.168.188.96\n\n\n\n\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio secure naming","url":"/2021/03/13/microService/istio/istio_secure_naming/","content":"\n## secure naming\nreference: https://istio.io/latest/docs/concepts/security/#secure-naming\n\n<!-- more -->\n\nSidecar and perimeter proxies work as [Policy Enforcement Points](https://www.jerichosystems.com/technology/glossaryterms/policy_enforcement_point.html) (PEPs) to secure communication between clients and servers.\n**`Sidecar`** 和 **`边缘代理`** 作为 Policy Enforcement Points(PEPs) 以保护客户端和服务器之间的通信安全.\n\n服务器身份（Server identities）被编码在证书里，但服务名称（service names）通过服务发现或 DNS 被检索。安全命名信息将服务器身份映射到服务名称。身份 A 到服务名称 B 的映射表示“授权 A 运行服务 B“。控制平面监视 apiserver，生成安全命名映射，并将其安全地分发到 PEPs。 以下示例说明了为什么安全命名对身份验证至关重要。\n\n假设运行服务 datastore 的合法服务器仅使用 infra-team 身份。恶意用户拥有 test-team 身份的证书和密钥。恶意用户打算模拟服务以检查从客户端发送的数据。恶意用户使用证书和 test-team 身份的密钥部署伪造服务器。假设恶意用户成功攻击了发现服务或 DNS，以将 datastore 服务名称映射到伪造服务器。\n\n当客户端调用 datastore 服务时，它从服务器的证书中提取 test-team 身份，并用安全命名信息检查 test-team 是否被允许运行 datastore。客户端检测到 test-team 不允许运行 datastore 服务，认证失败。\n\n安全命名能够防止 HTTPS 流量受到一般性网络劫持，除了 DNS 欺骗外，它还可以保护 TCP 流量免受一般网络劫持。如果攻击者劫持了 DNS 并修改了目的地的 IP 地址，它将无法用于 TCP 通信。这是因为 TCP 流量不包含主机名信息，我们只能依靠 IP 地址进行路由，而且甚至在客户端 Envoy 收到流量之前，也可能发生 DNS 劫持。\n\n## 双向 TLS 认证\nreference: https://istio.io/latest/zh/docs/concepts/security/#mutual-TLS-authentication\n\nIstio 通过客户端和服务器端 PEPs 建立服务到服务的通信通道，PEPs 被实现为Envoy 代理。当一个工作负载使用双向 TLS 认证向另一个工作负载发送请求时，该请求的处理方式如下：\n\n1. Istio 将出站流量从客户端重新路由到客户端的本地 sidecar Envoy。\n2. 客户端 Envoy 与服务器端 Envoy 开始双向 TLS 握手。在握手期间，客户端 Envoy 还做了安全命名检查，以验证服务器证书中显示的服务帐户是否被授权运行目标服务。\n3. 客户端 Envoy 和服务器端 Envoy 建立了一个双向的 TLS 连接，Istio 将流量从客户端 Envoy 转发到服务器端 Envoy。\n4. 授权后，服务器端 Envoy 通过本地 TCP 连接将流量转发到服务器服务。\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio authorization","url":"/2021/03/13/microService/istio/istio_authorization/","content":"\n\n## istio authorization\n\nReference\n概念: https://istio.io/latest/docs/concepts/security/#authorization\n测试用例: https://istio.io/latest/docs/tasks/security/authorization/authz-http/\n参数参考: https://istio.io/latest/docs/reference/config/security/authorization-policy/\n\n<!-- more -->\n\n## TCP authorization实例\n\n**`注意:`**\n - 测试authorization用例之前要先enable mtls\n - `k8s deployment` 资源要添加 `service account` 如: **`serviceAccountName: bookinfo-productpage`**\n - istio一些策略如： **`rules.to.operation.hosts/methods/paths`** 只能用在 HTTP authorization中\n\n### **deployment**\n\n```\napiVersion: v1\nkind: Service\nmetadata:\n  name: productpage\n  labels:\n    app: productpage\n    service: productpage\nspec:\n  ports:\n  - port: 9080\n    name: http\n  selector:\n    app: productpage\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: bookinfo-productpage\n  labels:\n    account: productpage\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: productpage-v1\n  labels:\n    app: productpage\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: productpage\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: productpage\n        version: v1\n    spec:\n      serviceAccountName: bookinfo-productpage\n      containers:\n      - name: productpage\n        image: docker.io/istio/examples-bookinfo-productpage-v1:1.16.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9080\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n```\n\n### **authentication**\n\nauthentication_namespace.yaml\n\n```\napiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"query-authentication-policy\"\n  namespace: \"query\"\nspec:\n  mtls:\n    mode: STRICT\n---\napiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"storage-rest-authentication-policy\"\n  namespace: \"storage-rest\"\nspec:\n  mtls:\n    mode: STRICT\n---\napiVersion: \"security.istio.io/v1beta1\"\nkind: \"PeerAuthentication\"\nmetadata:\n  name: \"fm-authentication-policy\"\n  namespace: \"fm\"\nspec:\n  mtls:\n    mode: STRICT\n```\n\n### **authorization deny**\n\nauthorization_deny_policy_namespace.yaml\n\n```\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: authorization-deny-all\n  namespace: storage-rest\nspec:\n  {}\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: authorization-deny-all\n  namespace: fm\nspec:\n  {}\n```\n\n### **authorization policy*\n\nauthorization_tcp_policy_namespace.yaml\n\n```\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: queryweb-to-storage-rest\n  namespace: storage-rest\nspec:\n  selector:\n    matchLabels:\n      app: storage-rest\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        #namespaces: [\"query\"]\n        principals: [\"cluster.local/ns/query/sa/query-web\"]  # service Account 路径\n    to:\n    - operation:\n        ports: [\"9900\"]\n        #paths: [\"/v1*\"]\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: queryrest-to-fm\n  namespace: fm\nspec:\n  selector:\n    matchLabels:\n      app: fm-master-fs\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        namespaces: [\"query\"]\n    to:\n    - operation:\n        ports: [\"8080\"]\n```\n\n## **HTTPauthorization实例**\n\n参考istio官网配置的bookinfo实例: https://istio.io/latest/docs/tasks/security/authorization/authz-http/\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"01 部署external etcd集群","url":"/2021/03/13/microService/kubernetes/01_etcd_high_availablity/","content":"\n## **部署etcd集群**\n\n| 组件 | 使用的证书 |\n| :------: | :------: |\n| etcd | ca.pem, server.pem, server-key.pem |\n| kube-apiserver | ca.pem, server.pem, server-key.pem |\n| kubelet | ca.pem, ca-key.pem |\n| kube-proxy | ca.pem, kube-proxy.pem, kube-proxy-key.pem |\n| kubectl | ca.pem, admin.pem, admin-key.pem |\n\n所有k8s证书,配置,安装包都放到/opt/kubernetes/目录下\n\n```shell\n\tmkdir -p /opt/kubernetes/{ssl,cfg,bin}\n\tls /opt/kubernetes/\n\tbin/  cfg/  ssl/\n```\n\n### 1. 生成ca-key.pem私匙和ca.pem证书\n\n```xml\n\tcat > ca-config.json << EOF\n\t{\n\t    \"signing\": {\n\t        \"default\": {\n\t            \"expiry\": \"87600h\"\n\t        },\n\t        \"profiles\": {\n\t            \"kubernetes\": {\n\t                \"expiry\": \"87600h\",\n\t                \"usages\": [\n\t                    \"signing\",\n\t                    \"key encipherment\",\n\t                    \"server auth\"\n\t                ]\n\t            }\n\t        }\n\t    }\n\t}\n\tEOF\n\n\tcat > ca-csr.json << EOF\n\t{\n\t    \"CN\": \"kubernetes\",\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\t\t\t// 注意是algo而不是also\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\t\t\t// 哪个国家的可以随便写\n\t            \"L\": \"Beijing\",\t\t// 地点随便写\n\t            \"ST\": \"Beijing\",\t// 地点随便写\n\t            \"O\": \"k8s\",\t\t\t// 用户组, 固定的， 不能随便写\n\t            \"OU\": \"System\"\t\t// 用户, 固定的，不能随便写\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n\n生成ca.pem和ca-key.pem\n\n```shell\n\t// -bare ca 表示生成以ca开头的证书和key\n\tcfssl gencert -initca ca-csr.json | cfssljson -bare ca -\n\tls ca*\n\tca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem\n```\n### 2. 生成server端证书和key, 用于k8s的etcd和kube-apiserver\n\n\n```xml\n\tcat > server-csr.json << EOF\n\t{\n\t    \"CN\": \"kubernetes\",\n\t    \"hosts\": [\t\t\t// 包含了哪些机器IP和域名需要此server端证书\n\t        \"127.0.0.1\",\n\t        \"10.239.140.133\",\t\t// 要使用改证书的etcd或其他服务部署所在节点IP地址或域名\n\t        \"10.239.131.206\",\n\t        \"10.239.141.145\",\n\t        \"10.239.141.194\",\n\t        \"kubernetes.default\",\n\t        \"kubernetes.default.svc\",\n\t        \"kubernetes.default.svc.cluster\",\n\t        \"kubernetes.default.svc.cluster.local\"\n\t    ],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"k8s\",\t\t\t//和下面一起代表了用户和组去请求集群\n\t            \"OU\": \"System\"\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n生成server端证书\n\n```shell\n\t// -bare server 表示生成以server开头的证书和key\n\tcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server\n\tls server*\n\tserver.csr  server-csr.json  server-key.pem  server.pem\n```\n\n### 3. 生成admin证书, 集群管理员通过证书访问集群\n\n\n```xml\n\tcat > admin-csr.json << EOF\n\t{\n\t    \"CN\": \"admin\",\n\t    \"hosts\": [],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"system:masters\",\t// 用户组, 不要改动, 否则会认证失败\n\t            \"OU\": \"System\"\t\t\t// 用户\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n\n生成管理员证书和key\n\n```shell\n\t// -bare admin 表示生成以admin开头的证书和key\n\tcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin\n\tls admin*\n\tadmin.csr  admin-csr.json  admin-key.pem  admin.pem\n```\n\n### 4. 生成kube-proxy证书\n工作节点通过kube-proxy组件访问api-server生成一些网络策略, 必须得有权限, 生成一个证书, 让kube-proxy携带证书去访问集群.\n\n```xml\n\tcat > kube-proxy-csr.json << EOF\n\t{\n\t    \"CN\": \"system:kube-proxy\",\t\t//固定,不能改变\n\t    \"hosts\": [],\n\t    \"key\": {\n\t        \"algo\": \"rsa\",\n\t        \"size\": 3072\n\t    },\n\t    \"names\": [\n\t        {\n\t            \"C\": \"CN\",\n\t            \"L\": \"Beijing\",\n\t            \"ST\": \"Beijing\",\n\t            \"O\": \"k8s\",\n\t            \"OU\": \"System\"\n\t        }\n\t    ]\n\t}\n\tEOF\n```\n生成kube-proxy证书和key\n\n```shell\n\tcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy\n\tls kube-proxy*\n\tkube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem\n```\n\n### 5. 只保留*.pem, 删除其它文件\n\n```shell\n\tls | grep -v pem | xargs -i rm {}\n\tls\n\tadmin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  server-key.pem  server.pem\n```\n\n## **关闭防火墙或开发端口**\n### 关闭防火墙\n\n```shell\n\tsetenforce 0\n\tsystemctl stop firewalld.service\n\tsysctl net.bridge.bridge-nf-call-iptables=1\n```\n\n### 如果使用firewalld作为防火墙，则需要开放端口\n\n```shell\n\tfirewall-cmd --zone=public --add-port=2379/tcp --permanent\n\tfirewall-cmd --zone=public --add-port=2380/tcp --permanent\n\tfirewall-cmd --reload\n\tfirewall-cmd --list-all\n```\n\n## **etcd安装**\netcd是由coreos公司开发在GitHub上开源的存储键值对的数据库\n\n### etcd 下载\n本次测试安装etcd的3.2.12版本\nhttps://github.com/etcd-io/etcd/releases/tag/v3.2.12\n下载解压并移动到指定目录\n\n```shell\n\twget -L https://github.com/etcd-io/etcd/releases/download/v3.2.12/etcd-v3.2.12-linux-amd64.tar.gz\n\ttar -zxvf etcd-v3.2.12-linux-amd64.tar.gz\n\tls etcd-v3.2.12-linux-amd64\n\tDocumentation/  etcd  etcdctl  README-etcdctl.md  README.md  READMEv2-etcdctl.md\n\t// 将解压后文件中的可执行文件etct和etcdctl移动到/opt/kubernetes/bin/目录下\n\tmv etcd-v3.2.12-linux-amd64/etcd /opt/kubernetes/bin/\n\tmv etcd-v3.2.12-linux-amd64/etcdctl /opt/kubernetes/bin/\n```\n\n### 创建ETCD的配置文件\n\n```\n\tcat > /opt/kubernetes/cfg/etcd << EOF\n\t#[Member]\n\tETCD_NAME=\"etcd01\"\n\tETCD_DATA_DIR=\"/var/lib/etcd/default.etcd\"\n\tETCD_LISTEN_PEER_URLS=\"https://10.239.140.133:2380\"\n\tETCD_LISTEN_CLIENT_URLS=\"https://10.239.133:2379\"\n\t\n\t#[Clustering]\n\tETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://10.239.140.133:2380\"\n\tETCD_ADVERTISE_CLIENT_URLS=\"https://10.239.140.133:2379\"\n\tETCD_INITIAL_CLUSTER=\"etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380\"\n\tETCD_INITIAL_CLUSTER_TOKEN=\"etcd-cluster\"\n\tETCD_INITIAL_CLUSTER_STATE=\"new\"\n\tEOF\n```\n### 使用systemd来管理etcd服务\n\n```xml\n\tcat > /usr/lib/systemd/system/etcd.service << EOF\n\t[Unit]\t\t\t\t\t//systemd依赖的一些服务, 网络服务启动之后再启动此服务\n\tDescription=Etcd Server\n\tAfter=network.target\n\tAfter=network-online.target\n\tWants=network-online.target\n\t\n\t[Service]\n\tType=notify\n\tWorkingDirectory=/var/lib/etcd/\t\t\t\t\t//看网上配置有这个参数, 自己配置过程中没有加入\n\tEnvironmentFile=-/opt/kubernetes/cfg/etcd\t\t//指定服务启动配置文件\n\tExecStart=/opt/kubernetes/bin/etcd \\\t\t\t//服务启动选项\n\t--name=${ETCD_NAME} \\\n\t--data-dir=${ETCD_DATA_DIR} \\\n\t--listen-peer-urls=${ETCD_LISTEN_PEER_URLS} \\\n\t--listen-client-urls=${ETCD_LISTEN_CLIENT_URLS},http://127.0.0.1:2379 \\\n\t--advertise-client-urls=${ETCD_ADVERTISE_CLIENT_URLS} \\\n\t--initial-advertise-peer-urls=${ETCD_INITIAL_ADVERTISE_PEER_URLS} \\\n\t--initial-cluster=${ETCD_INITIAL_CLUSTER} \\\n\t--initial-cluster-token=${ETCD_INITIAL_CLUSTER_TOKEN} \\\n\t--initial-cluster-state=new \\\n\t--cert-file=/opt/kubernetes/ssl/server.pem \\\t\t// 指定数字证书\n\t--key-file=/opt/kubernetes/ssl/server-key.pem \\\n\t--peer-cert-file=/opt/kubernetes/ssl/server.pem \\\n\t--peer-key-file=/opt/kubernetes/ssl/server-key.pem \\\n\t--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \\\n\t--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n\tRestart=on-failure\n\tLimitNOFILE=65536\n\t\n\t[Install]\n\tWantedBy=multi-user.target\n\tEOF\n```\n\n启动etcd服务\n\n```shell\n\t// 启动etcd服务发现卡住, 且过一会会报个错误, 原因是另外request另外两台etcd服务2380得不到响应\n\t// 待另外至少一台etcd服务启动后再回来查看etcd服务状态就正常了\n\tsystemctl daemon-reload\n\tsystemctl start etcd\n\tsystemctl enable etcd\n\t// 完整的启动etcd服务命令其实就是运行以下命令:\n\t/opt/kubernetes/bin/etcd --name=\"etcd01\" --data-dir=\"/var/lib/etcd/default.etcd\" --listen-peer-urls=\"https://10.239.140.133:2380\" --listen-client-urls=\"https://10.239.140.133:2379,http://127.0.0.1:2379\" --advertise-client-urls=\"https://10.239.140.133:2379\" --initial-advertise-peer-urls=\"https://10.239.140.133:2380\" --initial-cluster=\"etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380\" --initial-cluster-token=\"etcd-cluster\" --initial-cluster-state=\"new\" --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n```\n\n查看服务\n\n```shell\n\tps -ef | grep etcd \n\troot     23726     1  5 21:41 ?        00:00:00 /opt/kubernetes/bin/etcd --name=etcd01 --data-dir=/var/lib/etcd/default.etcd --listen-peer-urls=https://10.239.140.133:2380 --listen-client-urls=https://10.239.140.133:2379,http://127.0.0.1:2379 --advertise-client-urls=https://10.239.140.133:2379 --initial-advertise-peer-urls=https://10.239.140.133:2380 --initial-cluster=etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380 --initial-cluster-token=etcd-cluster --initial-cluster-state=new --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem\n\troot     23744 14957  0 21:41 pts/1    00:00:00 grep --color=auto etcd\n\t// 查看etcd服务状态和日志发现etcd一直尝试request其它两台机器, 状态不正常, 在其它两台也部署etcd后就可以了\n\tsystemctl status etcd\n\ttail /var/log/messages\n```\n\n### etcd拷贝到其它机器\n\n```shell\n\tscp -r /opt/kubernetes root@10.239.131.206:/opt\n\tscp /usr/lib/systemd/system/etcd.service  root@10.239.131.206:/usr/lib/systemd/system/etcd.service\n\t// 只需要修改其它机器上/opt/kubernetes/cfg/etcd文件里的ETCD_NAME和其它参数IP地址然后就可以直接运行命令启动etcd服务.\n\t// 并且启动后不会卡住\n\tsystemctl start etcd\n\tsystemctl enable etcd\n```\n\n### 测试etcd集群状态是否正常\n\n```shell\n\t/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --endpoints=\"https://10.239.140.133:2379,https://10.239.131.206:2379,https://10.239.141.145:2379\" cluster-health\n\tmember 723f8ab932b4c3f6 is healthy: got healthy result from https://10.239.141.145:2379\n\tmember 8af8f5fa5f0b0b39 is healthy: got healthy result from https://10.239.131.206:2379\n\tmember 91dd39fb14e3de97 is healthy: got healthy result from https://10.239.140.133:2379\n\tcluster is healthy\n```\n\n## 移除etcd集群\n分别登陆各台部署etcd的机器上执行如下命令:\n\n```shell\n\tsystemctl stop etcd\n\tsystemctl disable etcd\n\trm -rf /usr/lib/systemd/system/etcd.service\n```\n\n## **遇到的问题**\n查看Centos7系统日子\n\n```shell\n\tcat /var/log/messages\n\tsystemctl status etcd.service\n```\n\n出现以下错误:\n\netcd.service: main process exited, code=exited, status=2/INVALIDARGUMENT\n\n很明显是运行etcd命令时候的参数错误, 改对参数就可以了.\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"01 Kubernetes安装方式和配置条件","url":"/2021/03/13/microService/kubernetes/01_kubernetes安装方式和配置条件/","content":"\n## 安装方法\nreference: https://kubernetes.io/docs/setup/production-environment/tools/\n\n<!-- more -->\n\n * [Bootstrapping clusters with kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/)\n * [Installing Kubernetes with kops](https://kubernetes.io/docs/setup/production-environment/tools/kops/)\n * [Installing Kubernetes with Kubespray](https://kubernetes.io/docs/setup/production-environment/tools/kubespray/)\n\n## Before you begin\n * One or more machines running one of:\n  * Ubuntu 16.04+\n  * Debian 9+\n  * CentOS 7\n  * Red Hat Enterprise Linux (RHEL) 7\n  * Fedora 25+\n  * HypriotOS v1.0.1+\n  * Flatcar Container Linux (tested with 2512.3.0)\n * 2 GB or more of RAM per machine (any less will leave little room for your apps).\n * 2 CPUs or more.\n * Full network connectivity between all machines in the cluster (public or private network is fine).\n * Unique hostname, MAC address, and product_uuid for every node. See here for more details.\n * Certain ports are open on your machines. See here for more details.\n * Swap disabled. You MUST disable swap in order for the kubelet to work properly.\n\n## Verify the MAC address and product_uuid are unique for every node \n\n * 您可以使用命令 **`ip link`** 或 **`ifconfig -a`** 来获取网络接口的 MAC 地址\n * 可以使用 **`sudo cat /sys/class/dmi/id/product_uuid`** 命令对 product_uuid 校验\n一般来讲，硬件设备会拥有唯一的地址，但是有些虚拟机的地址可能会重复。Kubernetes 使用这些值来唯一确定集群中的节点。 如果这些值在每个节点上不唯一，可能会导致安装[失败](https://github.com/kubernetes/kubeadm/issues/31).\n\n## 检查网络适配器\n\n如果您有一个以上的网络适配器，同时您的 Kubernetes 组件通过默认路由不可达，我们建议您预先添加 IP 路由规则，这样 Kubernetes 集群就可以通过对应的适配器完成连接。\n\n## 确保 iptables 工具不使用 nftables 后端\n\n在 Linux 中，nftables 当前可以作为内核 iptables 子系统的替代品。 **iptables** 工具可以充当兼容性层，其行为类似于 iptables 但实际上是在配置 nftables。 nftables 后端与当前的 kubeadm 软件包不兼容：它会导致重复防火墙规则并破坏 **kube-proxy**。\n\n如果您系统的 **iptables** 工具使用 nftables 后端，则需要把 **iptables** 工具切换到“旧版”模式来避免这些问题。 默认情况下，至少在 Debian 10 (Buster)、Ubuntu 19.04、Fedora 29 和较新的发行版本中会出现这种问题。RHEL 8 不支持切换到旧版本模式，因此与当前的 kubeadm 软件包不兼容\n\n\n### iptables_legacy\n\n#### \"Debian 或 Ubuntu\"\n```bash\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy\nupdate-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\nupdate-alternatives --set arptables /usr/sbin/arptables-legacy\nupdate-alternatives --set ebtables /usr/sbin/ebtables-legacy\n```\n\n#### \"Fedora\"\n\n```bash\nupdate-alternatives --set iptables /usr/sbin/iptables-legacy\n```\n\nMake sure that the **br_netfilter** module is loaded. This can be done by running **`lsmod | grep br_netfilter`**. To load it explicitly call **`sudo modprobe br_netfilter`**.\n\nAs a requirement for your Linux Node's iptables to correctly see bridged traffic, you should ensure **`net.bridge.bridge-nf-call-iptables`** is set to 1 in your **`sysctl`** config, e.g.\n\n```\ncat <<EOF | sudo tee /etc/modules-load.d/k8s.conf\nbr_netfilter\nEOF\n\ncat <<EOF | sudo tee /etc/sysctl.d/k8s.conf\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nEOF\nsudo sysctl --system\n```\nFor more details please see the [Network Plugin Requirements page](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements).\n\n## 检查所需端口\n### 控制平面节点(s)\n| Protocol | Direction | Port Range | Purpose | Used By |\n| :------ | :----- | :------ | :------ | :------ |\n| TCP | Inbound | 6443* | Kubernetes API server | All |\n| TCP | Inbound | 2379-2380 | etcd server client API | kube-apiserver, etcd |\n| TCP | Inbound | 10250 | kubelet API | Self, Control plane |\n| TCP | Inbound | 10251 | kube-scheduler | Self |\n| TCP | Inbound | 10252 | kube-controller-manager | Self |\n\n### 工作节点(s)\n\n| Protocol | Direction | Port Range | Purpose | Used By |\n| :------ | :----- | :------ | :------ | :------ |\n| TCP | Inbound | 10250 | kubelet API | Self, Control plane |\n| TCP | Inbound | 30000-32767 | NodePort Services† | All |\n\n† D Default port range for [NodePort Services](https://kubernetes.io/docs/concepts/services-networking/service/).\n\nAny port numbers marked with * are overridable, so you will need to ensure any custom ports you provide are also open\n\nAlthough etcd ports are included in control-plane nodes, you can also host your own etcd cluster externally or on custom ports.\n\nThe pod network plugin you use (see below) may also require certain ports to be open. Since this differs with each pod network plugin, please see the documentation for the plugins about what port(s) those need.\n\n## 安装 runtime\n\nTo run containers in Pods, Kubernetes uses a [container runtime](https://kubernetes.io/docs/setup/production-environment/container-runtimes).\n从 v1.6.0 版本起，Kubernetes 开始默认允许使用 CRI（容器运行时接口）。\n\n从 v1.14.0 版本起，kubeadm 将通过观察已知的 UNIX 域套接字来自动检测 Linux 节点上的容器运行时。 下表中是可检测到的正在运行的 runtime 和 socket 路径。\n\n| 运行时 | 域套接字 |\n| :------ | :----- |\n| Docker | <div style=\"width: 200pt\">/var/run/docker.sock</div> |\n| containerd | /run/containerd/containerd.sock |\n| CRI-O | /var/run/crio/crio.sock |\n\n如果同时检测到 docker 和 containerd，则优先选择 docker。 这是必然的，因为 docker 18.09 附带了 containerd 并且两者都是可以检测到的。 如果检测到其他两个或多个运行时，kubeadm 将以一个合理的错误信息退出。\n\n在非 Linux 节点上，默认使用 docker 作为容器 runtime。\n\n如果选择的容器 runtime 是 docker，则通过内置 dockershim CRI 在 kubelet 的内部实现其的应用。\n\n基于 CRI 的其他 runtimes 有：\n\n- [containerd](https://github.com/containerd/cri) （containerd 的内置 CRI 插件）\n- [cri-o](https://cri-o.io/)\n- [frakti](https://github.com/kubernetes/frakti)\n\n请参考 [CRI 安装指南](/zh/docs/setup/production-environment/container-runtimes/)获取更多信息。\n\n## 安装 kubeadm、kubelet 和 kubectl\n\n您需要在每台机器上安装以下的软件包：\n\n* `kubeadm`：用来初始化集群的指令。\n\n* `kubelet`：在集群中的每个节点上用来启动 pod 和容器等。\n\n* `kubectl`：用来与集群通信的命令行工具。\n\nkubeadm **不能** 帮您安装或者管理 `kubelet` 或 `kubectl`，所以您需要确保它们与通过 kubeadm 安装的控制平面的版本相匹配。\n如果不这样做，则存在发生版本偏差的风险，可能会导致一些预料之外的错误和问题。\n然而，控制平面与 kubelet 间的相差一个次要版本不一致是支持的，但 kubelet 的版本不可以超过 API 服务器的版本。\n例如，1.7.0 版本的 kubelet 可以完全兼容 1.8.0 版本的 API 服务器，反之则不可以。\n\n有关安装 `kubectl` 的信息，请参阅[安装和设置 kubectl](/zh/docs/tasks/tools/install-kubectl/)文档。\n\n## **Warning**\n> 这些指南不包括系统升级时使用的所有 Kubernetes 程序包。这是因为 kubeadm 和 Kubernetes\n> 有[特殊的升级注意事项](/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/)。\n\n\n关于版本偏差的更多信息，请参阅以下文档：\n\n* Kubernetes [版本与版本间的偏差策略](/zh/docs/setup/release/version-skew-policy/)\n* Kubeadm-specific [版本偏差策略](/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#version-skew-policy)\n\n## k8s_install\n### \"Ubuntu、Debian 或 HypriotOS\"\n\n```bash\nsudo apt-get update && sudo apt-get install -y apt-transport-https curl\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\ncat <<EOF | sudo tee /etc/apt/sources.list.d/kubernetes.list\ndeb https://apt.kubernetes.io/ kubernetes-xenial main\nEOF\nsudo apt-get update\nsudo apt-get install -y kubelet kubeadm kubectl\nsudo apt-mark hold kubelet kubeadm kubectl\n```\n\n### \"CentOS、RHEL 或 Fedora\"\n\n```bash\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\nEOF\n\n# 将 SELinux 设置为 permissive 模式（相当于将其禁用）\nsetenforce 0\nsed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config\n\nyum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes\n\nsystemctl enable --now kubelet\n```\n\n  **请注意：**\n\n  - 通过运行命令 `setenforce 0` 和 `sed ...` 将 SELinux 设置为 permissive 模式可以有效的将其禁用。\n    这是允许容器访问主机文件系统所必须的，例如正常使用 pod 网络。\n    您必须这么做，直到 kubelet 做出升级支持 SELinux 为止。\n  - 一些 RHEL/CentOS 7 的用户曾经遇到过问题：由于 iptables 被绕过而导致流量无法正确路由的问题。您应该确保\n    在 `sysctl` 配置中的 `net.bridge.bridge-nf-call-iptables` 被设置为 1。\n\n    ```bash\n    cat <<EOF >  /etc/sysctl.d/k8s.conf\n    net.bridge.bridge-nf-call-ip6tables = 1\n    net.bridge.bridge-nf-call-iptables = 1\n    EOF\n    sysctl --system\n    ```\n  - 确保在此步骤之前已加载了 `br_netfilter` 模块。这可以通过运行 `lsmod | grep br_netfilter` 来完成。要显示加载它，请调用 `modprobe br_netfilter`。\n\nkubelet 现在每隔几秒就会重启，因为它陷入了一个等待 kubeadm 指令的死循环。\n\n\n## 在控制平面节点上配置 kubelet 使用的 cgroup 驱动程序\n\n使用 docker 时，kubeadm 会自动为其检测 cgroup 驱动并在运行时对 `/var/lib/kubelet/kubeadm-flags.env` 文件进行配置。\n\n如果您使用不同的 CRI，您需要使用 `cgroup-driver` 值修改 `/etc/default/kubelet` 文件（对于 CentOS、RHEL、Fedora，修改 `/etc/sysconfig/kubelet` 文件），像这样：\n\n```bash\nKUBELET_EXTRA_ARGS=--cgroup-driver=<value>\n```\n\n这个文件将由 `kubeadm init` 和 `kubeadm join` 使用以获取额外的用户自定义的 kubelet 参数。\n\n请注意，您 **只** 需要在您的 cgroup 驱动程序不是 `cgroupfs` 时这么做，因为它已经是 kubelet 中的默认值。\n\n需要重新启动 kubelet：\n\n```bash\nsystemctl daemon-reload\nsystemctl restart kubelet\n```\n\n自动检测其他容器运行时的 cgroup 驱动，例如在进程中工作的 CRI-O 和 containerd。\n\n## 故障排查\n\n如果您在使用 kubeadm 时遇到困难，请参阅我们的[故障排查文档](/zh/docs/setup/production-environment/tools/kubeadm/troubleshooting-kubeadm/)。\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"Istio 设置请求超时","url":"/2021/03/13/microService/istio/istio_设置请求超时/","content":"\nOfficial website: \nhttps://istio.io/latest/zh/docs/tasks/traffic-management/fault-injection/\nhttps://istio.io/latest/zh/docs/tasks/traffic-management/request-timeouts/\n\n## 设置请求超时实例\n### 应用默认目标规则\n\n```shell\n\tkubectl apply -f samples/bookinfo/networking/destination-rule-all.yaml\n```\n如果启用了双向 TLS，请执行以下命令：\n\n```shell\n\tkubectl apply -f samples/bookinfo/networking/destination-rule-all-mtls.yaml\n```\n等待几秒钟，以使目标规则生效。  \n您可以使用以下命令查看目标规则：\n\n```shell\n\tkubectl get destinationrules -o yaml\n```\n### 初始化应用的版本路由\n\n```shell\n\tkubectl apply -f samples/bookinfo/networking/virtual-service-all-v1.yaml\n```\n### 设置请求超时\n将请求路由到 reviews 服务的 v2 版本，它会发起对 ratings 服务的调用：\n\n```shell\n\tkubectl apply -f - <<EOF\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: reviews\n\tspec:\n\t  hosts:\n\t    - reviews\n\t  http:\n\t  - route:\n\t    - destination:\n\t        host: reviews\n\t        subset: v2\n\tEOF\n```\n\n给对 ratings 服务的调用添加 2 秒的延迟：\n\n```shell\n\tkubectl apply -f - <<EOF\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: ratings\n\tspec:\n\t  hosts:\n\t  - ratings\n\t  http:\n\t  - fault:\n\t      delay:\n\t        percent: 100\n\t        fixedDelay: 2s\n\t    route:\n\t    - destination:\n\t        host: ratings\n\t        subset: v1\n\tEOF\n```\n在浏览器中打开 Bookinfo 的网址 http://$GATEWAY_URL/productpage。\n\n这时可以看到 Bookinfo 应用运行正常（显示了评级的星型符号），但是每次刷新页面，都会有 2 秒的延迟。\n\n现在给对 reviews 服务的调用增加一个半秒的请求超时：\n\n```shell\n\tkubectl apply -f - <<EOF\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: reviews\n\tspec:\n\t  hosts:\n\t  - reviews\n\t  http:\n\t  - route:\n\t    - destination:\n\t        host: reviews\n\t        subset: v2\n\t    timeout: 0.5s\n\tEOF\n```\n刷新 Bookinfo 页面。\n\n这时候应该看到 1 秒钟就会返回，而不是之前的 2 秒钟，但 reviews 是不可用的。\n\n即使超时配置为半秒，响应仍需要 1 秒，是因为 productpage 服务中存在硬编码重试，因此它在返回之前调用 reviews 服务超时两次。\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"05 Kubernetes volumes","url":"/2021/03/13/microService/kubernetes/05_kubernetes_volume/","content":"\n## volumes\n> 我们可能希望新的容器可以在之前容器结束的位置继续运行，比如在物理机上重启进程。 可能不需要（或者不想要）整个文件系统被持久化， 但又希望能保存实际数据的目录\n> 在 pod 启动时创建卷， 并在删除 pod 时销毁卷. 在容器重新启动期间， 卷的内容将保持不变， 在重新启动容器之后， 新容器可以识别前一个容器写入卷的所有文件。 另外，如果一个 pod 包含多个容器， 那这个卷可以同时被所有的容器使用\n\nKubernetes 的卷是 pod 的一个组成部分， 因此像容器一样在 pod 的规范中就定义了。 \n它们不是独立的 Kubernetes 对象， 也不能单独创建或删除。 \npod 中的所有容器都可以使用卷， 但必须先将它挂载在每个需要访问它的容器中。 在每个容器中， 都可以在其文件系统的任意位置挂载卷\n卷类型\n * emptyDir 用于存储临时数据的简单空目录\n * gitRepo 通过检出Git仓库的内容来初始化的卷\n * hostPath 用于将目录从工作节点的文件系统挂载到pod中\n * configMap、secret、downwardAPI 用于将 Kubemetes 部分资源和集群信息公开给 pod 的特殊类型的卷 \n ......\n\nfortune-pod.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: fortune\n\tspec:\n\t  containers:\n\t  - image: luksa/fortune\n\t    name: html-generator\n\t    volumeMounts:\n\t    - name: html\n\t      mountPath: /var/htdocs\n\t  - image: nginx:alpine\n\t    name: web-server\n\t    volumeMounts:\n\t    - name: html\n\t      mountPath: /usr/share/nginx/html\n\t      readOnly: true\n\t    ports:\n\t    - containerPort: 80\n\t      protocol: TCP\n\t  volumes:\n\t  - name: html\n\t    emptyDir: {}\n```\n\n作为卷来使用的 emptyDir 是在承载 pod 的工作节点的实际磁盘上创建的，因此其性能取决于节点的磁盘类型。\n也可以通知 Kubemetes 在 tmfs 文件系统(存在内存而非硬盘)上创建 emptyDir. 因此，将 emptyDir 的 medium 设置为Memory\n\n```xml\n\t  volumes:\n\t  - name: html\n\t    emptyDir\n\t      medium: Memory\n```\n创建pod\n\n```shell\n\t$ kubectl create -f fortune-pod.yaml\n\n\t$ kuberctl port-forward fortune 8080:80\n\tForwarding from 127.0.0.1:8080 -> 80\n\tForwarding from [::1]:8080 -> 80\n\tHandling connection for 8080\n\t......\n```\n\n查看 pod 描述\n\n```shell\n\t$ kubectl describe po/fortune\n\t......\n\tContainers:\n\thtml-generator:\n\t......\n\t  Mounts:\n\t    /var/htdocs from html (rw)\n\t    /var/run/secrets/kubernetes.io/serviceaccount from default-token-b79jt (ro)\n\t......\n\tVolumes:\n\t  html:\n\t    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\n\t    Medium:     Memory\n\t    SizeLimit:  <unset>\n\t  default-token-b79jt:\n\t    Type:        Secret (a volume populated by a Secret)\n\t    SecretName:  default-token-b79jt\n\t    Optional:    false\n\t......\n```\n测试访问两种方式:\n\n * 第一种浏览器访问:\n浏览器输入 [http://127.0.0.1:8080](http://127.0.0.1:8080) 或 [http://localhost:8080](http://localhost:8080)\n即可看到每过10s按F5刷新一次HTML页面内容都不一样\n\n * 第二种 curl 或 wget 访问\n不去掉公司的proxy访问命令里加上 noproxy\n\n\n```shell\n\t$ curl http://127.0.0.1:8080 --noproxy \"*\"\t// \"*\" 对所有路径的访问都不经过配置的proxy\n```\n也可以去掉公司的proxy, 需要先执行 $ export http_proxy= 把公司的proxy去掉\n再执行如下命令即可访问\n\n\n```shell\n\t$ curl -s http://127.0.0.1:8080\n\t$ curl -s http://localhost:8080\n\t$ wget http://127.0.0.1:8080\n```\n\n开启两个终端进入这两个container，每过10s分别运行如下各自container里的cat html命令，发现HTML内容不断变化但两个容器的html内容一样\n如果打开浏览器，发现浏览器输出跟这两个container里相应路径下html的内容都是一样同步变化的\n\n```shell\n\t$ kubectl exec po/fortune -c web-server -it -- sh\n\t$ cat /usr/share/nginx/html/index.html\n\n\t$ kubectl exec po/fortune -c html-generator -it -- sh\n\t$ cat /var/htdocs/index.html\n```\n\n## gitRepo 卷\n> gitRepo 容器就像 emptyDir 卷一样， 基本上是一个专用目录， 专门用于包含卷的容器并单独使用。 当 pod 被删除时， 卷及其内容被删除。 然而， 其他类型的卷并不创建新目录， 而是将现有的外部目录挂载到 pod 的容器文件系统中.\n> 使用对应私有 Git repo 的 gitRepo 卷， 其实不可行。 Kubemetes 开发入员的共识是保待 gitRepo 卷的简单性， 而不添加任何通过 SSH 协议克隆私有存储库的支待， 因为这需要向 gitRepo 卷添加额外的配置选项。如果想要将私有的 Git repo 克隆到容器中， 则应该使用 gitsync sidecar 或类似的方法， 而不是使用 gitRepo 卷.\n\n## hostPath 卷\n> hostPath 卷指向节点文件系统上的特定文件或目录,在同一个节点上运行并在其 hostPath 卷中使用相同路径的 pod 可以看到相同的文件.\n> 持久性存储, 因为gitRepo 和 emptyDir 卷的内容都会在 pod 被删除时被删除， 而 hostPath 卷的内容则 不会被删除.\n> 如果删除了一个pod, 并且下一个 pod 使用了指向主机上相同路径的hostPath 卷， 则新 pod 将会发现上一个 pod 留下的数据， 但前提是必须将其调度到与第一个 pod 相同的节点上.\n> 不应该使用hostPath 卷作为存储数据库数据的目录. 因为卷的内容存储在特定节点的文件系统中, 当数据库 pod 被重新安排在另一个节点时， 会找不到数据, 这会使 pod 对预定规划的节点很敏感\n> 请记住仅当需要在节点上读取或写入系统文件时才使用 hos七Path, 切勿使用它们来持久化跨 pod的数据.\n\n```shell\n\t$ kubectl get pods -n kube-system\n\tNAME                                  READY   STATUS    RESTARTS   AGE\n\t......\n\tkube-apiserver-master-node            1/1     Running   5          2d20h\n\t......\n\n\t$ kubectl describe po/kube-apiserver-master-node -n kube-system\n\t......\n\tContainers:\n\t  kube-apiserver:\n\t  ......\n\t  Mounts:\n\t    /etc/kubernetes/pki from k8s-certs (ro)\n\t    /etc/pki from etc-pki (ro)\n\t    /etc/ssl/certs from ca-certs (ro)\n\t......\n\tVolumes:\n\t  ca-certs:\n\t    Type:          HostPath (bare host directory volume)\n\t    Path:          /etc/ssl/certs\n\t    HostPathType:  DirectoryOrCreate\n\t  etc-pki:\n\t    Type:          HostPath (bare host directory volume)\n\t    Path:          /etc/pki\n\t    HostPathType:  DirectoryOrCreate\n\t  k8s-certs:\n\t    Type:          HostPath (bare host directory volume)\n\t    Path:          /etc/kubernetes/pki\n\t    HostPathType:  DirectoryOrCreate\n```\nPod使用三个HostPath卷来访问宿主主机的/etc/ssl/certs, /etc/pki, /etc/kubernetes/pki三个目录.\n\n\n## PV & PVC & storageclass(sc)\n> 在 Kubemetes 集群中为了使应用能够正常请求存储资源， 同时避免处理基础设施细节， 引入了两个新的资源， 分别是待久卷和持久卷声明. 这名字可能有点误导，因为正如在前面看到的， 甚至常规的 Kubemetes 卷 也可以用来存储持久性数据\n> 在 pod 中使用 PersistentVolume (持久卷， 简称 PV) 要比使用常规的 pod 卷复杂一些\n> 研发人员无须向他们的 pod 中添加特定技术的卷， 而是由集群管理员设置底层存储， 然后通过 Kubernetes API 服务器创建持久卷并注册。 在创建持久卷时， 管理员可以指定其大小和所支持的访问模式.\n> 当集群用户需要在其 pod 中使用持久化存储时， 他们首先创建持久卷声明(PersistentVolumeClaim, 简称 PVC) 清单， 指定所需要的最低容量要求和访问模式，然后用户将待久卷声明清单提交给 Kubernetes API 服务器， Kubernetes 将找到可匹配的待久卷并将其绑定到持久卷声明\n> 持久卷声明可以当作 pod 中的一个卷来使用， 其他用户不能使用相同的持久卷，除非先通过删除持久卷声明绑定来释放.\n![](PV_PVC.PNG)\n\n```shell\n\t$ kubectl get pv\n\n\t$ kubectl get pvc -n default\n\tNAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE\n\tmysql-pv-claim   Bound    pvc-522c119b-a968-4a86-ae6c-521b64b775ee   20Gi       RWO            rook-ceph-block   5d2h\n\twp-pv-claim      Bound    pvc-4b83da7a-4552-4806-9f68-af96d4a56d96   20Gi       RWO            rook-ceph-block   5d2h\n\n\t$ kubectl get sc\t// storageclass的简写(sc)\n```\n\n\n## Additional:\n## CephFS example\n\n创建CephFS\n\n```xml\n\tapiVersion: ceph.rook.io/v1\n\tkind: CephFilesystem\n\tmetadata:\n\t  name: myfs\n\t  namespace: rook-ceph\n\tspec:\n\t  metadataPool:\n\t    replicated:\n\t      size: 3\n\t  dataPools:\n\t    - replicated:\n\t        size: 3\n\t  preservePoolsOnDelete: true\n\t  metadataServer:\n\t    activeCount: 1\n\t    activeStandby: true\n```\n\n创建sc(StorageClass), sc是不需要提前创建好PV, 而是根据PVC需求动态创建PV.\n\n```xml\n\tapiVersion: storage.k8s.io/v1\n\tkind: StorageClass\n\tmetadata:\n\t  name: rook-cephfs\n\t# Change \"rook-ceph\" provisioner prefix to match the operator namespace if needed\n\tprovisioner: rook-ceph.cephfs.csi.ceph.com\n\t  parameters:\n\t  # clusterID is the namespace where operator is deployed.\n\t  clusterID: rook-ceph\n\t  \n\t  # CephFS filesystem name into which the volume shall be created\n\t  fsName: myfs\n\t  \n\t  # Ceph pool into which the volume shall be created\n\t  # Required for provisionVolume: \"true\"\n\t  pool: myfs-data0\n\t  \n\t  # Root path of an existing CephFS volume\n\t  # Required for provisionVolume: \"false\"\n\t  # rootPath: /absolute/path\n\t  \n\t  # The secrets contain Ceph admin credentials. These are generated automatically by the operator\n\t  # in the same namespace as the cluster.\n\t  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner\n\t  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph\n\t  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node\n\t  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph\n\t  \n\treclaimPolicy: Delete\n```\n\n创建PVC和deployment\n> PV 是K8S全局资源\n> PVC 是指定在某个Namespace下的K8S资源, 如果PVC访问属性为ReadWriteMany， 多个不同Pod挂载此相同的PVC到容器指定目录, 该目录将共享文件\n> 多个不同Pod挂载不同的PVC到容器指定目录，则文件不能共享\n\n```xml\n\tapiVersion: v1\n\tkind: PersistentVolumeClaim\n\tmetadata:\n\t  name: my-pvc\n\t  namespace: kube-system\n\tspec:\n\t  accessModes:\t\t\t\t// PV 具有的访问部署属性， PVC绑定后也应具有的访问部署属性(可读,可写,多机部署)\n\t  - ReadWriteMany\n\t  resources:\n\t    requests:\n\t      storage: 1Gi\n\t  storageClassName: rook-cephfs\n\t---\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: my-test\n\t  namespace: kube-system\n\t  labels:\n\t    app: my-test\n\t    kubernetes.io/cluster-service: \"true\"\n\tspec:\n\t  replicas: 2\n\t  selector:\n\t    matchLabels:\n\t      app: my-test\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: my-test\n\t        kubernetes.io/cluster-service: \"true\"\n\t    spec:\n\t      containers:\n\t      - name: my-test\n\t        image: registry:2\n\t        imagePullPolicy: IfNotPresent\n\t        resources:\n\t          limits:\n\t            cpu: 100m\n\t            memory: 100Mi\n\t        volumeMounts:\n\t        - name: my-volume\n\t          mountPath: /var/lib/myVolume\n\t      volumes:\n\t      - name: my-volume\n\t         persistentVolumeClaim:\n\t           claimName: my-pvc\t\t\t// 绑定上面的PVC\n\t           readOnly: false\n\t       affinity:\n\t         podAffinity:\n\t           requiredDuringSchedulingIgnoredDuringExecution:\n\t               - labelSelector:\n\t                   matchExpressions:\n\t                   - key: app\n\t                     operator: In\n\t                     values:\n\t                     - helm\n\t                 topologyKey: kubernetes.io/hostname\n```\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"07 Kubernetes Etcd","url":"/2021/03/13/microService/kubernetes/07_kubernetes_Etcd/","content":"\n## Etcd\nhttps://github.com/coreos/etcd\n> Etcd 是 CoreOS 团队（同时发起了 CoreoS 、 Rocket 等热门项目）发起的一个开源分布式键值仓库项目，可以用于分布式系统中的配置信息管理和服务发现（ service discovery），目前已经被广泛应用到大量开源项目中，包括 Kubernetes 、 CloudFoundry 、 CoreOS Fleet 和Salesforce 等\n> Etcd 是 CoreOS 团队于 2013 年 6 月发起的开源项目，它的目标是构建一个高可用的分布式键值（ key-value ）仓库，遵循 Apache v2许可，基于 Go 语言实现\n> 分布式系统中最基本的问题之一就是实现信息的共识，在此基础上才能实现对服务配置信息的管理、服务的发现、更新、同步,Etcd 专门为集群环境设计，采用了更为简洁的 Raft 共识算法＠，同样可以实现数据强一.致性，并支持集群节点状态管理和服务自动发现等.\n * 简单：支持 RESTfulAPI 和 gRPCAPI;\n * 安全： 基于 TLS 方式实现安全连接访问 ；\n * 快速： 支持每秒一万次的并发写操作，超时控制在毫秒量级 ；\n * 可靠： 支持分布式结构 ， 基于 Raft 算法实现一致性 \n![](etcd_3.PNG)\n默认etcd在k8s集群中的 3 or 5 or 7 台node上部署.\n\n## 下载安装\n\n###  二进制文件方式下载\n\n```shell\n\t$ curl -L https://github.com/coreos/etcd/releases/download/v3.3.1/etcd-v3.3.1-linux-amd64.tar.gz\n\t$ tar xzvf etcd-v3.3.llinux-amd64.tar.gz\n\t$ cd etcd-v3.3.llinux-amd64.tar.gz | ls\n\t其中 etcd 是服务主文件， etcdctl 是提供给用户的命令客户端, 其他都是文档文件.\n\t$ sudo cp etcd* /usr/local/bin/\n```\nEtcd 安装到此完成\n\n```shell\n\t$ etcd --version\n```\n直接执行 Etcd 命令，将启动一个服务节点，监昕在本地的 2379 （客户端请求端口）和 2380 （其他节点连接端口 ） \n\n```shell\n\t$ etcd\n```\n可以通过 REST API 直接查看集群健康状态：\n\n```shell\n\t$ curl -L http://127.0.0.1:2379/health\n\t{”health”: ”true”}\n```\n也可以使用自带的 etcdctl 命令进行查看（实际上是封装了阻STAPI 调用）：\n\n```shell\n\t$ etcdctl cluster-health\n```\n通过 etcdctl 设置和获取键值, 设置键值对 testkey：\"hello world\"\n\n```shell\n\t$ etcdctl put testkey \"hello world111\"\n```\n也可以直接通过 HTTP 访问本地 2379 端口的方式来进行操作，例如查看 test key 的值：\n\n```shell\n\t$ curl -L -X PUT http://localhost:2379/v2/keys/testkey -d value=\"hello world\"\n```\n\n### Docker 镜像方式下载\n镜像名称为 quay.io/coreos/etcd:v3.3.1，可以通过下面的命令启动 etcd 服务监听到本地的 2379 和 2380 端口\n\n```shell\n\t$ docker run -p 2379:2379 -p 2380:2380 -v /etc/ssl/certs/:/etc/ssl/certs/ quay.io/coreos/etcd:v3.3.1\n```\n\n## Etcd 集群管理\n启动各个节点上的 etcd 服务, 指向主节点etcd存储\n\n### 时钟同步\n对于分布式集群来说，各个节点上的同步时钟十分重要， Etcd 集群需要各个节点时钟差异不超过 ls ，否则可能会导致 Raft 协议的异常.\n\n### 节点恢复\n> Etcd 集群中的节点会通过数据目录来存放修改信息和集群配置 。\n> 一般来说，当某个节点出现故障时候，本地数据已经过期甚至格式破坏. 如果只是简单地重启进程，容易造成数据的不一致。 这个时候，保险的做法是先通过命令（例如 etcdctlmember rm [member ］）来删除该节点，然后清空数据目录，再重新作为空节点加入.\n> Etcd 提供了－ strict-reconf ig-check 选项，确保当集群状态不稳定时候（例如启动节点数还不够达到 quorum ）拒绝对配置状态的修改\n\n### 重启集群\n> 极端情况下，集群中大部分节点都出现问题，需要重启整个集群.\n> 这个时候，最保险的办法是找到一个数据记录完整且比较新的节点，先以它为唯一节点创建新的集群，然后将其他节点一个一个地添加进来，添加过程中注意保证集群的稳定性.\n\n\n### K8s证书访问etcd server\n\nEnter etcd pod:\n\n```shell\n\t$ kubectl exec etcd-hci-node01 -n kube-system -i -t – sh\n```\n\nGet all key:\n\n```shell\n\t$ ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only\n```\n\nCheck the value of the key:\n\n```shell\n\t$ ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get /registry/statefulsets/kafka/kafka -w=json\n```\n\n向etcd数据库添加key,value\n\n```shell\n\t$ ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key put testkey \"hello world111\"\n```\n\n获取添加的key, value值\n\n```shell\n\t$ ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get testkey\n```\n\n\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"04 Kubernetes service","url":"/2021/03/13/microService/kubernetes/04_kubernetes_service/","content":"\n## Kubernetes Service\n> Kubemetes 服务是一种为一组功能相同的 pod 提供单一不变的接入点的资源.\n> 当服务存在时，它的 IP 地址和端口不会改变。 客户端通过 IP 地址和端口号建立连接，这些连接会被路由到提供该服务的任意一个 pod上.\n> 通过这种方式， 客户端不需要知道每个单独的提供服务的 pod 的地址， 这样这些 pod 就可以在集群中随时被创建或移除.\n\n通过为前端 pod 创建服务， 并且将其配置成可以在集群外部访问，可以暴露一个单一不变的 IP 地址让外部的客户端连接 pod。 \n同理，可以为后台数据库 pod 创建服务，并为其分配一个固定的 IP 地址。尽管 pod 的 IP 地址会改变，但是服务的 IP 地址固定不变。\n另外，通过创建服务，能够让前端的 pod 通过环境变量或 DNS 以及服务名来访问后端服务\nPod 控制器中使用标签选择器来指定哪些 pod 属于同一 Service。\n\n## service\n> 如果 pod 的标签与服务的 pod 选择器相匹配，那么 pod 就将作为服务的后端.只要创建了具有适当标签的新 pod ，它就成为服务的一部分，并且请求开始被重定向到 pod.\n如下所示Service和POD都采用命名端口的方式, 最大的好处就是即使更换spec pod中的端口号也无须更改服务 spec.\n * 第一步，创建service\n创建service yaml文件 kubia-svc.yaml\n\n\n```xml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: kubia\n\tsepc:\n\t// sessionAffinity: ClientIP\t// 默认此值是None, 若改为ClientIP，则SVC接受到的请求连接只会固定转发给同一个pod\n\t  ports:\n\t  - name: http\t\t\t// 端口别名，可以当作端口号用\n\t    port: 80\t\t\t// 该服务可用的端口\n\t    targetPort: http\t// 服务将连接转发到的POD端口, pod需要将http映射pod本身的8080或其它端口，否则这里只能填写端口号\n\t  - name: https\n\t    port: 443\n\t    targetPort: https\t// 含有label:app=kubia的pod需要将https映射pod本身8443或其它端口，否则这里只能填写端口号\n\t  selector:\n\t    app: kubia\t\t\t// 具有app=kubia标签的pod都属于该服务\n```\n\n创建了 一个名叫kubia的服务，它将在端口80接收请求并将连接路由到具有标签选择器是app=kubia的pod的8080端口上.\n在发布完YAML文件后， 可以在命名空间下列出来所有的服务资源, 新的服务已经被分配了一个内部集群IP, 只能在集群内部可以被访问.\n\n```shell\n\t$ kubectl get svc\n\tNAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\n\tkubernetes   ClusterIP   10.96.0.1      <none>        443/TCP   23h\n\tkubia        ClusterIP   10.98.229.76   <none>        80/TCP    20s\n```\n\n * 第二步，创建两个pod，一个添加标签app=kubia，另一个用来执行测试通过kubectl exec来访问第一个pod\nkubia.yaml\n\n\n```xml\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: kubia\t// name: kubia1; name: kubia2\n\tspec:\n\t  nodeSelector:\t\t// pod被分配到含有标签gpu=true的node上，当然也可以注释掉这两行\n\t    gpu: \"true\"\n\t  containers:\n\t  - image: luksa/kubia\n\t    name: kubia\n```\n\nkubia-label.yaml\n\n```xml\n\t apiVersion: v1\t\t\t\t\t// api服务版本\n\t kind : Pod\t\t\t\t\t\t// 资源类型\n\t metadata:\n\t   name: kubia-label\t\t\t// pod 名字\n\t   labels:\n\t     app: kubia\t\t\t\t\t// pod添加label\n\t spec :\n\t   nodeSelector:\n\t     gpu: \"true\"\t\t\t\t// node 选择器\n\t   containers:\n\t   - image: luksa/kubia\t\t\t// image 名字\n\t     name: kubia\t\t\t\t// container 名字\n\t     ports:\n\t     - name: http\t\t\t\t// pod端口映射，用http名字代替8080，名字随便取, 可以跟上面的service的targetPort对应起来\n\t       containerPort: 8080\t\t// 用上面的名字定义这个端口号的别名\n\t     - name: https\n\t       containerPort: 8443\n```\n\n查看POD并执行一个POD去通过上面创建的service(通过label)包含的pod提供的服务.\n其中pod kubia-label中container运行的服务进程监听了8080端口, POD对外也暴露了8080端口\n\n```shell\n\t$ kubectl get pod --show-labels\n\tNAME           READY   STATUS    RESTARTS   AGE    LABELS\n\tkubia          1/1     Running   0          101m   <none>\n\tkubia-label    1/1     Running   0          98m    app=kubia\n\tkubia-label1   1/1     Running   0          99s    app=kubia\n\tkubia-label2   1/1     Running   0          79s    app=kubia\n```\n\n * 第三步: 执行一个pod用curl命令访问另一个pod提供的服务\n双横杠(--)代表着kubectl命令项的结束.在两个横杠之后的内容是指在pod内部需要执行的命令.\nk8s 服务代理接续curl请求连接，三个包含label为app=kubia的pod任意选择一个pod\n访问服务三种方式,加不加端口都可以\n\n\n```shell\n\t<p>$ kubectl exec kubia -- curl -s http://10.98.229.76:http</p>\n\t$ kubectl exec kubia -- curl -s http://10.98.229.76:80\n\t$ kubectl exec kubia -- curl -s http://10.98.229.76\n\tYou've hit kubia-label\n\n\t$ kubectl exec kubia -- curl -s http://10.98.229.76\n\tYou've hit kubia-label2\n\t$ kubectl exec kubia -- curl -s http://10.98.229.76\n\tYou've hit kubia-label1\n```\n\n### Affinity 亲和性\nKubernetes 仅仅支持两种形式的会话亲和性服务： None 和 ClientIP\n这种方式将会使服务代理将来自同 一个 client IP 的所有请求转发至同 一个 pod上.\nKubernetes 服务不是在 HTTP 层面上工作。服务处理 TCP 和 UDP 包，并不关心其中的载荷内容。\n因为 cookie 是 HTTP 协议中的一部分，服务并不知道它们，这就解释了为什么会话亲和性不能基千 cookie。\n如果希望特定客户端产生的所有请求每次都指向同 一个 pod, 可以设置服务的 sessionAffinity 属性为 ClientIP (而不是 None,None 是默认值）\n\n```xml\n\tapiVersion: vl\n\tkind: Service\n\tspec:\n\t  sessionAffinity: ClientIP\n\t......\n```\n\n## 环境变量发现service\n### 创建replicaSet 管理 3 个 POD\n\n```xml\n\tapiVersion: apps/v1\n\tkind: ReplicaSet\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  replicas: 3\n\t  selector:\n\t    matchLabels:\n\t      app: kubia\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: kubia\n\t    spec:\n\t      nodeSelector:\n\t        gpu: \"true\"\n\t      containers:\n\t      - name: kubia\n\t        image: luksa/kubia\n\t        ports:\n\t        - name: http\n\t          containerPort: 8080\n\t        - name: https\n\t          containerPort: 8443\n```\n\n```shell\n\t$ kubectl get svc \n\tNAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubernetes   ClusterIP   10.96.0.1       <none>        443/TCP          29h\n\tkubia        ClusterIP   10.111.88.195   <none>        80/TCP,443/TCP   3h46m\n```\n\n查看pod所在的service对应的IP和端口\n\n```shell\n\t$ kubectl exec kubia-5rvfq env\n\t......\n\tKUBERNETES_SERVICE_PORT=443\n\tKUBIA_SERVICE_PORT=80\t\t\t\t// 服务的集群IP\n\t......\n\tKUBERNETES_SERVICE_HOST=10.96.0.1\n\tKUBIA_SERVICE_HOST=10.111.88.195\t// 服务所在的端口\n\t......\n```\npod 是否使用 内 部的 DNS 服务器是根据 pod 中 spec 的 dnsPolicy 属性来决定的\n\n进入容器后执行如下命令\n\n```shell\n\t$ kubectl exec kubia-5rvfq -it -- bash\t\t// -- 表示kubectl 命令执行完了，开始执行pod容器里要运行的命令\n\t$ curl http://kubia.default.svc.cluster.local\n\t$ curl http://kubia.default\n\t$ curl http://kubia\n\tYou've hit kubia-5rvfq\n\n\t$ cat /etc/resolv.conf\n\tnameserver 10.96.0.10\t\t// 对应kube-system 里的服务kube-dns服务IP\n\tsearch default.svc.cluster.local svc.cluster.local cluster.local sh.intel.com\n\toptions ndots:5\n\troot@kubia-5rvfq:/# curl http://svc.cluster.local\n\tcurl: (6) Could not resolve host: svc.cluster.local\n\n\t$ ping kubia\n\tPING kubia.default.svc.cluster.local (10.111.88.195): 56 data bytes\n\t^C--- kubia.default.svc.cluster.local ping statistics ---\n\t4 packets transmitted, 0 packets received, 100% packet loss\n```\n\n上面的 curl 这个服务是工作的，但是却 ping 不通。这是 因为服务的集群 IP 是一个虚拟 IP，并且只有在与服务端口结合时才有意义。 \n\n查看kube-system下面kube-dns信息\n\n```shell\n\t$ kubectl get svc -n kube-system\n\tNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\n\tkube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   2d3h\n```\n\n### 删除service\n\n```shell\n\t$ kubectl delete svc kubia\n```\n\n## Service samples\n### 查看service\n\n```shell\n\t$ kubectl get svc -n kube-system\n\t$ kubectl get svc -n istio-system\n\tNAME                        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                                                                      AGE\n\tgrafana                     ClusterIP      10.104.1.236     <none>        3000/TCP                                                                                                                                     3d5h\n\tistio-egressgateway         ClusterIP      10.107.177.52    <none>        80/TCP,443/TCP,15443/TCP                                                                                                                     3d5h\n\tistio-ingressgateway        LoadBalancer   10.97.82.221     <pending>     15020:31237/TCP,80:31556/TCP,443:30614/TCP,15029:32511/TCP,15030:32423/TCP,15031:30670/TCP,15032:30961/TCP,31400:30196/TCP,15443:31028/TCP   3d5h\n\tistio-pilot                 ClusterIP      10.97.192.70     <none>        15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP                                                                                     3d5h\n\tistiod                      ClusterIP      10.107.202.199   <none>        15012/TCP,443/TCP\n\t......\n\n\t$ kubectl get services\n\tNAME                        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\n\tkubernetes                  ClusterIP      10.3.240.l       <none>        443/TCP          34m\n\tkubia-http                  LoadBalancer   10.3.246.185     <pending>     8080:31348/TCP   4s\n\t暂时忽略 kubernetes 服务，仔细查看创建的kubian-http 服务 。 它还没有外部 IP 地址 ，因为 Kubernetes 运行的云基础设施创建负载均衡需要一段时间\n\t$ kubectl get services\n\tNAME                        TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE\n\tkubernetes                  ClusterIP      10.3.240.l       <none>            443/TCP          34m\n\tkubia-http                  LoadBalancer   103.246.185      104 155.74.57     8080:31348/TCP   4s\n\t现在有外部 IP 了，应用就可以从任何地方通过 http://104.155.74.57:8080 访问\n\t$ curl 104.155.74.57:8080\n\tYou’ve hit kubia-4jfyf\n```\n\n### 查看service的CRD信息\n\n```shell\n\t$ kubectl get svc istio-ingressgateway -n istio-system -oyaml\n```\n\n## endpoint 服务\n\n```shell\n\t$ kubectl describe svc kubia\n\tName:              kubia\n\tNamespace:         default\n\tLabels:            <none>\n\tAnnotations:       <none>\n\tSelector:          app=kubia\t\t// 用于创建endpoint列表的服务pod选择器\n\tType:              ClusterIP\n\tIP:                10.111.88.195\n\tPort:              http  80/TCP\n\tTargetPort:        8080/TCP\n\tEndpoints:         10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080\t\t// 服务endpoint的pod的IP和端口列表\n\tPort:              https  443/TCP\n\tTargetPort:        8443/TCP\n\tEndpoints:         10.44.0.1:8443,10.44.0.2:8443,10.44.0.3:8443\n\tSession Affinity:  ClientIP\n\tEvents:            <none>\n\n\t$ kubectl get po -o wide\n\tNAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES\n\tkubia         1/1     Running   0          15h   10.44.0.4   server02   <none>           <none>\n\tkubia-5rvfq   1/1     Running   0          15h   10.44.0.2   server02   <none>           <none>\n\tkubia-8cgnm   1/1     Running   0          15h   10.44.0.1   server02   <none>           <none>\n\tkubia-8kv8d   1/1     Running   0          15h   10.44.0.3   server02   <none>           <none>\n```\nEndpoint 资源和其他Kubernetes 资源一样，所以可以使用 kubectl info 来获取它的基本信息\n\n```shell\n\t$ kubectl get endpoints kubia\n\tNAME    ENDPOINTS                                                  AGE\n\tkubia   10.44.0.1:8443,10.44.0.2:8443,10.44.0.3:8443 + 3 more...   16h\n```\n\nEndpoint是一个单独的资源并不 是服务的一个属性, 必须手动创建\nexternal-service.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: external-service\n\tspec:\n\t  ports:\n\t  - port: 80\n```\n\nexternal-service-endpoints.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Endpoints\n\tmetadata:\n\t  name: external-service\n\tsubsets:\n\t  - addresses:\n\t    - ip: 11.11.11.11\n\t    - ip: 22.22.22.22\n\t    ports:\n\t    - port: 80\n```\n部署service和endpoint\n\n```shell\n\t$ kubectl create -f external-service.yaml\n\t$ kubectl create -f external-service-endpoints.yaml\n\t$ kubectl describe svc/external-service\n\tName:              external-service\n\tNamespace:         default\n\tLabels:            <none>\n\tAnnotations:       <none>\n\tSelector:          <none>\n\tType:              ClusterIP\n\tIP:                10.97.153.150\n\tPort:              <unset>  80/TCP\n\tTargetPort:        80/TCP\n\tEndpoints:         11.11.11.11:80,22.22.22.22:80\n\tSession Affinity:  None\n\tEvents:            <none>\n```\n\n## 暴露service\n • 将服务的类型设置成NodePort -- 每个集群节点都会在节点上打开一个端口， 对于NodePort服务， 每个集群节点在节点本身（因此得名叫NodePort)上打开一个端口，并将在该端口上接收到的流量重定向到基础服务。\n   该服务仅在内部集群 IP 和端口上才可访间， 但也可通过所有节点上的专用端口访问.\n • 将服务的类型设置成LoadBalance, NodePort类型的一种扩展 -- 这使得服务可以通过一个专用的负载均衡器来访问， 这是由Kubernetes中正在运行的云基础设施提供的。 负载均衡器将流量重定向到跨所有节点的节点端口。\n   客户端通过负载均衡器的 IP 连接到服务\n • 创建一 个Ingress资源， 这是一 个完全不同的机制， 通过一 个IP地址公开多个服务——它运行在 HTTP 层（网络协议第 7 层）上， 因此可以提供比工作在第4层的服务更多的功能\n\n\n### NodePort 类型 service\n指定端口不是强制性的。 如果忽略它，Kubemetes将选择一个随机端口.\n客户端发送请求的节点并不重要, 整个互联网可以通过任何节点上的30123(用户自己定义的)端口访问到pod,如下所示\n如在个人机器上生成如下service和pod，可以在以前做的项目的任何机器上通过如下访问\n\n```shell\n\tcurl -s http://10.239.140.186:30123 (master节点的NodeIP:port)访问服务\n\tcurl -s http://10.239.140.200:30123 (worker02节点的NodeIP:port)访问服务\n```\n\nkubia-svc-nodeport.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: kubia-nodeport\n\tspec:\n\t  type: NodePort\t\t\t// 设置服务类型\n\t  ports:\n\t  - port: 80\t\t\t\t// 服务集群IP端口号\n\t    targetPort: 8080\n\t    nodePort: 30123\t\t\t// 通过集群节点(master或worker)的NodeIP，加上30123端口可以访问服务\n\t  selector:\n\t    app: kubia\n```\n\n```shell\n\t$ kubectl create -f kubia-svc-nodeport.yaml\n\t\n\t$ kubectl get svc\n\tNAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubernetes           ClusterIP      10.96.0.1       <none>        443/TCP          43h\n\tkubia-nodeport       NodePort       10.103.7.50     <none>        80:30123/TCP     99m\n\t\n\t$ kubectl describe svc kubia-nodeport\n\tName:                     kubia-nodeport\n\tNamespace:                default\n\tLabels:                   <none>\n\tAnnotations:              <none>\n\tSelector:                 app=kubia\n\tType:                     NodePort\n\tIP:                       10.103.7.50\n\tPort:                     <unset>  80/TCP\n\tTargetPort:               8080/TCP\n\tNodePort:                 <unset>  30123/TCP\n\tEndpoints:                10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080\n\tSession Affinity:         None\n\tExternal Traffic Policy:  Cluster\n\tEvents:                   <none>\n\n\t$ kubectl get po -o wide\n\tNAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES\n\tkubia         1/1     Running   0          17h   10.44.0.4   server02   <none>           <none>\n\tkubia-5rvfq   1/1     Running   0          17h   10.44.0.2   server02   <none>           <none>\n\tkubia-8cgnm   1/1     Running   0          17h   10.44.0.1   server02   <none>           <none>\n\tkubia-8kv8d   1/1     Running   0          17h   10.44.0.3   server02   <none>           <none>\n```\n查看server02机器IP\n\n```shell\n\t$kubectl get node -o wide\n\tNAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\n\talpha      Ready    master   2d18h   v1.18.2   10.239.140.186   <none>        Ubuntu 18.04.4 LTS   5.3.0-28-generic    docker://19.3.6\n\tserver02   Ready    <none>   2d17h   v1.18.2   10.239.140.200   <none>        Ubuntu 18.04.4 LTS   4.15.0-76-generic   docker://19.3.6\n```\n两种访问方式\n * 第一种: 通过NodeIP:Port 访问:\n\n\n```shell\n\t$ curl -s http://10.239.140.200:30123\n\tYou've hit kubia-8kv8d\n\t$ curl -s http://10.239.140.186:30123\n\tYou've hit kubia-5rvfq\n```\n * 第二种: 通过 service的CLUSTER-IP：port 进入port进行访问\n\n\n```shell\n\t$ kubectl exec kubia -- curl -s http://10.103.7.50:80\n\tYou've hit kubia-8kv8d\n```\n\n### LoadBalancer 方式访问\n> 如果Kubemetes在不支持Load Badancer服务的环境中运行， 则不会调配负载平衡器， 但该服务仍将表现得像 一 个NodePort服 务。 这是因为LoadBadancer服务是NodePo江服务的扩展\n如果没有指定特定的节点端口， Kubernetes将会选择一个端口\n创建服务后， 云基础架构需要一段时间才能创建负载均衡器并将其 IP 地址写入服务对象。 一旦这样做了， IP 地址将被列为服务的外部 IP 地址\n\nkubia-svc-loadbalancer.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: kubia-loadbalancer\n\tspec:\n\t  type: LoadBalancer\n\t  ports:\n\t  - port: 80\n\t    targetPort: 8080\n\t  selector:\n\t    app: kubia\n```\n\n```shell\n\t$ kubectl create -f kubia-svc-loadbalancer.yaml\n\n\t$ kubectl describe svc/kubia-loadbalancer\n\tName:                     kubia-loadbalancer\n\tNamespace:                default\n\tLabels:                   <none>\n\tAnnotations:              <none>\n\tSelector:                 app=kubia\n\tType:                     LoadBalancer\n\tIP:                       10.99.184.62\n\tPort:                     <unset>  80/TCP\n\tTargetPort:               8080/TCP\n\tNodePort:                 <unset>  30994/TCP\t\t// yaml资源文件里没有指定, Kubemetes将会选择一个端口\n\tEndpoints:                10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080\n\tSession Affinity:         None\n\tExternal Traffic Policy:  Cluster\n\tEvents:                   <none>\n\n\t$ kubectl get svc\n\tNAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubernetes           ClusterIP      10.96.0.1       <none>        443/TCP          43h\n\tkubia-loadbalancer   LoadBalancer   10.99.184.62    <pending>     80:30994/TCP     4m11s\n```\n\n可以看到Kubemetes在不支持Load Badancer服务的环境中运行 EXTERNAL-IP显示为 <pending>状态，但仍然可以像NodePort方式一样访问服务\n\n```shell\n\t$ kubectl exec kubia -- curl -s http://10.99.184.62:80\n\tYou've hit kubia-8cgnm\n\t$ curl -s 10.239.140.186:30994\t\t// masterIP：svcPort\n\tYou've hit kubia-5rvfq\n\t$ curl -s 10.239.140.200:30994\t\t// worker01：svcPort\n\tYou've hit kubia-8kv8d\n```\n如果支持LoadBalancer且获得EXTERNAL-IP为 130.211.53.173\n可以通过 $ curl http://130.211.53.173 进行访问\n\n\n### Ingress 暴露服务\n> 需要 Ingress一个重要的原因是每个 LoadBalancer 服务都需要自己的负载均衡器， 以及独有的公有 IP 地址， 而 Ingress 只需要一个公网 IP 就能为许多服务提供访问\n> Ingress 在网络栈 (HTTP) 的应用层操作， 并且可以提供一 些服务不能实现的功能， 诸如基于 cookie 的会话亲和性 (session affinity) 等功能\n> Ingress 对象提供的功能之前，必须强调只有 Ingress控制器在集群中运行，Ingress 资源才能正常工作。 不同的 Kubernetes 环境使用不同的控制器实现， 但有些并不提供默认控制器\nIngress通常向外暴露 Service.Type=NodePort 或者 Service.Type=LoadBalancer 类型的服务，因此先创建一个NodePort类型svc.\n\n```shell\n\t$ kubectl get svc\n\tNAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubia-nodeport        NodePort       10.103.7.50     <none>        80:30123/TCP     144m\n```\n创建kubia-ingress.yaml资源文件\n\n```xml\n\tapiVersion: networking.k8s.io/v1beta1\n\tkind: Ingress\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  rules:\n\t  - host: kubia.example.com\t\t\t\t// Ingress 将域名kubia.example.com映射到您的服务\n\t    http:\n\t      paths:\n\t      - path: /\n\t        backend:\n\t          serviceName: kubia-nodeport\t// 将所有请求发送到kubia-nodeport服务的80端口\n\t          servicePort: 80\n```\n\n> kubectl创建ingress.yaml资源文件遇到webhook ...错误时修改master节点上/etc/kubernetes/manifests/kube-apiserver.yaml，将K8s默认用系统配置的proxy注释掉，稍后再运行kubectl create ...就可以了\n\n```shell\n\t$ kubectl create -f kubia-ingress.yaml\n\t$ kubectl get ingress\t\t// 自己机器上没有获得ADDRESS这列IP\n\tNAME    CLASS    HOSTS               ADDRESS            PORTS   AGE\n\tkubia   <none>   kubia.example.com   192.168.99.100     80      92s\n```\n> 一旦知道 IP 地址，通过配置 DNS 服务器将 kubia.example.com 解析为此 IP地址，或者在/ect/hosts(Windows系统为C:\\windows\\system32\\drivers\\etc\\hosts ）文件中添加下面一行内容：\n\n```\n\t192 168.99.100 kubia.example.com\n```\n通过Ingress访问pod, 环境都己经建立完毕，可以通过 http ：此ubia.example.com 地址访 问服务 （使用浏览器或者 curl 命令）\n\n```shell\n\t$ curl http://kubia.example.com\n```\n> 客户端如何通过 Ingress 控制器连接到 其 中 一个 pod。客户端首先对 kubia.example.com 执行 DNS 查 找， DNS 服务器（或本地操作系统）返回了In gress 控制器的 IP。\n> 客户端然后 向 Ingress 控制器发送 HTTP 请求，并在 Host 头中指定 kubia . example.com。\n> 控制器从该头部确定客户端尝试访 问哪个服务，通过与该服务关联 的 Endpo int 对象查看 pod IP ， 并将客户端的请求转发给其中一个pod。\n> Ingress 控制器不会将请求转发给该服务，只用它来选择一个pod。大多数（即使不是全部）控制器都是这样工作的.\n\nIngress规范的 rules 和 paths 都是数组，因此它们可以包含多个条目 \n一个 Ingress 可以将 多个主机和路径映射到多个服务\n * 客户端可以通过一个 IP 地址（ Ingress 控制器的 IP 地址 〉访问两种不同的服务\n * 同样，可以使用 Ingress 根据 HTTP 请求中的主机而不是（仅）路径映射到不同的服务\nkubia-ingress.yaml\n\n\n```xml\n\tapiVersion: networking.k8s.io/v1beta1\n\tkind: Ingress\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  rules:\n\t  - host: kubia.example.com\t\t\t\t// 对 kubia.example.com 的请求将会转发至kubia服务\n\t    http:\n\t      paths:\n\t      - path: /\n\t        backend:\n\t          serviceName: kubia-nodeport\n\t          servicePort: 80\n\t      - path: /kubia\t\t\t\t\t// 对 kubia.example.com/kubia 的请求将会转发至kubia服务\n\t        backend:\n\t          serviceName: kubia\n\t          servicePort: 80\n\t      - path: /foo\t\t\t\t\t\t// 对 kubia.example.com/foo 的请求将会转发至bar服务\n\t        backend:\n\t          serviceName: bar\n\t          servicePort: 80\n\t  - host: bar.example.com\t\t\t\t// 对 bar.example.com 的请求将会转发至bar服务\n\t    http:\n\t      paths:\n\t      - path: /\n\t        backend:\n\t          serviceName: bar\n\t          servicePort: 80\n```\n\n```shell\n\t$ kubectl get ingress\n\tNAME    CLASS    HOSTS                               ADDRESS   PORTS   AGE\n\tkubia   <none>   kubia.example.com,bar.example.com             80      3s\n```\nDNS 需要将 foo .example.com 和 bar.example.com 域名都指向 Ingress 控制器的 IP 地址.\n然后像上面一样配置/ect/hosts文件内容,就可以通过域名访问了\n\n### Ingress处理TLS传输\n配置 Ingress 以支持 TLS, Ingress 转发 HTTP 流量.\n\n\n\n## readiness Probe 就绪探针\n> 了解了存活探针，以及它们如何通过确保异常容器自动重启来保持应用程序的正常运行 。 与存活探针类似， Kubernetes 还允许为容器定义准备就绪探针\n> 就绪探测器会定期调用，并确定特定的 pod 是否接收客户端请求 。 当容器的准备就绪探测返回成功时，表示容器己准备好接收请求 \n就绪探针有三种类型:\n * Exec 探针，执行进程的地方。容器的状态由进程的退出状态代码确定 。\n * HTTP GET 探针，向容器发送 HTTP GET 请求，通过响应的 HTTP 状态代码判断容器是否准备好 。\n * TCP socket 探针，它打开一个 TCP 连接到容器的指定端口。如果连接己建立，则认为容器己准备就绪 \n\n启动容器时，可以为 Kubernetes 配置一个等待时间，经过等待时间后才可以执行第一次准备就绪检查。\n之后，它会周期性地调用探针，并根据就绪探针的结果采取行动。\n如果某个 pod 报告它尚未准备就绪，则会从该服务中删除该 pod。如果 pod再次准备就绪，则重新添加 pod.\n存活探针通过杀死异常的容器并用新的正常容器替代它们来保持 pod 正常工作，\n就绪探针确保只有准备好处理请求的 pod 才可以接收它们（请求）\n> 设想一组pod (例如， 运行应用程序服务器的pod)取决于另 一 个pod (例如，后端数据库）提供的服务。 如果任何一个前端连接点出现连接间题并且无法再访问数据库， 那么就绪探针可能会告知Kubemet es该pod没有准备好处理任何请求。 如果其他pod实例没有遇到类似的连接问题， 则它们可以正常处理请求。 就绪探针确保客户端只与正常的pod交互， 并且永远不会知道系统存在问题.\n通过kubectl ed江命令来向已存在的ReplicationController中的pod模板添加探针\nkubia-replicaset-renameport.yaml\n\n```xml\n\tapiVersion: apps/v1\n\tkind: ReplicaSet\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  replicas: 3\n\t  selector:\n\t    matchLabels:\n\t      app: kubia\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: kubia\n\t    spec:\n\t      nodeSelector:\n\t        gpu: \"true\"\n\t      containers:\n\t      - name: kubia\n\t        image: luksa/kubia\n\t        readinessProbe:\t\t\t// pod中的每个容器都会有一个就绪探针\n\t          exec:\n\t            command:\n\t            - ls\n\t            - /var/ready\n\t        ports:\n\t        - name: http\n\t          containerPort: 8080\n\t        - name: https\n\t          containerPort: 8443\n```\n\n创建RS资源并查看READY状态\n\n```shell\n\t$ kubectl create -f kubia-replicaset-renameport.yaml\n\t$ kubectl get po\n\tNAME          READY   STATUS    RESTARTS   AGE\n\tkubia-2xk54   0/1     Running   0          6m20s\n\tkubia-bj4tz   0/1     Running   0          6m20s\n\tkubia-hr9bx   0/1     Running   0          6m21s\n```\n\n通过创建/var/ready文件使其中一个文件的就绪探针返回成功，该文件的存在可以模拟就绪探针成功\n准备就绪探针会定期检查 默认情况下每 10 秒检查一次, 最晚 10 秒钟内， 该 pod 应该已经准备就绪.\n\n```shell\n\t$ kubectl exec po/kubia-2xk54 -- touch /var/ready\n\t$ kubectl get po\n\tNAME          READY   STATUS    RESTARTS   AGE\n\tkubia-2xk54   1/1     Running   0          6m26s\n\tkubia-bj4tz   0/1     Running   0          6m26s\n\tkubia-hr9bx   0/1     Running   0          6m27s\n\n\t$ kubectl describe po/kubia-2xk54\n\t......\n\tReadiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3\n\t......\n```\n\n修改创建过的RS的资源文件里的readiness命令是不生效的, 除非删了重建, 查看readiness Probe如下\n\n```shell\n\t$ kubectl edit rc kubia\n```\n\n### 再次测试 readiness\n\n```shell\n\t$ kubectl get po\n\tNAME          READY   STATUS    RESTARTS   AGE\n\tkubia-2xk54   1/1     Running   0          6m20s\n\tkubia-bj4tz   0/1     Running   0          6m20s\n\tkubia-hr9bx   0/1     Running   0          6m21s\n\n\t$ exec kubia-2xk54 -- rm -rf /var/ready\t\t// 过大概10s后\n\n\t$ kubectl get po\n\tNAME          READY   STATUS    RESTARTS   AGE\n\tkubia-2xk54   0/1     Running   0          6m30s\n\tkubia-bj4tz   0/1     Running   0          6m30s\n\tkubia-hr9bx   0/1     Running   0          6m31s\n\n\t$ kubectl exec kubia-bj4tz -- touch /var/ready\t// 过大概10s后\n\t$ kubectl get svc\n\tNAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES\n\tkubia-2xk54   0/1     Running   0          6m40s   10.44.0.2   server02   <none>           <none>\n\tkubia-bj4tz   1/1     Running   0          6m40s   10.44.0.3   server02   <none>           <none>\n\tkubia-hr9bx   0/1     Running   0          6m40s   10.44.0.1   server02   <none>           <none>\n```\n查看SVC\n\n```shell\n\t$ kubectl get svc\t// 也可以通过kubectl exec kubia-bj4tz env 来查看POD所支持的所有SVC的IP等信息\n\tNAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubia                 ClusterIP      10.111.88.195   <none>        80/TCP,443/TCP   22h\n\tkubia-loadbalancer    LoadBalancer   10.102.224.77   <pending>     80:32671/TCP     3h8m\n```\n\n两种访问方式，POD和Node\n第一种POD访问SvcIP:SvcPort方式\n\n```shell\n\t$kubectl exec kubia -- curl -s  http://10.111.88.195:80\t\t// SvcIP:SvcPort, SvcPort映射到PodIP, 可以通过describe svc查看\n\tYou've hit kubia-bj4tz\n```\n第二种NodeIP:NodePOrt方式\n\n```shell\n\t$ curl -s 10.239.140.186:32671\t\t// NodeIP:NodePort\n\tYou've hit kubia-bj4tz\n```\n\n应该通过删除 pod 或更改 pod 标签而不是手动更改探针来从服务中手动移除pod.\n如果想要从某个服务中手动添加或删除 pod, 请将 enabled=true 作为标签添加到 pod, 以及服务的标签选择器中。 当想要从服务中移除 pod 时，删除标签\n应该始终定义一 个就绪探针， 即使它只是向基准 URL 发送 HTTP 请求一样简单。\n\n## headless 服务\n让客户端连接到所有 pod, 需要找出每个 pod 的 IP.Kubemetes 允许客户通过 DNS 查找发现 pod IP.\n但是对千 headless 服务， 由于 DNS 返回了 pod 的 IP,客户端直接连接到该 pod, 而不是通过服务代理.\nheadless 服务仍然提供跨 pod 的负载平衡， 但是通过 DNS 轮询机制不是通过服务代理.\n> 如果告诉Kubemetes, 不需要为服务提供集群 IP (通过在服务 spec 中将 clusterIP 字段设置为 None 来完成此操作）， 则 DNS 服务器将返回 pod IP 而不是单个服务 IP\n> 将服务 spec中的clusterIP字段设置为None 会使服务成 为headless 服务，因为Kubemetes 不会 为其分配集群IP, 客户端可通过该IP将其连接到支持它的pod\nkubia-svc-headless.yaml\n\n```xml\n\tapiersion: v1\n\tkin: Service\n\tmetdata:\n\t  nme: kubia-headless\n\tspe:\n\t  custerIP: None\t\t// clusterIP字段设置为None 会使服务成 为headless服务\n\t  prts:\n\t  -port: 80\n\t   targetPort: 8080\n\t  slector:\n\t    app: kubia\n\n\t$ kubectl create -f kubia-svc-headless.yaml\n\t$ kubectl get svc\n\tNAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tkubia-headless        ClusterIP      None            <none>        80/TCP           103s\n```\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"03 Kubernetes nodes, namespace, pod","url":"/2021/03/13/microService/kubernetes/03_kubernetes_ns_pod/","content":"\n## nodes\n\n```shell\n$ kubectl get nodes -o wide\nNAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME\nhci-node01   Ready    master   5d    v1.18.1   10.67.108.211   <none>        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8\nhci-node02   Ready    <none>   5d    v1.18.1   10.67.109.142   <none>        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8\nhci-node03   Ready    <none>   5d    v1.18.1   10.67.109.147   <none>        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8\nhci-node04   Ready    <none>   5d    v1.18.1   10.67.109.144   <none>        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8\n$ kubectl describe node hci-node02\n输出显示了节点的状态、 CPU 和内存数据、系统信息、运行容器的节点等\n\n查看某台机器的资源\n$ kubectl describe node hci-node01\n```\n\n### 创建别名和补全\nkubectl 会被经常使用。很快你就会发现每次不得不打全命令是非常痛苦的。\n将下面的代码添加到 ~/.bashrc 或类似的文件中 ：\n\n```shell\n$ alias k=kubectl\n```\n为kuebctl配置 tab 补全\n需要先安装一个叫作 bashcompletio口的包来启用 bash 中的 tab 命令补全， 然后可以运行接下来的命令（也需要加到 ~/.bashrc 或类似的文件中）\n\n```shell\n$ source <{kubectl completion bash)\n$ kubectl desc<TAB> nod<TAB> hci<TAB>\n```\n但是需要注意的是， tab 命令行补全只在使用完整的 kubectl 命令时会起作用,(当使用别名 k 时不会起作用). 需要改变 kubectl completion 的输出来修复：\n\n```shell\n$ source <(kubectl completion bash | sed s/kubectl/k/g)\n```\n### node 标签\n\n```shell\n$ kubectl get nodes\n$ kubectl label node server02 gpu=false\n$ kubectl label node server02 gpu=true --overwrite\t//修改node标签\n$ kubectl get node -L gpu\t\t// 列出所有node，并添加GPU一列进行展示\n$ kubectl get node -l gpu\t\t// 只列出含标签的key为gpu的node\n$ kubectl get node -l gpu=false\t// 只列出含gpu=false的node\n```\n\n将POD调度到指定的node上: kubia-gpu.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\nname: kubia-gpu\t\t// 指定生成的POD名字\nspec:\nnodeSelector:\t\t// node选择器,选择含标签gpu=true的node机器\n\tgpu: \"true\"\t\t\ncontainers:\n- image: luksa/kubia\t// 要拉取的 image 名字\n\tname: kubia\t\t\t// 生成的 container 名字\n```\n```shell\n$ kubectl create -f kubia-gpu.yaml\n```\n如果没有标签为gpu=true的合适node， 通过 $ kubectl describe pod/kubia-nogpu 查看Message， 会报 0/2 nodes are available: 2 node(s) didn't match node selector.信息\n\n```shell\n$ kubectl describe pod/kubia-gpu\n......\nNode-Selectors:  gpu=true\n......\n```\n\n### taint污点\n给Node添加污点可以让配置tolerations的Pod部署上来，而不让平常的Pod部署.\n配置tolerations的Pod可以部署到添加污点的机器也可以部署到其它平常机器\n\n```shell\n// 查看node机器污点\n$ kubectl describe node/<Node-Name> | grep Taint\n  Taints:             node-role.kubernetes.io/master:NoSchedule\n// 去掉污点\n$  kubectl taint nodes <Node-Name> node-role.kubernetes.io/master:NoSchedule-\n\n$ kubectl taint nodes NodeName gpu=true:NoSchedule\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-demo\n  namespace: dev\nspec:\n  selector:\n    matchLabels:\n      app: web-demo\n  replicas: 3\t\t\t// 副本数3来测试能部署到哪些机器\n  template:\n    metadata:\n      labels:\n        app: web-demo\n    spec:\n      selector:\n        matchLabels:\n          app: web-demo\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: web-demo\n        spec:\n          containers:\n          - name: web-demo\n            image: hub.mooc.com/kubernetes/web:v1\n            ports:\n            - containerPort: 8080\n          tolerations:\t\t// 可以部署到有设置taint的机器，也可以部署到其它机器\n          - key: \"gpu\"\n            operator: \"Equal\"\n            value: \"true\"\n            effect: \"NoSchedule\"\n```\n\n> 典型的使用kubeadm部署和初始化的Kubernetes集群，master节点被设置了一个node-role.kubernetes.io/master:NoSchedule的污点，可以使用kubectl describe node <node-name>命令查看\n> 这个污点表示默认情况下master节点将不会调度运行Pod，即不运行工作负载, 对于使用二进制手动部署的集群设置和移除这个污点的命令如下:\n\n```shell\n$ kubectl taint nodes <node-name> node-role.kubernetes.io/master=:NoSchedule\n$ kubectl taint nodes <node-name> node-role.kubernetes.io/master:NoSchedule-\n```\n> kubeadm初始化的Kubernetes集群，master节点也被打上了一个node-role.kubernetes.io/master=的label，标识这个节点的角色为master。给Node设置Label和设置污点是两个不同的操作。设置Label和移除Label的操作命令如下\n\n设置Label\n\n```shell\n$ kubectl label node node1 node-role.kubernetes.io/master=\n```\n\n移除Label\n\n```shell\n$ kubectl label node node1 node-role.kubernetes.io/master-\n```\n\n## Namespace\n> 大多数对象的名称必须符合 RFC 1035 （域名）中规定的命名规范 ，这意味着它们可能只包含字母、数字、横杠（－）和点号，但命名空间（和另外几个）不允许包含点号\n\n### 隔离性\n> 名字的隔离只是 通过svc名称(DNS) 访问的隔离，通过svc的IP和Pod的IP再加上端口号(Port) 照样可以访问不同命名空间下的服务.\n\n### 设置默认命名空间\n> 默认Kubeclt获取default命名空间下的资源，可以通过设置K8s上下文配置文件如kube.config 使得某个命名空间变为默认namespace，获取pod时候不需要在加上 -n 参数\n\n### 创建命名空间\n> namespace不提供网络隔离, 如果命名空间 foo 中的某个 pod 知道命名空间 bar 中 pod 的 IP 地址，那它就可以将流量（例如 HTTP 请求）发送到另一个 pod\n第一种： commands方式\n\n```shell\n$ kubectl create namespace custom-namespace\n$ kubectl create ns custom-namespace\n```\n\n第二种： Yaml方式， 之所以选择使用 YAML 文件，只是为了强化Kubemetes中的所有内容都是一 个 API 对象这一概念\n\n```shell\n$ touch custom-namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: custom-namespace\n$ kubectl create -f custom-namespace.yaml\n```\n\n### 划分方式\n\n* 按环境划分: dev(开发), test(测试)\n* 按团队划分\n* 自定义多级划分\n\n### 标记命名空间\n\n```shell\n$ kubectl label namespace default istio-injection=enabled --overwrite         // enabled\n$ kubectl label namespace default istio-injection=disabled --overwrite        // disabled\n$ kubectl label namespace default istio-injection= --overwrite                // cancel set\nnamespace/default labeled\n```\n\n### 查看标记 istio-injection=enabled 标签的命名空间\n\n```shell\n$ kubectl get namespace -L istio-injection\nNAME              STATUS   AGE   ISTIO-INJECTION\ndefault           Active   85m   enabled\nistio-system      Active   25m   disabled\nkube-node-lease   Active   85m\nkube-public       Active   85m\nkube-system       Active   85m\n[root@hci-node01 istio-1.5.2]#\n```\n\n### 删除namespace\n删除当前命名空间中的所有资源，可以删除ReplicationCcontroller和pod,以及我们创建的所有service\n第一个 all 指定正在删除所有资源类型, --all 选项指定将删除所有资源实例, 而不是按名称指定它们\n使用 all 关键字删除所有内容并不是真的完全删除所有内容。 一些资源比如Secret会被保留下来， 并且需要被明确指定删除\n\n```shell\n$ kubectl delete all --all\t\t// 命令也会删除名为 kubernetes 的Service, 但它应该会在几分钟后自动重新创建\n```\n可以简单地删除整个命名空间（ pod 将会伴随命名空间 自动删除〉\n\n```shell\n$ kubectl delete ns custom-namespace\n```\n强制删除NAMESPACE\n\n```shell\n$ kubectl delete namespace NAMESPACENAME --force --grace-period=0\n```\n进入kube-system下得etcd pod 删除需要删除的NAMESPACE\n\n```shell\n$ kubectl get po -n kube-system\nNAME                                 READY   STATUS    RESTARTS   AGE\netcd-hci-node01                      1/1     Running   5          16d\n......\n\n$ kubectl exec -it etcd-hci-node01 sh -n kube-system\n$ etcdctl del /registry/namespaces/NAMESPACENAME\n```\n\n## POD\n\n### 查看pod解释\n\n```xml\n$ kubectl explain pod\nKIND:     Pod\nVERSION:  v1\n\nDESCRIPTION:\n\tPod is a collection of containers that can run on a host. This resource is\n\tcreated by clients and scheduled onto hosts.\n\nFIELDS:\napiVersion   <string>\n\tAPIVersion defines the versioned schema of this representation of an\n\tobject. Servers should convert recognized schemas to the latest internal\n\tvalue, and may reject unrecognized values. More info:\n\thttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\nkind <string>\n\tKind is a string value representing the REST resource this object\n\trepresents. Servers may infer this from the endpoint the client submits\n\trequests to. Cannot be updated. In CamelCase. More info:\n\thttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\nmetadata     <Object>\n\tStandard object's metadata. More info:\n\thttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\nspec <Object>\n\tSpecification of the desired behavior of the pod. More info:\n\thttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status\n\nstatus       <Object>\n\tMost recently observed status of the pod. This data may not be up to date.\n\tPopulated by the system. Read-only. More info:\n\thttps://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status\n```\n\n深入理解POD属性\n\n```shell\n$ kubectl explain pod.apiVersion\n$ kubectl explain pod.kind\n$ kubectl explain pod.spec\n```\npods 的缩写是 po, service 的缩写是 SVC, replicationcontroller 的缩写 rc\n\n```shell\n$ kubectl get pods -n kube-system\n```\n> 我们提到过每个 pod 都有自己的 IP 地址，但是这个地址是集群 内部的，不能从集群外部访问。\n> 要让 pod 能够从外部访问 ， 需要通过服务对象公开它， 要创建一个特殊的 LoadBalancer 类型的服务。\n> 因为如果你创建一个常规服务（ 一个 Cluster IP 服务）， 比如 pod ，它也 只能从集群内部访问。\n> 通过创建 LoadBalanc er 类型 的服务，将创建一个外部的负载均衡 ，可以通过 负载均衡的公共 IP 访问 pod \n\n### 创建POD\n通过上传 JSON 或 YAML 描述文件到 Kubemetes API 服务器来创建 pod.\nkubectl create -f 命令用于从YAML或JSON文件创建任何资源（不只是 pod).\n\n```shell\n$ kubectl create -f kubia-manual.yaml\n$ kubectl create -f kubia-gpu.yaml -n custom-namespace\t//创建pod到custom-namespace命名空间下\n$ kubectl describe pod/kubia\n......\nEvents:\nType    Reason     Age    From               Message\n----    ------     ----   ----               -------\nNormal  Scheduled  5m12s  default-scheduler  Successfully assigned default/kubia-liveness to server02\nNormal  Pulling    5m8s   kubelet, server02  Pulling image \"luksa/kubia-unhealthy\"\n```\n编写好yaml文件在本地某个目录后, cd到此目录, 用一条command全部创建或删除资源\n\n```shell\n$ kubectl apply -f .\t// 添加所有资源\n$ kubectl delete -f .\t// 删除所有资源\n```\n### 一直查看pod状态\n\n```shell\n$ kubectl get pods -w\n```\n### pod标签labels\n\n```shell\n$ kubectl get po --show-labels\n```\n查看pod标签的key值为creation_method 和 env 的信息\n\n```shell\n$ kubectl get po -L creation_method,env\nNAME                           READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\nkubia                          1/1     Running   0          16h\nkubia-manual-v2                1/1     Running   0          34m   manual            pod\n```\nPOD添加标签\n\n```shell\n$ kubectl label po kubia  creation_method=manual\nNAME                           READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV\nkubia                          1/1     Running   0          16h   manual\nkubia-manual-v2                1/1     Running   0          43m   manual            pod\n```\n更改现有标签, 在更改现有标签时， 需要使用--overwrite选项\n\n```shell\n$ kubectl label po kubia-manual-v2 env=debug --overwrite\n```\n使用标签列出POD\n\n```shell\n$ kubectl get po -1 creation_method=manual\n$ kubectl get po -l env\n```\n同样列出没有env标签的pod\n确保使用单引号来圈引 !env, 这样bash shell才不会解释感叹号（译者注：感叹号在bash中有特殊含义， 表示事件指示器)\n\n```shell\n$ kubectl get po -l '!env'\ncreation_method!=manual 选择带有creation_method标签， 并且值不等于manual的pod\nenv in (prod, devel)选择带有env标签且值为prod或devel的pod\nenv notin (prod, devel)选择带有env标签， 但其 值不是prod或devel的pod\napp=pc,rel=beta 选择pc微服务的beta版本pod\n```\n\n### pod 注解\n\n```shell\n$ kubectl annotate pod kubia-gpu mycompany.com/someannotion=\"foo bar\"\n$ kubectl describe pod/kubia-gpu\n......\nAnnotations:  mycompany.com/someannotion: foo bar\n......\n```\n### 查看该 pod 的完整描述文件：\n\n```shell\n$ kubectl get po kubia-manual -o yaml\t// 获取yaml格式信息\n$ kubect1 get po kubia-manual -o json\t// 获取json格式信息\n```\n\n### 执行pod容器\n直接执行:\n\n```shell\n$ kubectl exec fortioclient-f8d65c6bb-5k4td -c captured date -n twopods\n```\n进入容器执行, 当pod中只有一个容器时可以不加-c参数指定某个容器\n\n```shell\n$ kubectl exec fortioclient-f8d65c6bb-5k4td -c captured -i -t /bin/sh -n twopods\n```\n### 容器进程, 网络等\n查看进程command完整信息\n\n```shell\n$ ps auxwww\n$ pa -ef\n```\n查看网络\n\n```shell\n$ netstat -ntlp\n```\n### 查看Pod, svc日志\n\n```shell\n$ kubectl logs pod/istiod-774777b79-ddfk4 -n istio-system\n$ kubectl logs -f pod/<pod_name> #类似tail -f的方式查看(tail -f 实时查看日志文件 tail -f 日志文件log)\n$ kubectl logs svc/istiod -n istio-system\n如果该pod中有其他容器， 可以通过如下命令获取其日志：\n$ kubectl logs kubia-manual -c kubia\n```\n\n查看容器重启后前一个容器为什么重启的日志信息\n\n```shell\n$ kubectl logs mypod --previous\n```\n### 部署应用程序\n\n```shell\n$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\n```\n\n### 重启Pod\n\n```shell\n$ kubectl get pod {podname} -n {namespace} -o yaml | kubectl replace --force -f -\n```\n\n### 删除Pod\n\n```shell\n$ kubectl delete pod PODNAME -n custom-namespace\t\t// 删除指定命名空间下的POD\n$ kubectl delete po -l creation_method=manual\t\t\t// 通过标签选择器来删除\n$ kubectl delete po --all -n custom-namespace\t\t\t// 删除当前命名空间中的所有 pod\n$ kubectl delete all --all -n custom-namespace\t\t\t// 删除所有pod和svc，系统带的kubernetes服务会过一会重启\n```\n可使用kubectl中的强制删除命令删除POD\n\n```shell\n$ kubectl delete pod PODNAME --force --grace-period=0\n```\n直接从ETCD中删除源数据\n删除default namespace下的pod名为pod-to-be-deleted-0\n\n```shell\n$ ETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0\n```\n\n## livenessProbe 存活探针, readinessProbe\nKubemetes 可以通过存活探针 (liveness probe) 检查容器是否还在运行.\nKubemetes 可以通过readinessProbe探针 检查容器是否准备完毕可以挂到负载均衡上供外部访问.\nlivenessProbe与readinessProbe探针用法完全一样, 都有三种，下面介绍这三种健康检查方式.\n\n可以为 pod 中的每个容器单独指定存活探针。 如果探测失败， Kubemetes 将定期执行探针并重新启动容器\n\n * 第一种健康检查方式: 执行命令检查存活探针是否存活\n\n\n```xml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-demo\n  namespace: dev\nspec:\n  selector:\n    matchLabels:\n      app: web-demo\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web-demo\n    spec:\n      selector:\n        matchLabels:\n          app: web-demo\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: web-demo\n        spec:\n          containers:\n          - name: web-demo\n            image: hub.mooc.com/kubernetes/web:v1\n            ports:\n            - containerPort: 8080\n            livenessProbe:\t\t\t\t// 检查应用是否存活的探针, 和容器一个级别\n              exec:\t\t\t\t\t\t// 第一种健康检查方式, 通过执行命令\n                command:\n                - /bin/sh\n                - -c\n                - ps -ef|grep java|grep -v grep\n              initialDelaySeconds: 10\t\t// 容器起来后过10s开始检查\n              periodSeconds: 10\t\t\t\t// 每隔10s检查一次\n              failureThreshold: 2\t\t\t// 连续健康检查失败2次放弃检查, 重启容器\n              successThreshold: 1\t\t\t// 检查一次满足条件就认为健康检查通过\n              timeoutSeconds: 5\t\t\t\t// 每次健康检查delay时间是5s, 超时也认为健康检查失败, 重启容器\n            readinessProbe:\t\t\t\t// readinessProbe与livenessProbe用法完全一样.\n              exec:\t\t\t\t\t\t// 第一种检查方式, 通过执行命令\n                command:\n                - /bin/sh\n                - -c\n                - ps -ef|grep java|grep -v grep\n              initialDelaySeconds: 10\t\t// 容器起来后过10s开始检查\n              periodSeconds: 10\t\t\t\t// 每隔10s检查一次\n              failureThreshold: 2\t\t\t// 连续健康检查失败2次放弃检查, 重启容器\n              successThreshold: 1\t\t\t// 检查一次满足条件就认为健康检查通过\n              timeoutSeconds: 5\t\t\t\t// 每次健康检查delay时间是5s, 超时也认为健康检查失败, 重启容器\n```\n\n * 第二种健康检查方式: 执行网络请求检查存活探针是否存活\n\n\n```xml\n$ touch kubia-liveness-probe.yaml\n  apiVersion: v1\n  kind: Pod\n  metadata:\n    name: kubia-liveness\n  spec:\n    containers:\n    - image: luksa/kubia-unhealthy\n      name: kubia\n      livenessProbe:\t\t// 一个 HTTP GET 存活探针\n        httpGet:\t\t\t// 第二种健康检查方式, 通过httpGet\n          path: /\t\t\t// 应用要访问的路径\n          port: 8080\t\t// 容器本身启动的端口\n          scheme: HTTP\n        initialDelaySeconds: 15\t\t// 容器起来后过10s开始检查\n        periodSeconds: 5\n```\n```shell\n$ kubectl get pods\n  NAME                           READY   STATUS    RESTARTS   AGE\n  kubia-liveness                 1/1     Running   1          13m\n```\n查看该pod描述\n\n```shell\n$ kubectl describe pod/kubia-liveness\n  ......\n  Last State:     Terminated\n    Reason:       Error\n    Exit Code:    137\n    Started:      Wed, 13 May 2020 15:49:36 +0800\n    Finished:     Wed, 13 May 2020 15:51:25 +0800\n  Ready:          True\n  Restart Count:  1\n  Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3\n  ......\n```\n数字137是两个 数字的总和：128+x, 其中x是终止进程的信号编号.\n在这个例子中，x等于9, 这是SIGKILL的信号编号，意味着这个进程被强行终止.\n当容器被强行终止时，会创建一个全新的容器—-而不是重启原来的容器.\ndelay=Os部分显示在容器启动后立即开始探测.\ntimeout仅设置为1秒，因此容器必须在1秒内进行响应， 不然这次探测记作失败.\n每10秒探测一次容器(period=lOs), 并在探测连续三次失败(#failure=3)后重启容器.\n定义探 针时可以自定义这些附加参数。例如，要设 置初始延迟，请将initialDelaySeconds属性添加到存活探针的配置中.\n\n```xml\n    livenessProbe:\t\t// 一个 HTTP GET 存活探针\n      httpGet:\n        path: /\n        port: 8080\n      initialDelaySeconds: 15\t// Kubernetes会在第—次探测前等待15秒 \n```\n\n```shell\n\t$ kubectl describe pod/kubia-liveness\n\t......\n\tLiveness:       http-get http://:8080/ delay=15s timeout=1s period=10s #success=1 #failure=3\n\n * 第三种健康检查方式: 通过TCP检查端口是否处于监听状态\n    livenessProbe:\t\t// 一个 HTTP GET 存活探针\n      tcpSocket:\n        port: 8080\n      initialDelaySeconds: 20\t// Kubernetes会在第—次探测前等待15秒 \n      periodSeconds: 5\n```\n> 如果没有设置初始延迟，探针将在启动时立即开始探测容器， 这通常会导致探测失败， 因为应用程序还没准备好开始接收请求.\n> 务必记得设置一个初始延迟未说明应用程序的启动时间.\n> 对于在生产中运行的pod, 一定要定义一个存活探针。没有探针的话，Kubemetes无法知道你的应用是否还活着。只要进程还在运行， Kubemetes会认为容器是健康的\n> Kubernetes会在你的容器崩溃或其存活探针失败时， 通过重启容器来保持运行。 这项任务由承载pod的节点上的Kubelet 执行 一— 在主服务器上运行的Kubernetes Control Plane组件不会参与此过程.\n> 但如果节点本身崩溃， 那么Control Plane 必须为所有随节点停止运行的pod创建替代品。 它不 会为你直接创建的pod执行此操作 。 这些pod只被Kubelet 管理.\n\n### 查看 readinessProbe, healthProbe\n\n```shell\n$ kubectl edit po -n istio-system istio-ingressgateway-6489d9556d-wjr58\n$ kubectl edit deployment -n istio-system istio-ingressgateway\n$ kubectl logs po/istio-ingressgateway-6489d9556d-wjr58 -n istio-system\n$ kubectl get po -A\n```\n\n### affinity\n匹配Node标签, Pod部署到哪台机器上.\n\n```xml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-demo\n  namespace: dev\nspec:\n  selector:\n    matchLabels:\n      app: web-demo\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web-demo\n    spec:\n      selector:\n        matchLabels:\n          app: web-demo\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: web-demo\n        spec:\n          containers:\n          - name: web-demo\n            image: hub.mooc.com/kubernetes/web:v1\n            ports:\n            - containerPort: 8080\n          affinity:\n            nodeAffinity:\t\t// node亲和性, 要部署到哪台机器，不要部署到哪台机器\n              requiredDuringSchedulingIgnoredDuringExecution:\t// 必须满足下面条件才会执行调度\n                nodeSelectorTerms:\t\t// 数组形式, 下面可以定义多个Terms, 它们之间是或的关系\n                - matchExpressions:\t\t// 数组形式, 如果定义多个matchExpressions它们之间是与的关系\n                  - key: beta.kubernetes.io/arch\t// 节点的label含有的key名字, 这里的是由K8s根据机器自动生成的\n                    operator: In\n                    values:\t\t\t\t// 前提是Node机器有amd64标签K8s才会把容器部署到次Node机器\n                    - amd64\t\t\t\t// 通过kubectl get nodes NodeName -o yaml进行查看\n              preferredDuringSchedulingIgnoredDuringExecution:\t// 最好是怎样调度\n              - weight: 1\t\t\t\t// 权重\n                perference:\n                  matchExpressions:\n                  - key: disktype\t\t// 通过kubectl get nodes --show-labels查看\n                    operator: NotIn\n                    values:\n                    - ssd\n            podAffinity:\t\t// Pod亲和性, 想和哪些Pod部署到一台机器, 不想和哪些Pod部署在一台机器\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: app\n                    operator: In\t\t// 要跟app=web-demo的Pod运行在同一个节点上\n                    values:\n                    - web-demo-node\n                topologyKey: kubernetes.io/hostname\t\t// 节点的label名字\n              preferredDuringSchedulingIgnoredDuringExecution:\n              - weight: 100\n                podAffinityTerm:\n                  labelSelector:\n                    matchExpressions:\n                    - key: app\n                      operator: In\n                      values:\n                      - web-demo-node\n                  topologyKey: kubernetes.io/hostname\n            podAntiAffinity:\t\t// Pod反亲和性, 不想和哪些Pod部署到一台机器, 用法和podAffinity用法完全一样\n            pod反亲和性用的很多的是上面的replicas: 的值 >=2 时候会把容器副本分别部署到不同机器\n```\n\n### Pod启动停止控制\nPod容器启动时候和停止前所做的事\n\n```xml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-demo\n  namespace: dev\nspec:\n  selector:\n    matchLabels:\n      app: web-demo\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: web-demo\n    spec:\n      selector:\n        matchLabels:\n          app: web-demo\n      replicas: 1\n      template:\n        metadata:\n          labels:\n            app: web-demo\n        spec:\n          containers:\n          - name: web-demo\n            image: hub.mooc.com/kubernetes/web:v1\n            ports:\n            - containerPort: 8080\n            volumeMounts:\n            - name: shared-volume\n              mounthPath: /shared-web\n            lifecycle:\t\t\t\tPod里容器启动前和停止前要做的事\n              postStart:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"echo web starting ... >> /var/log/messages\"]\n              preStop:\n                exec:\n                  command: [\"/bin/sh\", \"-c\", \"echo web stopping ... >> /var/log/messages && sleep 3\"]\n```\n\n## ReplicationController\n一个ReplicationController有三个主要部分\n • label selector ( 标签选择器）， 用于确定ReplicationController作用域中有哪些pod\n • replica count (副本个数）， 指定应运行的pod 数量\n • pod template (pod模板）， 用于创建新的pod 副本\n使用 ReplicationController 的好处\n •确保一 个 pod (或多个 pod副本）持续运行， 方法是在现有pod 丢失时启动一个新 pod。\n • 集群节点发生故障时， 它将为故障节 点 上运 行的所有 pod (即受ReplicationController 控制的节点上的那些 pod) 创建替代副本。\n • 它能轻松实现 pod的水平伸缩 手动和自动都可以\n\n### 由RC创建POD\nkubia-rc.yaml, 内容如下:\n\n```xml\napiVersion: v1\nkind: ReplicationController\t\t// 这里的配置定义了ReplicationController(RC)\nmetadata:\n  name: kubia\t\t// ReplicationController 的名字\nspec:\n  replicas: 3\t\t// pod 实例的目标数目\n  selector:\t\t\t// selector也可以不写，replica 直接根据下面的template模板里的lables标签选择创建POD\n    app: kubia\t\t// pod 选择器决定了 RC 的操作对象\n  template:\t\t\t// 从此以下都是创建新 pod 所用的 pod 模板, 与单独创建的pod定义yaml文件内容几乎相同\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n        ports:\n        - containerPort: 8080\n```\n\n创建ReplicationController并由其创建pod\n\n```shell\n$ kubectl create -f kubia-rc.yaml\n$ kubectl get po -o wide\n  NAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES\n  kubia-6wnj5   1/1     Running   0          56s   10.44.0.3   server02   <none>           <none>\n  kubia-788p8   1/1     Running   0          56s   10.44.0.2   server02   <none>           <none>\n  kubia-c9kn6   1/1     Running   0          56s   10.44.0.1   server02   <none>           <none>\n\n$ kubectl delete po kubia-6wnj5\n  NAME          READY   STATUS              RESTARTS   AGE\n  kubia-6ntgt   0/1     ContainerCreating   0          12s\n  kubia-6wnj5   1/1     Terminating         0          3m3s\n  kubia-788p8   1/1     Running             0          3m3s\n  kubia-c9kn6   1/1     Running             0          3m3s\n```\n\n上面重新列出pod会显示四个， 因为你删除的pod己终止， 并且己创建一个新的pod\n虽然ReplicationController会立即收到删除pod的通知 (API 服务器允许客户端监听资源和资源列表的更改），但这不是它创建替代pod的原因。\n该通知会触发控制器检查实际的pod数量并采取适当的措施.\n\n```shell\n$ kubectl get po -o wide\n  NAME          READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES\n  kubia-6ntgt   1/1     Running   0          53s     10.44.0.4   server02   <none>           <none>\n  kubia-788p8   1/1     Running   0          3m44s   10.44.0.2   server02   <none>           <none>\n  kubia-c9kn6   1/1     Running   0          3m44s   10.44.0.1   server02   <none>           <none>\n```\n\n### 获取有关 ReplicationController 的信息\n\n```shell\n$ kubectl get rc -o wide\n$ kubectl get rc -o wide -n default\t\t// RC是针对某个namespace下做的副本pod控制\n  NAME    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR\n  kubia   3         3         3       17m   kubia        luksa/kubia   app=kubia\n\n获取RC的详细信息\n$ kubectl describe rc kubia\n```\n\n如果你更改了 一个 pod 的标签，使它不再 与 ReplicationController 的标签选择器相匹配 ， 那么该 pod 就变得和其他手动创建的 pod 一样了\n更改 pod 的标签时， ReplicationController 发现一个 pod 丢失了 ， 并启动一个新的pod替换它.\n\n给其中一个 pod 添加了 type=special 标签，再次列出所有 pod 会显示和以前一样的三个 pod 。 因为从 ReplicationCon位oiler 角度而言， 没发生任何更改.\n\n```shell\n$ kubectl label pod/kubia-6ntgt type=special\n$ kubectl get pod --show-labels\n  NAME          READY   STATUS    RESTARTS   AGE   LABELS\n  kubia-6ntgt   1/1     Running   0          27m   app=kubia,type=special\n  kubia-788p8   1/1     Running   0          30m   app=kubia\n  kubia-c9kn6   1/1     Running   0          30m   app=kubia\n```\n\n更改app标签该 pod 不再与 RC 的标签选择器相匹配\n\n```shell\n$ kubectl label pod/kubia-6ntgt app=foo --overwrite\n$ kubectl get pod --show-labels\n  NAME          READY   STATUS              RESTARTS   AGE   LABELS\n  kubia-6ntgt   1/1     Running             0          30m   app=foo,type=special\n  kubia-788p8   1/1     Running             0          33m   app=kubia\n  kubia-c9kn6   1/1     Running             0          33m   app=kubia\n  kubia-dqshz   0/1     ContainerCreating   0          4s    app=kubia\n```\n使用 -L app 选项在列 中显示 app 标签\n\n```shell\n$ kubectl get pod -L app\n  NAME          READY   STATUS    RESTARTS   AGE     APP\n  kubia-6ntgt   1/1     Running   0          32m     foo\n  kubia-788p8   1/1     Running   0          35m     kubia\n  kubia-c9kn6   1/1     Running   0          35m     kubia\n  kubia-dqshz   1/1     Running   0          2m17s   kubia\n```\n可能有一个 bug 导致你的 pod 在特定时间或特定事件后开始出问题。\n如果你知道某个 pod 发生了故障， 就可以将它从 Replication-Controller 的管理范围中移除， 让控制器将它替换为新 pod, 接着这个 pod 就任你处置了。 完成后删除该pod 即可。\n\n### 编辑RC的YAML配置\n用默认文本编辑器中打开ReplicationController的YAML配置，会在/tmp目录生成一个临时yaml文件，退出后/tmp目录下的yaml文件也会删掉\n如果你想使用nano编辑Kubernetes资源，请执行以下命令（或将其放入 ~/.bashrc或等效文件中）\nexport KUBE_EDITOR=\"/usr/bin/nano\"\n\n```xml\n$ kubectl edit rc kubia\n......\n spec:\n   replicas: 3\n   selector:\n     app: kubia1\t\t\t\t RC selector 修改，需要配合下面的label一起修改\n   template:\n     metadata:\n       creationTimestamp: null\n       labels:\n         app: kubia1\t\t\t Pod label 修改，需要配合上面的 RC selector 一起修改\n     spec:\n       containers:\n       - image: luksa/kubia\n         imagePullPolicy: Always\n         name: kubia\n         ports:\n         - containerPort: 8080\n           protocol: TCP\n......\n```\n```shell\n$ kubectl get pod\n  NAME          READY   STATUS              RESTARTS   AGE   APP\n  kubia-279wl   0/1     ContainerCreating   0          2s    kubia1\n  kubia-6ntgt   1/1     Running             0          44m   foo\n  kubia-788p8   1/1     Running             0          47m   kubia\n  kubia-c9kn6   1/1     Running             0          47m   kubia\n  kubia-dqshz   1/1     Running             0          14m   kubia\n  kubia-m6vml   0/1     Pending             0          2s    kubia1\n  kubia-xxjqr   0/1     ContainerCreating   0          2s    kubia1\n```\n\n### RC 扩容\n扩展/缩容 RC管理的pod为5个\n第一种，commands方式:\n\n```shell\n$ kubectl scale rc kubia --replicas=5\n```\n第二种， edit rc yaml文件\n\n```shell\n$ kubectl edit rc kubia\n  ......\n  spec:\n    replicas: 5\n  ......\n```\n\n### 删除RC\n当使用 kubectl delete 删除 ReplicationController 时， 可以通过给命令增加 --cascade= false 选项来保持 pod 的运行.\n\n$ kubectl delete rc kubia --cascade=false\n已经删除了 ReplicationController, 所以这些 pod 独立了， 它们不再被管理。但是你始终可以使用适当的标签选择器创建新的 ReplicationController, 并再次将它们管理起来\n\n\n## ReplicaSet\n> 最 初， ReplicationController 是用于复制和在异常时重新调度节点的唯 一Kubemetes 组件， 后来又引入了 一个名为 ReplicaSet 的类似资源 。 它是新一代的ReplicationController, 并且将其完全替换掉 (ReplicationController 最终将被弃用）。\n> 也就是说从现在起， 你应该始终创建 ReplicaSet 而不是 ReplicationController。 它们几乎完全相同， 所以你不会碰到任何麻烦\n> ReplicaSet 的行为与ReplicationController 完全相同， 但pod 选择器的表达能力更强\n> ReplicationController 都无法仅基千标签名的存在来匹配 pod, 而ReplicaSet 则可以。 例如， ReplicaSet 可匹配所有包含名为 env 的标签的 pod, 无论ReplicaSet 的实际值是什么（可以理解为 env=*)\n\nkubia-replicaset.yaml\n\n```xml\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: kubia\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kubia\n  template:\n    metadata:\n      labels:\n        app: kubia\n    spec:\n      containers:\n      - name: kubia\n        image: luksa/kubia\n        ports:\n        - containerPort: 8080\n```\n检查replicaset:\n\n```shell\n$ kubectl get rs\n```\n\n### matchExpressions选择器\n创建个yaml文件\nkubia-replicaset-matchexpressions.yaml\n\n```xml\n selector:\n   matchExpressions:\n     - key: app\n       operator: In\n       values:\n         - kubia\n```\n每个表达式都必须 包含一个key, 一个operator (运算符），并且可能还有一个values的列表（取决于 运算符）.\n• In : Label的值 必须与其中 一个指定的values 匹配。\n• Notln : Label的值与任何指定的values 不匹配。\n• Exists : pod 必须包含一个指定名称的标签（值不重要）。使用此运算符时，\n不应指定 values字段。\n• DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定\n如果同时指定matchLabels和matchExpressions, 则所有标签都必须匹配，并且所有表达式必须计算为true以使该pod与选择器匹配.\n\n### 查看 replicaset 和 deployment 的详细信息\n\n```shell\n$ kubectl describe deployment details-v1\n$ kubectl describe rs details-v1-6fc55d65c9\n```\n\n### 删除ReplicaSet\n删除ReplicaSet会删除所有的pod,这种情况下是需要列出pod来确认.\n\n```shell\n$ kubectl delete rs kubia\n```\n## DaemonSet\n如果节点下线， DaemonSet不会在其他地方重新创建pod。 但是， 当将一个新节点添加到集群中时， DaemonSet会立刻部署一个新的pod实例。\n如果有人无意中删除了 一个 pod ， 那么它也会重新创建 一个新的 pod。\n与 ReplicaSet一样，DaemonSet 从配置的 pod 模板创建 pod.\n\n> 如果节点可以被设置为不可调度的 ， 防止 pod 被部署到节点上. DaemonSet 甚至会将 pod 部署到这些节点上，因为无法调度的属性只会被调度器使用，而 DaemonSet 管理的 pod 则完全绕过调度器. 这是预期的，因为DaemonSet的目的是运行系统服务，即使是在不可调度的节点上，系统服务通常也需要运行.\n\n给node节点打上label\n\n```shell\n$ kubectl label node server02 disk=ssd\n```\n\nssd-monitor-daemonset.yaml\n\n```xml\napiVersion: apps/v1\t\t\t// DaemooSet在apps的API组 中，版本是v1\nkind: DaemonSet\nmetadata:\n  name: ssd-monitor\nspec:\n  selector:\n    matchLabels:\n      app: ssd-monitor\n  template:\n    metadata:\n      labels:\n        app: ssd-monitor\n    spec:\n      nodeSelector:\t\t\t// pod模板包含 会选择有disk=ssd标签的节点 一个节点选择器\n        disk: ssd\n      containers:\n      - name: main\n        image: luksa/ssd-monitor\n```\n```shell\n$ kubectl create -f ssd-monitor-daemonset.yaml\n```\n### 查看DaemonSet\n\n```shell\n$ kubectl get ds\n  NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES              SELECTOR\n  ssd-monitor   1         1         1       1            1           disk=ssd        3m29s   main         luksa/ssd-monitor   app=ssd-monitor\n```\n如果你有多个节点并且其他的节点也加上了同样的标签，将会看到 DaemonSet 在每个节点上都启动 pod.\n给其中一个节点修改标签disk=hdd, 假设它的硬盘换成磁盘而不是SSD, 那个节点上的pod会如预期中被终止.\n如果还有其他的 pod在运行， 删除 DaemonSet 也会一起删除这些 pod。\n\n### 删除ds\n删除ds会删除由ds控制schedule到每个节点的pod\n\n```shell\n$ kubectl delete ds ssd-monitor\n```\n\n## Job资源\nKubemetes 通过 Job 资源提供了对此的支持，它允许你运行一种 pod, 该 pod 在内部进程成功结束时， 不重启容器。\n一旦任务完成， pod 就被认为处于完成状态.\n由Job管理的pod会一直被重新安排，直到它们成功完成任务.\n\nexporter.yaml\n\n```xml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: batch-job\nspec:\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      restartPolicy: OnFailure\t\t// 默认为Always,Job pod不能使用默认策略， 因为它们不是要无限期地运行\n      containers:\n      - name: main\n        image: luksa/batch-job\t\t// 运行luksa/batch-job镜像，该镜像调用 一个运行120秒的进程，然后退出\n```\n需要明确地将重启策略 restartPolicy 设置为 OnFailure 或 Never。 此设置防止容器在完成任务时重新启动\n\n```shell\n$ kubectl create -f exporter.yaml\n\n$ kubectl get pod\n  NAME                READY   STATUS    RESTARTS   AGE\n  batch-job-lhnfg     1/1     Running   0          113s\n\n$ kubectl get jobs\n  NAME        COMPLETIONS   DURATION   AGE\n  batch-job   0/1           111s       111s\n```\n\n等待两三分钟后\n\n```shell\n$ kubectl get pod\n  NAME                READY   STATUS      RESTARTS   AGE\n  batch-job-lhnfg     0/1     Completed   0          3m21s\n\n$ kubectl get job\n  NAME        COMPLETIONS   DURATION   AGE\n  batch-job   1/1           2m41s      3m27s\n```\n完成后pod未被删除的原因是允许你查阅其日志\n\n```\n$ kubectl logs po/batch-job-lhnfg\n  Thu May 14 05:04:38 UTC 2020 Batch job starting\n  Thu May 14 05:06:38 UTC 2020 Finished succesfully\n```\npod 可以被直接删除， 或者在删除创建它的Job时被删除.  \n作业可以配置为创建多个pod实例，并以并行或串行方式运行它们.  \n在Job配置中设置 completions和parallelism属性来完成的.  \n如果你需要 一个Job运行多次，则可以将comple巨ons设为你希望作业的pod运行多少次.  \n\n```xml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: multi-completion-batch-job\nspec:\n  completions: 5\t// job将一个接一个地运行五个pod\n  parallelism: 2\t// 最多两个pod可以并行运行\n  template:\n    metadata:\n      labels:\n        app: batch-job\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: main\n        image: luksa/batch-job\n```\n\n它最初创建一个pod, 当pod的容器运行完成时，它创建第二个pod, 以此类推，直到五个pod成功完成。\n如果其中 一个pod发生故障，工作会创建一个新的pod, 所以Job总共可以创建五个以上的pod.\n\n```shell\n$ kubectl create -f multi-completion-batch-job.yaml\n  NAME                               READY   STATUS              RESTARTS   AGE\n  multi-completion-batch-job-8kzd5   0/1     ContainerCreating   0          4s\n  multi-completion-batch-job-9rnxs   0/1     ContainerCreating   0          4s\n```\n只要其中 一个pod完成任务，工作将运行下 一个pod, 直到五个pod都成功完成任务.\n\n```shell\n$ kubectl get po\n  NAME                               READY   STATUS    RESTARTS   AGE\n  multi-completion-batch-job-8kzd5   1/1     Running   0          2m8s\n  multi-completion-batch-job-9rnxs   1/1     Running   0          2m8s\n\n$ kubectl get job\n  NAME                         COMPLETIONS   DURATION   AGE\n  multi-completion-batch-job   0/5           2m13s      2m13s\n```\nPOD虽然创建，但是POD里的进程任务还没有完成，因此job显示任然是0/5没有一个pod任务完成\n再等待一会时间\n\n```shell\n$ kubectl get po\n  NAME                               READY   STATUS      RESTARTS   AGE\n  multi-completion-batch-job-8kzd5   0/1     Completed   0          4m18s\n  multi-completion-batch-job-9rnxs   0/1     Completed   0          4m18s\n  multi-completion-batch-job-blntb   1/1     Running     0          107s\n  multi-completion-batch-job-qhsr5   1/1     Running     0          92s\n  ssd-monitor-jbhpd                  1/1     Running     0          176m\n\n$ kubectl get job\n  NAME                         COMPLETIONS   DURATION   AGE\n  multi-completion-batch-job   2/5           4m16s      4m16s\n```\n如上显示已经有2个POD任务完成，POD退出.\n甚至可以在 Job 运行时更改 Job 的 parallelism 属性, command如下，实验环境没有成功使用\n\n```shell\n$ kubectl scale job multi-completion-batch-job --replicas 3\n```\n\n> 通过在 pod 配置中设置 activeDeadlineSeconds 属性，可以限制 pod的时间。如果 pod 运行时间超过此时间， 系统将尝试终止 pod, 并将 Job 标记为失败。\n> 通过指定 Job manifest 中的 spec.backoff巨m辽字段， 可以配置 Job在被标记为失败之前可以重试的次数。 如果你没有明确指定它， 则默认为6\n\n### 删除job\n删除job时，由job创建的pod也被直接删除\n\n```shell\n$ kubectl delete job multi-completion-batch-job\n```\n\n## CornJob 资源\n> 批处理任务需要在特定的时间运行，或者在指定的时间间隔内重复运行,在 Linux 和类 UNIX 操作系统中， 这些任务通常被称为 cron 任务。 Kubemetes 也支持这种任务\n> Kubemetes 中的 cron 任务通过创建 CronJob 资源进行配置, 运行任务的时间表以知名的 cron 格式指定\n时间表从左到右包含以下五个条目\n• 分钟\n• 小时\n• 每月中的第几天\n• 月\n• 星期几\n\n创建资源文件(kube API 对象文件)cronjob.yaml\n\n```xml\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  schedule: \"0,15,30,55 * * * *\"\n  startingDeadlineSeconds: 15\t// pod最迟必须在预定时间后15秒开始运行， 如果因为任何原因到该启动时间15s后仍不启动，任务将不会运行，并将显示为Failed\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: periodic-batch-job\n        spec:\n          restartPolicy: OnFailure\n          containers:\n          - name: main\n            image: luksa/batch-job\n```\n\n> 希望每 15 分钟运行一 次任务因此 schedule 字段的值应该是\"0, 15, 30, 45****\" 这意味着每小时的 0 、 15 、 30和 45 分钟（第一个星号），每月的每一天（第二个星号），每月（第三个星号）和每周的每一天（第四个星号）。\n> 相反，如果你希望每隔 30 分钟运行一 次，但仅在每月的第一天运行，则应将计划设置为 \"0,30 * 1 * *\", 并且如果你希望它每个星期天的 3AM 运行，将它设置为 \"0 3 * * 0\" (最后一个零代表星期天）。\n\n### 查看cronjob\n\n```shell\n$ kubectl get cronjob -o wide\nNAME                              SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE    CONTAINERS   IMAGES            SELECTOR\nbatch-job-every-fifteen-minutes   0,15,30,55 * * * *   False     0        18m             112m   main         luksa/batch-job   <none>\n```\n\n### cronjob运行状态\n\n```shell\n$ kubectl get po\nNAME                                               READY   STATUS      RESTARTS   AGE\nbatch-job-every-fifteen-minutes-1589439300-4v8wd   0/1     Completed   0          36m\nbatch-job-every-fifteen-minutes-1589439600-99pns   0/1     Completed   0          31m\nbatch-job-every-fifteen-minutes-1589440500-vs5vm   0/1     Completed   0          16m\nbatch-job-every-fifteen-minutes-1589441400-52rzb   1/1     Running     0          112s\n\n$ kubectl get job\nNAME                                         COMPLETIONS   DURATION   AGE\nbatch-job-every-fifteen-minutes-1589439300   1/1           2m24s      36m\nbatch-job-every-fifteen-minutes-1589439600   1/1           2m24s      31m\nbatch-job-every-fifteen-minutes-1589440500   1/1           2m22s      16m\nbatch-job-every-fifteen-minutes-1589441400   0/1           116s       116s\n```\n再过一点时间查看\n\n```shell\n$ kubectl get po\nNAME                                               READY   STATUS      RESTARTS   AGE\nbatch-job-every-fifteen-minutes-1589439600-99pns   0/1     Completed   0          32m\nbatch-job-every-fifteen-minutes-1589440500-vs5vm   0/1     Completed   0          17m\nbatch-job-every-fifteen-minutes-1589441400-52rzb   0/1     Completed   0          2m34s\n\n$ kubectl get job\nNAME                                         COMPLETIONS   DURATION   AGE\nbatch-job-every-fifteen-minutes-1589439600   1/1           2m24s      32m\nbatch-job-every-fifteen-minutes-1589440500   1/1           2m22s      17m\nbatch-job-every-fifteen-minutes-1589441400   1/1           2m20s      2m37s\n```\n\n总结: CornJob过指定的时间执行一次POD，执行完退出，会保留三个POD和Job记录.\n\n### 删除cronjob\n运行中的 Job 将不会被终止，不会删除 Job 或 它们的 Pod。为了清理那些 Job 和 Pod，需要列出该 Cron Job 创建的Job，然后删除它们.\n\n```shell\n$ batch-job-every-fifteen-minutes\ncronjob.batch \"batch-job-every-fifteen-minutes\" deleted\n```\n## secret\n\n```shell\n$ kubectl get secret -n default\nNAME                  TYPE                                  DATA   AGE\ndefault-token-gtcjx   kubernetes.io/service-account-token   3      32d\n\n$ kubectl get pods -o wide\nNAME                               READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES\nwordpress-7bfc545758-vtfvm         1/1     Running   5          10d     10.36.0.10   hci-node04   <none>           <none>\nwordpress-mysql-764fc64f97-sjnjk   1/1     Running   0          10d     10.36.0.8    hci-node04   <none>           <none>\n\n$ kubectl get po/wordpress-7bfc545758-vtfvm -o yaml\n......\nspec:\n  containers:\n  - env:\n    volumeMounts:\n    - mountPath: /var/www/html\n      name: wordpress-persistent-storage\n    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount\n      name: default-token-gtcjx\n      readOnly: true\n......\n  volumes:\n  - name: wordpress-persistent-storage\n    persistentVolumeClaim:\n      claimName: wp-pv-claim\n  - name: default-token-gtcjx\n    secret:\n      defaultMode: 420\t\t// 访问权限\n      secretName: default-token-gtcjx\n......\n```\n\n进入wordpress-7bfc545758-vtfvm所在机器的容器里查看/var/run/secrets/kubernetes.io/serviceaccount路径文件\n\n```shell\n$ docker exec -it daec0458a397 /bin/sh\n# ls /var/run/secrets/kubernetes.io/serviceaccount\n  ca.crt  namespace  token\n```\n\n### 创建自己的Secret\nserviceAccount 用来跟Apiserver通信，用来授权, 可以创建自己的Secret\n编写Secret配置文件 secret.yaml\n\n```xml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dbpass\ntype: Opaque\t\t// 不透明，浑浊的.\ndata:\n  username: aW1vb2M=\t\t// base64加密的用户名\n  passwd: aW1vb2MxMjM=\t\t// base64加密的密码\n```\n\n把字符串生成base64很简单，命令如下\n\n```shell\n\t$ echo -n imooc | base64\t// -n 表示换行\n\taW1vb2M=\n```\n编写Pod资源配置文件 pod-secret.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\nspec:\n  containers:\n  - name: springbook-web\n    image: hub.mooc.com/kubernetes/springboot-web:v1\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: db-secret\n      mountPath: /db-secret\n      readOnly: true\n  volumes:\n  - name: db-secret\n    projected:\n      sources:\t\t\t// secret 来源\n      - secret:\n        name: dbpass\t// secret 名字\n```\n生成Pod并进入查看\n\n```shell\n$ / # cd /db-secret/\n$ ls\n  passwd username\n$ cat -n username\t\t// 查看容器里存放的是base64解码过的数据\n  immoc\n$ cat -n passwd\n  imooc123\n```\n可以通过修改secret.yaml文件修改secret账号密码等再$ kubectl apply -f secret.yaml来更改密码.\n\n## Configmap\n> configmap常用来存储不需要加密的数据, 比如应用的启动参数，一些参数的配置等\n * 第一种向k8s添加很多key value的键值对属性值，就可以用configmap\n\n\n```shell\n$ touch game.properties\n$ vim game.properties\n  enemies=aliens\n  lives=3\n  enemies.cheat=true\n  secret.code.allowed=true\n  ......\n```\n配置到K8S里\n\n```shell\n$ kubectl create configmap web-game --from-file game.properties\n$ kubectl get cm\n```\n\n使用configmap, Pod-game.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-game\nspec:\n  containers:\n  - name: web\n    image: hub.mooc.com/kubernetes/springboot-web:v1\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n    - name: game\n      mountPath: /etc/config/game\n      readOnly: true\n  volumes:\n  - name: game\n    configMap:\n      name: web-game\n```\n生成Pod并进入查看\n\n```shell\n$ cd /etc/config/game\n$ ls\n  game.properties\n$ cat game.properties\n  enemies=aliens\n  lives=3\n  enemies.cheat=true\n  secret.code.allowed=true\n  ......\n```\n\n可以通过kubectl edit 修改configMap账号密码等\n\n```shell\n$ kubectl edit cm web-game -o yaml\n  enemies.cheat=false\t//等等操作\n```\n\n * 第二种配置文件方式创建configMap\nconfigmap.yaml\n\n\n```xml\napeVersion: v1\nkind: Configmap\nmetadata:\n  name: configs\ndata:\n  Java_OPTS: -Xms1024m\n  LOG_LEVEL: DEBUG\n```\n\n```shell\n$ kubectl create -f configmap.yaml\n```\n\n编写资源配置文件pod-env.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\nspec:\n  containers:\n  - name: web\n    image: hub.mooc.com/kubernetes/springboot-web:v1\n    ports:\n    - containerPort: 8080\n    env:\n      - name: LOG_LEVEL_CONFIG\n        valueFrom:\n          configMapKeyRef:\n            name: configs\t\t// 指定configMap名字\n            key: LOG_LEVEL\t\t// configs下面的LOG_LEVEL\n```\n进入容器查看环境变量\n\n```shell\n$ env | grep LOG\n  LOG_LEVEL_CONFIG=DEBUG\n```\n之后次容器就可以通过环境变量获取值\n\n * 第三种 通过命令行方式传进参数\n也是先跟第二种一样创建configMap资源\n\n\n```shell\n$ kubectl create -f configmap.yaml\n```\n编写资源配置文件pod-cmd.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-cmd\nspec:\n  containers:\n  - name: web\n    image: hub.mooc.com/kubernetes/springboot-web:v1\n    command: [\"/bin/sh\", \"-c\", \"java -jar /springboot-web.jar -DJAVA_OPTS=$(JAVA_OPTS)\"]\n    ports:\n    - containerPort: 8080\n    env:\n      - name: Java_OPTS\n        valueFrom:\n          configMapKeyRef:\n            name: configs\t\t// 指定configMap名字\n            key: Java_OPTS\t\t// configs下面的LOG_LEVEL\n```\n进入容器查看进程\n\n```shell\n$ ps -ef\n  java -jar /springboot-web.jar -DJAVA_OPTS=-Xms1024m\n```\n\n## downwardAPI\ndownwardAPI主要作用是在程序中取得Pod对象本身的一些相关信息\npod-downwardapi.yaml\n\n```xml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-downwardapi\n  labels:\n    app： downwardapi\n    type: webapp\nspec:\n  containers:\n  - name: web\n    image: hub.mooc.com/kubernetes/springboot-web:v1\n    ports:\n    - containerPort: 8080\n    volumeMounts:\n      - name: podinfo\n        mountPath: /etc/podinfo\n  volumes:\n    - name: podinfo\n      projected:\n        sources:\n        - downwardAPI:\n          items:\n            - path: \"labels\"\n              fieldRef:\n                fieldPath: metadata.labels\n            - path: \"name\"\n              fieldRef:\n                fieldPath: metadata.name\n            - path: \"namespace\"\n              fieldRef:\n                fieldPath: metadata.namespace\n            - path: \"mem-request\"\n              resourceFieldRef:\n                containerName: web\n                resource: limits.memory\n```\n\n进入容器查看文件信息\n\n```shell\n$ cd /etc/podinfo\n$ ls -l\n  labels mem-request name namespace\n$ cat -n labels\n  app=\"downwardapi\"\n  type=\"webapp\"\n$ cat -n namespace\n  default\n$ cat -n name\n  pod-downwardapi\n```\n\n## 删除evicted pod\n\n通过脚本删除很多的Evicted Pod.\n\n;;; id001 第一种\n``` shell\nkubectl get pods | grep Evicted | awk '{print $1}' | xargs kubectl delete pod\n```\n;;;\n\n;;; id001 第二种\n``` shell\n#!/bin/bash\nfor each in $(kubectl get pods|grep Evicted|awk '{print $1}');\ndo\n  kubectl delete pods $each\ndone\n```\n;;;\n\n\n## 重启Pod\n\n这种方式适用于多种对象。  重启 Pod 并不会修复运行程序的 bug，想要解决程序的意外终止，最终还是得要修复 bug.\n\n``` shell\nkubectl get pod {podname} -n {namespace} -o yaml | kubectl replace --force -f -\n```\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"06 Kubernetes deployment","url":"/2021/03/13/microService/kubernetes/06_kubernetes_deployment/","content":"\n## deployment\nDeployment 是一种更高阶资源， 用千部署应用程序并以声明的方式升级应用, 而不是通过 ReplicationController 或 ReplicaSet 进行部署， 它们都被认为是更底层的概念.\n当创建一个 Deployment 时， ReplicaSet 资源也会随之创建.\n在使用 Deployment 时， 实际的 pod是由 Deployment 的 Replicaset 创建和管理的， 而不是由 Deployment 直接创建和管理的.\n> 创建Deployment与创建ReplicationController并没有任何区别。Deployment也是由标签选择器、期望副数和pod模板组成的。此外，它还包含另 一 个字段，指定一 个部署策略，该策略定义在修改Deployment资源时应该如何执行更新\nDeployment可以同时管理多个版本的 pod, 所以在命名时不需要指定应用的版本号\nkubia-svc-loadbalancer.yaml\n\n```xml\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: kubia-loadbalancer\n\tspec:\n\t  type: LoadBalancer\n\t  ports:\n\t  - port: 80\n\t    targetPort: 8080\n\t  selector:\n\t    app: kubia\n```\n\nkubia-deployment-v1.yaml\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  replicas: 3\n\t  selector:\n\t    matchLabels:\n\t      app: kubia\n\t  template:\n\t    metadata:\n\t      name: kubia\n\t      labels:\n\t        app: kubia\n\t    spec:\n\t      containers:\n\t      - image: luksa/kubia:v1\n\t        name: nodejs\n```\n\n创建deployment\n\n```shell\n\t$ kubectl create -f kubia-deployment-v1.yaml --record\n```\n查看deployment过程\n\n```shell\n\t$ kubectl rollout status deployment kubia\n\tWaiting for deployment \"kubia\" rollout to finish: 1 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 1 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 2 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 2 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 2 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 2 out of 3 new replicas have been updated...\n\tWaiting for deployment \"kubia\" rollout to finish: 1 old replicas are pending termination...\n\tWaiting for deployment \"kubia\" rollout to finish: 1 old replicas are pending termination...\n\tWaiting for deployment \"kubia\" rollout to finish: 1 old replicas are pending termination...\n\tdeployment \"kubia\" successfully rolled out\n```\n当使用 ReplicationController 创建 pod 时， 它们的名称是由 Controller 的名称加上一个运行时生成的随机字符串.\n由 Deployment 创建的三个 pod 名称中均包含一个额外的数字, 这个数字实际上对应 Deployment 和 ReplicaSet 中的 pod 模板的哈希值\n\n```shell\n\t$ kubectl get po\n\tkubia-59d857b444-2nrr7   1/1     Running   0          5m15s\n\tkubia-59d857b444-f9pnx   1/1     Running   0          5m15s\n\tkubia-59d857b444-wzgnj   1/1     Running   0          5m15s\n\n\t$ kubectl get replicasets\n\tNAME               DESIRED   CURRENT   READY   AGE\n\tkubia-59d857b444   3         3         3       8m48s\n\n\t$ kubectl get svc\n\tNAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\n\tkubernetes           ClusterIP      10.96.0.1       <none>        443/TCP        4d22h\n\tkubia-loadbalancer   LoadBalancer   10.100.58.157   <pending>     80:31170/TCP   3s\n```\n查看master node 的IP为10.239.140.186\n\n```shell\n\t$ curl 10.239.140.186:31170\n\tThis is v1 running in pod kubia-59d857b444-wzgnj\n```\n\n查看deployment详细信息\n\n```xml\n\t$ kubectl describe deployment kubia\n\tName:                   kubia\n\tNamespace:              default\n\tCreationTimestamp:      Mon, 18 May 2020 15:04:12 +0800\n\tLabels:                 <none>\n\tAnnotations:            deployment.kubernetes.io/revision: 1\n\t                        kubernetes.io/change-cause: kubectl create --filename=kubia-deployment-v1.yaml --record=true\n\tSelector:               app=kubia\n\tReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\n\tStrategyType:           RollingUpdate\n\tMinReadySeconds:        0\n\tRollingUpdateStrategy:  25% max unavailable, 25% max surge\n\tPod Template:\n\t  Labels:  app=kubia\n\t  Containers:\n\t   nodejs:\n\t    Image:        luksa/kubia:v1\n\t    Port:         <none>\n\t    Host Port:    <none>\n\t    Environment:  <none>\n\t    Mounts:       <none>\n\t  Volumes:        <none>\n\tConditions:\n\t  Type           Status  Reason\n\t  ----           ------  ------\n\t  Available      True    MinimumReplicasAvailable\n\t  Progressing    True    NewReplicaSetAvailable\n\tOldReplicaSets:  <none>\n\tNewReplicaSet:   kubia-59d857b444 (3/3 replicas created)\n\tEvents:\n\t  Type    Reason             Age    From                   Message\n\t  ----    ------             ----   ----                   -------\n\t  Normal  ScalingReplicaSet  7m12s  deployment-controller  Scaled up replica set kubia-59d857b444 to 3\n```\n\n略微减慢滚动升级的速度， 以便观察升级过程确实是以滚动的方式执行的。 可以通过在Deployment上设置minReadySeconds属性来实现\nminReadySeconds的主要功能是避免部署出错版本的应用， 而不只是单纯地减慢部署的速度.\nminReadySeconds属性指定新创建的pod至少要成功运行多久之后，才能将其视为可用.\n通常情况下需要 将minReadySeconds设置为更高的值， 以确保pod在它们真正开始接收实际流量之后可以持续保持就绪状态.\n\n如kubectl patch命令将其设置为10秒\n\n```shell\n\t$ kubectl patch deployment kubia -p '{\"spec\": {\"minreadyseconds\": 10}}'\n```\n## 重新部署几种方式\n重建(recreate)，滚动(rollingUpdate)，蓝绿，金丝雀\n\n### recreate\n停止Pod并重新创建Pod\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: web-recreate\n\t  namespace: dev\n\tspec:\n\t  strategy:\n\t    type: Recreate\t\t// 停止Pod并重新创建Pod\n\t  selector:\n\t    matchLabels:\n\t      app: web-recreate\n\t  replicas: 2\n\t  template:\n\t......\n\tapiVersion: extensions/v1beta1\n\tkind: Ingress\n\tmetadata:\n\t  name: web-recreate\n\t  namespace: dev\n\tspec:\n\t  rules:\n\t  - host: web-recreate.mooc.com  //配置下/etc/hosts 文件 \"IP web-recreate.mooc.com\"\n\t    http:\n\t      paths:\n\t      - path: /\n\t        backend:\n\t          serviceName: web-recreate\n\t          servicePort: 80\n```\n\n### 滚动更新deployment\n滚动更新过程svc的IP等不会变，只是改变pod版本，修改pod的镜像就可以了\n> 实际上， 如何达到新的系统状态的过程是由 Deployment 的升级策略决定的，默认策略是执行滚动更新（策略名为 RollingUpdate)。 另 一种策略为 Recreate, 它会一次性删除所有旧版本的 pod, 然后创建新的 pod, 整个行为类似千修改ReplicationController 的 pod 模板， 然后删除所有的 pod\n使用kubectl set image命令来更改任何包含容器资源的镜像(ReplicationController、ReplicaSet、 Deployment等）\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: web-rollingupdate\n\t  namespace: dev\n\tspec:\n\t  strategy:\n\t    rollingUpdate:\t\t\t// 滚动更新, 不设置的话都会设成跟下面一样的默认值, $kubectl get deployment Name -o yaml查看\n\t      maxSurge: 25%\t\t\t// 可以最大超出实力数的百分比，4个replicas，就是每次最多多启动1个实例\n\t      maxUnavailable: 25%\t// 不可用实力百分比，4个replicas，就是每次必须有3个实例可用\n\t    type: RollingUpdate\n\t  selector:\n\t    matchLabels:\n\t      app: web-rollingupdate\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: web-bluegreen\n\t        version: v1.0\n\t    spec:\n\t......\n```\n\n```shell\n\t$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\n\tdeployment.apps/kubia image updated\n```\n创建rs，然后新的rs会创建一个pod，然后原来的rs删除一个pod，新的rs再创建一个pod，依次完成设定的3个pod都完成更新.\n通过命令定时查看输出:\n\n```shell\n\t$ while sleep 0.2; do curl \"http://web-rollingupdate.mooc.com/hello?name=michael\"; echo \"\"; done\n```\n\n查看Pod:\n\n```shell\n\t$ kubectl get po\n\tNAME                     READY   STATUS              RESTARTS   AGE\n\tfortune                  2/2     Running             0          3d\n\tkubia-59d857b444-6nq74   1/1     Running             0          72s\n\tkubia-59d857b444-jbln7   1/1     Terminating         0          72s\n\tkubia-59d857b444-lr2zw   1/1     Running             0          72s\n\tkubia-7d5c456ffc-69gg5   0/1     ContainerCreating   0          2s\n\tkubia-7d5c456ffc-vmz74   1/1     Running             0          14s\n```\n\n滚动更新后旧的 ReplicaSet 仍然会被保留\n\n```shell\n\t$ kubectl get rs\n\tNAME               DESIRED   CURRENT   READY   AGE\n\tkubia-59d857b444   3         3         3       9m27s\n\tkubia-7d5c456ffc   1         1         0       3s\n```\n再等待一会再查看rs\n\n```shell\n\t$ kubectl get rs\n\tNAME               DESIRED   CURRENT   READY   AGE\n\tkubia-59d857b444   0         0         0       10m\n\tkubia-7d5c456ffc   3         3         3       72s\n```\n\nDeployment可以非常容易地回滚到先前部署的版本，它可以让Kubernetes 取消最后一次部署的 Deployment\n\n```shell\n\t$ kubectl set image deployment kubia nodejs=luksa/kubia:v2\n\t$ kubectl get rs\n\tNAME               DESIRED   CURRENT   READY   AGE\n\tkubia-59d857b444   0         0         0       3h3m\n\tkubia-79b84b44f4   3         3         3       96s\n\tkubia-7d5c456ffc   0         0         0       174m\n```\nundo 命令也可以在滚动升级过程中运行，并直接停止滚动升级。 在升级过程中已创建的 pod 会被删除并被老版本的 pod 替代\n\n```shell\n\t$ kubectl rollout undo deployment kubia\n\tdeployment.apps/kubia rolled back\n\t$ kubectl get rs\n\tNAME               DESIRED   CURRENT   READY   AGE\n\tkubia-59d857b444   0         0         0       3h7m\n\tkubia-79b84b44f4   0         0         0       5m19s\n\tkubia-7d5c456ffc   3         3         3       177m\n```\n\nkubectl rollout history 来显示升级的版本\n\n```shell\n\t$ kubectl rollout history deployment kubia\n\tdeployment.apps/kubia\n\tREVISION  CHANGE-CAUSE\n\t1         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n\t2         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n\t3         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n```\n\n### 回滚到一个特定的 Deployment 版本\n\n```shell\n\t$ kubectl rollout undo deployment kubia --to-revision=l\n\tdeployment.apps/kubia rolled back\n```\n> 旧版本的 ReplicaSet 过多会导致 ReplicaSet 列表过于混乱，可以通过指定Deployment 的 re visionHistoryLimit 属性来限制历史版本数量。默认值是 2\n\n```shell\n\t$ kubectl rollout history deployment kubia\n\tdeployment.apps/kubia\n\tREVISION  CHANGE-CAUSE\n\t2         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n\t3         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n\t4         kubectl create --filename=kubia-deployment-v1.yaml --record=true\n```\n\n### 滚动升级速率\n在 Deployment 的滚动升级期间，有两个属性会决定一次替换多少个pod: maxSurge 和 maxUnavailable。可以通过 Deployment 的 strategy 字 段下rollingUpdate 的子属性来配置.\n\n```xml\n\tspec:\n\t  strategy:\n\t    rollingUpdate:\n\t      maxSurge· 1\n\t      maxUnavailable: 0\n\t    type: RollingUpdate\n```\n\n> * maxSurge: 决定了 Deployment 配置中期望的副本数之外，最多允许超出的 pod 实例的数量。默认值为 25%，所以 pod 实例最多可以比期望数量多25%. 这个值也可以不是百分数而是绝对值(例如，可以允许最多多出一个成两个pod).\n> * maxUnavailable: 决定了在滚动升级期间 ，相对于期望副本数能够允许有多少 pod 实例处于不可用状态。默认值也是25%, 所以可用 pod 实例的数量不能低于期望副本数的75%. 与 maxSurge 一样，也可以指定绝对值而不是百分比.\n\n### 暂停滚动升级\n\n```shell\n\t$ kubectl set image deployment kubia nodejs=luksa/kubia:v4\n\tdeployment \"kubia\" image updated\n\t$ kubectl rollout pause deployment kubia\n\tdeployment \"kubia\" paused\n```\n\n### 恢复滚动升级\n如果部署被暂停， 那么在恢复部署之前， 撤销命令不会撤销它\n\n```shell\n\t$ kubectl rollout resume deployment kubia\n\tdeployment \"kubia\" resumed\n```\n\n### readinessProbe\nkubia-deployment-v3-with-readinesscheck.yaml\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: kubia\n\tspec:\n\t  replicas: 3\t\t\t\t\t// POD副本个数为3\n\t  minReadySeconds: 10\t\t\t// 设置minReadySeconds 值为10s,指定新创建的pod至少要成功运行多久之后，才能将其视为可用.\n\t  progressDeadlineSeconds: 120\t// 设置升级失败超时时间，超过则自动停止升级，如下describe deployment 可查看到\"Progressing False...\"信息，需要手动undo来取消升级\n\t  strategy:\n\t    rollingUpdate:\n\t      maxSurge: 1\n\t      maxUnavailable: 0\t\t\t// 设置maxUnavailable的 中值为0来确保升级过程 pod被挨个替换\n\t    type: RollingUpdate\n\t  selector:\n\t    matchLabels:\n\t      app: kubia\n\t  template:\n\t    metadata:\n\t      name: kubia\n\t      labels:\n\t        app: kubia\n\t    spec:\n\t      containers:\n\t      - image: luksa/kubia:v3\t// 修改image\n\t        name: nodejs\n\t        readinessProbe:\n\t          periodSeconds: 1\t\t// 定义一个就绪探针，并且每隔—秒钟执行一次\n\t          httpGet:\t\t\t\t// 就绪探针会执行发送HTTP GET请求到容器\n\t            path: /\n\t            port: 8080\n```\n\n直接使用kubectl apply来升级Deployment\napply命令可以用YAML 文件中声明的字段来更新Deployment。不仅更新镜像，而且还添加了就绪探针， 以及在 YAML 中添加或修改的其他声明。 \n如果新的 YAML也包含rep巨 cas字段， 当它与现有Deployment中的数量不一致时， 那么apply 操作也会对Deployment进行扩容.\n\n```shell\n\t$ kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml\n\tdeployment \"kubia\" configured\n```\n\n### 取消滚动升级\n默认情况下， 在10分钟内不能完成滚动升级的话， 将被视为失败。 如果运行kubectl describe deployment命令， 将会显示一条ProgressDeadlineExceeded的记录.\n判定Deployment滚动升级失败的超时时间， 可以通过设定Deployment spec中的progressDeadlineSeconds来指定.\n如果达到了progressDeadlineSeconds指定的时间， 则滚动升级过程会自动取消.\n\n```shell\n\t$ kubectl describe deployment kubia\n\tConditions:\n\t  Type           Status  Reason\n\t  ----           ------  ------\n\t  Available      True    MinimumReplicasAvailable\n\t  Progressing    True    ReplicaSetUpdated\n\t......\n```\n过了3，4分钟后再次执行\n\n```shell\n\t$ kubectl describe deployment kubia\n\tName:                   kubia\n\t......\n\tConditions:\n\t  Type           Status  Reason\n\t  ----           ------  ------\n\t  Available      True    MinimumReplicasAvailable\n\t  Progressing    False   ProgressDeadlineExceeded\n\tOldReplicaSets:  kubia-59d857b444 (3/3 replicas created)\n\tNewReplicaSet:   kubia-7d6c89d47b (1/1 replicas created)\n\t......\n```\n\n因为滚动升级过程不再继续， 所以只能通过rollout undo命令来取消滚动升级, 其实就是回滚上一个版本.\n\n```shell\n\t$ kubectl rollout undo deployment kubia\n\tdeployment.apps/kubia rolled back\n```\n\n### 蓝绿部署\n可以通过修改Service下的不同version值，选择应用不同版本的Pod来提供服务.\n新版本运行起来一段时间没问题后才可以删除旧版本.\n一般保持有两个版本的应用也就是Pod保持运行状态, 只有下个版本部署一段时间没问题后，第一个版本的可以删除.\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: web-rollingupdate\n\t  namespace: dev\n\tspec:\n\t  strategy:\n\t    rollingUpdate:\t\t\t// 滚动更新, 不设置的话都会设成跟下面一样的默认值, $kubectl get deployment Name -o yaml查看\n\t      maxSurge: 25%\t\t\t// 可以最大超出实力数的百分比，4个replicas，就是每次最多多启动1个实例\n\t      maxUnavailable: 25%\t// 不可用实力百分比，4个replicas，就是每次必须有3个实例可用\n\t    type: RollingUpdate\n\t  selector:\n\t    matchLabels:\n\t      app: web-rollingupdate\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: web-bluegreen\n\t        version: v2.0\t\t// 蓝绿部署，与下面的version一致，部署不同版本的应用\n\t    spec:\n\t......\n\t---\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: web-bluegreen\n\t  namespace: dev\n\tspec:\n\t  ports:\n\t  - ports: 80\n\t    protocol: TCP\n\t    targetPort: 8080\n\t  selector:\n\t    app: web-bluegreen\n\t    version: v2.0\t\t\t// 与上面的version一致，部署不同版本应用, 下次部署单独抽离Service，选择不同版本version就可以了\n\t  type: ClusterIP\n\t---\n\t......\n```\n\n### 金丝雀发布\n> 在蓝绿部署的基础之上，去掉version, 修改selector就是金丝雀部署.\n> 一个新的 pod 会被创建， 与此同时所有旧的 pod 还在运行。 一旦新的 pod 成功运行， 服务的一部分请求将被切换到新的 pod。 这样相当于 运行了一个金丝雀版本。金丝雀发布是一种可以将应用程序的出错版本和其影响到的用户的风险化为最小的技术.\n> 与其直接向每个用户发布新版本， 不如用新版本替换 一个或一小部分的 pod。通过 这种方式， 在升级的初期只有少数用户会访问新版本。 验证新版本是否正常工作之后， 可以将剩余的 pod 继续升级或者回滚到上一个的版本\n\n### 删除deployment\n删除deployment会连带删除创建的replicaset和由replicaset创建的pod\n\n```shell\n\t$ kubectl delete deployment kubia\n\tdeployment.apps \"kubia\" deleted\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"08 Kubernetes samples and problems","url":"/2021/03/13/microService/kubernetes/08_kubernetes_samples_problems/","content":"\n## 01samples\nReference link:\nhttps://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/\n\n1. 填写yaml文件\n\n```shell\n\t$ touch test.yaml\n```\n\n添加如下内容\n\n```xml\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\tlabels:\n\t\tapp.kubernetes.io/name: load-balancer-example\n\tname: hello-world\n\tspec:\n\treplicas: 5\n\tselector:\n\t\tmatchLabels:\n\t\tapp.kubernetes.io/name: load-balancer-example\n\ttemplate:\n\t\tmetadata:\n\t\tlabels:\n\t\t\tapp.kubernetes.io/name: load-balancer-example\n\t\tspec:\n\t\tcontainers:\n\t\t- image: gcr.io/google-samples/node-hello:1.0\n\t\t\tname: hello-world\n\t\t\tports:\n\t\t\t- containerPort: 8080\n```\n\n2. 执行yaml生成POD\n\n```shell\n\t$ kubectl apply -f test.yaml\n```\n\n3. Display information about the Deployment:\n\n```shell\n\t$ kubectl get deployments hello-world\n\t$ kubectl describe deployments hello-world\n```\n\n4. Display information about your ReplicaSet objects:\n\n```shell\n\t$ kubectl get replicasets\n\t$ kubectl describe replicasets\n```\n\n5. Create a Service object that exposes the deployment:\n\n```shell\n\t$ kubectl expose deployment hello-world --type=LoadBalancer --name=my-service\n```\n\n6. Display information about the Service:\n\n```shell\n\t$ kubectl get services my-service\n\tNAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE\n\tmy-service   LoadBalancer   10.103.40.210   <pending>     8080:30972/TCP   16m\n```\n\n7. Display detailed information about the Service:\n\n```shell\n\t$ kubectl describe services my-service\n\tName:                     my-service\n\tNamespace:                default\n\tLabels:                   app.kubernetes.io/name=load-balancer-example\n\tAnnotations:              <none>\n\tSelector:                 app.kubernetes.io/name=load-balancer-example\n\tType:                     LoadBalancer\n\tIP:                       10.103.40.210\n\tPort:                     <unset>  8080/TCP\n\tTargetPort:               8080/TCP\n\tNodePort:                 <unset>  30972/TCP\n\tEndpoints:                10.44.0.2:8080,10.44.0.3:8080,10.44.0.4:8080 + 2 more...\n\tSession Affinity:         None\n\tExternal Traffic Policy:  Cluster\n\tEvents:                   <none>\n```\n8. 通过 NODE-IP + NodePort 访问\n如自己cubic中的开发机IP是10.239.140.186\n * 第一种访问方式:\n\n```shell\n\t$ curl http://10.239.140.186:30972\n\tHello Kubernetes!\n```\n\n * 第二种访问方式:\n\n```\n浏览器输入http://10.239.140.186:30972 也能正常显示 Hello Kubernetes!\n```\n但是公司lab实验室的开发机同样用上面方式部署之后，无法通过 NODE-IP + NodePort 访问， 问题估计跟网络有关\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"09 Kubernetes Certificates","url":"/2021/03/13/microService/kubernetes/09_kubernetes_certificate/","content":"\nReference:\n最新版本:  https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/\n中文版本:  https://kubernetes.io/zh/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/\n1.19版本: https://v1-19.docs.kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/\n\n## K8s Certificates\n\n !()[k8s_certificates.PNG]\n\n## 证书颁发\n\n1. 自签证书, 多用在内部服务之间, K8s, etcd就是使用自签证书.\n\n2. 权威机构: GeoTrust RSA CA 2018, GTS CA 101, Intel Internal Issuing CA 5A\n例如赛门铁克, 分不同档次, 中档3000元左右, 如www.ctnrs.com, 也有针对所有域名的 *.ctnrs.com, 不过更贵\n\n根证书: 每个浏览器都内置了各个信任的权威机构, 是权威机构颁发的证书浏览https访问路径上的锁显示安全, 否则显示不可信任的网站\n\n自签证书和权威机构都会颁发两个证书:\n-- 1. crt： 数字证书;\n-- 2. key： 私钥;\n将这两证书配置到web服务器就可以了\n\n不管是自签还是权威机构都有CA, 会颁发多个证书出来, \n\n## simulate CA\n![](CA_simulate.PNG)\n\n## 使用cfssl或openssl生成自签证书\nhttps://kubernetes.io/zh/docs/concepts/cluster-administration/certificates/\n\ncfssl 在 k8s 中比较流行, 使用json文件传入来生成证书, 要比 openssl 更直观也简单点.\n\n## 查看证书\nkubernetes 1.19版本\n```\nkubeadm alpha certs check-expiration\n```\n\n\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"k8s command, args, entrypoint, cmd 区别","url":"/2021/03/13/microService/kubernetes/k8s_command_args_entrypoint_cmd区别/","content":"\n## **Dockerfile 中ENTRYPOINT,CMD的比较**\n### ENTRYPOINT 的两种格式：\n\n```\n\tENTRYPOINT [\"executable\", \"param1\", \"param2\"] (exec格式，推荐)\n\tENTRYPOINT command param1 param2 (shell 格式)\n```\n\n### CMD指令有三种格式：\n\n```\n\tCMD [\"executable\",\"param1\",\"param2\"] (exec 格式，推荐)\n\tCMD command param1 param2 (shell 格式)\n```\n\n**注意：**\n * Dockerfile 中多个CMD 最后一个生效\n * shell和exec格式的区别，只有shell形式才会获取相关环境变量（这里环境变量指例如:$HOME）\n * Docker run CMD 会覆盖 Dockerfile 中的 CMD\n\n## **k8s yaml 中command、args的比较**\n命令和参数说明：\n\n`command`、`args`两项实现覆盖Dockerfile中`ENTRYPOINT`的功能\n\n具体的command命令代替ENTRYPOINT的命令行，args代表集体的参数.\n\n1. 如果command和args均没有写，那么用Dockerfile的配置。\n2. 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command（不带任何参数，当然command中可自带参数）。\n3. 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令行会被执行，并且将args中填写的参数追加到ENTRYPOINT中。\n比如:\n\nDockerfile：\n\n```xml\n\tFROM alpine:latest\n\tCOPY \"executable_file\" /\n\tENTRYPOINT [ \"./executable_file\" ]\n```\n\nKubernetes yaml文件：\n\n```xml\n\tspec:\n\t   containers:\n\t     - name: container_name\n\t       image: image_name\n\t       args: [\"arg1\",\"arg2\",\"arg3\"]\n```\n\n4. 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数。比如：\n\n\n```xml\n\tcommand：/test.sh,p1,p2\n\t\n\targs: p3,p4\n```\n**另：** 多命令执行使用`sh,-c,[command;command,...]`的形式，单条命令的参数填写在具体的command里面，例如：\n\n```xml\n\tcommand：[\"/bin/sh\", \"-c\", \"echo '123'; ./test.sh,p1,p2,p3,p4\"]\n```\nargs: 不用填\n\n\n\n","tags":["istio"],"categories":["microService","kubernetes"]},{"title":"10 Kubernetes plugin","url":"/2021/03/13/microService/kubernetes/10_kubernetes_plugin/","content":"\n## K8s plugin\n\n官网: https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/\nk8s安装插件原理简单来说是kubectl会在linux的$PATH路径里查找以`kubectl-`开头的可执行文件,都认为是kubectl的插件.  \n因此想要做自定义k8s插件需要在文件名前加上`kubectl-`.  \n\n```shell\n\t// 查看kubectl版本\n\t$ kubectl --version\n\tClient Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:30:33Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\tServer Version: version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", GitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", BuildDate:\"2020-08-26T14:23:04Z\", GoVersion:\"go1.15\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\t// 查看kubectl所有插件, 看到出错信息, 意思是kubectl在$PATH路径里没找到kubectl-开头的可执行文件\n\t// 最后也没有发现/root/bin/目录存在, 因此报错\n\t$ kubectl plugin list\n\tUnable read directory \"/root/bin\" from your PATH: open /root/bin: no such file or directory. Skipping...\n\terror: unable to find any kubectl plugins in your PATH\n\n\t// 创建/root/bin目录, 或者直接在/usr/local/bin等目录创建kubectl插件都可以\n\t$ mkdir /root/bin\n\t\n\t$ cat << EOF >> /root/bin/kubectl-foo\n\t#!/bin/bash\n\t# optional argument handling\n\tif [[ \"$1\" == \"version\" ]]\n\tthen\n\t    echo \"1.0.0\"\n\t    exit 0\n\tfi\n\t# optional argument handling\n\tif [[ \"$1\" == \"config\" ]]\n\tthen\n\t    echo \"$KUBECONFIG\"\n\t    exit 0\n\tfi\n\techo \"I am a plugin named kubectl-foo\"\n\tEOF\n\t\n\t// 增加可执行权限\n\t$ chmod +x /root/bin/kubectl-foo\n```\n\n简单执行k8s自定义的插件\n\n```shell\n\t$ kubectl foo\n\tI am a plugin named kubectl-foo\n\t\n\t$ kubectl foo version\n\t1.0.0\n\t\n\t$ kubectl foo config\n\t/etc/kubernetes/admin.conf\n```\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"使用端口转发(Port Forwarding)来访问集群中的应用","url":"/2021/03/13/microService/kubernetes/k8s_port_forward/","content":"\n## port forward\nreference: https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#port-forward\n\n<!-- more -->\n\nUsage： \n```\n$ kubectl port-forward TYPE/NAME [options] [LOCAL_PORT:]REMOTE_PORT [...[LOCAL_PORT_N:]REMOTE_PORT_N]\n```\nFlags\n\n| Name | Shorthand| Default | Usage |\n| :---: | :---: | :--- | :--- |\n| address | <div style=\"width: 70pt\"></div> | <div style=\"width: 60pt\">[localhost]</div> | Addresses to listen on (comma separated).<br/> Only accepts IP addresses or localhost as a value<br/>. When localhost is supplied, kubectl will try to bind on both 127.0.0.1 and ::1 and will fail if neither of these addresses are available to bind. |\n| pod-running-timeout | | 1m0s | The length of time (like 5s, 2m, or 3h, higher than zero) to wait until at least one pod is running |\n\n\n**Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod**\n```\nkubectl port-forward pod/mypod 5000 6000\n```\n\n**Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the deployment**\n```\nkubectl port-forward deployment/mydeployment 5000 6000\n```\n\n**Listen on port 8443 locally, forwarding to the targetPort of the service's port named \"https\" in a pod selected by the service**\n```\nkubectl port-forward service/myservice 8443:https\n```\n\n**Listen on port 8888 locally, forwarding to 5000 in the pod**\n```\nkubectl port-forward pod/mypod 8888:5000\n```\n\n**Listen on port 8888 on all addresses, forwarding to 5000 in the pod**\n```\nkubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000\n```\n\n**Listen on port 8888 on localhost and selected IP, forwarding to 5000 in the pod**\n```\nkubectl port-forward --address localhost,10.19.21.23 pod/mypod 8888:5000\n```\n\n**`Note:`** 如果您没有明确指定 hostIP 和 protocol，Kubernetes 将使用 0.0.0.0 作为默认 hostIP 和 TCP 作为默认 protocol。\n\n**Listen on a random port locally, forwarding to 5000 in the pod**\n```\nkubectl port-forward pod/mypod :5000\n```\n\n\n## 实例\nreference: https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\n\nCreate a Deployment that runs Redis:\n\n```\nkubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml\n```\n\n转发一个本地端口到 pod 端口\n从 Kubernetes v1.10 开始，kubectl port-forward 允许使用资源名称 （例如 pod 名称）来选择匹配的 pod 来进行端口转发。\n\n```\nkubectl port-forward redis-master-765d459796-258hz 7000:6379 \n```\n这相当于\n\n```\nkubectl port-forward pods/redis-master-765d459796-258hz 7000:6379\n```\n或者\n\n```\nkubectl port-forward deployment/redis-master 7000:6379 \n```\n或者\n\n```\nkubectl port-forward rs/redis-master 7000:6379\n```\n或者\n\n```\nkubectl port-forward svc/redis-master 7000:redis\n```\n以上所有命令都应该有效。输出应该类似于：\n\n```\nI0710 14:43:38.274550    3655 portforward.go:225] Forwarding from 127.0.0.1:7000 -> 6379\nI0710 14:43:38.274797    3655 portforward.go:225] Forwarding from [::1]:7000 -> 6379\n```\n\n与本地 7000 端口建立的连接将转发到运行 Redis 服务器的 pod 的 6379 端口。 通过此连接，您可以使用本地工作站来调试在 pod 中运行的数据库。\n\n警告： 由于已知的限制，目前的端口转发仅适用于 TCP 协议。 在 issue 47862 中正在跟踪对 UDP 协议的支持。\n\nOptionally let kubectl choose the local port:\n```\nkubectl port-forward deployment/redis-master :6379\n```\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"k8s configmap","url":"/2021/03/13/microService/kubernetes/k8s_configmap/","content":"\n## configmap\n\n<!-- overview -->\n\nConfigMap 并不提供保密或者加密功能。\n如果你想存储的数据是机密的，请使用 [secret](https://kubernetes.io/zh/docs/concepts/configuration/secret/),\n或者使用其他第三方工具来保证你的数据的私密性，而不是用 ConfigMap。\n\n<!-- more -->\n\n## 创建configmap\n查看宿主机配置文件\n\n```shell\n\t$ ls ./configmap/   //包含两个配置文件\n\tapplication.properties  ui.properties\n\n\t$ vim ./configmap/ui.properties // 配置文件里都是键值对\n\tcolor.good=purple\n\tcolor.bad=yellow\n\tallow.textmode=true\n\thow.nice.to.look=fairlyNice\n```\n\n1. 将宿主机`./configmap/`目录下的所有配置文件打包到configmap中.\n\n\n```shell\n\tkubectl create configmap myconfigmap -n test --from-file=./configmap/\n```\n\n## configmap在pod中使用\npod.yaml\n\n```yaml\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  namespace: test\n\t  name: test-configmap\n\tspec:\n\t  containers:\n\t    - name: test-configmap\n\t      image: k8s.gcr.io/busybox\n\t      command: [\"/bin/sh\", \"-c\", \"ls /etc/config\"]\n\t      //command: [\"/bin/sh\", \"-c\", \"for ((i=0; i<100; i++)); do  cat /etc/config/ui.properties | head -n 1; sleep 1; done\"]\n\t      volumeMounts:\n\t      - name: config-volume\n\t        mountPath: /etc/config\n\t  volumes:\n\t    - name: config-volume\n\t      configMap:\n\t        name: myconfigmap\n\t  restartPolicy: Never\n```\n\n查看日志\n\n```shell\n\t$ kubectl logs po/test-configmap -n test\n\tapplication.properties\n\tui.properties\n```\n\n发现在pod的/etc/config目录有两个配置文件可供pod里进程读取\n\n## 运行时修改configmap内容\n修改 ui.properties 在configmap中的key和value值\n\n```shell\n\t$ kubectl edit configmap myconfigmap -n test\n\tcolor.good=purple --改为--> color.favorite=blue\n```\n登陆pod查看mount到pod 路径/etc/config/ui.properties文件中的值变化\n**`NOTE: `** 修改configmap中键值对后, pod中对应mount的内容会`过一会才会变化`.\n\n```shell\n\t$ kubectl exec po/test-configmap -n test -it -- /bin/sh\n\t// 等一会时间再查看\n\t$ cat /etc/config/ui.properties\n\tcolor.favorite=blue\n\tcolor.bad=yellow\n\tallow.textmode=true\n\thow.nice.to.look=fairlyNice\n```\n\n## 不可变更的 ConfigMap\n\nKubernetes Beta 特性 _不可变更的 Secret 和 ConfigMap 提供了一种将各个\nSecret 和 ConfigMap 设置为不可变更的选项。对于大量使用 ConfigMap 的\n集群（至少有数万个各不相同的 ConfigMap 给 Pod 挂载）而言，禁止更改\nConfigMap 的数据有以下好处：\n\n- 保护应用，使之免受意外（不想要的）更新所带来的负面影响。\n- 通过大幅降低对 kube-apiserver 的压力提升集群性能，这是因为系统会关闭\n  对已标记为不可变更的 ConfigMap 的监视操作。\n\n此功能特性由 `ImmutableEphemeralVolumes`\n[特性门控](/zh/docs/reference/command-line-tools-reference/feature-gates/)\n来控制。你可以通过将 `immutable` 字段设置为 `true` 创建不可变更的 ConfigMap。\n例如：\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  ...\ndata:\n  ...\nimmutable: true\n```\n\n一旦某 ConfigMap 被标记为不可变更，则 _无法_ 逆转这一变化，，也无法更改\n`data` 或 `binaryData` 字段的内容。你只能删除并重建 ConfigMap。\n因为现有的 Pod 会维护一个对已删除的 ConfigMap 的挂载点，建议重新创建\n这些 Pods。\n\n\n\n\n","tags":["istio"],"categories":["microService","kubernetes"]},{"title":"k8s_对象名称和IDs","url":"/2021/03/13/microService/kubernetes/k8s_对象名称和IDs/","content":"\n## 对象名称和 IDs\n\n集群中的每一个对象都有一个名称 来标识在同类资源中的唯一性。\n\n<!-- more -->\n\n每个 Kubernetes 对象也有一个UID 来标识在整个集群中的唯一性。\n\n比如，在同一个名字空间 中有一个名为 `myapp-1234` 的 Pod, 但是可以命名一个 Pod 和一个 Deployment 同为 `myapp-1234`.\n\n对于用户提供的非唯一性的属性，Kubernetes 提供了 标签（Labels）和 注解（Annotation）机制。\n\n## 名称\n\n客户端提供的字符串，引用资源 url 中的对象，如/api/v1/pods/some name。\n\n某一时刻，只能有一个给定类型的对象具有给定的名称。但是，如果删除该对象，则可以创建同名的新对象。\n\n以下是比较常见的三种资源命名约束。\n\n### DNS 子域名\n\n很多资源类型需要可以用作 DNS 子域名的名称。 DNS 子域名的定义可参见 RFC 1123。 这一要求意味着名称必须满足如下规则：\n\n * 不能超过253个字符\n * 只能包含字母数字，以及'-' 和 '.'\n * 须以字母数字开头\n * 须以字母数字结尾\n\n### DNS 标签名 \n\n某些资源类型需要其名称遵循 RFC 1123 所定义的 DNS 标签标准。也就是命名必须满足如下规则：\n\n * 最多63个字符\n * 只能包含字母数字，以及'-'\n * 须以字母数字开头\n * 须以字母数字结尾\n\n### 路径分段名称\n某些资源类型要求名称能被安全地用作路径中的片段。 换句话说，其名称不能是 `.` 、`..`，也不可以包含 `/` 或 `%` 这些字符。\n\n下面是一个名为nginx-demo的 Pod 的配置清单：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-demo\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.14.2\n    ports:\n    - containerPort: 80\n```\n\n说明： 某些资源类型可能具有额外的命名约束。\n\n\n## UIDs\n\nKubernetes 系统生成的字符串，唯一标识对象。\n\n在 Kubernetes 集群的整个生命周期中创建的每个对象都有一个不同的 uid，它旨在区分类似实体的历史事件。\n\nKubernetes UIDs 是全局唯一标识符（也叫 UUIDs）。 UUIDs 是标准化的，见 ISO/IEC 9834-8 和 ITU-T X.667.\n\n\n","tags":["istio"],"categories":["microService","kubernetes"]},{"title":"k8s resources和驱逐策略","url":"/2021/03/13/microService/kubernetes/k8s_resources和驱逐策略/","content":"\n## Resources\n查看某台机器的资源\n\n```shell\n\t$ kubectl describe node hci-node01\n```\n\n### memory, CPU\nrequests, limits关键字.\n当没有哪个Node节点满足requests的资源时候，container会一直处于pending状态, 而不会受limits资源影响.\n如Node只有10GB内存，一个container request memory配置7GB内存大小，那么其余container只能分配剩余的3GB内存大小.\n配置K8s的container文件\n\n```xml\n\tapiVersion: v1\n\t......\n\tspec:\n\t  containers:\n\t  - name: web-demo\n\t    image: hub.**\n\t    ports:\n\t    - containerPort: 8080\n\t    resources:\n\t      requests:\n\t        memory: 100Mi\t\t// Ki: KB大小, Mi: MB大小, Gi: GB大小\n\t        cpu: 100m\t\t\t// 如机器有12个内核, 而1个cpu内核=1000m, 100m为0.1个cpu内核, milli core, 千分之一core\n\t      limits:\n\t        memory: 1000Mi\n\t        cpu: 200m\n```\n\n登陆container运行的机器，查看容器设置参数\n\n```shell\n\t$ docker inspect containerID\n\t......\n\t\"CpuShares\" 102,\t\t// 100m转为为0.1内核，再用0.1 * 1024 = 102，当很多个容器运行在某台机器上发生CPU资源抢占时候用该值作为权重进行分配\n\t\"Memory\": 1048576000,\t// 对应K8s资源文件的limits->memory值：1000MB大小\n\t......\n\t\"CPUPeriod\": 100000,\t// docker 默认值, 单位是ns, 转化为毫秒是100\n\t\"CpuQuota\": 20000,\t\t// 0.2 core * 100000 = 20000, 与上面一起使用，表示100ms内最多分配给容器的cpu量是0.2个core\n```\n\n当容器里进程所用内存超过requests值时, 占用资源最多的进程会被kill掉, 但是容器不会退出重启.\n而CPU超出limits值时进程不会退出，因为CPU是可压缩资源，而内存不是.\n\n```shell\n\t$ docker stats ContainerID\t\t\t// 查看container CPU和内存使用情况\n```\n\n进入容器通过以下command命令模拟多个后台进程占用CPU\n\n```shell\n\t$ dd if=/dev/zero of=/dev/null &\t//\n\t$ killall dd\t\t\t\t\t\t// 删除所有dd命令\n```\n一般情况都是设置requests == limits\n\n### LimitRange\n在一个namespace下对Pod和container申请的CPU和memory资源进行限制\n\n```xml\n\tapiVersion: v1\n\tkind: LimitRange\n\tmetadata:\n\t  name: test-limits\n\tspec: \n\t  limits:\n\t  - max:\n\t      cpu: 4000m\n\t      memory: 2Gi\n\t    min:\n\t      cpu: 100m\n\t      memory: 100Mi\n\t    maxLimitRequestRatio:\n\t      cpu: 3\t\t\t\t// 对POD显示，运行起来后占用机器cpu资源数是在limits.cpu最大以内且不能超过requests.cpu的3倍.\n\t      memory: 2\n\t    type: Pod\t\t\t\t// 上面限制类型为Pod\n\t  - default:\n\t      cpu: 300m\t\t\t\t// 默认Container 0.3个内核\n\t      memory: 200Mi\n\t    defaultRequest:\n\t      cpu: 200m\n\t      memory: 100Mi\n\t    max:\n\t      cpu: 2000m\n\t      memory: 1Gi\n\t    min:\n\t      cpu: 100m\n\t      memory: 100Mi\n\t    maxLimitRequestRatio:\n\t      cpu: 5\n\t      memory: 4\n\t    type: Container\t\t\t// 上面显示类型为Container\n```\n\n运行如下命令查看limits限制\n\n```shell\n\t$ kubectl describe limits -n namespaces\n```\n\n> 当新建deployment或RS或Pod时，会根据LimitRange限制检测新建的Pod或Container是否满足条件，否则创建失败, 可以通过kubectl get ** -o yaml查看事件message.\n\n### ResourceQuota\n对每个namespace能够使用的资源进行限制.\n\n```xml\n\tapiVersion: v1\n\tkind: ResourceQuota\n\tmetadata:\n\t  name: resource-quota\n\tspec:\n\t  hard:\n\t    pods: 4\n\t    requests.cpu: 2000m\n\t    requests.memory: 4Gi\n\t    limits.cpu: 4000m\n\t    limits.memory: 8Gi\n\t    configmaps: 10\n\t    persistentvolumeclaims: 4\n\t    replicationcontrollers: 20\n\t    secrets: 10\n\t    services: 10\n```\n\n查看quota类型资源\n\n```shell\n\t$ kubectl get quota -n NS\n\t$ kubectl describe quota *** -n NS\t\t// 查看硬件资源是否使用饱和\n```\n\n### Eviction-Pod驱逐\n常见驱逐策略配置\n * 在1m30s时间内可利用内存持续小于1.5GB时进行驱逐\n\n```\n\t--eviction-soft=memory.available<1.5Gi\n\t--eviction-soft-grace-period=memory.available=1m30s\n```\n\n * 当可利用内存小于100MB或磁盘小于1GB或剩余的节点小于5%立即执行驱逐策略\n\n\n```\n\t--eviction-hard=memory.available<100Mi,nodefs.available<1Gi,nodefs.inodesFree<5%\n```\n\n驱逐策略:\n磁盘紧缺时候:\n * 删除死掉的Pod和容器\n * 删除没用的image\n * 按优先级和资源占用情况驱逐Pod\n\n\n内存紧缺时候:\n * 驱逐不可靠的Pod(没有设置requests和limits资源的pod都是不可靠的Pod)\n * 驱逐基本可靠的pod(实际使用资源大于requests资源是基本可靠,超出的越多优先删除，否则删除占用内存最多的Pod)\n * 驱逐可靠的Pod(requests资源==limits资源的Pod为可靠的Pod, 如果都一样驱逐策略跟上面驱逐基本可靠Pod一致)\n\n\n### label\nkey=value 贴标签(Node, Deployment, Service, Pod)\n\n```xml\n\t#deploy\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: web-demo\n\t  namespace: dev\n\tspec:\n\t  selector:\n\t    matchLabels:\t\t\t\t// 选择含有指定标签的Pod,必须跟下面一致，或者不写只配置下面Pod的label也可以\n\t      app: web-demo\n\t    matchExpressions:\n\t      - {key: group, operator: In, values: [dev, test]}\t\t// Key in values的Pod都会被选中\n\t  replicas: 1\n\t  template:\t\t\t\t\t\t// 设置Pod模板\n\t    metadata:\n\t      labels:\t\t\t\t\t// 设置Pod标签\n\t        app: web-demo\n\t        group: dev\n\t    spec:\n\t      containers:\n\t      - name: web-demo\n\t        image: hub**\n\t        ports:\n\t        - containerPort: 8080\n\t      nodeSelector:\t\t\t\t// 选择含有某个标签的Node机器部署Pod\n\t        disktype: ssd\n\t---\n\t# service\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: web-demo\n\t  namespace: dev\n\tspec:\n\t  ports:\n\t  - port: 80\n\t    protocol: TCP\n\t    targetPort: 8080\n\t  selector:\n\t    app: web-demo\n\t  type: ClusterIP\n```\n\n查询Pod\n\n```shell\n\t$ kubectl get pods -l \"group in (dev, test)\" -n NS\n\t$ kubectl get pods -l \"group notin (dev)\" -n NS\n\t$ kubectl get pods  -l group=dev,group=test -n NS\n```\n\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"k8s node 节点reset后重新加入集群","url":"/2021/03/13/microService/kubernetes/k8s_node节点reset后重新加入集群/","content":"\n## node节点(重新)加入集群\n加入新的node节点或在原来node节点不小心执行 `kubectl reset` 命令后都可用下面命令重新加入集群.  \n\n登陆master机器, 重新获取Token\n\n```shell\n# 添加新机器的IP和域名\nvim /etc/hosts\n  ......\n  <IP> <域名>\n\n# 添加新机器的IP和域名到 NO_PROXY\nexport NO_PROXY=$NO_PROXY,<IP>,<域名>\n\n# 获取token\nkubeadm token create --print-join-command\n  kubeadm join 192.168.180.121:6443 --token p08kmc.nci5h0xfmlcw92vg     --discovery-token-ca-cert-hash sha256:44315d59e08f4d94bc75d20730b861818dfeda6517c1b228399f061f4256329b\n```\n\n子节点重新加入\n\n```shell\n# 添加集群其它所有机器的IP和域名\nvim /etc/hosts\n  ......\n  <机器1 IP> <机器1 域名>\n  <机器2 IP> <机器2 域名>\n  ......\n\n# 添加所有机器的IP和域名到 NO_PROXY\nexport NO_PROXY=$NO_PROXY,<机器1 IP>,<机器2 IP>,<机器1 域名>,<机器2 域名>\n\n# 加入集群\nkubeadm join 192.168.180.121:6443 --token p08kmc.nci5h0xfmlcw92vg     --discovery-token-ca-cert-hash sha256:44315d59e08f4d94bc75d20730b861818dfeda6517c1b228399f061f4256329b\n  [preflight] Running pre-flight checks\n          [WARNING IsDockerSystemdCheck]: detected \"cgroupfs\" as the Docker cgroup driver. The recommended driver is \"systemd\". Please follow the guide at https://kubernetes.io/docs/setup/cri/\n  [preflight] Reading configuration from the cluster...\n  [preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'\n  [kubelet-start] Downloading configuration for the kubelet from the \"kubelet-config-1.16\" ConfigMap in the kube-system namespace\n  [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n  [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n  [kubelet-start] Activating the kubelet service\n  [kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n  \n  This node has joined the cluster:\n  * Certificate signing request was sent to apiserver and a response was received.\n  * The Kubelet was informed of the new secure connection details.\n  \n  Run 'kubectl get nodes' on the control-plane to see this node join the cluster.\n```\n\n查看nodes\n\n```shell\nkubectl get nodes\n  NAME         STATUS   ROLES    AGE     VERSION\n  k8s-master   Ready    master   3d4h    v1.16.1\n  k8s-node01   Ready    <none>   2m35s   v1.16.1\n  k8s-node02   Ready    <none>   3d3h    v1.16.1\n```\n\n## 设置污点\nreference link: \n官网: https://kubernetes.io/zh/docs/concepts/scheduling-eviction/taint-and-toleration/\n博客: http://www.mydlq.club/article/69/#wow2\n\n污点内容一般组成为 key、value 及一个 effect 三个元素，表现为：\n```\n<key>=<value>:<effect>\n```\n上面 value 可以设置为空.\n\n设置污点\n``` shell\nkubectl taint node [node] key=value[effect]\n\nkubectl taint nodes node1 key1=value1:NoSchedule\n```\n其中 effect 可取值如下：\n\n * PreferNoSchedule： 尽量不要调度。\n * NoSchedule： 一定不能被调度。\n * NoExecute： 不仅不会调度, 还会驱逐 Node 上已有的 Pod。\n\n查看污点\n``` shell\nkubectl describe nodes node1 | grep Taints\n```\n\n移除污点\n``` shell\nkubectl taint nodes node1 key1=value1:NoSchedule-\n```\n\n## master节点重新加入集群\n\n最好把iptables也清理一下 iptables -F\n\n去到现有的master节点上生成token\n\n### 生成token\n\n```shell\nkubeadm  token create --print-join-command\n  kubeadm join 10.239.140.201:6443 --token 9ks5ps.g0fhcxbzl604k8v0     --discovery-token-ca-cert-hash sha256:2d291498e3c0739c53f33b85c4498fc7ef2ab362926e970671159b4f392d43dc\n```\n\n### 生成key\n\n```shell\nkubeadm init phase upload-certs --upload-certs\n  W0805 14:41:18.070434   16460 version.go:101] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n  W0805 14:41:18.070565   16460 version.go:102] falling back to the local client version: v1.16.2\n  [upload-certs] Storing the certificates in Secret \"kubeadm-certs\" in the \"kube-system\" Namespace\n  [upload-certs] Using certificate key:\n  6314a9877893263374fdf33bedf9a225640a97215784c8c8f387549966d0565d\n```\n\n拿到上述内容之后，拼接；前面的token加上-control-plane –certificate-key ,在要加入的master机器节点上运行，加入集群。\n\n```shell\nkubeadm join 172.31.17.49:9443 --token kjjguy.pmqxvb1nmgf1nq4q --discovery-token-ca-cert-hash sha256:dcadd5b87024c304e5e396ba06d60a4dbf36509a627a6a949c126172e9c61cfb --control-plane --certificate-key 6314a9877893263374fdf33bedf9a225640a97215784c8c8f387549966d0565d\n```\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"k8s secret","url":"/2021/03/13/microService/kubernetes/k8s_secret/","content":"\n## Secret\nSecret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者 容器镜像 中来说更加安全和灵活。 参阅 [Secret 设计文档](https://git.k8s.io/community/contributors/design-proposals/auth/secrets.md) 获取更多详细信息。\n\n<!-- more -->\n\nSecret 是一种包含少量敏感信息例如密码、令牌或密钥的对象。 这样的信息可能会被放在 Pod 规约中或者镜像中。 用户可以创建 Secret，同时系统也创建了一些 Secret.\n\nKubernetes 会验证 Secret 作为卷来源时所给的对象引用确实指向一个类型为 Secret 的对象。因此，Secret 需要先于任何依赖于它的 Pod 创建。\n\nSecret API 对象处于某名字空间 中。它们只能由同一命名空间中的 Pod 引用。\n\n每个 Secret 的 `大小限制为 1MB`。这是为了防止创建非常大的 Secret 导致 API 服务器 和 kubelet 的内存耗尽。然而，创建过多较小的 Secret 也可能耗尽内存。 更全面得限制 Secret 内存用量的功能还在计划中。\n\nkubelet 仅支持从 API 服务器获得的 Pod 使用 Secret。 这包括使用 kubectl 创建的所有 Pod，以及间接通过副本控制器创建的 Pod。 它不包括通过 kubelet --manifest-url 标志，--config 标志或其 REST API 创建的 Pod（这些不是创建 Pod 的常用方法）。\n\n以环境变量形式在 Pod 中使用 Secret 之前必须先创建 Secret，除非该环境变量被标记为可选的。 Pod 中引用不存在的 Secret 时将无法启动。\n\n使用 secretKeyRef 时，如果引用了指定 Secret 不存在的键，对应的 Pod 也无法启动。\n\n对于通过 `envFrom` 填充环境变量的 Secret，如果 Secret 中包含的键名无法作为合法的环境变量名称，对应的键会被跳过，该 Pod 将被允许启动。 不过这时会产生一个事件，其原因为 `nvalidVariableNames`，其消息中包含被跳过的无效键的列表。 下面的示例显示一个 Pod，它引用了包含 2 个无效键 1badkey 和 2alsobad。\n\n```\nkubectl get events\n输出类似于：\nLASTSEEN   FIRSTSEEN   COUNT     NAME            KIND      SUBOBJECT                         TYPE      REASON\n0s         0s          1         dapi-test-pod   Pod                                         Warning   InvalidEnvironmentVariableNames   kubelet, 127.0.0.1      Keys [1badkey, 2alsobad] from the EnvFrom secret default/mysecret were skipped since they are considered invalid environment variable names.\n```\n\n## Secret 与 Pod 生命周期的关系\n\n通过 API 创建 Pod 时，不会检查引用的 Secret 是否存在。一旦 Pod 被调度，kubelet 就会尝试获取该 Secret 的值。如果获取不到该 Secret，或者暂时无法与 API 服务器建立连接， kubelet 将会定期重试。kubelet 将会报告关于 Pod 的事件，并解释它无法启动的原因。 一旦获取到 Secret，kubelet 将创建并挂载一个包含它的卷。在 Pod 的所有卷被挂载之前， Pod 中的容器不会启动。\n\n## Secret 概览\n\n要使用 Secret，Pod 需要引用 Secret。\nPod 可以用三种方式之一来使用 Secret：\n\n- 作为挂载到一个或多个容器上的 [卷](https://kubernetes.io/zh/docs/concepts/storage/volumes/)中的[文件](https://kubernetes.io/zh/docs/concepts/configuration/secret/#using-secrets-as-files-from-a-pod)\n  中的[文件](#在Pod中使用Secret文件)\n- 作为[容器的环境变量](#以环境变量的形式使用Secrets)\n- 由 [kubelet在为Pod拉取镜像时使用](#使用imagePullSecret)\n\nSecret 对象的名称必须是合法的 [DNS 子域名](https://kubernetes.io/zh/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names). 在为创建 Secret 编写配置文件时，你可以设置 `data` 与/或 `stringData` 字段。 `data` 和 `stringData` 字段都是可选的。 `data` 字段中所有键值都必须是 base64 编码的字符串。如果不希望执行这种 base64 字符串的转换操作，你可以选择设置 stringData 字段，其中可以使用任何字符串作为其取值。\n\n## Secret 的类型\n在创建 Secret 对象时，你可以使用 [Secret](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#secret-v1-core) 资源的 `type` 字段，或者与其等价的 `kubectl` 命令行参数（如果有的话）为其设置类型。 Secret 的类型用来帮助编写程序处理 Secret 数据。\n\nKubernetes 提供若干种内置的类型，用于一些常见的使用场景。 针对这些类型，Kubernetes 所执行的合法性检查操作以及对其所实施的限制各不相同。\n\n| 内置类型 | 用法 |\n| :------ | :----- |\n| Opaque | <div style=\"width: 300pt\">用户定义的任意数据</div> |\n| kubernetes.io/service-account-token | 服务账号令牌 |\n| kubernetes.io/dockercfg | ~/.dockercfg 文件的序列化形式 |\n| kubernetes.io/dockerconfigjson | ~/.docker/config.json 文件的序列化形式 |\n| kubernetes.io/basic-auth | 用于基本身份认证的凭据 |\n| kubernetes.io/ssh-auth | 用于 SSH 身份认证的凭据 |\n| kubernetes.io/tls | 用于 TLS 客户端或者服务器端的数据 |\n| bootstrap.kubernetes.io/token | 启动引导令牌数据 |\n\n通过为 Secret 对象的 type 字段设置一个非空的字符串值，你也可以定义并使用自己 Secret 类型。如果 type 值为空字符串，则被视为 Opaque 类型。 Kubernetes 并不对类型的名称作任何限制。不过，如果你要使用内置类型之一， 则你必须满足为该类型所定义的所有要求。\n\n### Opaque Secret\n\n当 Secret 配置文件中未作显式设定时，默认的 Secret 类型是 Opaque。 当你使用 kubectl 来创建一个 Secret 时，你会使用 generic 子命令来标明 要创建的是一个 Opaque 类型 Secret。 例如，下面的命令会创建一个空的 Opaque 类型 Secret 对象：\n\n```\nkubectl create secret generic empty-secret\nkubectl get secret empty-secret\n```\n\n输出类似于\n\n```\nNAME           TYPE     DATA   AGE\nempty-secret   Opaque   0      2m6s\n```\n\nDATA 列显示 Secret 中保存的数据条目个数。 在这个例子种，0 意味着我们刚刚创建了一个空的 Secret。\n\n\n### 服务账号令牌 Secret\nKubernetes 在创建 Pod 时会自动创建一个服务账号 Secret 并自动修改你的 Pod 以使用该 Secret。该服务账号令牌 Secret 中包含了访问 Kubernetes API 所需要的凭据。\n\n如果需要，可以禁止或者重载这种自动创建并使用 API 凭据的操作。 不过，如果你仅仅是希望能够安全地访问 API 服务器，这是建议的工作方式。\n\n参考 [ServiceAccount](https://kubernetes.io/zh/docs/tasks/configure-pod-container/configure-service-account/) 文档了解服务账号的工作原理。你也可以查看 [Pod](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#pod-v1-core) 资源中的 automountServiceAccountToken 和 serviceAccountName 字段文档，了解 从 Pod 中引用服务账号。\n\n\n### Docker 配置 Secret\n\n你可以使用下面两种 `type` 值之一来创建 Secret，用以存放访问 Docker 仓库 来下载镜像的凭据。\n\n * `kubernetes.io/dockercfg`\n * `kubernetes.io/dockerconfigjson`\n\n下面是一个 kubernetes.io/dockercfg 类型 Secret 的示例：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-dockercfg\ntype: kubernetes.io/dockercfg\ndata:\n  .dockercfg: |\n        \"<base64 encoded ~/.dockercfg file>\"\n```\n\n> **说明：**\n> 如果你不希望执行 base64 编码转换，可以使用 stringData 字段代替。\n\n当你使用清单文件来创建这两类 Secret 时，API 服务器会检查 data 字段中是否 存在所期望的主键，并且验证其中所提供的键值是否是合法的 JSON 数据。 不过，API 服务器不会检查 JSON 数据本身是否是一个合法的 Docker 配置文件内容。\n\n```\nkubectl create secret docker-registry secret-tiger-docker \\\n  --docker-username=tiger \\\n  --docker-password=pass113 \\\n  --docker-email=tiger@acme.com\n```\n\n上面的命令创建一个类型为 kubernetes.io/dockerconfigjson 的 Secret。 如果你对 data 字段中的 .dockerconfigjson 内容进行转储，你会得到下面的 JSON 内容，而这一内容是一个合法的 Docker 配置文件。\n\n```\n{\n  \"auths\": {\n    \"https://index.docker.io/v1/\": {\n      \"username\": \"tiger\",\n      \"password\": \"pass113\",\n      \"email\": \"tiger@acme.com\",\n      \"auth\": \"dGlnZXI6cGFzczExMw==\"\n    }\n  }\n}\n```\n\n### 基本身份认证 Secret \n`kubernetes.io/basic-auth` 类型用来存放用于基本身份认证所需的凭据信息。 使用这种 Secret 类型时，Secret 的 data 字段必须包含以下两个键：\n\n * `username`: 用于身份认证的用户名；\n * `password`: 用于身份认证的密码或令牌。\n以上两个键的键值都是 base64 编码的字符串。 当然你也可以在创建 Secret 时使用 `stringData` 字段来提供明文形式的内容。 下面的 YAML 是基本身份认证 Secret 的一个示例清单：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-basic-auth\ntype: kubernetes.io/basic-auth\nstringData:\n  username: admin\n  password: t0p-Secret\n```\n\n提供基本身份认证类型的 Secret 仅仅是出于用户方便性考虑。 你也可以使用 Opaque 类型来保存用于基本身份认证的凭据。 不过，使用内置的 Secret 类型的有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。\n\n### SSH 身份认证 Secret\n\nKubernetes 所提供的内置类型 `kubernetes.io/ssh-auth` 用来存放 `SSH` 身份认证中 所需要的凭据。使用这种 Secret 类型时，你就必须在其 `data` （或 `stringData`） 字段中提供一个 ssh-privatekey 键值对，作为要使用的 SSH 凭据。\n\n下面的 YAML 是一个 SSH 身份认证 Secret 的配置示例：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-ssh-auth\ntype: kubernetes.io/ssh-auth\ndata:\n  # 此例中的实际数据被截断\n  ssh-privatekey: |\n          MIIEpQIBAAKCAQEAulqb/Y ...\n```\n\n提供 SSH 身份认证类型的 Secret 仅仅是出于用户方便性考虑。 你也可以使用 Opaque 类型来保存用于 SSH 身份认证的凭据。 不过，使用内置的 Secret 类型的有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。\n\n> **注意**： SSH 私钥自身无法建立 SSH 客户端与服务器端之间的可信连接。 需要其它方式来建立这种信任关系，以缓解“中间人（Man In The Middle）” 攻击，例如向 ConfigMap 中添加一个 known_hosts 文件。\n\n\n### TLS Secret\n\nKubernetes 提供一种内置的 `kubernetes.io/tls` Secret 类型，用来存放证书 及其相关密钥（通常用在 TLS 场合）。 此类数据主要提供给 Ingress 资源，用以终结 TLS 链接，不过也可以用于其他 资源或者负载。当使用此类型的 Secret 时，Secret 配置中的 `data` （或 `stringData`）字段必须包含 `tls.key` 和 `tls.crt` 主键，尽管 API 服务器 实际上并不会对每个键的取值作进一步的合法性检查。\n\n下面的 YAML 包含一个 TLS Secret 的配置示例：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-tls\ntype: kubernetes.io/tls\ndata:\n  # 此例中的数据被截断\n  tls.crt: |\n        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...\n  tls.key: |\n        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...\n```\n\n提供 TLS 类型的 Secret 仅仅是出于用户方便性考虑。 你也可以使用 Opaque 类型来保存用于 TLS 服务器与/或客户端的凭据。 不过，使用内置的 Secret 类型的有助于对凭据格式进行归一化处理，并且 API 服务器确实会检查 Secret 配置中是否提供了所需要的主键。\n\n当使用 kubectl 来创建 TLS Secret 时，你可以像下面的例子一样使用 tls 子命令：\n\n```\nkubectl create secret tls my-tls-secret \\\n  --cert=path/to/cert/file \\\n  --key=path/to/key/file\n```\n\n这里的公钥/私钥对都必须事先已存在。用于 `--cert` 的公钥证书必须是 `.PEM` 编码的 （Base64 编码的 DER 格式），且与 `--key` 所给定的私钥匹配。 私钥必须是通常所说的 PEM 私钥格式，且未加密。对这两个文件而言，PEM 格式数据 的第一行和最后一行（例如，证书所对应的 `--------BEGIN CERTIFICATE-----` 和 `-------END CERTIFICATE----`）都不会包含在其中。\n\n### 启动引导令牌 Secret\n\n通过将 Secret 的 `type` 设置为 `bootstrap.kubernetes.io/token` 可以创建 启动引导令牌类型的 Secret。这种类型的 Secret 被设计用来支持节点的启动引导过程。 其中包含用来为周知的 ConfigMap 签名的令牌。\n\n启动引导令牌 Secret 通常创建于 `kube-system` 名字空间内，并以 `bootstrap-token-<令牌 ID>` 的形式命名；其中 `<令牌 ID>` 是一个由 6 个字符组成 的字符串，用作令牌的标识。\n\n\n## 创建Secret\n有几种不同的方式来创建 Secret：\n\n * [使用 kubectl 命令创建 Secret](https://kubernetes.io/zh/docs/tasks/configmap-secret/managing-secret-using-kubectl/)\n * [使用配置文件来创建 Secret](https://kubernetes.io/zh/docs/tasks/configmap-secret/managing-secret-using-config-file/)\n * [使用 kustomize 来创建 Secret](https://kubernetes.io/zh/docs/tasks/configmap-secret/managing-secret-using-kustomize/)\n\n## 编辑 Secret\n你可以通过下面的命令编辑现有的 Secret：\n\n```\nkubectl edit secrets mysecret\n```\n这一命令会打开默认的编辑器，允许你更新 data 字段中包含的 base64 编码的 Secret 值\n\n## 使用 Secret\n\nSecret 可以作为数据卷被挂载，或作为[环境变量](https://kubernetes.io/zh/docs/concepts/containers/container-environment/) 暴露出来以供 Pod 中的容器使用。它们也可以被系统的其他部分使用，而不直接暴露在 Pod 内。 例如，它们可以保存凭据，系统的其他部分将用它来代表你与外部系统进行交互。\n\n### 在Pod中使用Secret文件\n\n在 Pod 中使用存放在卷中的 Secret：\n\n1. 创建一个 Secret 或者使用已有的 Secret。多个 Pod 可以引用同一个 Secret。\n2. 修改你的 Pod 定义，在 `spec.volumes[]` 下增加一个卷。可以给这个卷随意命名， 它的 `spec.volumes[].secret.secretName` 必须是 Secret 对象的名字。\n3. 将 `spec.containers[].volumeMounts[]` 加到需要用到该 Secret 的容器中。 指定 `spec.containers[].volumeMounts[].readOnly = true` 和 `spec.containers[].volumeMounts[].mountPath` 为你想要该 Secret 出现的尚未使用的目录。\n4. 修改你的镜像并且／或者命令行，让程序从该目录下寻找文件。 Secret 的 data 映射中的每一个键都对应 `mountPath` 下的一个文件名。\n\n这是一个在 Pod 中使用存放在挂载卷中 Secret 的例子：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret\n```\n\n您想要用的每个 Secret 都需要在 `spec.volumes` 中引用。\n\n如果 Pod 中有多个容器，每个容器都需要自己的 `volumeMounts` 配置块， 但是每个 Secret 只需要一个 `spec.volumes`。\n\n您可以打包多个文件到一个 Secret 中，或者使用的多个 Secret，怎样方便就怎样来。\n\n\n### 将 Secret 键名映射到特定路径\n\n我们还可以控制 Secret 键名在存储卷中映射的的路径。 你可以使用 spec.volumes[].secret.items 字段修改每个键对应的目标路径：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n      readOnly: true\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret\n      items:\n      - key: username\n        path: my-group/my-username\n```\n\n将会发生什么呢：\n\n * `username` Secret 存储在 `/etc/foo/my-group/my-username` 文件中而不是 `/etc/foo/username` 中。\n * `password` Secret 没有被映射\n如果使用了 `spec.volumes[].secret.items`，只有在 `items` 中指定的键会被映射。 要使用 Secret 中所有键，就必须将它们都列在 `items` 字段中。 所有列出的键名必须存在于相应的 Secret 中。否则，不会创建卷\n\n### Secret 文件权限\n你还可以指定 Secret 将拥有的权限模式位。如果不指定，默认使用 0644。 你可以为整个 Secret 卷指定默认模式；如果需要，可以为每个密钥设定重载值。\n\n例如，您可以指定如下默认模式：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret\n      defaultMode: 256\n```\n\n之后，Secret 将被挂载到 /etc/foo 目录，而所有通过该 Secret 卷挂载 所创建的文件的权限都是 0400。\n\n请注意，JSON 规范不支持八进制符号，因此使用 256 值作为 0400 权限。 如果你使用 YAML 而不是 JSON，则可以使用八进制符号以更自然的方式指定权限。\n\n注意，如果你通过 `kubectl exec` 进入到 Pod 中，你需要沿着符号链接来找到 所期望的文件模式。例如，下面命令检查 Secret 文件的访问模式：\n\n```\nkubectl exec mypod -it sh\n\ncd /etc/foo\nls -l\n```\n\n输出类似于：\n\n```\ntotal 0\nlrwxrwxrwx 1 root root 15 May 18 00:18 password -> ..data/password\nlrwxrwxrwx 1 root root 15 May 18 00:18 username -> ..data/username\n```\n\n### 挂载的 Secret 会被自动更新\n\n当已经存储于卷中被使用的 Secret 被更新时，被映射的键也将终将被更新。 组件 kubelet 在周期性同步时检查被挂载的 Secret 是不是最新的。 但是，它会使用其本地缓存的数值作为 Secret 的当前值\n\n> **Note:** 说明： 使用 Secret 作为[子路径卷](https://kubernetes.io/zh/docs/concepts/storage/volumes/#using-subpath)挂载的容器 不会收到 Secret 更新。\n\n\n### 以环境变量的形式使用Secrets\n\n将 Secret 作为 Pod 中的[环境变量](https://kubernetes.io/zh/docs/concepts/containers/container-environment/)使用：\n\n1. 创建一个 Secret 或者使用一个已存在的 Secret。多个 Pod 可以引用同一个 Secret。\n2. 修改 Pod 定义，为每个要使用 Secret 的容器添加对应 Secret 键的环境变量。 使用 Secret 键的环境变量应在 `env[x].valueFrom.secretKeyRef` 中指定 要包含的 Secret 名称和键名。\n3. 更改镜像并／或者命令行，以便程序在指定的环境变量中查找值。\n\n这是一个使用来自环境变量中的 Secret 值的 Pod 示例：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-env-pod\nspec:\n  containers:\n  - name: mycontainer\n    image: redis\n    env:\n      - name: SECRET_USERNAME\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: username\n      - name: SECRET_PASSWORD\n        valueFrom:\n          secretKeyRef:\n            name: mysecret\n            key: password\n  restartPolicy: Never\n```\n\n#### 使用来自环境变量的 Secret 值\n在一个以环境变量形式使用 Secret 的容器中，Secret 键表现为常规的环境变量，其中 包含 Secret 数据的 base-64 解码值。这是从上面的示例在容器内执行的命令的结果：\n\n```\necho $SECRET_USERNAME\n```\n\n**`Secret 更新之后对应的环境变量不会被更新`**\n\n如果某个容器已经在通过环境变量使用某 Secret，对该 Secret 的更新不会被 容器马上看见，除非容器被重启。有一些第三方的解决方案能够在 Secret 发生 变化时触发容器重启。\n\n## 不可更改的 Secret\n\nFEATURE STATE: Kubernetes v1.19 [beta]\n\nKubernetes 的 alpha 特性 不可变的 Secret 和 ConfigMap 提供了一种可选配置， 可以设置各个 Secret 和 ConfigMap 为不可变的。对于大量使用 Secret 的集群（至少有成千上万各不相同的 Secret 供 Pod 挂载）， 禁止变更它们的数据有下列好处：\n\n防止意外（或非预期的）更新导致应用程序中断\n通过将 Secret 标记为不可变来关闭 kube-apiserver 对其的监视，从而显著降低 kube-apiserver 的负载，提升集群性能。\n\n这个特性通过 ImmutableEmphemeralVolumes 特性门控 来控制，从 v1.19 开始默认启用。 你可以通过将 Secret 的 immutable 字段设置为 true 创建不可更改的 Secret。 例如：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  ...\ndata:\n  ...\nimmutable: true\n```\n\n> **说明：**\n> 一旦一个 Secret 或 ConfigMap 被标记为不可更改，撤销此操作或者更改 data 字段的内容都是 不 可能的。 只能删除并重新创建这个 Secret。现有的 Pod 将维持对已删除 Secret 的挂载点 - 建议重新创建这些 Pod。\n\n\n## 使用imagePullSecret\n\n`imagePullSecrets` 字段中包含一个列表，列举对同一名字空间中的 Secret 的引用。 你可以使用 `imagePullSecrets` 将包含 Docker（或其他）镜像仓库密码的 Secret 传递给 kubelet。kubelet 使用此信息来替你的 Pod 拉取私有镜像。 关于 `imagePullSecrets` 字段的更多信息，请参考 [PodSpec API](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.20/#podspec-v1-core) 文档。\n\n\n## 使用案例\n\n### 案例：以环境变量的形式使用 Secret\n\n创建一个 Secret 定义：\n\n```\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  USER_NAME: YWRtaW4=\n  PASSWORD: MWYyZDFlMmU2N2Rm\n```\n\n生成 Secret 对象：\n\n```\nkubectl apply -f mysecret.yaml\n```\n使用 `envFrom` 将 Secret 的所有数据定义为容器的环境变量。 Secret 中的键名称为 Pod 中的环境变量名称：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\nspec:\n  containers:\n    - name: test-container\n      image: k8s.gcr.io/busybox\n      command: [ \"/bin/sh\", \"-c\", \"env\" ]\n      envFrom:\n      - secretRef:\n          name: mysecret\n  restartPolicy: Never\n```\n\n### 案例：包含 SSH 密钥的 Pod\n创建一个包含 SSH 密钥的 Secret：\n\n```\nkubectl create secret generic ssh-key-secret \\\n  --from-file=ssh-privatekey=/path/to/.ssh/id_rsa \\\n  --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub\n```\n输出类似于：\n\n```\nsecret \"ssh-key-secret\" created\n```\n你也可以创建一个带有包含 SSH 密钥的 secretGenerator 字段的 kustomization.yaml 文件。\n\n> **`注意：`** 发送自己的 SSH 密钥之前要仔细思考：集群的其他用户可能有权访问该密钥。 你可以使用一个服务帐户，分享给 Kubernetes 集群中合适的用户，这些用户是你要分享的。 如果服务账号遭到侵犯，可以将其收回。\n\n现在我们可以创建一个 Pod，令其引用包含 SSH 密钥的 Secret，并通过存储卷来使用它：\n\n```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-test-pod\n  labels:\n    name: secret-test\nspec:\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: ssh-key-secret\n  containers:\n  - name: ssh-test-container\n    image: mySshImage\n    volumeMounts:\n    - name: secret-volume\n      readOnly: true\n      mountPath: \"/etc/secret-volume\"\n```\n\n容器中的命令运行时，密钥的片段可以在以下目录找到：\n\n```\n/etc/secret-volume/ssh-publickey\n/etc/secret-volume/ssh-privatekey\n```\n然后容器可以自由使用 Secret 数据建立一个 SSH 连接\n\n### 案例：包含生产/测试凭据的 Pod \n\n下面的例子展示的是两个 Pod。 一个 Pod 使用包含生产环境凭据的 Secret，另一个 Pod 使用包含测试环境凭据的 Secret。\n\n你可以创建一个带有 `secretGenerator` 字段的 `kustomization.yaml` 文件，或者执行 `kubectl create secret`：\n```\nkubectl create secret generic prod-db-secret \\\n  --from-literal=username=produser \\\n  --from-literal=password=Y4nys7f11\n```\n\n```\nkubectl create secret generic test-db-secret \\\n  --from-literal=username=testuser \\\n  --from-literal=password=iluvtests\n```\n\n>**说明：**\n>特殊字符（例如 $、\\、*、= 和 !）会被你的 Shell解释，因此需要转义。 在大多数 Shell 中，对密码进行转义的最简单方式是用单引号（'）将其括起来。 例如，如果您的实际密码是 S!B\\*d$zDsb，则应通过以下方式执行命令：\n>```\n>kubectl create secret generic dev-db-secret --from-literal=username=devuser --from-literal=password='S!B\\*d$zDsb='\n>```\n>您无需对文件中的密码（--from-file）中的特殊字符进行转义。\n\n创建 pod ：\n\n``` yaml\n$ cat <<EOF > pod.yaml\napiVersion: v1\nkind: List\nitems:\n- kind: Pod\n  apiVersion: v1\n  metadata:\n    name: prod-db-client-pod\n    labels:\n      name: prod-db-client\n  spec:\n    volumes:\n    - name: secret-volume\n      secret:\n        secretName: prod-db-secret\n    containers:\n    - name: db-client-container\n      image: myClientImage\n      volumeMounts:\n      - name: secret-volume\n        readOnly: true\n        mountPath: \"/etc/secret-volume\"\n- kind: Pod\n  apiVersion: v1\n  metadata:\n    name: test-db-client-pod\n    labels:\n      name: test-db-client\n  spec:\n    volumes:\n    - name: secret-volume\n      secret:\n        secretName: test-db-secret\n    containers:\n    - name: db-client-container\n      image: myClientImage\n      volumeMounts:\n      - name: secret-volume\n        readOnly: true\n        mountPath: \"/etc/secret-volume\"\nEOF\n```\n\n将 Pod 添加到同一个 kustomization.yaml 文件\n\n```\n$ cat <<EOF >> kustomization.yaml\nresources:\n- pod.yaml\nEOF\n```\n\n通过下面的命令应用所有对象\n\n```\nkubectl apply -k .\n```\n\n两个容器都会在其文件系统上存在以下文件，其中包含容器对应的环境的值：\n\n```\n/etc/secret-volume/username\n/etc/secret-volume/password\n```\n\n请注意，两个 Pod 的规约配置中仅有一个字段不同；这有助于使用共同的 Pod 配置模板创建 具有不同能力的 Pod。\n\n您可以使用两个服务账号进一步简化基本的 Pod 规约：\n\n1. 名为 prod-user 的服务账号拥有 prod-db-secret\n2. 名为 test-user 的服务账号拥有 test-db-secret\n\n然后，Pod 规约可以缩短为：\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: prod-db-client-pod\n  labels:\n    name: prod-db-client\nspec:\n  serviceAccount: prod-db-client\n  containers:\n  - name: db-client-container\n    image: myClientImage\n```\n\n### 案例：Secret 卷中以句点号开头的文件\n\n你可以通过定义以句点开头的键名，将数据“隐藏”起来。 例如，当如下 Secret 被挂载到 `secret-volume` 卷中：\n\n``` yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: dotfile-secret\ndata:\n  .secret-file: dmFsdWUtMg0KDQo=\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secret-dotfiles-pod\nspec:\n  volumes:\n  - name: secret-volume\n    secret:\n      secretName: dotfile-secret\n  containers:\n  - name: dotfile-test-container\n    image: k8s.gcr.io/busybox\n    command:\n    - ls\n    - \"-l\"\n    - \"/etc/secret-volume\"\n    volumeMounts:\n    - name: secret-volume\n      readOnly: true\n      mountPath: \"/etc/secret-volume\"\n```\n\n卷中将包含唯一的叫做 `.secret-file` 的文件。 容器 `dotfile-test-containe` 中，该文件处于 `/etc/secret-volume/.secret-file` 路径下。\n\n> **说明：** 以点号开头的文件在 `ls -l` 的输出中会被隐藏起来； 列出目录内容时，必须使用 `ls -la` 才能看到它们。\n\n### 案例：Secret 仅对 Pod 中的一个容器可见\n\n考虑一个需要处理 HTTP 请求、执行一些复杂的业务逻辑，然后使用 HMAC 签署一些消息的应用。 因为应用程序逻辑复杂，服务器中可能会存在一个未被注意的远程文件读取漏洞， 可能会将私钥暴露给攻击者。\n\n解决的办法可以是将应用分为两个进程，分别运行在两个容器中： 前端容器，用于处理用户交互和业务逻辑，但无法看到私钥； 签名容器，可以看到私钥，响应来自前端（例如通过本地主机网络）的简单签名请求。\n\n使用这种分割方法，攻击者现在必须欺骗应用程序服务器才能进行任意的操作， 这可能比使其读取文件更难。\n\n## 最佳实践\n### 客户端使用 Secret API\n\n当部署与 Secret API 交互的应用程序时，应使用 [鉴权策略](https://kubernetes.io/zh/docs/reference/access-authn-authz/authorization/)， 例如 [RBAC](https://kubernetes.io/zh/docs/reference/access-authn-authz/rbac/)，来限制访问。\n\n\n## 安全属性\n\n### 保护\n\n因为 Secret 对象可以独立于使用它们的 Pod 而创建，所以在创建、查看和编辑 Pod 的流程中 Secret 被暴露的风险较小。系统还可以对 Secret 对象采取额外的预防性保护措施， 例如，在可能的情况下避免将其写到磁盘。\n\n只有当某节点上的 Pod 需要用到某 Secret 时，该 Secret 才会被发送到该节点上。 Secret 不会被写入磁盘，而是被 kubelet 存储在 tmpfs 中。 一旦依赖于它的 Pod 被删除，Secret 数据的本地副本就被删除。\n\n同一节点上的很多个 Pod 可能拥有多个 Secret。 但是，只有 Pod 所请求的 Secret 在其容器中才是可见的。 因此，一个 Pod 不能访问另一个 Pod 的 Secret。\n\n同一个 Pod 中可能有多个容器。但是，Pod 中的每个容器必须通过 volumeeMounts 请求挂载 Secret 卷才能使卷中的 Secret 对容器可见。 这一实现可以用于在 Pod 级别[构建安全分区](https://kubernetes.io/zh/docs/concepts/configuration/secret/#secret-visible-to-only-one-container)。\n\n在大多数 Kubernetes 发行版中，用户与 API 服务器之间的通信以及 从 API 服务器到 kubelet 的通信都受到 SSL/TLS 的保护。 通过这些通道传输时，Secret 受到保护。\n\nFEATURE STATE: Kubernetes v1.13 [beta]\n\n你可以为 Secret 数据开启[静态加密](https://kubernetes.io/zh/docs/tasks/administer-cluster/encrypt-data/)， 这样 Secret 数据就不会以明文形式存储到etcd 中。\n\n\n## 风险\n\n * API 服务器上的 Secret 数据以纯文本的方式存储在 etcd 中，因此：\n  - 管理员应该为集群数据开启静态加密（要求 v1.13 或者更高版本）。\n  - 管理员应该限制只有 admin 用户能访问 etcd；\n  - API 服务器中的 Secret 数据位于 etcd 使用的磁盘上；管理员可能希望在不再使用时擦除/粉碎 etcd 使用的磁盘\n  - 如果 etcd 运行在集群内，管理员应该确保 etcd 之间的通信使用 SSL/TLS 进行加密。\n * 如果您将 Secret 数据编码为 base64 的清单（JSON 或 YAML）文件，共享该文件或将其检入代码库，该密码将会被泄露。 Base64 编码不是一种加密方式，应该视同纯文本。\n * 应用程序在从卷中读取 Secret 后仍然需要保护 Secret 的值，例如不会意外将其写入日志或发送给不信任方。\n * 可以创建使用 Secret 的 Pod 的用户也可以看到该 Secret 的值。即使 API 服务器策略不允许用户读取 Secret 对象，用户也可以运行 Pod 导致 Secret 暴露。\n * 目前，任何节点的 root 用户都可以通过模拟 kubelet 来读取 API 服务器中的任何 Secret。 仅向实际需要 Secret 的节点发送 Secret 数据才能限制节点的 root 账号漏洞的影响， 该功能还在计划中\n\n","tags":["istio"],"categories":["microService","kubernetes"]},{"title":"查看 linux 系统日志 和 服务的日志","url":"/2021/03/13/linux/linux日志_服务日志/","content":"\n## 查看linux 系统日志\n\n```shell\n\t$ cat /var/log/messages\n```\n\n## 查看某个服务日志\n\n```shell\n\t$ journalctl -fu docker\n\t$ journalctl -fu kubelet\n```\n\n查看服务日志详细内容:\n\n```shell\n\tjournalctl -u kubelet --no-pager\n```\n\n## 重新加载系统配置\n\n```shell\n\t$ systemctl daemon-reload\n```\n\n","categories":["linux"]},{"title":"linux查看运行的服务","url":"/2021/03/13/linux/linux查看运行的服务/","content":"\n## Centos\n\n```shell\n\t$ systemctl list-units --type=service\n```\n\n\n","categories":["linux"]},{"title":"maven","url":"/2021/03/13/linux/maven/","content":"\n## Centos7 maven 安装\n\n安装maven前提是安装有jdk, yum安装maven时候会自动安装JDK.\n\n```shell\n\t(optional)// 多出来这两步是因为原来机器kernel版本台老, 原生的yum库安装maven自带JDK也比较老, 所以clean再cache, 然后安装maven\n\tyum clean all\n\t(optional)\n\tyum makecache\n\t\n\twget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo\n\tyum -y install apache-maven\n\t\n\t// 查找包路径\n\trpm -qa|grep apache-maven\n```\n\n### 配置阿里云镜像仓库\n\n```xml\n\tvim /etc/maven/settings.xml\n\t// 定位到mirrors节点下添加下面配置\n\t  <mirrors>\n\t     <mirror>\n\t        <id>nexus-aliyun</id>\n\t        <mirrorOf>central</mirrorOf>\n\t        <name>Nexus aliyun</name>\n\t        <url>http://maven.aliyun.com/nexus/content/groups/public</url>\n\t     </mirror>\n\t  </mirrors>\n```\n\n### 配置proxy\n\n```xml\n\tvim /etc/maven/settings.xml\n\t    <proxy>\n\t       <id>my-proxy1</id>\n\t       <active>true</active>\n\t       <protocol>http</protocol>\n\t       <host>child-prc.intel.com</host>\n\t       <port>913</port>\n\t    </proxy>\n\t    <proxy>\n\t       <id>my-proxy2</id>\n\t       <active>true</active>\n\t       <protocol>https</protocol>\n\t       <host>child-prc.intel.com</host>\n\t       <port>913</port>\n\t    </proxy>\n```\n\n### 配置本地仓库\nyum安装完maven后默认的本地仓库地址:\n\n```shell\n\tls /root/.m2/repository\n```\n\n配置新的本地仓库:\n\n```xml\n\tvim /etc/maven/settings.xml\n\t// 定位到这个节点进行编写\n\t<localRepository>/home/maven/repo</localRepository>\n```\n\n### 指定JDK版本\n配置创建项目的版本默认为 JDK8\n\n```xml\n\tvim /etc/maven/settings.xml\n\t  <profiles>\n\t    <profile>    \n\t         <id>jdk-1.8</id>    \n\t         <activation>    \n\t           <activeByDefault>true</activeByDefault>    \n\t           <jdk>1.8</jdk>    \n\t         </activation>    \n\t           <properties>    \n\t             <maven.compiler.source>1.8</maven.compiler.source>    \n\t             <maven.compiler.target>1.8</maven.compiler.target>    \n\t             <maven.compiler.compilerVersion>1.8</maven.compiler.compilerVersion>    \n\t           </properties>    \n\t    </profile>\n\t  <profiles>\n```\n\n### (optional)设置tomcat用户名和密码\n如果tomcat安装时候或安装后tomcat的配置文件没有设置用户名和密码此处可忽略\n\n```xml\n\t  <servers>\n\t    <server>\n\t      <id>tomcat8</id>\n\t      <username>admin</username>\n\t      <password>123456</password>\n\t    </server>\n\t  </servers>\n```\n\n### 测试maven\n\n```shell\n\t// 查看maven版本\n\tmvn -v\n\t\n\t// 自动到配置的Ali的Maven中央仓库下载缺省的或者Maven中央仓库更新的各种配置文件和类库（jar包)到Maven配置的本地仓库`/home/maven/repo`中\n\tmvn help:system\n```\n\n## VScode远程连接Centos, 搭建maven项目简单步骤\n首先需要安装一些VScode扩展插件\n * maven for java\n * language support for java\n * debugger for java\n * java test runner:junit\n * java extension pack\n\n1. `Ctrl+shift+p`, 搜索`maven`, 选择`Maven:Create Maven Project`\n![](1.PNG)\n2. 选择`archetype-quickstart-jdk8`\n![](2.PNG)\n3. 选择最新的`2.0`\n![](3.PNG)\n4. 选择文件夹来创建项目\n\n5. 输入相关参数\n\n * groupid: 公司或组织的域名倒序+当前项目名称, 会跟源代码文件中第一行package指定的一致.\n * artifactId： 当前项目的模块名称, 会在根目录创建此模块名称的文件夹, 所有创建的项目文件都在这个文件夹下.\n * version： 版本, 可以默认直接enter键\n\n6. 可以发现在项目目录下多出了如下两个文件\n\n```shell\n\tls /<Project path>/\n\tpom.xml src/\n```\n\n7. 编译, 测试\n\n```shell\n\t// 1. 编译, 会在项目目录下生成一个target/文件夹\n\tmvn compile\n\tls \n\tpom.xml  src/  target/\n\t\n\t// 2. 编写单元测试代码, 测试源码函数.\n\tmvn test\n\t\n\t// 3. 执行, 执行的文件名字一定要是`<groupId-name>.<class-name>`\n\tmvn exec:java -Dexec.mainClass=\"cloud.App\"\n\t\n\t// 4. 清理, 删除target文件夹\n\tmvn clean\n```\n\n中途可能会出错遇到一些问题, 具体查看下面的Problems.\n\n## 下载指定jar包\n在 Maven 官网上查找，可以很方便地查询到自己需要的 jar文件 和 pom引用.  \nMaven 官网 地址如下：  \nhttps://mvnrepository.com/\n\n\n## VScode给maven项目添加单元测试(unit test)\n要明白一点的是单元测试要测试的是函数方法, 利用单元测试工具junit不用手动调用main函数去调用要测试的方法, junit会自动执行要测试的方法.\n因此在单元测试中不会手写main函数.\n\n**`junit单元测试, 对测试方法顺序是乱序的, 某个方法要先执行可以加上@BeforeAll, 且方法声明为static, @Test方法不能加static.`**\njunit5+版本更新后的参考文档: https://developer.ibm.com/zh/tutorials/j-introducing-junit5-part1-jupiter-api/\n\n![](4.PNG)\nVScode写的函数方法等存储在hce/src/main/java/下的calculate/和hello/两个文件夹下\nVScode写的单元测试方法存储在hce/src/test/java/下的calculate/和hello/两个文件夹下, 注意文件夹名和上述同名, 不写成同命要手动改写引入源码包路径.\n\npom.xml:(junit和junit.jupiter区别没有比较过, 可以在(maven包官网)[https://mvnrepository.com/]搜索junit查看, 这里是用junit.jupiter)\n\n```xml\n\t  <properties>\n\t    <junit.version>5.6.0</junit.version>\n\t  </properties>\n\t......\n\t  <dependencies>\n\t    <dependency>\n\t      <groupId>org.junit.jupiter</groupId>\n\t      <artifactId>junit-jupiter-api</artifactId>\n\t      <version>${junit.version}</version>\n\t      <scope>test</scope>\n\t    </dependency>\n\t    <dependency>\n\t      <groupId>org.junit.jupiter</groupId>\n\t      <artifactId>junit-jupiter-engine</artifactId>\n\t      <version>${junit.version}</version>\n\t      <scope>test</scope>\n\t    </dependency>\n\t  </dependencies>\n```\n\nCalculate.java\n\n```java\n\tpackage calculate;\n\t\n\t/**\n\t * Hello world!\n\t */\n\tpublic final class Calculate {\n\t    private Calculate() {\n\t    }\n\t\n\t    public static int add(int firstNumber, int secondNumber) {\n\t        return firstNumber + secondNumber;\n\t    }\n\t\n\t    public static int multiply(int multiplicand, int multiplier) {\n\t        return multiplicand * multiplier;\n\t    }\n\t}\n```\n\nHello.java\n\n```java\n\tpackage hello;\n\t\n\t/**\n\t * Hello world.\n\t */\n\tpublic final class Hello {\n\t\n\t    private Hello() {\n\t    }\n\t\n\t    public static String initMessage(String initInfo) {\n\t        return initInfo;\n\t    }\n\t\n\t    /**\n\t     * say Something.\n\t     */\n\t    public static String saySomething(String initInfo, String msg) {\n\t        String message = initInfo + \" \" + msg;\n\t        return message;\n\t    }\n\t\n\t    /**\n\t     * say nothing.\n\t     */\n\t    public static String sayNothing(String nomsg) {\n\t        return null;\n\t    }\n\t}\n```\n\nTestCalculate.java\n\n```java\n\tpackage calculate;\n\t\n\timport org.junit.jupiter.api.Test;\n\t\n\timport static org.junit.jupiter.api.Assertions.assertEquals;\n\t\n\t/**\n\t * Unit test for simple App.\n\t */\n\tclass TestCalculate {\n\t\n\t    @Test\n\t    public void add_TwoPlusTwo_ReturnsFour() {\n\t        // Arrange\n\t        final int expected = 4;\n\t        // Act\n\t        final int actual = Calculate.add(2, 2);\n\t        // Assert\n\t        assertEquals(expected, actual);\n\t        System.out.println(\"add successfully!\");\n\t    }\n\t\n\t    @Test\n\t    public void multiply_FourTimesTwo_ReturnsEight() {\n\t        // Arrange\n\t        final int expected = 8;\n\t        // Act\n\t        final int actual = Calculate.multiply(4, 2);\n\t        // Assert\n\t        assertEquals(expected, actual);\n\t        System.out.println(\"multiply successfully!\");\n\t    }\n\t}\n```\n\nTestHello.java\n\n```java\n\tpackage hello;\n\t\n\timport org.junit.jupiter.api.BeforeAll;\n\timport org.junit.jupiter.api.Test;\n\t\n\timport static org.junit.jupiter.api.Assertions.assertEquals;\n\timport static org.junit.jupiter.api.Assertions.assertNotNull;\n\timport static org.junit.jupiter.api.Assertions.assertNull;\n\t\n\t/**\n\t * Unit test for simple App.\n\t */\n\tpublic class TestHello {\n\t\n\t    private static String initInfo;\n\t\n\t    /**\n\t     * @BeofreAll 单元测试, 方法必须为static静态方法.\n\t     * 被注解的（静态）方法将在当前类中的所有 @Test 方法前执行一次。\n\t     */\n\t    @BeforeAll\n\t    public static void testInitMessage() {\n\t        initInfo = Hello.initMessage(\"Hello\");\n\t        assertNotNull(initInfo);\n\t    }\n\t\n\t    /**\n\t     * @Test 单元测试, 方法不能加static.\n\t     * 各个@Test方法间执行顺序为乱序\n\t     */\n\t    @Test\n\t    public void testSaySomething() {\n\t        String info = null;\n\t        info = Hello.saySomething(initInfo, \"World!\");\n\t        assertNotNull(info);\n\t        System.out.println(info);\n\t    }\n\t\n\t    @Test\n\t    public void testSayNothing() {\n\t        String info = null;\n\t        info = Hello.sayNothing(\"Hello World!\");\n\t        assertNull(info);\n\t    }\n\t\n\t    @Test\n\t    public void testHello() {\n\t        assertEquals(1, 1);\n\t        System.out.println(\"test something else successfully!\");\n\t    }\n\t}\n```\n\n打开终端, 到hce项目根目录, 依次执行`mvn compile`编译\n![](5.PNG)\n\n```shell\n\t[root@hci-node01 hce]# mvn compile\n\t[INFO] Scanning for projects...\n\t[INFO] \n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] Building hce 1.0-SNAPSHOT\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] \n\t[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (default) @ hce ---\n\t[INFO] \n\t[INFO] --- maven-checkstyle-plugin:3.1.0:check (checkstyle) @ hce ---\n\t[INFO] \n\t[INFO] --- jacoco-maven-plugin:0.8.4:prepare-agent (pre-unit-test) @ hce ---\n\t[INFO] argLine set to -javaagent:/home/maven/repo/org/jacoco/org.jacoco.agent/0.8.4/org.jacoco.agent-0.8.4-runtime.jar=destfile=/home/zhan/download/storage/minIO/client/java/hce/target/jacoco.exec\n\t[INFO] \n\t[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hce ---\n\t[INFO] Using 'UTF-8' encoding to copy filtered resources.\n\t[INFO] skip non existing resourceDirectory /home/zhan/download/storage/minIO/client/java/hce/src/main/resources\n\t[INFO] \n\t[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hce ---\n\t[INFO] Nothing to compile - all classes are up to date\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] BUILD SUCCESS\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] Total time: 1.354 s\n\t[INFO] Finished at: 2020-09-24T17:42:25+08:00\n\t[INFO] Final Memory: 28M/1570M\n\t[INFO] ------------------------------------------------------------------------\n\t[root@hci-node01 hce]#\n```\n\n再执行`mvn test`进行单元测试.\n![](6.PNG)\n\n```shell\n\t[root@hci-node01 hce]# mvn test\n\t[INFO] Scanning for projects...\n\t[INFO] \n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] Building hce 1.0-SNAPSHOT\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] \n\t[INFO] --- maven-enforcer-plugin:3.0.0-M3:enforce (default) @ hce ---\n\t[INFO] \n\t[INFO] --- maven-checkstyle-plugin:3.1.0:check (checkstyle) @ hce ---\n\t[INFO] \n\t[INFO] --- jacoco-maven-plugin:0.8.4:prepare-agent (pre-unit-test) @ hce ---\n\t[INFO] argLine set to -javaagent:/home/maven/repo/org/jacoco/org.jacoco.agent/0.8.4/org.jacoco.agent-0.8.4-runtime.jar=destfile=/home/zhan/download/storage/minIO/client/java/hce/target/jacoco.exec\n\t[INFO] \n\t[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ hce ---\n\t[INFO] Using 'UTF-8' encoding to copy filtered resources.\n\t[INFO] skip non existing resourceDirectory /home/zhan/download/storage/minIO/client/java/hce/src/main/resources\n\t[INFO] \n\t[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ hce ---\n\t[INFO] Nothing to compile - all classes are up to date\n\t[INFO] \n\t[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ hce ---\n\t[INFO] Using 'UTF-8' encoding to copy filtered resources.\n\t[INFO] skip non existing resourceDirectory /home/zhan/download/storage/minIO/client/java/hce/src/test/resources\n\t[INFO] \n\t[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ hce ---\n\t[INFO] Changes detected - recompiling the module!\n\t[INFO] Compiling 2 source files to /home/zhan/download/storage/minIO/client/java/hce/target/test-classes\n\t[INFO] \n\t[INFO] --- maven-surefire-plugin:3.0.0-M4:test (default-test) @ hce ---\n\t[INFO] \n\t[INFO] -------------------------------------------------------\n\t[INFO]  T E S T S\n\t[INFO] -------------------------------------------------------\n\t[INFO] Running hello.TestHello\n\ttest something else successfully!\n\tHello World!\n\t[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.039 s - in hello.TestHello\n\t[INFO] Running calculate.TestCalculate\n\tmultiply successfully!\n\tadd successfully!\n\t[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.003 s - in calculate.TestCalculate\n\t[INFO] \n\t[INFO] Results:\n\t[INFO] \n\t[INFO] Tests run: 5, Failures: 0, Errors: 0, Skipped: 0\n\t[INFO] \n\t[INFO] \n\t[INFO] --- jacoco-maven-plugin:0.8.4:report (post-unit-test) @ hce ---\n\t[INFO] Loading execution data file /home/zhan/download/storage/minIO/client/java/hce/target/jacoco.exec\n\t[INFO] Analyzed bundle 'hce' with 2 classes\n\t[INFO] \n\t[INFO] --- jacoco-maven-plugin:0.8.4:check (check-unit-test) @ hce ---\n\t[INFO] Loading execution data file /home/zhan/download/storage/minIO/client/java/hce/target/jacoco.exec\n\t[INFO] Analyzed bundle 'hce' with 2 classes\n\t[INFO] All coverage checks have been met.\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] BUILD SUCCESS\n\t[INFO] ------------------------------------------------------------------------\n\t[INFO] Total time: 2.964 s\n\t[INFO] Finished at: 2020-09-24T17:43:04+08:00\n\t[INFO] Final Memory: 39M/1581M\n\t[INFO] ------------------------------------------------------------------------\n\t[root@hci-node01 hce]#\n```\n\n## Problems\n\n### 问题1: Maven Version\nDetected Maven Version: 3.5.2 is not in the allowed range 3.6.3.  \n原因是如linux系统装的maven版本是3.5.2, 用VScode新建maven项目时候自动生成的pom.xml添加maven版本号为3.6.3, 因此可以修改pom.xml里的maven版本, 改为机器安装的maven版本就可以了.\n修改相应错误产生的包中的pox.xml文件的 requireMavenVersion, 改为所需的3.5.2\n\n```xml\n\t            <configuration>\n\t              <rules>\n\t                <requireMavenVersion>\n\t                  <version>3.5.2</version>\n\t                </requireMavenVersion>\n\t              </rules>\n\t              <fail>true</fail>\n\t            </configuration>\n```\n\n### 问题2：No compiler\nMaven编报错：No compiler is provided in this environment. Perhaps you are running on a JRE rather a JDK?  \n\n**第一步:** 参看上面的**指定JDK版本**确定是否在安装maven时候, 把jdk配置进maven的配置文件/etc/maven/setting.xml文件里\n\n**第二步:** 查看pox.xml文件中的compiler和linux系统中`java -v`和`javac -v`版本是否一致.\n\n```xml\n\t    <maven.compiler.source>1.8</maven.compiler.source>\n\t    <maven.compiler.target>1.8</maven.compiler.target>\n```\n\n### 问题3: ImportOrder\njava导入包编译时候出错报ImportOrder: Wrong order for 'io.minio.errors.MinioException' import\n原因是checkstyle对java import包的顺序有要求\n**方法一:** 修改pom.xml直接禁用:\n\n```xml\n\t<plugin>\n\t    <groupId>org.apache.maven.plugins</groupId>\n\t    <artifactId>maven-checkstyle-plugin</artifactId>\n\t    <configuration>\n\t        <skip>true</skip>\n\t    </configuration>\n\t</plugin>\n```\n\n**方法二:** 根据checkstyle定义, 修改导入包顺序\ncheckstyle导入包顺序定义官网：[ImportOrder](https://checkstyle.sourceforge.io/config_imports.html#ImportOrder)  \n记住要按字典顺序排序导入这一条: ensures that imports within each group are in lexicographic order\n\n\n\n","categories":["linux"]},{"title":"linux向其它终端发送消息","url":"/2021/03/13/linux/linux向其它终端发送消息/","content":"\n## 查看所有打开的终端\n\n```shell\n\t$ w -f // 或者只输入`w`查看打开的所有终端, `w`是who的简写而不是write\n\t   16:41:45 up 53 days, 47 min,  5 users,  load average: 0.17, 0.31, 0.38\n\t  USER     TTY        LOGIN@   IDLE   JCPU   PCPU WHAT\n\t  root     pts/0     14:23    1.00s  0.07s  0.00s w -f\n\t  ai       :0        18Jun20 ?xdm?   8days  7.89s /usr/libexec/gnome-session-binary --session gnome-classic\n\t  ai       pts/1     18Jun20 53days  3:28m  3:28m /usr/lib/virtualbox/VirtualBox\n\t  ai       pts/2     16:29   10:49   0.07s  0.02s bash\n\t  root     pts/3     16:27    9:37   0.02s  0.02s -bash\n```\n\n## 向指定终端发送消息\n\n```shell\n\t$ write ai pts/2\n\t  123\n```\n\n## 广播消息, 向所有终端发送消息\n\n```shell\n\t$ wall 123\n\t  Broadcast message from root@master-node (pts/0) (Mon Aug 10 16:40:49 2020):\n\t  123\n```\n\n","categories":["linux"]},{"title":"linux给grub添加内核启动参数","url":"/2021/03/13/linux/linux给grub添加内核启动参数/","content":"如果你想在系统启动时加载一个内核参数，需修改GRUB的配置模板(/etc/default /grub),添加\"名称=值”的键值对到GRUB_CMDLINE_LINUX变量,添加多个时用空格隔开,例如GRUB_CMDLINE_LINUX=\"...... name=value\"(如果没有GRUB_CMDLINE_LINUX变量时,用GRUB_CMDLINE_LINUX_DEFAULT替代即可).\n1. Debian or Ubuntu\n\n```shell\n$ sudo update-grub  //生成grub的配置文件\n$ sudo apt-get install grub2-common  //没有 update-grub命令时,先运行这个安装命令  \n```\n\n2. Fedora or CentOS7\n\n```shell\n$ sudo grub2-mkconfig -o /boot/grub2/grub.cfg //生成grub2的配置文件\n$ sudo yum install grub2-tools.x86_64 //没有grub2-mkconfig命令时,先安装grub2-tools\n```\n带EFI的系统,grub.cfg文件会是在/boot/efi下,比如CentOS7:/boot/efi/EFI/centos/grub.cfg\n","categories":["linux"]},{"title":"nginx","url":"/2021/03/13/linux/nginx/","content":"\n## nginx\n\n### nginx端口转发\n\n```shell\nvim /usr/lib/webserver/nginx/conf/nginx.conf \n```\n\n重启:\n\n```shell\n\tcd ../sbin/\n\t./nginx -s reload\n```\n","categories":["linux"]},{"title":"ntpdate 同步各个系统时间","url":"/2021/03/13/linux/ntpdate_synchronize_os_time/","content":"\n## ntpdate\nCentos下载ntpdate\n\n```shell\n\t$ yum install ntp ntpdate -y\n```\n\n集群操作，同步各个系统时间, 与某一台服务器时间保持一致.\n\n```\n0 12 * * * */usr/sbin/ntpdate 192.168.0.1\n```\n每天12点强制将系统时间设置为192.168.0.1服务器时间\n\n\n\n\n\n","categories":["linux"]},{"title":"perf","url":"/2021/03/13/linux/perf/","content":"## perf 命令\n\n/root/spdk/examples/nvme/perf/perf -q 64 -s 1024 -w write -t 10 -c 0x01 -o 4096 -D -LL -r 'trtype:PCIe traddr:0000:5e:00.0' \n","categories":["linux"]},{"title":"python 安装","url":"/2021/03/13/linux/python安装/","content":"\n## **Centos7 python3.6 安装**\n\n```shell\n\t$ yum install epel-release\n\t$ yum search python | grep python36\n\t$ yum install python36\n\t$ whereis python\n\t$ rm /usr/bin/python // 删除原来系统自带的python2.7版本\n\t$ ln -s /usr/bin/python3.6 /usr/bin/python\n\t$ ls -alh /usr/bin/python\n\tlrwxrwxrwx 1 root root 18 Sep 11 11:19 /usr/bin/python -> /usr/bin/python3.6\n```\n\n有时候我们保留系统默认的的python 2.7版本`/usr/bin/python -> /usr/bin/python2.7`.  \n可以直接用`$ python3`来运行python3版本, 原因是安装python36时候会自动生成`/usr/bin/python3`链接向`/usr/bin/python3.6`.  \n因此其实也可以直接用`$ python3.6`来执行python3版本.  \n\n```shell\n\t$ ls -alh /usr/bin/python3\n\tlrwxrwxrwx 1 root root 9 Sep 11 11:10 /usr/bin/python3 -> python3.6\n```\n\n","categories":["linux"]},{"title":"scp机器间复制文件","url":"/2021/03/13/linux/scp机器间复制文件/","content":"跨服务器拷贝需要用到的命令是scp.\n----------------------拷贝文件夹----------------------------------------------\n把当前文件夹tempA拷贝到 目标服务器10.127.40.25 服务器的 /tmp/wang/文件夹下\n\n```shell\nscp -r /tmp/tempA/ wasadmin@10.127.40.25:/tmp/wang/\n```\n\n其中wasadmin是目标服务器的用户名，执行命令提示输入密码，然后输入密码即可\n \n----------------------拷贝文件----------------------------------------------\n把当前文件夹tempA.txt拷贝到 目标服务器10.127.40.25 服务器的 /tmp/wang/文件夹下\n\n```shell\nscp  /tmp/tempA.txt wasadmin@10.127.40.25:/tmp/wang/\n```\n\n其中wasadmin是目标服务器的用户名，执行命令提示输入密码，然后输入密码即可\n\nFrom <https://www.cnblogs.com/xiayahui/p/5437556.html> \n\n","categories":["linux"]},{"title":"screen程序在后台运行","url":"/2021/03/13/linux/screen程序在后台运行/","content":"\n## screen\n\nlinux下安装：\n\n * centos:\n```shell\n$ yum install screen\n```\n\n * ubuntu:\n```shell\n$ apt-get install screen\n```\n\n启动时添加选项-L（Turn on output logging.），会在当前目录下生成screenlog.0文件。\n![](screenlog.0.png)\n### 连接会话\n1. <font color='red'><b>screen -L -dmS test</b></font>的意思是启动一个开始就处于断开模式的会话，会话的名称是test。\n```shell\n$ screen -r test // 连接该会话，在会话中的所有屏幕输出都会记录到screenlog.0文件。\n```\n\n\n### 让每个screen会话窗口有单独的日志文件\n\n在screen配置文件/etc/screenrc最后添加下面一行：\n```\n<font color=\"red\" size=5><b>logfile /tmp/screenlog_%t.log</b></font>  \n```\n%t是指window窗口的名称，对应screen的-t参数。所以我们启动screen的时候要指定窗口的名称，例如：\n![](screen_logs.png)\n<font color=\"red\" size=5><b>screen -L -t window1 -dmS test</b></font>的意思是启动test会话，test会话的窗口名称为window1。屏幕日志记录在/tmp/screenlog_window1.log。如果启动的时候不加-L参数，在screen session下按ctrl+a H，日志也会记录在/tmp/screenlog_window1.log。\n\n```shell\n$ screen -S yourname -> 新建一个叫yourname的session\n```\n\n然后在里面执行你要执行的程序\n\n比如 `java -jar xxx.jar`\n\n然后 `ctrl+a+d` 退出会话\n\n```shell\nscreen -ls -> 列出当前所有的session\n\nscreen -r yourname -> 回到yourname这个session\n\nscreen -d yourname -> 远程detach某个session\n\nscreen -d -r yourname -> 结束当前session并回到yourname这个session\n\nscreen -ls\n```\n会有如下显示：\n122128.test     (12/04/2017 08:35:43 PM)        (Attached)\n删除它\n```shell\nscreen -X -S 122128 quit\n```\n\n再screen -ls就没了\n\n```shell\n\t[root@wlp10 test_faiss]# screen -ls\n\tThere are screens on:\n\t        11174.pts-1.wlp10       (Detached)\n\t        11054.zh        (Detached)\n\t2 Sockets in /var/run/screen/S-root.\n\t\n\t[root@wlp10 test_faiss]# screen -X -S 11174.pts-1 quit\n\tbash: screen -X -S 11174.pts-1 quit: command not found...\n\t[root@wlp10 test_faiss]# ps aux|grep 11174\n\troot      11174  0.0  0.0 127908  2888 ?        Ss   15:03   0:00 SCREEN -l\n\troot      11530  0.0  0.0 168680  5068 pts/1    S+   15:06   0:00 grep --color=auto 11174\n\t[root@wlp10 test_faiss]# kill 11174\n\t[root@wlp10 test_faiss]# pa aux|grep 11054\n\tbash: pa: command not found...\n\t[root@wlp10 test_faiss]# ps aux|grep 11054\n\troot      11054  0.0  0.0 127908  2848 ?        Ss   15:02   0:00 SCREEN -S zh\n\troot      11561  0.0  0.0 168680  5072 pts/1    S+   15:07   0:00 grep --color=auto 11054\n\t[root@wlp10 test_faiss]# screen -ls\n\tThere is a screen on:\n\t        11054.zh        (Detached)\n\t1 Socket in /var/run/screen/S-root.\n\t\n\t[root@wlp10 test_faiss]#\n```\n\n1.想永远关闭screen的闪屏功能，需要修改配置文件。在CentOS中可以修改/etc/screenrc，修改这个文件将对所有用户生效。   \n    Vbell on 改为 vbell off\n    之后新建的screen按Backspace键到头就没有闪烁了\n\n2.只修改自己的配置文件，在$HOME/.screenrc(没有的话新建~/.screenrc文件)中     加入下面的话：\n    vbell off\n    之后新建的screen按Backspace键到头就没有闪烁了\n","categories":["linux"]},{"title":"linux添加修改网络命名空间设备","url":"/2021/03/13/linux/linux添加修改网络命名空间设备/","content":"\n## 网卡, iptables, 路由表\n网卡相当于整个计算机的网关, 有唯一的MAC地址, 用来识别计算机并用来跟其它计算机通信, 网卡位于OSI七层模型的第二层链路层.\n\n查看iptables: `$ iptables -nvL`用来查看流量经过网卡进入计算机后的流向, 指定哪些端口服务.\n\n查看路由表: `$ route -n`, 定义网络流量最终可以通过哪个网络设备出去.\n\n## ping和traceroute命令\n操作网络OSI模型2层链路层网命令ping, traceroute\n\n```shell\n//ping\n$ ping -c 3 127.0.0.1\t//ping 3次 环回地址\n$ ping -c 3 127.33.99.11\t//环回地址不止是指127.0.0.1, 只要是以127开头的网段都可以.\n\n// traceroute\n$ yum install -y net-tools\n$ yum install -y traceroute\n$ traceroute 127.0.0.1\n```\n\n## ip命令查看管理网络命名空间(namespace)\n`ip 命令`是在`centos 7`引入进来的, 基本不用以前的ifconfig命令.  \n`$ docker exec -it container cmd...` docker 命令操作实际上是在linux的网络命名空间发展来的.  \n`$ kubectl exec -it pod -- cmd ...` kubernetes 命令操作实际上也是在linux的网络命名空间发展来的.  \n**linux网络命名空间是属于linux内核的一个功能,必须要内核支持, docker是在此基础上做了扩展和封装.**\n\n### **ip 基本命令**\n\n```shell\n$ ip netns add eden\t\t\t//添加Linux network namespace, 与宿主机的网络命名空间隔离\n$ ip netns ls\t\t\t\t//查看\n$ ip netns delete eden\t\t//删除 network namespace\n$ ip netns add greenland\n```\n\n进入新建的网络命名空间(与`docker exec -it container cmd...`非常类似).  \n\n```shell\n$ ip netns exec greenland bash\t\t// 进入网络命名空间, 但是用的User和主机提示符跟原来相同\n$ ip link\t\t\t//查看有什么设备\n$ ip addr\t\t\t//查看ip设备地址\n$ exit\n```\n\n重命名网络命名空间的User和主机提示符\n\n```shell\n$ ip netns exec greenland bash -rcfile <(echo \"PS1=\\\"greenland\\\"\")\t// 修改User和主机提示符为greenland\ngreenland> ip link\t\t\t\t\t// 1. 查看网络设备是处于\"state DOWN mode DEFAULT\"\ngreenland> ping -c 3 127.0.0.1\t\t// 2. 返回: connect: Network is unreachable\ngreenland> ip link set lo up\t\t// 3. 启用设备\ngreenland> ip link\t\t\t\t\t// 4. 再次查看网络设备处于\"up\"\ngreenland> ping -c 3 127.8.8.8\t\t// 5. 查看可以ping通了\n\ngreenland> ip addr\ngreenland> route -n\t\t\t\t\t// 查看路由表\ngreenland> iptables -nvL -t nat\t\t// 查看iptables\ngreenland> ifconfig\t\t\t\t\t// 返回没有内容因为设备都处于没有启动状态\ngreenland> exit\t\t\t\t\t\t// 退出\n```\n\n不进入网络命名空间而直接执行新建的网络命名空间里的命令\n\n```shell\n$ ip netns exec greenland ip addr\t\t// 去掉bash, 不进入网络命名空间\n```\n\n### **实例: 西门庆和潘金莲的私会**\n**类比西门庆(李瓶儿)和潘金莲(武大郎)在各自家里(network namespace)通过连接打造好的相关联放在各自家里固定位置(IP address)的一半梯子(跨network namespace)来互相访问**\n![](link_1.PNG)\n1. 创建西门庆和潘金莲的家(network namespace)\n\n```shell\n$ ip netns add ximenqing-ns\n$ ip netns add panjinlian-ns\n$ ip netns ls\n```\n\n2. 打造相关联的用来拼接梯子的两半梯子:\n\n```shell\n//veth 从名字上来看是 Virtual ETHernet(虚拟以太网)的缩写，它的作用很简单，就是要把从一个 network namespace 发出的数据包转发到另一个 namespace。veth 设备是成对的，一个是 container 之中，另一个在 container 之外，即在真实机器上能看到的\n$ ip link add ximenqing type veth peer name panjinlian\n$ ip ls\n```\n\n3. 将打造好的两半梯子各自放到西门庆和潘金莲家里, 在需要的时候再拿出来拼接, 梯子状态是down.  \n梯子放进西门庆和潘金莲家里后, 宿主机network namespace里的xinmenqing和panjinlian桥(网络设备)就消失了.\n\n```shell\n$ ip link set ximenqing netns ximenqing-ns\t\t// 把ximenqing这个梯子(设备)塞到ximenqing家里(network namespace)\n$ ip netns exec ximenqing ip link\t//查看西门庆家中的设备\n$ ip link set panjinlian netns panjinlian-ns\t\t// 把ximenqing这个梯子(设备)塞到ximenqing家里(network namespace)\n$ ip netns exec panjinlian ip link\t//查看潘金莲家中的设备\n```\n\n4. 设定好各自一半梯子连接时候把梯子都放到各自家里确定位置,连接时候才能定位到确切位置(设置好各自的IP地址)\n\n```shell\n$ ip netns exec ximenqing-ns ip addr add dev ximenqing 192.168.188.96/24\t// 192网段采用24子网掩码\n$ ip netns exec ximenqing-ns ip addr\t\t//查看梯子是否已固定好(IP已设定)\n$ ip netns exec panjinlian-ns ip addr add dev panjinlian 192.168.188.69/24\n$ ip netns exec panjinlian-ns ip addr\n```\n\n5. 梯子固定好位置后把ximenqing和panjinlian家的梯子都up起来, 只up一家没用.\n\n```shell\n$ ip netns exec ximenqing-ns ip link set ximenqing up\n$ ip netns exec ximenqing-ns ip addr\t//可以查看到ximenqing这个网络设备状态是\"LOWERLAYERDOWN\"单方面起来了, 因为还需要把它相关联的panjinlian的那一半梯子也起来.  \n$ ip netns exec panjinlian-ns ip link set ximenqing up\n$ ip netns exec panjinlian-ns ip addr\n```\n6. 西门庆通过梯子去找潘金莲, 或潘金莲通过梯子去找西门庆.\n\n```shell\n$ ip netns exec ximenqing-ns ping -c 3 192.168.188.69\n$ ip netns exec panjinlian-ns ping -c 3 192.168.188.96\n```\n\n### **实例: 王婆帮助西门庆和潘金莲私会**\n西门庆和潘金莲打造好的各自的一半梯子和王婆家里的桥对应的一半梯子相连, 西门庆和潘金莲通过王婆家相连进行私会.  \n![](link_2_1.PNG)\n![](link_2_2.PNG)\n1. 创建wangpo这家连接桥\n\n```shell\n$ ip link add wangpo type bridge\t// 创建wangpo这个桥, 和panjinlian-ns和ximenqing-ns不一样\n$ ip netns ls\n```\n\n2. 打造西门庆和王婆家的桥相关联的用来拼接的两半梯子:\n\n```shell\n$ ip link add wp2xmq type veth peer name xmq2wp\n$ ip link\t\t// 查看创建好的西门庆到王婆家的一半梯子和王婆家到西门庆家的一半梯子\n$ ip link add pjl2wp type veth peer name wp2pjl\n$ ip link\n```\n\n3. 将打造好的四半梯子，一个放到西门庆家，一个放到潘金莲家，两个连接到王婆家的桥上两端.\n\n```shell\n$ ip link set xmq2wp netns ximenqing-ns\t\t// xiq2wp这半桥塞到西门庆家里\n$ ip netns exec ximenqing-ns ip link\t\t\t// 查看有没有塞进去\n$ ip link set pjl2wp netns panjinlian-ns\n$ ip netns exec panjinlian-ns ip link\n\n$ ip link set wp2xmq master wangpo\t\t\t// wp2xmq这半桥连接王婆这个桥, 还是可以在宿主机总network namespace看得到\n$ ip link set wp2pjl master wangpo\n$ ip link\n$ bridge link\t\t//查看王婆家刚添加的网络设备wp2xmq, wp2pjl\n```\n\n4. 固定(设立ip address)西门庆家的梯子, 而不需要固定连接王婆家的那一半梯子\n\n```shell\n$ ip netns exec ximenqing-ns ip addr add dev xmq2wp 192.168.188.96/24\n$ ip netns exec ximenqing-ns ip addr\n$ ip netns exec panjinlian-ns ip addr add dev pjl2wp 192.168.188.69/24\n$ ip netns exec panjinlian-ns ip addr\n```\n\n5. 把西门庆, 潘金莲的梯子都up起来, 也要up王婆家的桥\n\n```shell\n$ ip netns exec ximenqing-ns ip link set xmq2wp up\t//up 塞到西门庆家里的一半梯子\n$ ip netns exec ximenqing-ns ip link\n$ ip netns exec panjinlian-ns ip link set pjl2wp up\t//up 塞到潘金莲家里的一半梯子\n$ ip netns exec panjinlian-ns ip link\n\n$ ip link set wangpo up\t\t//up 王婆家的桥\n$ ip link set wp2xmq up \t//up王婆家的桥连接西门庆家的梯子\n$ ip link set wp2pjl up\t\t//up王婆家的桥连接潘金莲家的梯子\n$ bridge link\t\t\t\t//查看连接王婆家桥的梯子是否up\n```\n\n6. 王婆起到桥接作用(虚拟网桥), 西门庆和潘金莲通过王婆家桥进行私会.\n\n```shell\n$ ip netns ximenqing-ns ping -c 3 192.168.188.69\n$ ip netns panjinlian-ns ping -c 3 192.168.188.96\n```\n\n\n\n\n\n\n","tags":["IP"],"categories":["linux"]},{"title":"RPM, yum","url":"/2021/03/13/linux/rpm_yum/","content":"\n## RPM\n> RPM是”Redhat Package Manager”的缩写，根据名字也能猜到这是Redhat公司开发出来的。RPM 是以一种数据库记录的方式来将你所需要的套件安装到你的Linux 主机的一套管理程序。也就是说，你的linux系统中存在着一个关于RPM的数据库，它记录了安装的包以及包与包之间依赖相关性。RPM包是预先在linux机器上编译好并打包好的文件，安装起来非常快捷。但是也有一些缺点，比如安装的环境必须与编译时的环境一致或者相当；包与包之间存在着相互依赖的情况；卸载包时需要先把依赖的包卸载掉，如果依赖的包是系统所必须的，那就不能卸载这个包，否则会造成系统崩溃。\n> 每一个rpm包的名称都由”-“和”.”分成了若干部分。就拿 a2ps-4.13b-57.2.el5.i386.rpm 这个包来解释一下，a2ps 为包名；4.13b则为版本信息；57.2.el5为发布版本号；i386为运行平台。其中运行平台常见的有i386, i586, i686, x86_64 ，需要你注意的是cpu目前是分32位和64位的，i386,i586和i686都为32位平台，x86_64则代表为64位的平台。另外有些rpm包并没有写具体的平台而是noarch，这代表这个rpm包没有硬件平台限制。例如 alacarte-0.10.0-1.fc6.noarch.rpm.\n\n### 安装一个rpm包\n\n```shell\n$ rpm -ivh **.rpm\n```\n-i ：安装的意思\n-v ：可视化\n-h ：显示安装进度\n--force 强制安装，即使覆盖属于其他包的文件也要安装\n--nodeps 当要安装的rpm包依赖其他包时，即使其他包没有安装，也要安装这个包\n\n### 升级一个rpm包\n\n```shell\n$ rpm -Uvh filename -U ：即升级的意思\n```\n\n### 卸载一个rpm包\n\n```shell\n$ rpm -e filename 这里的filename是通过rpm的查询功能所查询到的\n```\n\n### 查询一个包是否安装\n\n```shell\n$ rpm -q rpm包名（这里的包名，是不带有平台信息以及后缀名的）\n```\n\n### 查询当前系统中所安装的所有rpm包, 列出前十个\n\n```shell\n$ rpm -qa | head -n 10\n```\n\n### rpm包的相关信息\n\n```shell\n$ rpm -qi 包名 （同样不需要加平台信息与后缀名）\n```\n\n### 列出rpm包安装的文件\n\n```shell\n$ rpm -ql 包名\n```\n通过上面的命令可以看出vim是通过安装vim-enhanced-7.0.109-6.el5这个rpm包得来的。那么反过来如何通过一个文件去查找是由安装哪个rpm包得来的\n\n### 列出某一个文件属于哪个rpm包\n\n```shell\n$ rpm -qf 文件的绝对路径\n$ rpm -qf `which tree`\n$ rpm -qf `which screen`\n```\n### rpm包安装的时候要手动配置环境变量\n\n\n## Yum\n> yum是Redhat所特有的安装RPM程序包的工具，使用起来相当方便。因为使用RPM安装某一个程序包有可能会因为该程序包依赖另一个程序包而无法安装。而使用yum工具就可以连同依赖的程序包一起安装。当然CentOS同样可以使用yum工具，而且在CentOS中你可以免费使用yum，但Redhat中只有当你付费后才能使用yum，默认是无法使用yum的\n\n### 设置proxy\n\n```xml\n\t# vim /etc/yum.conf\n\t[main]\n\tcachedir=/var/cache/yum/$basearch/$releasever\n\tkeepcache=0\n\tdebuglevel=2\n\tlogfile=/var/log/yum.log\n\texactarch=1\n\tobsoletes=1\n\tgpgcheck=1\n\tplugins=1\n\tinstallonly_limit=5\n\tbugtracker_url=http://bugs.centos.org/set_project.php?project_id=23&ref=http://bugs.centos.org/bug_report_page.php?category=yum\n\tdistroverpkg=centos-release\n\tproxy=http://child-prc.intel.com:913\n```\n\n### k8s repo\n\n```shell\n\t$ touch /etc/yum.repos.d/kubernetes.repo\n\t[kubernetes]\n\tname=Kubernetes\n\tbaseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64\n\tenabled=1\n\tgpgcheck=1\n\trepo_gpgcheck=1\n\tgpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg\n```\n\n### 安装rpm包\n\n * 列出所有可用的rpm包\n\n```shell\n$ yum list\n```\n\n * 搜索一个rpm包\n\n```shell\n$ yum search [相关关键词]\n```\n\n * 安装一个rpm包\n\n```shell\n$ yum install [-y] [rpm包名]\n```\n * 卸载一个rpm包\n\n```shell\n$ yum remove [-y] [rpm包名]\n```\n * 升级一个rpm包\n\n```shell\n$ yum update [-y] [rpm包]\n```\n\n\n\n\n\n","categories":["linux"]},{"title":"nc, netstat, tee","url":"/2021/03/13/linux/nc_netstat_tee/","content":"\n## **nc**\nnc命令用于设置路由器, 执行本指令可设置路由器的相关参数.  \n\n```\n$ nc [-hlnruz][-g<网关...>][-G<指向器数目>][-i<延迟秒数>][-o<输出文件>][-p<通信端口>][-s<来源位址>][-v...][-w<超时秒数>][主机名称][通信端口...]\n * -g<网关> 设置路由器跃程通信网关，最多可设置8个。\n * -G<指向器数目> 设置来源路由指向器，其数值为4的倍数。\n * -h 在线帮助。\n * -i<延迟秒数> 设置时间间隔，以便传送信息及扫描通信端口。\n * -l 使用监听模式，管控传入的资料。\n * -n 直接使用IP地址，而不通过域名服务器。\n * -o<输出文件> 指定文件名称，把往来传输的数据以16进制字码倾倒成该文件保存。\n * -p<通信端口> 设置本地主机使用的通信端口。\n * -r 乱数指定本地与远端主机的通信端口。\n * -s<来源位址> 设置本地主机送出数据包的IP地址。\n * -u 使用UDP传输协议。\n * -v 显示指令执行过程。\n * -w<超时秒数> 设置等待连线的时间。\n * -z 使用0输入/输出模式，只在扫描通信端口时使用。\n```\n\n### **实例**\n\n```shell\n// TCP端口扫描\n$ nc -v -z -w2 192.168.0.3 1-100\t// 扫描192.168.0.3 的端口 范围是 1-100\n// 扫描UDP端口\n$ nc -u -z -w2 192.168.0.1 1-1000\t// 扫描192.168.0.3 的端口 范围是 1-1000\n// 扫描指定端口\n$ nc -nvv 192.168.0.1 80 \t\t\t// 扫描 80端口\n\n$ nc -v -z -w2 10.239.140.133 6443\n  Ncat: Version 7.50 ( https://nmap.org/ncat )\n  Ncat: Connected to 10.239.140.133:6443.\n  Ncat: 0 bytes sent, 0 bytes received in 0.02 seconds.\n```\n\n## **netstat**\n安装\n\n```shell\nyum install net-tools\n```\n\nnetstat 命令用于显示网络状态, 利用 netstat 指令可让你得知整个 Linux 系统的网络情况.  \n\n```shell\nnetstat [-acCeFghilMnNoprstuvVwx][-A<网络类型>][--ip]\n```\n常用的命令行参数:\n```\n * -a或--all 显示所有连线中的Socket.\n * -n或--numeric 直接使用IP地址，而不显示域名(别名).\n * -t或--tcp 显示TCP传输协议的连线状况.\n * -u或--udp 显示UDP传输协议的连线状况.\n * -p或--programs 显示正在使用Socket的程序识别码和程序名称.\n * -l或--listening 显示监控中的服务器的Socket, 仅列出有在Listen(监听)的服务状态.\n * -r或--route 显示Routing Table.\n * -c或--continuous 持续列出网络状态, 每隔一个固定时间, 执行该netstat命令.\n```\n\n其它的可选参数:\n```\n * -A<网络类型>或--<网络类型> 列出该网络类型连线中的相关地址。\n * -C或--cache 显示路由器配置的快取信息。\n * -e或--extend 显示网络其他相关信息。\n * -F或--fib 显示FIB。\n * -g或--groups 显示多重广播功能群组组员名单。\n * -h或--help 在线帮助。\n * -i或--interfaces 显示网络界面信息表单。\n * -M或--masquerade 显示伪装的网络连线。\n * -N或--netlink或--symbolic 显示网络硬件外围设备的符号连接名称。\n * -o或--timers 显示计时器。\n * -s或--statistics 显示网络工作信息统计表。\n * -v或--verbose 显示指令执行过程。\n * -V或--version 显示版本信息。\n * -w或--raw 显示RAW传输协议的连线状况。\n * -x或--unix 此参数的效果和指定\"-A unix\"参数相同。\n * --ip或--inet 此参数的效果和指定\"-A inet\"参数相同。\n```\n### **实例**\n列出所有信息\n\n```shell\n$ netstat | less\nActive Internet connections (w/o servers)\nProto Recv-Q Send-Q Local Address           Foreign Address         State\n......\n```\n * `Local Address`可以看作是服务端IP和提供服务的监听端口, `Foreign Address`可以看作是客户端IP和发起链接请求的IP地址和请求端口.  \n * `ESTABLISHED`表示客户端与服务端已经建立tcp长链接.  \n * `LISTEN`表示服务端提供服务的端口仍处于监听状态, 等待客户端发起请求.  \nTCP才能再Foreign Address看到链接的客户端IP和端口, 而UDP无状态是没有的.\n\n**最经典的搭配**\n\n```shell\n$ netstat -nltp | head -n 5\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1415/sshd\ntcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve\ntcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip\n```\n查看所有链接本机6443服务端口的客户端IP地址, 地址一致的合并, 然后连接数从高到底排序.\n\n```shell\n$ netstat -antp | grep :6443 | awk '{print $5}' | awk -F \":\" '{print $1}' | sort | uniq -c | sort -r -n\n      4 10.239.4.100\t// 表示从10.239.4.100客户端请求访问本机6443服务端口的进程数为4\n      3 10.239.4.80\n      3 10.239.141.194\n      3 10.239.141.145\n      3\n      2 10.40.0.6\n      2 10.239.140.53\n      2 10.239.140.133\n      2 10.109.19.69\n      1 10.40.0.9\n      1 10.40.0.2\n      1 10.40.0.1\n      1 10.109.19.68\n```\n正如输出显示的sshd服务, 进程编号为1415, 监听端口为22, 因此我们平时用ssh协议链接linux主机时候用的端口默认是22.\n显示详细的网络状况\n\n```shell\n\t$ netstat -a\n```\n显示当前户籍UDP连接状况\n\n```shell\n$ netstat -antp\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\ntcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1415/sshd\ntcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve\ntcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip\ntcp        0      0 10.239.140.133:10000    10.239.141.145:48554    ESTABLISHED 21353/kube-vip\n```\n\n显示当前户籍UDP连接状况, `由于UDP是无状态的, 因此State那一列都为空.`\n\n```shell\n$ netstat -anup\nActive Internet connections (servers and established)\nProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\nudp        0      0 224.0.0.251:5353        0.0.0.0:*                           4507/chrome\nudp6       0      0 :::111                  :::*                                1022/rpcbind\n```\n\n显示UDP端口号的使用情况\n\n```shell\n$ netstat -apu\n```\n显示网卡列表\n\n```shell\n$ netstat -i\n```\n显示组播组的关系\n\n```shell\n$ netstat -g\n```\n显示网络统计信息\n\n```shell\n$ netstat -s\n```\n显示监听的套接口\n\n```shell\n$ netstat -l\n```\n\n## **tee**\nLinux tee命令用于读取标准输入的数据，并将其内容输出成文件.  \ntee指令会从`标准输入`设备读取数据，将其内容输出到标准`输出设备`，同时保存成文件.  \n\n```shell\n$ tee [-ai][--help][--version][文件...]\n```\n * -a或--append 　附加到既有文件的后面，而非覆盖它．\n * -i或--ignore-interrupts 　忽略中断信号。\n * --help 　在线帮助。\n * --version 　显示版本信息。\n\n### **实例**\n使用指令\"tee\"将用户输入的数据同时保存到文件\"file1\"和\"file2\"中，输入如下命令：\n\n```shell\n$ tee file1.sh file2.sh                   #在两个文件中复制内容 \n  以上命令执行后，将提示用户输入需要保存到文件的数据，输入以下内容\n  123456 回车\n  123456\n  ctrl + c/d\t\t# 推荐用ctrl+d\n$ cat file1.sh file2.sh\n  123456\n  123456\n```\n\n使用通道 `\"|\"`\n\n```shell\n$ echo \"Hello\" | tee -a test.txt\n$ cat test.txt\n  Hello!\n```\n\n\n","categories":["linux"]},{"title":"vim","url":"/2021/03/13/linux/vim/","content":"\n## 对比两个文件不同\n\n```shell\n\t$ vimdiff file1 file2\n```\n\n## Linux下复制粘贴快捷键\n 1. 在终端下：\n```\n复制命令：Ctrl + Shift + C  组合键.\n粘贴命令：Ctrl + Shift + V  组合键.\n```\n\n 2. 在控制台下：\n```\n复制命令：Ctrl + Insert  组合键　　或　　用鼠标选中即是复制。\n粘贴命令：Shift + Insert  组合键　　或　　单击鼠标滚轮即为粘贴。\n```\n================================================================================\n\n## 快速查找单词：\n\n1， 最快的方式是让光标停留在想要查找的单词的任意一个字母上面， 然后输入Shift + *  ，即可快速选中该单词，并且可以通过 n  或  N 进行下一个或上一个的匹配。\n\n2，在一个vim文件中选中单词，另vim文件中查找改单词:\n  让光标停留在vim文件中单词的第一个字母上， 然后直接敲击键盘yw两个按键拷贝该单词\n  另一个vim或本文件输入 / (Ctrl + R) 0 （即 /”0），回车， 就查找到了第一个匹配的单词， 并且可以  通过n或N进行下一个或上一个的匹配。\n\n================================================================================\n\n## 多行注释和取消注释\n多行注释：\n\n1. 进入命令行模式，按ctrl + v进入 visual block模式，然后按j或↓, k或↑选中多行，把需要注释的行标记起来\n\n2. 按大写字母I，再插入注释符，例如//\n\n3. 按（两次）esc键就会全部注释了\n\n取消多行注释：\n\n1. 进入命令行模式，按ctrl + v进入 visual block模式，按字母l(小写\"L\")或←，→键横向选中列的个数，例如 // 需要选中2列\n\n2. 按字母j，或者k选中注释符号\n\n3. 按d键就可全部取消注释\n\n\n## 全选，全部复制，全部删除\n全选（高亮显示）：按esc后，然后ggvG或者ggVG\n\n全部复制：按esc后，然后ggyG\n\n全部删除：按esc后，然后dG\n\n\n解析：\n\ngg：是让光标移到首行，在vim才有效，vi中无效 \n\nv ： 是进入Visual(可视）模式 \n\nG ：光标移到最后一行 \n\n选中内容以后就可以其他的操作了，比如： \nd  删除选中内容 \ny  复制选中内容到0号寄存器 \n\"+y  复制选中内容到＋寄存器，也就是系统的剪贴板，供其他程序用 \n\n\n## 复制剪切n行\n\n剪切>:\n快捷键方式：\ndd：剪切光标所处当前行\nn + dd：剪切光标所在行及以下共 n 行\n按 p 粘贴在光标所在行\n命令行方式：\n例如剪切1到10行，并粘贴在12行处：\n1，10 m 12\n\n复制>：\n快捷键方式：\nyy：复制光标所处当前行\nn + yy：复制光标所在行及以下共 n 行\n按 p 粘贴在光标所在行\n命令行方式：\n例如复制1到10行，并粘贴在12行处：\n1，10 co 12\n\n删除>:\n快捷键方式：\ndd：删除光标所处当前行\nn + dd：删除光标所在行及以下共 n 行\n命令行方式：\n例如删除1到10行：\n1，10 de\n\n进入命令行:>\n\"shift + :\" ：进入命令行模式\n\"set nu\"或者\"set number\" ：显示行号, \"set nonu\"或者\"set nonumber\"取消显示行号\n\"etc + etc\" ：退出命令行模式\n实际情况下，按 p 粘贴时有的情况是粘贴在光标所在的下一行，自己操作下便可知晓。\n\n\n## 撤销上一次操作:\n\nu   撤销上一步的操作\nCtrl+r 恢复上一步被撤销的操作\n注意：\n    如果你输入“u”两次，你的文本恢复原样，那应该是你的Vim被配置在Vi兼容模式了。\n\n## 实现代码折叠功能\n \n \n解决方法：直接使用vim自带的快捷键和命令，便可以实现功能强大的折叠\n \n小试折叠：\n1  :set fdm=marker  在vim中执行该命令\n2  5G  将光标跳转到第5行\n3  zf10G  折叠第5行到第10行的代码，vim会在折叠的开始和结束自动添加三个连续的花括号作为标记\n4  zR  打开所有折叠\n5  zM  关闭所有折叠\n6  zE  删除所有的折叠标签\n7  退出vim窗口再次打开，执行2-6步。依然可以折叠，但是没有标记了。重新打开后折叠信息会丢失。\n \n折叠方法：\n1  manual  （不常用）默认折叠方法，如上面第7步即为该方法，关闭vim折叠会丢失。如果想保持折叠信息，可运行 :mkview 命令，重启后用 :loadview 命令回复。该命令生成的缓存文件位于 ~/.vim/view 文件夹中。移动或重命名文件，折叠信息依然会丢失。\n2  indent  （常用）缩进折叠方法，相同的缩进中代码会被折叠。 \n3  syntax  （不常用）语法高亮折叠，在c/c++中会折叠花括号部分，其它格式代码中有的不能自动折叠。 \n4  marker  （常用）标记折叠方法，如上面1-6所使用的方法。关闭vim折叠信息不会丢失，而且易用控制和标注。\n5  还有两种 diff 和 expr，目前我还没有用过。\n \n具体介绍：\n1  以 marker 为例，可以在vim中运行 :set fdm=marker 来设置折叠方法设置。折叠方法的时候，= 两边不能有空格。也可以将该命令添加到~/.vimrc中，实现vim自动加载。\n2  在使用小试折叠中 2 3 步折叠的时候，vim会自动添加三个连续的花括号作为标记，可在开始的花括号前添加介绍，花括号后添加级别号，级别号不能为0。\n\n3  级别的定义稍显复杂。在一般编码中，通常把不需要修改的代码添加标记折叠。没有必要在给折叠分等级。如果想快速折叠就切换为indent折叠方面，适用于任何有缩进的代码。\n \n折叠命令：\n1  zf  创建折叠，可以按照前面的方式进行折叠，也可以选中代码后进行折叠。 \n2  zF  在当前行创建折叠。当一开始就计划要折叠所写代码的时候，可以用该命令创建一对折叠符号，然后再往里面填写内容。\n3  :5,10fo  在vim中运行该命令会在折叠 5-10 行中的代码，可以用其它数字代替之。\n4  zd  删除光标下的折叠。\n5  zD  删除光标下的折叠，以及嵌套的折叠。\n6  zE  删除窗口内的所有折叠。仅当 manual 和 marker 折叠方法下有效。\n \n打开和关闭折叠：\n1  zo  打开光标下的折叠。\n2  zO  打开光标下的折叠，以及嵌套的折叠。\n3  zc  关闭光标下的折叠。\n4  zC  关闭光标下的折叠，以及嵌套的折叠。\n5  za  当光标在关闭折叠上时，打开之。在打开折叠上时，关闭之。\n6  zA  和za类似，不过对当前折叠和其嵌套折叠都有效。\n7  zv  打开当前光标所在折叠，仅打开足够的折叠使光标所在的行不被折叠。\n8  zr和zm  一层一层打开折叠和一层一层关闭折叠，这两个命令会递减和递增一个叫foldlevel的变量。如果你发现zm和zr不灵了，那有可能是你连续按的zr或zm次数多了，只要多按几次让foldlevel回到正常状态即可。执行以下zR和zM也可直接让foldlevel回到正常状态。\n9  zR和zM  打开所有折叠，设置foldlevel为最高级别。关闭所有折叠，设置foldlevel为0。\n \n在折叠间移动：\n1  [z  到当前打开折叠的开始。如果已在开始处，移到包含这个折叠的折叠开始处。\n2  ]z  到当前打开折叠的结束。如果已在结束处，移到包含这个折叠的折叠结束处。\n3  zj  把光标移动到下一个折叠的开始处。\n4  zk  把光标移动到前一个折叠的结束处。\n \n \n参考：\n在vim中运行 :h Folding 命令，查看折叠的帮助文档。\n\n\nturbostat 由 kernel-tools 数据包提供。\n是对 Intel® 64 位处理器中处理器的拓扑、频率、空闲的电源状态统计、温度和用电量的报告。\n\n## vim缩进参数解析\n缩进用 tab 制表符还是空格，个人爱好问题。但是在大多项目中，习惯使用空格。关于缩进，vim中可以通过如下四个参数进行配置\n\nset tabstop=4\n  <--->  set ts=4\nset softtabstop=4\n\nset shiftwidth=4\n\nset noexpandtab / expandtab\n解析： \n\ntabstop \n表示按一个tab之后，显示出来的相当于几个空格，默认的是8个。 \n\nsofttabstop \n表示在编辑模式的时候按退格键的时候退回缩进的长度。 \n\nshiftwidth \n表示每一级缩进的长度，一般设置成跟 softtabstop 一样 \n\nexpandtab与noexpandtab \n\n当设置成 expandtab 时，缩进用空格来表示，noexpandtab 则是用制表符表示一个缩进。个人习惯使用 ｀set expandtab｀ \n\n＃标志tab与空格 \n\n在vim中，默认情况下，没法区分空格和缩进，所以我们需要配置，使其能够区分。\n\n\n## vim内容左对齐\n:{range}left [margin]\n\":3,6 left\" 命令后，3~6行代码就左对齐，距离边缘0\n\":3,6 left 4\" 命令后，3~6行代码就左对齐，距离边缘4\n上面的命令会将代码直接都 “左对齐”了，但代码块并不都是左对齐的啊，所以，还是在命令行模式下，使用 ctrl+v 去选中代码块，按下=号，从而让代码块对齐比较好用。\n\n方法一:\n\n命令模式下：=:1,$\n\n方法二：\ngg（把关标定位到最上面），V（进入VISUAL模式），shift+g（选中整篇文本），然后＝。\n\n方法三：\ngg=G\n\n\n## 查看某个函数定义\nshift + k\n\n## 分屏\n\n水平分屏：\n：sp\t\t水平分屏打开当前文件\n：sp filename\t水平分屏打开另一个文件\nvim -o exe18.c Makefile hello.c\t\t默认打开3个文件并将它们水平分屏\nvim -o3 exe18.c Makefile hello.c\t打开3个文件并将它们水平分屏\nvim -on ... (n可以省略，表示分屏的个数)\n\nvim -o *.c 垂直打开所有.c文件\n\n垂直分屏\n：vsp\t\t垂直分屏打开当前文件\n：vsp filename\t垂直分屏打开另一个文件\nvim -O exe18.c Makefile hello.c\t\t默认打开3个文件并将它们垂直分屏\nvim -O3 exe18.c Makefile hello.c\t打开3个文件并将它们垂直分屏\n     ↓\n   （大写o）\n\n切换屏幕\nctrl+两次w，或者ctrl + ←||→ 切换屏幕\n\n退出屏幕\n:q \t退出当前光标在的屏幕\n：qall\t退出所有屏幕\n\n保存\n：w\t保存当前屏幕\n：wall\t保存所有打开的屏幕\n：wqall\t保存退出所有打开的屏幕\n\n为了让鼠标可以在几个屏幕间自由切换。\n按：\"Esc\"键 + “：”，输入：set mouse=a 。然后，回车(Enter)，这样鼠标就可以在多屏幕之间自由移动了，但是不能复制vim内容\n：set mouse=v，即可选中复制内容到粘贴板\n\n\n## 显示所打开的文件名\n1、:set laststatus=2\n:file或:f\n\n2、CTRL+g\n\n3、在/etc/vim/vimrc中添加\nset statusline=%F\\ [%{&fenc}\\ %{&ff}\\ L%l/%L\\ C%c]\\ %=%{strftime('%Y-%m-%d\\ %H:%M')}\nset laststatus=2\n\n\n## 补全代码\nvim里面有个快捷键可以用来补全代码 C-n  （Ctrl+n）\n  需要先有头文件，如vim中输入str后，按ctrl+n不会显示strlen()函数，只有在vim中添加\n  #include<string.h>头文件后才会智能提示strlen()函数\n\n”Ctrl + }“ 可以跳转到函数或变量的定义处，“Ctrl + o”可以返回上一个跳转页面。\n\n\n## 比较两个文件file1和file2\nvim file1 file2 -O\n\t-o(小写)水平打开两个文件\n\t-O(大写)垂直打开两个文件\n\n4294967295000 / (1000 * 1000) = 4294967.295\n\n垂直分屏\n：vsp filename\t垂直分屏打开另一个文件\n为了让鼠标可以在几个屏幕间自由切换。\n按：\"Esc\"键 + “：”，输入：set mouse=a 。然后，回车(Enter)，这样鼠标就可以在多屏幕之间自由移动了。\n\n================================================================================\nThe ycmd server SHUT DOWN (restart with ':YcmRestartServer'). \nUnexpect ...ype ':YcmToggleLogs ycmd_*.log' to check the logs\n:YcmToggleLogs ycmd_37783_stderr_zcxys8hw.log\n\tImprotError: Python version mismatch:module was compiled for Python 3.6, \n\tbut the interpreter version is incompatible:3.5.2(default), Nov 12 2018, 13:43:14\n:YcmToggleLogs ycmd_44993_stderr_uid3uor7.log\n:YcmToggleLogs ycmd_49853_stderr_yckx0mks.log\n\n================================================================================\nVim如何跳转到光标的上次位置\nCtrl + O\nCtrl + I\n装了Ctag的插件可以跳转 Ctrl+】。查看函数调用。 \n================================================================================\n\nvim复制代码包含注释格式如\"//\"会乱掉\n\t乱码问题\n\t一般来说只需要正确设置vim的编码识别序列就很少会遇到乱码问题：\n\tset fileencodings=ucs-bom,utf-8,utf-16,gbk,big5,gb18030,latin1\n\n\t1. 每次复制代码时，如果代码里有 // 这样的注释就容易让格式乱掉，通过下面的设置就可以避免这种情况。  \n\t2.   \n\t3. 粘贴代码时取消自动缩进  \n\t4. VIM在粘贴代码时会自动缩进，把代码搞得一团糟糕，甚至可能因为某行的一个注释造成后面的代码全部被注释掉，我知道有同学这个时候会用vi去打开文件再粘贴上去（鄙人以前就是这样），其实需要先设置一下  \n\t5. set paste  \n\t6. 然后再进入插入模式粘贴，代码就不会被自动缩进。可是敲代码的时候需要自动缩进，又得改回来:  \n\t7. set nopaste  \n\t8. 最方便的方法就是在.vimrc中加一句：  \n\t9. set pastetoggle=<F9>  \n\t10. 以后在插入模式下，只要按F9键就可以切换自动缩进。\n\nFrom <https://www.iteye.com/blog/haoningabc-1907479> \n\n\n\n================================================================================\n\n","categories":["linux"]},{"title":"tmux 操作","url":"/2021/03/13/linux/tmux操作/","content":"\n## Tmux安装\nubuntu版本下直接apt-get安装\n\n```shell\n\tsudo apt-get install tmux\n```\ncentos7版本下直接yum安装\n\n```shell\n\tyum install -y tmux\n```\n## 创建会话\n默认创建一个会话，以数字命名。（不推荐）\n\n```shell\n\ttmux\n```\n新建会话，比如新创建一个会话以\"ccc\"命名\n\n```shell\n\ttmux new -s ccc\n```\n加上参数-d，表示在后台新建会话\n\n```shell\n\ttmux new -s shibo -d\n```\n查看创建得所有会话\n\n```shell\n\ttmux ls\n```\n登录一个已知会话。即从终端环境进入会话。\n第一个参数a也可以写成attach。后面的aaa是会话名称。\n\n```shell\n\ttmux a -t aaa\n```\n退出会话不是关闭：\n登到某一个会话后, 依次按键`ctrl-b + d`, 这样就会退化该会话, 但不会关闭会话。\n如果直接ctrl + d, 就会在退出会话的通话也关闭了该会话！\n\n## 重命名会话\n\n```shell\n$ tmux ls  \nwangshibo: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached)\n\n$ tmux rename -t wangshibo kevin\n\n$ tmux ls\nkevin: 1 windows (created Sun Sep 30 10:17:00 2018) [136x29] (attached)\n```\n\n## 关闭会话（销毁会话）\n\n```shell\n$ tmux ls\naaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22]\nbbb: 1 windows (created Wed Aug 30 19:02:09 2017) [112x22]\n\n$ tmux kill-session -t bbb\n\n$ tmux ls\naaa: 2 windows (created Wed Aug 30 16:54:33 2017) [112x22]\n```\n\n## tmux快捷键\nctrl+b,shift + / 查看tmux所有快捷键\n\nctrl+b  激活控制台；此时以下按键生效\n\nctrl+b，shift + % 左右分割屏幕，\n\nctrl+b，shift + ' 上下分割屏幕\n\nexit 退出当前的分割屏，会自动切换到其它分割屏\n\nctrl + b + z 挂起(分割屏)\n\nctrl + b + 方向键 以1个单元格为单位移动边缘以调整当前面板大小\n\nctrl + b, alt + 方向键 以5个单元格为单位移动边缘以调整当前面板大小\n\nctrl+b 空格键       采用下一个内置布局，这个很有意思，在多屏时，用这个就会将多有屏幕竖着展示\n\nctrl + b, 单独按z最大化当前pane, 然后ctrl+b再单独按z恢复原来分割屏状态\n\nctrl+b, 单独按x键， 删除当前面板，同下\n\nctrl + a + d, 删除当前面板，同上\n\nctrl+b, 单独q键， 显示panel号\n\nctrl+b, 单独s键， 选择并切换会话；在同时开启了多个会话时使用\n\nctrl+b, 单独d键，detached tmux，tmux仍在后台运行\n\n\n\n","categories":["linux"]},{"title":"time","url":"/2021/03/13/linux/time/","content":"\n## 第一种查看并修改时间\n安装在虚拟机上的CentOS7的时间分为系统时间和硬件时间。二者都修改，重启系统（init 6 )才会永久生效。\n\n修改步骤如下\n\n查看当前系统时间\n\n```shell\n\tdate\n\tTue Sep 15 16:00:51 CST 2020\n\t//`CST` 代表China Standard Time\n\t\n\tdate -R\n\tTue, 15 Sep 2020 16:00:55 +0800\n\t// +0800表示我们国家的东八区; -0800表示西八区，是美国旧金山所在的时区\n```\n\n修改当前系统时间 `date -s \"2018-2-22 19:10:30\"`\n查看硬件时间 `hwclock --show`\n修改硬件时间 `hwclock --set --date \"2018-2-22 19:10:30\"`\n同步系统时间和硬件时间 `hwclock --hctosys`\n保存时钟 `clock -w`\n重启系统（init 6）后便发现系统时间被修改了\n\ninit0:关机\ninit1：单用户形式，只root进行维护\ninit2：多用户，不能使用net file system\ninit3：完全多用户\ninit5：图形化\ninit6：重启\ninit是Linux系统操作中不可缺少的程序之一。所谓的init进程，它是一个由内核启动的用户级进程。 \n内核自行启动（已经被载入内存，开始运行，并已初始化所有的设备驱动程序和数据结\n构等。之后，就通过启动一个用户级程序init的方式，完成引导进程。所以,init始终是第一个进程（其进程编号始终为1）。 \n内核会在过去曾使用过init的几个地方查找它，它的正确位置（对Linux系统来是/sbin/init）。如果内核找不到init，它就会试着运行/bin/sh，如果运行失败，系统的启动会失败。\n\n## 选择时区\n\n```shell\n\ttzselect\n```\n然后根据提示输入5选择`5) Asia`回车, 然后9选择`9) China`回车,然后1选择`1) Beijing Time`.  \ntzselect命令只告诉你选择的时区的写法，并不会生效。所以现在它还不是东8区北京时间。你可以在.profile、.bash_profile或者/etc/profile中设置正确的TZ环境变量并导出。 例如在.bash_profile里面设置 TZ='Asia/Shanghai'; export TZ并使其生效。\n\n\n\n\n\n","categories":["linux"]},{"title":"解压缩命令合集","url":"/2021/03/13/linux/解压缩命令合集/","content":"\n## 解压缩命令\n\n### tar\n```\n-c: 建立压缩档案\n-x：解压\n-t：查看内容\n-r：向压缩归档文件末尾追加文件\n-u：更新原压缩包中的文件\n```\n\n这五个是独立的命令，压缩解压都要用到其中一个，可以和别的命令连用但只能用其中一个。下面的参数是根据需要在压缩或解压档案时可选的。\n\n```\n-z：有gzip属性的\n-j：有bz2属性的\n-Z：有compress属性的\n-v：显示所有过程\n-O：将文件解开到标准输出\n```\n\n下面的参数-f是必须的.  \n-f: 使用档案名字，切记，这个参数是最后一个参数，后面只能接档案名.  \n```shell\ntar -cf all.tar *.jpg \n```\n\n这条命令是将所有.jpg的文件打成一个名为all.tar的包。-c是表示产生新的包，-f指定包的文件名。 \n\n```shell\ntar -rf all.tar *.gif\n```\n这条命令是将所有.gif的文件增加到all.tar的包里面去。-r是表示增加文件的意思。 \n\n```shell\ntar -uf all.tar logo.gif\n```\n\n这条命令是更新原来tar包all.tar中logo.gif文件，-u是表示更新文件的意思。 \n```shell\ntar -tf all.tar\n```\n这条命令是列出all.tar包中所有文件，-t是列出文件的意思 \n```shell\ntar -xf all.tar\n```\n这条命令是解出all.tar包中所有文件，-x是解开的意思 \n\n### 压缩\n\n```shell\ntar –cvf jpg.tar *.jpg //将目录里所有jpg文件打包成tar.jpg\n\ntar –czf jpg.tar.gz *.jpg   //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩，生成一个gzip压缩过的包，命名为jpg.tar.gz\n\ntar –cjf jpg.tar.bz2 *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用bzip2压缩，生成一个bzip2压缩过的包，命名为jpg.tar.bz2\n\ntar –cZf jpg.tar.Z *.jpg   //将目录里所有jpg文件打包成jpg.tar后，并且将其用compress压缩，生成一个umcompress压缩过的包，命名为jpg.tar.Z\n\nrar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linux\n\nzip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux\n```\n\n### 解压\n\n```shell\ntar –xvf file.tar //解压 tar包\n\ntar -xzvf file.tar.gz //解压tar.gz\n\ntar -xjvf file.tar.bz2   //解压 tar.bz2\n\ntar –xZvf file.tar.Z   //解压tar.Z\n\nunrar e file.rar //解压rar\n\nunzip file.zip //解压zip\n```\n\n### 总结\n\n```\n1、*.tar 用 tar –xvf 解压\n\n2、*.gz 用 gzip -d或者gunzip 解压\n\n3、*.tar.gz和*.tgz 用 tar -xzf 解压\n\n4、*.bz2 用 bzip2 -d或者用bunzip2 解压\n\n5、*.tar.bz2用tar -xjf 解压\n\n6、*.Z 用 uncompress 解压\n\n7、*.tar.Z 用tar –xZf 解压\n\n8、*.rar 用 unrar e解压\n\n9、*.zip 用 unzip 解压\n\n10、*.tgz 用tar zxvf  *.tgz  -C  ./\n```\n\nStay hungry, stay foolish!\n\nFrom <https://www.cnblogs.com/wi100sh/p/4178021.html> \n\n","categories":["linux"]},{"title":"ssh-keygen,known_hosts 免密登陆","url":"/2021/03/13/linux/ssh-keygen_免密登陆/","content":"\n## **ssh-keygen**\n\n| Nodes | IP |\n| :------: | :------: |\n| node01 | 10.67.0.1 |\n| node02 | 10.67.0.2 |\n| node03 | 10.67.0.3 |\n\n设置免密登陆原理就是每台要免密登陆的机器上在指定路径下都存放有其它机器的public key就可以了.  \n\n```shell\n\tcat << EOF >> /etc/hosts\n\t10.67.0.1 node01\n\t10.67.0.2 node02\n\t10.67.0.3 node03\n\tEOF\n```\n\n### 1. 三个节点都生成公私匙   \n\n```shell\n\t$ ssh-keygen\t// 直接回车就可以了\n\t......\n\t\n\t$ ls /root/.ssh/\n\tid_rsa  id_rsa.pub  known_hosts\n```\n### 2. 三个节点把公匙文件复制为authorized_keys文件\n\n```shell\n\t$ cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys\n```\n### 3. node01,node02上的authorized_keys内容都追加到node03上的authorized_keys里, 然后再拷贝node03上的authorized_keys文件到node01和node02上覆盖原来的authorized_keys就可以了.  \n每台机器上查看/root/.ssh/authorized_keys文件内容都能看到三台机器的id_rsa.pub内容\n\n```shell\n\t// node01\n\tscp /root/.ssh/authorized_keys root@node03:~/.ssh/authorized_keys_centos1\n\t\n\t// node02\n\tscp /root/.ssh/authorized_keys root@node03:~/.ssh/authorized_keys_centos2\n\t\n\t// node03\n\tcat /root/.ssh/authorized_keys_centos1 >> /root/.ssh/authorized_keys\n\tcat /root/.ssh/authorized_keys_centos2 >> /root/.ssh/authorized_keys\n\tscp /root/.ssh/authorized_keys root@node01:/root/.ssh/authorized_keys\n\tscp /root/.ssh/authorized_keys root@node02:/root/.ssh/authorized_keys\n```\n### 4. 可以直接免密登陆\n\n```shell\n\t$ ssh root@<node-IP>\n\t$ ssh root@<Node-Name>\t//前提是已在/etc/hosts中做过了IP与主机名的映射\n```\n\n## ssh-copy-id\n在一台机器上执行`ssh-keygen`命令之后可以使用`ssh-copy-id`快速拷贝到其它机器\n\n```shell\n\tssh-copy-id root@node01\n\tssh-copy-id root@node02\n\tssh-copy-id root@node03\n```\n\n## known_hosts\n用ssh命令第一次访问某台机器时会记录此机器的指纹(fingerprint)信息到本地/root/.ssh/known_hosts文件中\n\n```shell\n\t$ ssh node02\n\tThe authenticity of host 'node02 (10.67.0.2)' can't be established.\n\t`ECDSA key fingerprint` is SHA256:7SpY56wn********sKdq6fqxHxXB/7b34yWeHUdv11o.\n\t`ECDSA key fingerprint` is MD5:5d:28:********:95:91:0f:8f:c0:51:1f:1a:fe:bc.\n\tAre you sure you want to continue connecting (yes/no)?yes\n```\n\n","categories":["linux"]},{"title":"Istio 01 conception","url":"/2021/03/13/microService/istio/istio_01_conception/","content":"\n## Istio架构组件简介\n> 官网: https://istio.io/\n> 官网中文: https://istio.io/zh/\n> Github: https://github.com/istio/istio\n> 中文社区: https://istio.cn/\n\n### 简介\n> Istio 是一个由谷歌、IBM 与 Lyft 共同开发的开源项目，旨在提供一种统一化的微服务连接、安全保障、管理与监控方式。具体来说，Istio 是一个开源服务网格平台，它确保微服务在处理故障时以指定的方式相互连接。\n\n### 架构\n> Istio 服务网格逻辑上分为数据平面和控制平面。\n> * 数据平面由一组以 sidecar 方式部署的智能代理（Envoy）组成。这些代理可以调节和控制微服务及 Mixer 之间所有的网络通信。\n> * 控制平面负责管理和配置代理来路由流量。此外控制平面配置 Mixer 以实施策略和收集遥测数据。\n![](istio_arch.jpeg)\n\n### Envoy\n![](envoy_xds.png)\n\n```\nListeners: 设置监听IP:Port, 到达sidecar的请求都会到这里来.\nRoutes: 根据route, 如: - match: {prefix:”/”} route: {cluster: service_envoy}选择 Cluster.\nClusters: 找到相应的Cluster name: service_envoy, 定义了一些轮询规则等.\nEndpoints: 指定路由转发到哪里, IP:Port\n```\n\nEnvoy是经过CNCF(Cloud Native Computing Foundation,云原生计算基金会(是一个开源软件基金会，它致力于云原生（Cloud Native）技术的普及和可持续发展。)非常成功毕业的sidecar软件.\n从CNCF毕业的软件还有Prometheus, Kubernetes.\n\n> Istio 使用 Envoy 代理的扩展版本，Envoy 是以 C++ 开发的高性能代理，用于调解服务网格中所有服务的所有入站和出站流量。Envoy 的许多内置功能被 istio 发扬光大，例如：\n> Envoy 被部署为 sidecar，和对应服务在同一个 Kubernetes pod 中。这允许 Istio 将大量关于流量行为的信号作为属性提取出来，而这些属性又可以在 Mixer 中用于执行策略决策，并发送给监控系统，以提供整个网格行为的信息。\n> Sidecar 代理模型还可以将 Istio 的功能添加到现有部署中，而无需重新构建或重写代码。可以阅读更多来了解为什么我们在设计目标中选择这种方式。\n> * 动态服务发现\n> * 负载均衡\n> * TLS 终止\n> * HTTP/2 & gRPC 代理\n> * 熔断器\n> * 健康检查、基于百分比流量拆分的灰度发布\n> * 故障注入\n> * 丰富的度量指标\n\n### Mixer\n> Mixer 是一个独立于平台的组件，负责在服务网格上执行访问控制和使用策略，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级属性，发送到 Mixer 进行评估。有关属性提取和策略评估的更多信息，请参见 Mixer 配置。\n> Mixer 中包括一个灵活的插件模型，使其能够接入到各种主机环境和基础设施后端，从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。\n\n### Pilot\n> 控制面中负责流量管理的组件为Pilot\n\n### 流量管理\nIstio 维护了一个内部服务注册表 (service registry)，它包含在服务网格中运行的一组服务及其相应的服务 endpoints。Istio 使用服务注册表生成 Envoy 配置。\nIstio 不提供服务发现，尽管大多数服务都是通过 Pilot adapter 自动加入到服务注册表里的，而且这反映了底层平台（Kubernetes、Consul、plain DNS）的已发现的服务。还有就是，可以使用 ServiceEntry 配置手动进行注册。\n\n\n### Envoy\n> Istio 选择了开源的Envoy作为它的网络代理，envoy是2016年用c++的高性能的代理组件，用于管理所有服务的入口和出口流量.\n> envoy内置了很多功能，像服务发现，负载均衡，多种协议的支持，断路器，流量分割，健康检查，模拟一些故障，还有监控指标等等.\n> envoy会以sidecar的形式部署在每一个POD中负责完成Istio的核心功能，如果把Istio比作公司，envoy就是这个公司的一些员工，所有的具体工作都是由envoy干的\n\n### Pilot\n![](Pilot_Architecture.png)\n> 控制面中负责流量管理的组件为Pilot\n> Pilot 和 Envoy组件有了Istio就可以运转起来了，其它组件可以不需要.\n> Pilot 好比是envoy的直接领导，顶头上司，告诉并协助envoy怎样工作，因为envoy跑在一个容器里的.\n * Pilot告诉envoy集群都有哪些服务，envoy就可以做服务发现了.\n * Pilot根据用户配置的流量管理和服务信息转换成envoy能够识别的格式，然后分发给envoy，告诉envoy哪些POD需要多少流量，envoy就能做A/B测试，蓝绿部署.\n * Pilot告诉envoy多长时间算超时，应该重试几次.\n> Pilot 为 Envoy sidecar 提供服务发现功能，为智能路由（例如 A/B 测试、金丝雀部署等）和弹性（超时、重试、熔断器等）提供流量管理功能。它将控制流量行为的高级路由规则转换为特定于 Envoy 的配置，并在运行时将它们传播到 sidecar。\n> Pilot 将平台特定的服务发现机制抽象化并将其合成为符合 Envoy 数据平面 API 的任何 sidecar 都可以使用的标准格式。这种松散耦合使得 Istio 能够在多种环境下运行（例如，Kubernetes、Consul、Nomad），同时保持用于流量管理的相同操作界面。\n * Platform Adapter: 平台适配器。针对多种集群管理平台实现的控制器，得到API server的DNS服务注册信息（即service名与podIP的对应表）、入口资源以及存储流量管理规则的第三方资源\n * Abstract Model：维护了envoy中对service的规范表示。接收上层获取的service信息转化为规范模型\n * Envoy API：下发服务发现、流量规则到envoy上\n * Rules API：由运维人员管理。可通过API配置高级管理规则\n\n\n\n### Mixer (独立组件)\n两大功能: 策略，遥测\n> 策略: 为整个集群执行访问控制, 哪些用户可以访问哪些服务，还有一些策略的管理，像对一些服务的访问速度的限制等，比如服务A最多接受100QPS，超过的就会被直接扔掉\n> 遥测: 数据的收集和汇报，从envoy(proxy)收集数据，收集的是服务之间流转的数据, 收集的数据会汇报给其它对象，每个汇报的对象都有Adapter(适配器)转换汇报的数据为自己认识的格式，如Prometheus.\n可以不用Mixer也可以把Istio跑起来\n\n### Galley\n> 最初负责验证配置，Istio1.1之后升级为Istio整个平面的配置管理中心，校验各种配置是否正确\n\n### Citadel\n> 安全相关的，为用户到服务，服务到服务之间提供安全的通信，可以让http服务无感知的升级成为https服务，还有服务的访问授权等.\n\n\n## Istio 解决的问题\n> * 原来的单个应用拆分成多个微服务, 它们之间相互调用才能完成微服务，组件(服务模块)越多，出错概率越大，就会非常难以排查.\n> * 用户请求出现问题: 1.错误， 2.慢响应\n>  - 请求错误，得知道哪个步骤出错了，微服务之间哪些能调用成功哪些失败.\n>  - 请求响应太慢, 各个链路调用，耗时是多少，哪些并发执行哪些串行.\n> * 设置timeout\n> * 设置重试机制\n> * 某些节点异常(如load高)\n\n\n\n\n### A/B测试\n * A/B测试的基本思想是什么?\n> A/B测试的基本思想包括：提供两个方案并行测试。不同方案之间只存在一个变量，排除其他干扰因素。以某种标准判定结果优劣，筛出最优方案。其中第二点，即单变量，需要重点关注。因为某种方案的优劣，不光与方案本身有关，也可能与方案所适配的环境密不可分。\n> 所谓A/B测试，简单说来就是针对想调研的问题提供两种不同的备选解决方案（比如两个下单页面），然后让一部分用户使用方案A，另一部分用户使用方案B像LinkedIn的升级迭代一样，最终通过数据观察对比确定最优方案。\n\n * A/B测试案例\n> 在互联网产品开发过程中，我们经常面临多种方案的抉择。有些选择轻而易举，例如面向大众的偏理性产品，主色调定位蓝色最权威稳重而广为接受（比如百度、Facebook）；强调安全的服务，绿色是常规之选。但有些时候，备选方案模棱两可，甚至区别看起来无关紧要，例如某个按钮是用橙色还是红色，摆放的位置偏左还是偏右。面对这种情况，传统解决方式要么是根据设计师的审美来定，要么是一群人共同表决，要么由某个领导拍板决定。无论哪种方式，都不免受到个人主观因素的制约，未必代表的了广大用户在实际使用场景中的认知。“不识庐山真面目，只缘身在此山中。”通常解决此类情况，最合理的办法之一，就是进行A/B测试（A/B Testing）。所谓A/B测试，简单说来就是针对想调研的问题提供两种不同的备选解决方案（比如两个下单页面），然后让一部分用户使用方案A，另一部分用户使用方案B，最终通过数据观察对比确定最优方案。在现实生活中，达尔文《物种起源》中提到的物竞天择原理，本质上就是在谈同一物种的不同变体经过外部环境测试后存留延续下的最优结果，也算是一种A/B测试。\n\n * 团队使用A/B测试有什么好处？\n> 在数据面前任何妄加揣测的评断都可能是不准确的。通过A/B测试，产品团队能够获悉哪些对激发用户活跃度有所帮助，哪些又徒劳无功（有时候后者显得更为重要，因为它能终结一场无意义的争吵，或是缩减许多不必要的消耗）。一次测试或许能带来良好的改善效果，但也绝非意味着提升空间已然消失。微软公司的测试经验表明，在他们做的所有测试中，有三分之一被验证为成功有效，剩下的三分之二平平无奇或压根就是失败的。谷歌在2009年做了12000多次测试，但其中只有10％带来了业务变化。产品不息，测试不止。永远不要满足于当下的结果，因为世界上总有更好的解决方案\n\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio 04 sidecar inject","url":"/2021/03/13/microService/istio/istio_04_inject/","content":"\n## **What kind of resources can be injected**\nJob, DaemonSet, ReplicaSet, Pod, Deployment, 被istio sidecar 注入后会进行iptable重新网络初始化等操作\n\nService, Secrets, ConfigMap 这三个被istio注入后不会有啥改变\n\n## **sidecar 注入步骤实例**\n\n```shell\n$ touch jiuxi-deployment.yaml\n$ kubectl create ns jiuxi-ns\n$ kubectl apply -f jiuxi-deployment.yaml -n jiuxi-ns\n$ istioctl kube-inject -f jiuxi-deployment.yaml -o jiuxi-deployment-inject.yaml\n$ istioctl kube-inject -f jiuxi-deployment.yaml | kubectl apply -f - -n jiuxi-ns\n```\n### **1. 源文件部署, no sidecar:**\n\n\n```xml\n\t$ vim jiuxi-deployment.yaml\n\tapiVersion: app/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: jiuxi\n\t  labels:\n\t    app: jiuxi\n\tspec:\n\t  replicas: 1\n\t  selector:\n\t    matchLabels:\n\t      app: jiuxi\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: jiuxi\n\t    spec\n\t      containers:\n\t      - name: nginx\n\t        image:nginx:1.14-alpine\n\t        imagePullPolicy: IfNotPresent\n\t        ports:\n\t        - containerPort: 80\n```\n部署:\n```shell\n\t$ kubectl apply -f jiuxi-deployment.yaml -n jiuxi-ns\n```\n查看pod提供对外服务的端口号:\n\n```shell\n\t$ kubect exec -it po/jiuxi-*** -n jiuxi-ns -- netstat -ntlp\n\tActive Internete connections (only servers)\n\tProto Recv-Q Send-Q Local Address       Foreign Address      State      PID/Program name\n\ttcp     0          0.0.0.0:80             0.0.0.0:*          LISTEN     1/nginx: master pro\n```\n可以看到有个master主进程对外提供80端口服务\n\n### **2. sidecar注入**\n是先生成全新的deployment部署pod, 再同时删除原来的deployment和pod资源，从pod的名字哈希后缀可以观察到变化.\n\n```shell\n\t$ istioctl kube-inject -f jiuxi-deployment.yaml | kubectl apply -f - -n jiuxi-ns\n```\n可以观察到READY的有2个pod且pod名称hash部分有变化, dump到文件中查看inject后的资源配置信息 \n\n```shell\n\t$ istioctl kube-inject -f jiuxi-deployment.yaml > jiuxi-deployment-inject.yaml\n```\nsidecar注入后可以观察到pod里有两个运行的容器 `nginx`, `istio-proxy`, 还有一个已运行结束的`istio-init`容器.  \n`istio-init`容器是用来初始化网络命名空间, 使得`nginx`和`istio-proxy`处于相同的网络空间中.\n\n```shell\n\t$ kubectl exec -it -n jiuxi-ns po/jiuxi-* -c nginx -- ifconfig\t// 或者将ifcongig替换为route -n(查看路由表)\n\teth0: ......\n\t      inet addr:10.244.10.11......\n\tlo:\n\t    ......\n\t$ kubectl exec -it -n jiuxi-ns po/jiuxi-* -c istio-proxy -- ifconfig // 或者将ifcongig替换为route -n(查看路由表)\n\teth0: ......\n\t      inet addr:10.244.10.11......\n\tlo:\n\t    ......\n```\n从以上可以看到`nginx`和`istio-proxy`处于相同的网络空间中.  \nsidecar 注入后查看pod提供对外服务的端口号:\n\n```shell\n\t$ kubect exec -it po/jiuxi-*** -c nginx -n jiuxi-ns -- netstat -ntlp\n\t$ kubect exec -it po/jiuxi-*** -c istio-proxy -n jiuxi-ns -- netstat -ntlp\n\tProto Recv-Q Send-Q Local Address       Foreign Address      State      PID/Program name\n\ttcp     0          0.0.0.0:80             0.0.0.0:*          LISTEN     1/nginx: master pro\n\t......//Pod对外服务端口号会多增加5个\n```\n部署istio后搭建bookinfo实例, 查看productpage 网络\n\n```shell\n\t$ k exec -it -n book-info productpage-v1-7df7cb7f86-gjtfz -c istio-proxy -- netstat -ntlp\n\tActive Internet connections (only servers)\n\tProto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name\n\ttcp        0      0 0.0.0.0:15090           0.0.0.0:*               LISTEN      35/envoy\n\ttcp        0      0 127.0.0.1:15000         0.0.0.0:*               LISTEN      35/envoy\n\ttcp        0      0 0.0.0.0:15001           0.0.0.0:*               LISTEN      35/envoy\n\ttcp        0      0 0.0.0.0:15006           0.0.0.0:*               LISTEN      35/envoy\n\ttcp        0      0 0.0.0.0:15021           0.0.0.0:*               LISTEN      35/envoy\n\ttcp6       0      0 :::9080                 :::*                    LISTEN      -\n\ttcp6       0      0 :::15020                :::*                    LISTEN      1/pilot-agent\n```\n### **3. istio-iptables**\ntcp/ip协议栈一般都是由OS内核去实现的,无论是linux, windows, mac. 因为编码时候不会考虑传输层是什么, 怎么封报, 只是在应用层用http协议或调一些SDK或者调用开源包来实现数据转发或者说是报文的封装等.  \n但是真正实现网络层协议的还是在操作系统内核来干这些事情.  \n操作系统内核又一个模块 `netfilter`, 它有一部分功能就是来制定些网络策略, 如不允许某一个IP或某一个网段的IP或者POD来访问你的主机.  \n我们不太可能直接去操作位于内核的`netfilter`来制定这些策略和管理.  \nLinux 给我们了一个客户端工具`iptables`，通过这个工具可以跟OS的`netfilter模块`进行交互来向它发送指令执行策略制定.  \n`iptables`可以看作一个客户端, `netfilter`可以看作一个server端.\n\nistio-init容器在启动后不久就停止运行，改变了容器的网络策略，查看此容器日志: \n\n```shell\n\t$ kubectl logs -f -n jiuxi-ns jiuxi-*** -c istio-init \n\t......\n\t* nat\t\t\t//nat表增加下面四条链\n\t:PREROUTING ACCEPT [0:0]\n\t:INPUT ACCEPT [0:0]\n\t:OUTPUT ACCEPT [0:0]\n\t:POSTROUTING ACCEPT [0:0]\n\t:ISTIO_INBOUND - [0:0]\n\t:ISTIO_IN_REDIRECT - [0:0]\n\t:ISTIO_OUTPUT - [0:0]\n\t:ISTIO_REDIRECT - [0:0]\n\t-A PREROUTING -p tcp -j ISTIO_INBOUND\n\t-A OUTPUT -p tcp -j ISTIO_OUTPUT\n\t-A ISTIO_INBOUND -p tcp -m tcp --dport 22 -j RETURN\n\t-A ISTIO_INBOUND -p tcp -m tcp --dport 15090 -j RETURN\n\t-A ISTIO_INBOUND -p tcp -m tcp --dport 15021 -j RETURN\n\t-A ISTIO_INBOUND -p tcp -m tcp --dport 15020 -j RETURN\n\t-A ISTIO_INBOUND -p tcp -j ISTIO_IN_REDIRECT\n\t-A ISTIO_IN_REDIRECT -p tcp -j REDIRECT --to-ports 15006\n\t-A ISTIO_OUTPUT -s 127.0.0.6/32 -o lo -j RETURN\n\t-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --uid-owner 1337 -j ISTIO_IN_REDIRECT\n\t-A ISTIO_OUTPUT -o lo -m owner ! --uid-owner 1337 -j RETURN\n\t-A ISTIO_OUTPUT -m owner --uid-owner 1337 -j RETURN\n\t-A ISTIO_OUTPUT ! -d 127.0.0.1/32 -o lo -m owner --gid-owner 1337 -j ISTIO_IN_REDIRECT\n\t-A ISTIO_OUTPUT -o lo -m owner ! --gid-owner 1337 -j RETURN\n\t-A ISTIO_OUTPUT -m owner --gid-owner 1337 -j RETURN\n\t-A ISTIO_OUTPUT -d 127.0.0.1/32 -j RETURN\n\t-A ISTIO_OUTPUT -j ISTIO_REDIRECT\n\t-A ISTIO_REDIRECT -p tcp -j REDIRECT --to-ports 15001\t// 从任意地方来的和到任意地方去的流量, 协议是tcp/ip协议, 都会转到15001这个端口\n\tCOMMIT\n\t# Completed on Thu Aug 13 08:59:39 2020\n```\n\n查看此pod被调度到哪台机器上\n\n```shell\n\t$ docker ps | grep -i istio-proxy\n\t// 要使用privileged权限\n\t$ docker exec -it --privileged <istio-proxy_container_ID> bash\n\tistio-proxy@jiuxi-***:/$\n\tistio-proxy@jiuxi-***:/$ sudo su root\n\tistio-proxy@jiuxi-***:/$ iptables -nvL -t nat\t//通过iptables去查nat表里的iptables明细\n\t......\n\tChain ISTIO_INBOUND ()\t\t// 可以看到nat表增加了4条链\n\t......\n\tChain ISTIO_IN_REDIRECT ()\n\t......\n\tChain ISTIO_OUTPUT ()\n\t......\n\tChain ISTIO_REDIRECT (1 references)\t\t// 从任意地方来的和到任意地方去的流量, 协议是tcp/ip协议, 都会转到15001这个端口\n\t pkts bytes target    prot  opt  in    out    source     destination\n\t 0      0   REDIRECT  tcp   --    *     *     0.0.0.0/0   0.0.0.0/0      redir ports 15001\n```\n\n### **4. istio-proxy容器进程pilot-agent 和 envoy**\n下图中的istio-pilot也就是pilot-agent进程.  \n![](pilot-agent_envoy.PNG)\n**pilot-agent作用**\n1. pilot-agent(可以看作pilot的客户端)不停的跟pilot服务端(istiod pod里运行的pilot-discovery进程可以看作是pilot的服务端)进行通讯拿取从api-server获取的最新的资源路由等规则.  \n2. pilot-agent进程启动并生成envoy进程的启动配置.  \n3. pilot-agent启动envoy进程.  \n4. pilot-agent监控并管理envoy的运行情况, 比如envoy出错时负责重启, envoy配置变更后重新加载; 外部流控的一些规则发生变化后, pilot-agent会监听这种变化, 然后将envoy以新的配置重新加载.  \n\n\n```shell\n\t$ kubectl exec -it -n jiuxi-ns jiuxi-*** -c istio-proxy -- ps -ef\n\tUID   PID  PPID  C STIME  TTY    TIME    CMD\n\t                                         /usr/local/bin/pilot-agent proxy\n\t                                         /usr/local/bin/envoy -c /etc/ist\n\t                                         ps -ef\n```\n\n**envoy作用**\n![](ports_used_by_istio.PNG)\n之所以不用nginx而用envoy做sidecar原因是nginx功能啥都有,太重了, envoy更偏轻量级, 更方便使用.  \n\n```shell\n\t$ kubectl exec -it -n jiuxi-ns jiuxi-*** -c istio-proxy -- netstat -ntlp\n```\n可以看到15000端口号只允许本容器的127.0.0.1环回地址访问, 只能供本容器的进程访问.\n\n### 用同样的image运行了不同进程解析\nistio-init容器和istio-proxy容器都使用相同的image 如: docker.io/istio/proxyv2:1.5.0, 但是istio-init容器运行iptables进程，istio-proxy容器运行pilot-envoy和envoy两个进程，原因如下:  \n\n当用户同时在kubernetes中的yaml中写了command和args时候自然是可以覆盖DockerFile中ENTRYPOINT的命令行和参数，完整情况如下:  \n * 如果command 和 args 均没有写，那么用Docker默认的配置.\n * 如果command写了, 但args没有写，那么Docker默认的配置会被忽略而且仅仅执行.yaml文件的command(不带任何参数的).\n * 如果command没写, 但args写了，那么Docker默认配置的ENTRYPOINT的命令行会被执行, 但是调用的参数是.yaml中的args.(istio-proxy和istio-init容器都是这样)\n * 如果command和args都写了, 那么Docker默认的配置被忽略, 使用.yaml的配置\n\n\n1. 查看istio/proxyv2:1.5.0 image 原数据\n\n\n```shell\n\t$ docker images | grep -i proxyv2\n\t$ docker inspect <istio/proxyv2:1.5.0_ID>\n\t......\n\t\"Cmd\": null,\n\t......\n\t\"Entrypoint\":[\t//image被运行成容器时候执行的命令\n\t    \"usr/local/bin/pilot-agent\"\n\t],\n\t......\n```\n2. 查看istio-init 容器yaml资源配置\n\n\n```shell\n\t$ kubectl get po -n jiuxi-ns jiuxi-***\n\t......\n\tinitContainers:\n\t- args:\t\t\t// 有command, 因此容器会仅执行此command而且忽略image原数据里的args参数\n\t  - istio-iptables\n\t  - -p\n\t......\n```\n查看istio-init容器进程\n\n```shell\n\t$ docker ps -a --no-trunc| grep istio-init\n\t......\n\t\"/usr/local/bin/pilot-agent istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i * -x  -b * -d 15090,15021,15020\"\n\t......\n```\n3. 查看 istio-proxy 容器yaml资源配置\n\n\n```shell\n\t$ kubectl get po -n jiuxi-ns jiuxi-***\t//可以在同一个pod资源文件查看istio-proxy和istio-init的yaml配置\n\t......\n\t- args:\t\t\t\t// 有args但是没有command, 因此用image原数据里的command并用此yaml里的args参数.\n\t  - proxy\n\t  - sidecar\n\t  - domain\n\t  - $(POD_NAMESPACE).svc.cluster.local\n\t  - configPath\n\t  /etc/istio/proxy\t\t\t// pilot-agent生成envoy配置文件\n\t  - --binaryPath\n\t  - /usr/local/bin/envoy\t// pilot-agent启动envoy进程\n\t......\n```\n查看istio-proxy容器进程\n\n```shell\n\t$ docker ps --no-trunc| grep istio-proxy\n\t......\n\t\"/usr/local/bin/pilot-agent proxy sidecar --domain istio-system.svc.cluster.local istio-proxy-prometheus --proxyLogLevel=warning --proxyComponentLogLevel=misc:error --controlPlaneAuthPolicy NONE --trust-domain=cluster.local\"\n\t......\n```\n## sidecar 自动注入\n\n```shell\n\t$ kubectl label ns jiuxi-ns istio-injection=enabled\t//添加标签，部署到此namespace空间下的pod自动sidecar注入\n\t$ kubectl label ns jiuxi-ns istio-injection-\t\t//取消标签操作\n```\n\n## sidecar注入到svc等其它资源\n\n```shell\n\t$ kubectl expose deployment jiuxi -n deployment\t\t// 在deployment资源基础上自动生成svc\n\t$ kubectl get svc -n jiuxi-ns jiuxi -o yaml > jiuxi-svc.yaml\n\t$ istioctl kube-inject -f jiuxi-svc.yaml -o jiuxi-svc-inject.yaml //sidecar 注入svc，会发现svc的yaml文件内容并没有变化\n```\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"Istio 03 configuration & dashboard","url":"/2021/03/13/microService/istio/istio_03_configuration_dashboard/","content":"\n## Architecture\n\n> 新的 Mixer 模型使用 Envoy 中的扩展来提供更多功能。Istio 社区正在领导 Envoy 的 WebAssembly（Wasm）运行时的实现，Wasm 让我们可以使用超过 20 种的语言来开发模块化、沙盒化的扩展。可以在代理继续提供流量的同时动态加载、重载扩展\n> 通过 preview 配置文件安装 Istio 1.5 不会再安装 Mixer\n> 如果有需要，您可以保持安装并启用 Mixer。最终，Mixer 将成为 Istio 单独的发行组件，成为 istio-ecosystem 的一部分\n> 简化其余控制平面的 deployment。为此，我们将几个控制平面组件合并为一个组件：Istiod。该二进制文件包括 Pilot、Citadel、Galley 和 Sidecar 注入器的功能。这种方法从许多方面改善了 Istio 的安装和管理，降低了安装和配置的复杂性、维护工作量以及问题诊断时间，同时提高了响应速度。 关于 Istiod 的更多内容请查看 [Christian Posta 的这篇博客](https://blog.christianposta.com/microservices/istio-as-an-example-of-when-not-to-do-microservices/).  \n> Istiod 作为 1.5 中所有配置文件的默认配置\n从图片来看，我们正在从这里：\n![](istio_arch_19.JPG)\n迁移到这里：\n![](istio_arch_20.JPG)\n2020 年，我们将继续专注于普及，实现默认 零配置 的目标，该默认设置不需要您更改应用程序的任何配置即可使用 Istio 的大多数功能。\n\n## **Security Policy**\n![](security_policy.PNG)\n\n### **AuthorizationPolicy**\n授权认证, 精细化管理网格内的服务可以被哪些服务访问\n * From-来源: 指定从哪里来的服务可以访问此由Label Selector指定的服务\n * To-操作: 能访问到哪些路径,如使用get方法, Path路径等\n * When- 条件: 当满足某条件的时候可以访问服务\n\n\n```xml\n\t$ cat mtls-auth.yaml\n\tapiVersion: security.istio.io/v1beta1\n\tkind: AuthorizationPolicy\n\tmetadata:\n\t  name: box-nginx\n\t  namespace: rancher\n\tspec:\n\t  action: ALLOW\n\t  selector:\n\t    matchLabels:\n\t      app: nginx-primary\n\t  rules:\n\t  - from:\n\t    - source:\t\t\t\t//包括principals, host主机, IP等等更细致话配置\n\t      principals: [\"cluster.local/ns/rancher/sa/box\"]\t// 指定从namespace:rancher, serviceAccount:box 的服务可以访问标签为app=nginx-primary的服务\n```\n\n\n### **PeerAuthentication**\n对等授权认证, 三种模式:\n * PERMISSIVE-宽容 (default,服务间正常访问)\n * STRICT-严格(网格内服务启用mtls)\n * DISABLE-取消网格内的mtls\n\n```xml\n\t$ cat peer.yaml\n\tapiVersion: security.istio.io/v1beta1\n\tkind: PeerAuthentication\n\tmetadata:\n\t  name: rancher-policy\n\t  namespace: rancher\n\tspec:\n\t  mtls:\n\t  mode: STRICT\t// 网格内服务必须经过mtls双向握手才能进行通信, 加强网格内服务安全性, 网格外服务是不会访问进来的\n```\n\n### **RequestAuthentication JWT**\n\n## **Official website demo bookinfo**\n![](traffic_control.PNG)\n\n### **Gateway & VirtualService**\n\n```xml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: Gateway\t\t\t# 相当于外部服务要访问网格内服务的第二道大门, 第一道大门是istio默认的ingressgateway网关\n\tmetadata:\n\t  name: bookinfo-gateway\n\tspec:\n\t  selector:\n\t    istio: ingressgateway # use istio default controller\n\t  servers:\n\t  - port:\n\t      number: 80\t\t# 定义一个80端口供别人访问, 并把80端口暴露到网关上面\n\t      name: http\n\t      protocol: HTTP\n\t    hosts:\t\t\t# \"*\" 表示访问任何主机的80端口都映射都下面的VirtualService下的destination对应的productpage\n\t    - \"*\"\t\t\t# 也指定为\"httpbin.example.com\", 当访问此网址时去找对应的下面的VirtualService, 但需要做解析，否则浏览器无法访问得到.\n\t---\t\t\t\t# 解析方法: linux: 修改/etc/hosts, windows: 修改/Windows/System32/drivers/etc/hosts\n\t\t\t\t\t\t# 添加内容: `Host-Name httpbin.example.com`， 浏览器就可输入http://httpbin.example.com/productpage/200, 然后鼠标右击->Inspect->Network->Headers， 刷新网页即可看到报头\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: bookinfo\n\tspec:\t\t\t\t\t# * 经常会跟Gateway一起使用.\n\t  hosts:\t\t\t\t# * 表示VirtualService对整个k8s可寻址的资源如Service,Ingress, 都使用同样的路由规则\n\t  - \"*\"\t\t\t\t\t# 也指定为\"httpbin.example.com\", 与上面Gateway的hosts保持一致, 表示VirtualService的路由规则只适用于此流量可路由的域名资源httpbin.example.com.\n\t  gateways:\n\t  - bookinfo-gateway\t# 绑定上面Gateway, 从外部如浏览器可以访问如: curl -s http://ingressgateway-Host-IP:NodePortNumber/productpage\n\t  http:\t\t\t\t\t# 访问协议是http的时候才会进一步匹配\n\t  - match:\n\t    - uri:\n\t        exact: /productpage\n\t    - uri:\n\t        prefix: /static\n\t    - uri:\n\t        exact: /login\n\t    - uri:\n\t        exact: /logout\n\t    - uri:\n\t        prefix: /api/v1/products\n\t    route:\n\t    - destination:\n\t        host: productpage\t# 匹配上面满足后路由到对应k8s部署的svc名字叫productpage的服务, 通过kubectl get svc -n book-info查看\n\t        port:\n\t          number: 9080\t\t# 路由到服务productpage的9080端口提供的服务\n\t  route:\t\t\t\t\t# 上命没有匹配到默认到productpage service\n\t  - destination:\n\t      host: productpage\n\t      port:\n\t        number: 9080\n```\n\n**VirtualService**\n\n```xml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: VirtualService\n\tmetadata:\n\t  name: reviews\n\tspec:\t\t\t\t\t# 没有绑定gateway, 因此从外部还无法访问\n\t  hosts:\n\t    - reviews\n\t  http:\n\t  - route:\n\t    - destination:\n\t        host: reviews\n\t        subset: v1\t\t# 对应下名DestinationRule reviews中的name: v1\n\t      weight: 80\t\t# 路由权重\n\t    - destination:\n\t        host: reviews\n\t        subset: v1\t\t# 对应下名DestinationRule reviews中的name: v2\n\t      weight: 20\n```\n\n### **DestinationRule**\n\n```xml\n\tapiVersion: networking.istio.io/v1alpha3\n\tkind: DestinationRule\t\t# 路由指向由host指定的svc下面通过Label selector包含的特定POD\n\tmetadata:\n\t  name: reviews\n\tspec:\n\t  host: reviews\t\t\t# 对应k8s部署的svc名字叫productpage, kubectl get svc -n book-info\n\t  trafficPolicy:\n\t    loadBalancer:\t\t# 1.负载均衡 2.断路器 3.TLS 等等\n\t      simple: RANDOM\n\t    tls:\n\t      mode: ISTIO_MUTUAL\n\t  subsets:\n\t  - name: v1\n\t    labels:\n\t      version: v1\t\t# 对应pod的Labels标签里有version=v1, 可通过kubectl describe po/reviews-** -n book-info查看Labels.\n\t  - name: v2\n\t    labels:\n\t      version: v2\n\t  - name: v3\n\t    labels:\n\t      version: v3\n```\n\n通过网关访问服务:\n\n```xml\n\t$ curl -I -HHost:httpbin.example.com http://$INGRESS_HOST:$INGRESS_PORT/productpage/200\n\t上面命令将浏览器访问http://httpbin.example.com/producpage 对应的解析为 http://$INGRESS_HOST:$INGRESS_PORT/productpage/200\n\t这里-H选项是设置主机的HTTP报头为httpbin.example.com\n```\n\n## **Dashboard & log**\n\nOfficial website: https://istio.io/latest/docs/tasks/observability/distributed-tracing/\n\n### Dashboard Overall\ncontrolz: 日志, 也可用kubectl logs查看不过不够直观\nenvoy\ngrafana\njaeger: 查看traces\nkiali: 服务网格的可视化展示\nprometheus\nzipkin: 查看traces\n\n查询istio提供了哪些dashboard\n\n```shell\n\t$ istioctl dashboard\n```\n查看或修改指定pod的log\n\n```shell\n\t$ istioctl dashboard controlz istiod-5f47bf5895-lzm6w -n istio-system\n\t$ istioctl dashboard controlz istiod-5f47bf5895-lzm6w -n istio-system --address=192.168.22.184 // 添加了address监听地址\n\thttp://localhost:40230\n```\n然后浏览器输入 http://localhost:40230 就可看到log信息\n\n查看envoy dashboard, 前提是跑业务的pod中部署了envoy proxy\n\n```shell\n\t$ istioctl dashboard envoy prometheus-xxx -n istio-system\n\t$ istioctl dashboard envoy prometheus-xxx -n istio-system --address=192.168.22.184\n```\n\n### log\n查看某个pod日志\n\n```shell\n\t$ kubectl logs po/XXX -n Namespace\n```\n修改整个istio的日志模式\n\n```shell\n\t$ kubectl edit cm istio -n istio-system\n\t......\n\tdata:\n\t  mesh: |-\n\t    accessLogEncoding: TEXT\t\t// 日志默认是TEXT格式输出，可以改为json模式, 再序列话可以很直观查看进出istio envoy的流量的方向, 上下流等.\n```\n\n### **kiali**\n\n```shell\n\t$ istioctl dashboard kiali\n\t$ while true; do curl http://10.239.186.141/productpage; done; \n```\n![](kiali_1.PNG)\n\n### **jaeger**\n\n```shell\n\tkubectl apply -f samples/addons/jaeger.yaml -n istio-system\n\tistioctl dashboard jaeger\n```\n\n### **zipkin**\n\n```shell\n\tkubectl apply -f samples/addons/extras/zipkin.yaml -n istio-system\n\tistioctl dashboard zipkin\n```\n","tags":["istio"],"categories":["microService","istio"]},{"title":"Istio 02 Installation and upgrade","url":"/2021/03/13/microService/istio/istio_02_installation_upgrade/","content":"\n## **Download the specific version of Istio**\n\n```shell\n\t$ curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.6.4 sh -\n\t$ cd istio/bin/\n\t$ ./istioctl -h\n\t  Istio configuration command line utility for service operators to\n\t  debug and diagnose their Istio mesh.\n\t  \n\t  Usage:\n\t    istioctl [command]\n\t  \n\t  Available Commands:\n\t    analyze         Analyze Istio configuration and print validation messages\n\t    authz           (authz is experimental. Use `istioctl experimental authz`)\n\t    convert-ingress Convert Ingress configuration into Istio VirtualService configuration\n\t    dashboard       Access to Istio web UIs\n\t    deregister      De-registers a service instance\n\t    experimental    Experimental commands that may be modified or deprecated\n\t    help            Help about any command\n\t    install         Applies an Istio manifest, installing or reconfiguring Istio on a cluster.\n\t    kube-inject     Inject Envoy sidecar into Kubernetes pod resources\n\t    manifest        Commands related to Istio manifests\n\t    operator        Commands related to Istio operator controller.\n\t    profile         Commands related to Istio configuration profiles\n\t    proxy-config    Retrieve information about proxy configuration from Envoy [kube only]\n\t    proxy-status    Retrieves the synchronization status of each Envoy in the mesh [kube only]\n\t    register        Registers a service instance (e.g. VM) joining the mesh\n\t    upgrade         Upgrade Istio control plane in-place\n\t    validate        Validate Istio policy and rules (NOTE: validate is deprecated and will be removed in 1.6. Use 'istioctl analyze' to validate configuration.)\n\t    verify-install  Verifies Istio Installation Status or performs pre-check for the cluster before Istio installation\n\t    version         Prints out build version information\n\t  \n\t  Flags:\n\t        --context string          The name of the kubeconfig context to use\n\t    -h, --help                    help for istioctl\n\t    -i, --istioNamespace string   Istio system namespace (default \"istio-system\")\n\t    -c, --kubeconfig string       Kubernetes configuration file\n\t    -n, --namespace string        Config namespace\n\t  \n\t  Additional help topics:\n\t    istioctl options         Displays istioctl global options\n\t  \n\t  Use \"istioctl [command] --help\" for more information about a command.\n```\n\n## **istio目录介绍**\n\n``` shell\n\t$ll istio-*\n\t  bin/ ,***, manifests/, samples/, tools/\n```\nbin/目录存放istioctl客户端工具.  \nmanifests/目录存放istio安装表单.  \nsamples/目录存放一些使用样例.  \ntools/目录下的istioctl.bash文件是能在输入$ istioctl m 时候自动补全命令,如 $istioctl manifest.  , 需要先执行$ source istioctl.bash才能生效，新版本istio好像不用设置也能有自动补全功能.  \n\n## **Istioctl工具介绍**\nofficial website: https://istio.io/latest/docs/ops/diagnostic-tools/proxy-cmd/\n\n**Istioctl 与 Kubectl关系**\n从一开始istioctl和kubectl工具有一定的融合部分，如apply, delete等共同功能, 随着istioctl版本提升启用了一些与kubectl重合的功能并完善开发了自己的一套功能.\n\n```shell\n\t// 查看pilot和envoy配置策略等的同步情况, pilot是否完全将配置信息同步到envoy\n\t$ ./istioctl proxy-status\n\t// 查询属于某个pod的envoy规则等\n\t$ ./istioctl proxy-config <clusters|listeners|routes|endpoints|bootstrap> <pod-name[.namespace]>\n\n\t// 检测安装是否成功\n\t$ ./istioctl verify-install\n\t  ......\n\t  Checked 25 custom resource definitions\t// 可以看到Istio 1.6.4只有25个crd\n\t  Checked 3 Istio Deployments\n\t  Istio is installed successfully\n```\n\n## **istioctl command**\n\n### **istioctl profile**\n查看profile list\n\n```shell\n\t$ istioctl profile list\n```\n输出profile\n```shell\n\t$ istioctl profile dump demo > demo.yaml\n\tvim demo.yaml\n```\n## **安装 istio到kubernetes**\n\n**第一种:默认安装**\n```shell\n\tistioctl install --set profile=demo\n```\n**第二种:**\n```shell\n\tcd istio-1.6.4\n\tvim manifests/profiles/demo.yaml\n\t ......\n\t 34             - port: 80          // Service 端口\n\t 35               targetPort: 8080  // 指向Pod的端口, containerPort是container容器暴露出来的端口, 类似Dockerfile中的expose操作\n\t 36               nodePort: 31500   // 添加nodePort, 固定service的80端口映射到的宿主机的31500端口\n\t 37               name: http2\n\t ......\n\tistioctl install -f manifests/profiles/demo.yaml\n```\n\n## **Istioctl analyze**\n```shell\n\t$ istioctl analyze -n book-info\n\t  √ No Validation issues found when analyzing namespace: book-info\n```\n## **Istio upgrade and rollback**\n因为技术变更太快, 请先参考官网最新教程：\nOfficial website: https://istio.io/latest/zh/docs/setup/upgrade/istioctl-upgrade/\n\nistio1.6 提供了简单的升级命令方式，直接通过命令 $ istioctl upgrade 就可以更新Istio control plane in-place\n且提供了金丝雀发布方式，更新和回滚过程可以看到有两个istiod在istio-system命名空间下.\n\n使用istioctl升级时候, 老版本和新版本之间版本号要接近，相差太远如由istio1.0升级到istio1.6.8有可能会失败.\n\n**升级步骤**\n查看升级前的istioctl 版本\n```shell\n\t$ istioctl version\n\tclient versin: 1.5.0\t\t\t// 指istioctl 这个二进制客户端工具\n\tcontrol plane version: 1.5.0\t// istio部署在k8s上的控制面资源, 如pilot等\n\tdataplane version: 1.5.0\t\t// istio部署在k8s上的数据面资源, 如envoy\n```\n**1. 升级istioctl客户端二进制工具**\nDownload istio最新版本如istio-1.6.8, 解压缩`tar -zxvf istio-1.6.8.tar.gz`进入istio-1.6.8/bin  \nlinux Path环境变量中的老版本istioctl去掉添加新版本的istioctl客户端工具.  \n\n**2. 升级istio在k8s上的数据面和控制面**\n升级时候要保证升级的是同样的profile, 如demo, 或者default\n\n```shell\n\t$ istioctl profile list \t\t// 查看istioctl有哪些profile\n\t$ istioctl profile dump demo > demo.yaml\t// 先dump出跟老版本相同选择的新版本的profile, 如新老版本都采用demo 这个profile\n```\n\n第一种:根据新版本的demo.yaml来进行升级\n\n```shell\n\t$ vim demo.yaml \t//先修改下dump出来的新版本的demo.yaml\n\tjwtPolicy: third-party-jwt ---> 改为 jwtPolicy: first-party-jwt\n\t如果这里不修改会报证书无法挂载情况.\n\t$ istioctl upgrade -f demo.yaml\n```\n第二种:\n\n```shell\n$ istioctl manifest apply -set profile=demo --set values.global.jwtPolicy=first-party-jwt\n```\n查看升级后的istioctl 版本\n\n```shell\n\t$ istioctl version\n\tclient versin: 1.6.8\t\t\t// 指istioctl 这个二进制客户端工具\n\tcontrol plane version: 1.6.8\t// istio部署在k8s上的控制面资源, 如pilot等\n\tdataplane version: 1.5.0 (1 proxies), 1.6.8 (3 proxies)\t// istio部署在k8s上的数据面资源, 如envoy, 发现还有老版本注入的envoy, 也需要`手动`或`自动升级`\n```\n**3. 升级已经注入到pod中的sidecar**\n如果以前采用的是自动sidecar注入, 则将所有pods通过滚动更新来更新sidecar\n\n```shell\n\t$ kubectl rollout restart deployment --namespace <namespace with auto injection>\n```\n如果以前采用的是手动sidecar注入, 则更新sidecar通过执行如下命令:\n\n```shell\n\t$ kubectl apply -f < (istioctl kube-inject -f <original-application-deployment.yaml>) //实例如下\n\t$ istioctl kube-inject -f nginx.yaml | kubectl apply -f -\n```\n通过`kubectl get pods -n <Namespace>` 来观察到新版本生成后老版本的pod才terminate.  \n重新查看istioctl version\n\n```shell\n\t$ istioctl version\n\tclient versin: 1.6.8\n\tcontrol plane version: 1.6.8\n\tdataplane version: 1.6.8 (4 proxies)\n```\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"RHEL8.0开机总是要dhclient手动获取IP","url":"/2021/03/13/linux/RHEL8.0开机总是要dhclient手动获取IP/","content":"\n使用#dhclient前提是网线得确保连上且没问题\n```shell\n$ kill -9 dhclient_thread_ID\n```\n\n解决RHEL8.0开机自动获取IP方法如下:\n\n```shell\n\t$ ifconfig                                                     //查看第一个网卡名称ifcfg-enp0s20f0u4u1\n\t$ cd /etc/sysconfig/network-scripts/\n\t$ ls\n\t  ifcfg-enp1s0                                                //发现只有一个网卡配置文件，与#ifconfig查看到得网卡名字都不一致\n\t$ cp ifcfg-enp1s0 ifcfg-enp0s20f0u4u1    //手动copy一个与存在得网卡名相同得网卡配置文件\n\t$ vim  ifcfg-enp0s20f0u4u1\n\t ○ 删除UUID那一行\n\t ○ 修改NAME=ifcfg-enp0s20f0u4u1\n\t ○ 修改DEVICE=ifcfg-enp0s20f0u4u1\n\t ○ 修改ONBOOT=yes                              //开机即启动网络连接\n```\n","categories":["linux"]},{"title":"alias","url":"/2021/03/13/linux/alias/","content":"\n\n## alias\n\n```shell\n\t$ alias\n\talias cp='cp -i'\t// -i 表示交互, 覆盖文件时候会提示是否确定要overrite.\n\t......\n\t$ alias cdnet=\"cd /etc/sysconfig/network-scripts\"\n\t$ cdnet\n\n\t//cp命令时候不想提示是否覆盖文件\n\t// 方法一:\n\t$ unalias cp\n\t// 方法二: cp前加上\"\\\"表示不用alias的别名 'cp -i', 而 直接用 'cp'\n\t$ \\cp -fv dir1/file1  dir2/file2\t//-v表示输出复制命令的过程细节\n```\n\n\n","categories":["linux"]},{"title":"亚马逊aws-sdk-cpp","url":"/2021/03/13/linux/aws-sdk-cpp亚马逊/","content":"\ngithub: https://github.com/aws/aws-sdk-cpp\n\n## Centos 编译执行\n1. 更新下curl, 可以参考\"curl升级教程\"\n2. 更新gcc和g++, 可以参考\"Devtoolset 升级gcc到8.3.1\"\n\n```shell\n\twget https://github.com/aws/aws-sdk-cpp/archive/1.8.55.tar.gz\n\ttar -zxvf 1.8.55.tar.gz\n\tcd aws-sdk-cpp-1.8.55\n\tmkdir build\n\tcd build\n\tcmake <path-to-root-of-this-source-code(aws-sdk-cpp-1.8.55的绝对路径)> -DCMAKE_BUILD_TYPE=Debug\n\tmake\n\tsudo make install\n```\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"U盘安装OS","url":"/2021/03/13/linux/U盘安装OS/","content":"\n## **安装启动盘(U 盘)**\n\n```shell\nfdisk -l\numount /dev/sdb1\nmkfs.vfat /dev/sdb -I\nsudo dd if=~/Downloads/ubuntu-16.04-desktop-amd64.iso of=/dev/sdb status=progress\n```\n\n## **安装Ubuntu_18.04**\n### 1. 开启root访问权限\n```shell\nsudo passwd root\n```\n输入user用户密码，再设置root密码，下次就可以$ su root 获取root权限了\n\n### Add new user\n```shell\nadduser $USERNAME\npasswd $PASSWD\n```\n\n### 2. 设置网络\n* 点击屏幕右上角向下三角\n + Wired Connected -> Wired Settings -> Network Proxy -> Manual -> HTTP Proxy等输入child-prc.intel.com, port输入913\n + 到此可以试着打开浏览器看能不能上网\n```shell\n$ vi ~/.bashrc 添加如下内容\nexport http_proxy=http://child-prc.intel.com:913\nexport https_proxy=$http_proxy\nexport HTTP_PROXY=$http_proxy\nexport HTTPS_PROXY=$http_proxy\n```\n\n```shell\nsource ~/.bashrc 使得上面添加的内容生效\napt-get update\napt-get upgrade\napt install net-tools\t\t// 到此可以用ifconfig查看ip\napt-get install vim\t\t// 可以用vim命令了\nvim ~/.vimrc\t// 如果没有则新建.vimrc文件, 添加如下内容\nset nu\nset tabstop=4\n```\n\n### 3. 安装ssh并开启root远程登录\n\n```shell\n$ apt-get install openssh-server\n$ vim /etc/ssh/sshd_config\n  PermitRootLogin yes\t\t// 去掉PermitRootLogin前面的\"#\"注释，后面值改为yes, 如果不设置则远程只能登录user用户如ai而不能直接登录root\n$ /etc/init.d/ssh start\t\t// 启动SSH服务\n$ ps -e | grep sshd\t\t\t// 查看SSH是否启动成功\n$ /etc/init.d/ssh stop\t\t// 关闭SSH服务, 如果先开启服务再设置PermitRootLogin为yes则需要关掉ssh服务再启动ssh服务使其修改生效\n```\n\n### 4. 安装build-essential等必要的编译配置运行程序工具如gcc等\n```shell\napt install build-essential\t// 该命令将安装一堆新包，包括gcc，g ++和make.\ngcc --version\t\t// gcc -v\n```\n\n### 安装curl，会安装后运行可能出现的问题\n * 安装:\n```shell\napt-get update\napt-get upgrade\napt-get install curl\ncurl --version\n```\n\n * 提前设置好系统的proxy如:\n```shell\nexport http_proxy=child-prc.intel.com:913\nexport https_proxy=child-prc.intel.com:913 // https的proxy与上面的http的要一样\n```\n\n * 运行curl可能出现的问题:\n  curl: error while loading shared libraries: libcurl.so.4: cannot open shared object file: No such file or directo\n```shell\n$ find -name \"libcurl.so.4\" /\n```\n查看得到库文件在/usr/local/lib/libcurl.so.4\n解决方法一: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib\t//当前终端生效\n解决方法二: 把export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/lib添加到 ~/.bashrc, 然后运行source ~/.bashrc使其永久生效\n\n### Ubuntu18.04 apt-get卸载软件步骤\n```shell\n$ whereis curl\n  curl: /usr/bin/curl /usr/local/curl\n$ apt-get remove ***\n$ apt-get autoremove\n$ apt-get autoclean(或者clean)\n```\n\n### Ubuntu18.04 添加镜像源\n```shell\n$ vim /etc/apt/sources.list.d/kubernetes.list\n  deb https://apt.kubernetes.io/ kubernetes-xenial main\n```\n\n### Modify hostname\n```shell\n$ hostnamectl set-hostname $HOSTNAME\n$ reboot\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n一: 查询安装路径\n1. dpkg -L 软件名\n```shell\ndpkg -L gedit\n```\n\n2. whereis 软件名\n```shell\nwhereis gedit\n```\n\n二: 查询版本\n```shell\naptitude show 软件名\ndpkg -l 软件名\n```\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n### cpupower\n```shell\napt install linux-tools-common\n```\n\n```shell\nyum install epel-release\nyum install cmake\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n\n## 安装Centos Minimal\n\n### 配置网络启动\n\n```shell\n$ vi /etc/sysconfig/network-scripts/ifcfg-ens33\nONBOOT=yes\t\t\t// ONBOOT=no，改为ONBOOT=yes\n\n//重启网络\n$ ifup <网卡名字如：ens33>\t\t// ifup 网卡名字\n$ service network restart\t// 重启网络\n```\n\n### 配置yum代理\n\n```shell\n$ echo proxy=http://Proxy:port >> /etc/yum.conf\n$ yum install vim\n```\n\n### 配置系统proxy\n\n```shell\n$ vim ~/.bashrc\nexport http_proxy=http://Proxy:port\nexport https_proxy=http://Proxy:port\n```\n\n### 安装网络工具\n\n```shell\n$ yum install net-tools\n```\n### 开启远程登陆\n\n```shell\n$ vim /etc/ssh/sshd_config\nPort 22\n#AddressFamily any\n#ListenAddress 0.0.0.0\n#ListenAddress ::\n\n#LoginGraceTime 2m\nPermitRootLogin yes\n#StrictModes yes\n#MaxAuthTries 6\n#MaxSessions 10\n\n启动sshd\n$ /bin/systemctl start sshd.service\n```\n\n## **遇到的问题**\n### **缺少网卡驱动**\n在裸组装机而不是虚拟机上装Centos7之后$ ifconfig发现只有 lo环回网络地址，缺少网卡如\"enps3\"等\n解决方法是在网上查找网卡对应的驱动， 然后下载安装(本次没有找到对应网卡版本的驱动但安装后仍然成功查到网址并能正常使用)\n\n```shell\n$ ip addr\t\t\t//发现只有lo网路地址, 或者还有其它的如virbr0等的虚拟网络地址, 但都不是想要的网络IP\n$ lsmod | grep e1000\n  e1000e                263116  0\n  ptp                    19231  1 e1000e\n$ modprobe e1000e\n$ lspci -vvv | less\n$ rmmod e1000e\t\t//移除原来的网卡驱动\n$ rmmod e1000\n$ lsmod | grep e1000\n\n\n$ lspci\t\t\t\t//查看设备，其中有Intel网卡, I219-V为网卡设备号\n  00:1f.6 Ethernet controller: Intel Corporation Ethernet Connection (12) I219-V\n根据网卡设备号I219-V在网上查找相应驱动, 并下载解压 e1000e-3.8.4.tar.gz\n$ tar -zxvf e1000e-3.8.4.tar.gz\n$ cd e1000e-3.8.4\n$ cd src/\n$ make\n$ ls\t\t\t\t//会编译生成内核文件e1000e.ko\n  e1000e.ko\n$ insmod e1000e.ko\t//将内核文件加载进内核\n$ dmesg\n$ lsmod | grep e1000e\n$ ifconfig\t\t\t//再次查看发现有网络IP了\n$ make install\t\t//设置开机后会自动加载内核网络文件\n$ dmidecode -vvv | less\t\t//查看下网卡信息\n```\n机器运行期间执行过$ yun update, 选择yes, 可能会涉及到内核的升级或改变, 重新开机后会默认选择新添加的内核启动, 登陆后发现没有又没有网卡驱动.\n**第一种解决方法**\n可以在登陆时选择原来的内核启动就可以了.\n**第二种解决方法**\n因为换不同内核, 需要重新安装这个第三方网卡驱动, 重新进入e1000e-3.8.4/src目录$ make clean, $ make, $ make install  \n查看有没有旧的内核驱动, 如果又需要先卸载$ lsmod | grep e1000e, $ rmmod e1000e.ko  \n然后执行$ insmod e1000e.ko, 如果报insmod: ERROR: could not insert module xxxxx.ko: Unknown symbol in module错误可以先忽略直接重启系统选择新的内核启动问题就解决了.  \n\n\n\n","categories":["linux"]},{"title":"awk, sed, find, grep","url":"/2021/03/13/linux/awk_sed_find_grep/","content":"\n## **grep**\n\n```shell\nuname - a\n\t查看内核版本\nroot@Alpha:# uname -a\nLinux Alpha 4.13.0-36-generic #40~16.04.1-Ubuntu SMP Fri Feb 16 23:25:58 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n```\n\n*************************************************************************************************************\n```shell\ngrep      grep --help\n\t-c\n\t-i    不区分大小写\n\t-h\n\t-l\n\t-n\n\t-s\n\t-v\n```\n\n``` shell\ngrep -n \"IndexIVFFlat*\" *.py  //查找本目录所有后缀为.py的文件是否包含IndexIVFFlat*内容\ngrep 'test' aa bb cc\ngrep -r \"exit\" ./ --color=auto\ngrep -r \"exit\" ./ -h\ngrep -r \"exit\" ./ -n    --显示搜索内容在文件中的行号\nroot@Alpha:~/zhan/system/day4# grep  -r  \"exit\"  process_work.c   -n\n29:             exit(1);\nroot@Alpha:~/zhan/system/day4#\n```\n## **find**\nfind     find --help\nfind ./ -name \"exit\"\n```\nroot@Alpha:~/zhan/system/day4# find ./ -size +2k    --搜索大于2k文件的当前目录文件\n./\n./pthread_attr.o\n./pthread_cancel.o\n./condition.o\n./process_work.o\n./rwlock.o\n./mutex.o\n./pthread_create.o\nroot@Alpha:~/zhan/system/day4#\n\nroot@Alpha:~/zhan/system/day4# find ./ -size +2k -size 9k    --搜索大于2k小于9k的当前目录文件\n./pthread_attr.o\n./pthread_cancel.o\n./pthread_create.o\nroot@Alpha:~/zhan/system/day4#\n\nfind ./ -size +200 -size -500   -->不加-size参数时候默认单位: 512B(扇区大小0.5K)，不指定单位默认按照扇区大小\n磁盘最小扇区512B(512字节)\n内存最小块4K(4096字节)\n内存分配1000B和4096B空间对计算机来讲没什么区别都是以4096大小来做映射\n\nfind  ./ -type f  --默认递归搜索所有第一级到第n级目录中的文件，而不包含目录\nfind ./ -maxdepth 1 -type f   --只找当前目录文件，不找子级目录文件\nfind ./ -maxdepth 1 -type f -size +2k\nfind ./ -maxdepth 1 -type f -size +2k | ls -l  不生效，find不能跟管道\"|\"一起使用, 如下使用\nfind ./ -maxdepth 1 -type f -size +2k -exec ls -l {} \\;\n\t-exec指明要执行\"{}\"里面的内容，\"{}\"的内容由ls -l 传参, 执行完要有结束标志分号\";\"， \"\\\"符号转义字符将分号\";\"转义\n```\n\n## **sed**\nsed  --Stream Editor(流编辑器)\n早起Unix系统 -- ed 编辑器 , 很多sed命令和vi的末行命令是相同的\n\t\t\t    |       | \n\t                  vi      sed\n\n:/\t查找\n:%/the/this/g\t替换文件中的所有the为this\n\nsed option 'script' file1 file2 ……                sed 参数  's/the/this/', 待处理文件     ---the 替换为 this\n\tsed 's/char/int/' test.c -i                test.c文件中的char改为int                           \n\tsed 's/char/int/' test.c | tee tt.c         test.c文件中的char改为int并另存为tt.c文件\nsed option -f scriptfile file1 file2 ……         sed 参数  -f '脚本文件里写正则等'  待处理文件\n\n选项含义：\n--version\n--help\n-n, --quiet, --silent                静默输出\n-i, --in-place                           直接修改源文件\n-e script                        允许多个脚本指令被执行\n-f  script-file            \n--file=script-file             从文件中读取脚本指令，对编写自动脚本程序来说很棒\n-l   N, --line-length=N   该选项指定l指令可以输出的行长度\n--posix                            禁用GNU sed扩展功能\n-r，--regexp-extended   在脚本指令中使用扩展正则表达式\n-s，--separate                 默认情况下，sed将把命令行指定的多个文件名作为一个长的连续的输入流，\n                                        而GNU sed则允许把他们当做单独的文件，这样如正则表达式则不进行跨文件匹配.\n-u， --unbuffered           最低限度的缓存输入与输出\n\na，    append              追加\ni，     insert                   插入\nd，    delete                 删除\ns，     substitution        替换\n如：$ sed -i \"2a itcast\" ./testfile    在testfile文件中第2行后添加\"itcast\"\n       $ sed -i \"3d\" testfile          删除testfile文件中的第三行\n\nsed程序一行一行读出待处理文件，如果某一行与pattern匹配，则执行相应的action，如果一条命令没有patter而只有action，这个action将作用于待处理文件的每一行， 如\"s/the/this/\"  \"the\"就是pattern\n```\n$ sed 's/bc/-&-/' testfile\n123\na-bc-\n456\npattern2 中的&表示源文件当前行中与pattern1相匹配的字符串\n\n$ sed 's/\\([0-9]\\)\\([0-9]\\)/-\\1-~\\2~/' testfile\n-1-~2~3\nAbc\n-4-~5~6\npattern2中的\\1表示与pattern1的第一个()括号相匹配的内容，\\2表示与pattern1的第二个()括号相匹配的内容弄。\nsed默认使用Basic正则表达式规范，如果制定了-r选项则使用Extended规范，那么()括号就不必转移了，\n如：sed -r 's/([0-9])([0-9])/-\\1-~\\2~/' testfile\n\n\tsed '/def/p' out    --- \"p\"表示打印输出包含echo内容的行\n\troot@Alpha:~/zhan/test# sed '/def/p' abc.c\n\tabc\n\tdef\n\tdef      --由参数p打印输出的\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed -n '/def/p' abc.c   -n 表示静默输出，只输出改变的内容\n\tdef\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed '/def/d' abc.c\n\tabc\n\tghi\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed  -i 's/def/ddd/' abc.c == sed  -i  '/def/s/def/ddd/' abc.c\n\tabc                                                                                             sed  参数 pattern/action(动作)   目标文件\n\tddd                                                                                                                         pattern大多数情况可以省略如替换\n\tghi\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed '1,1s/def/ddd/' abc.c\n\tabc\n\tdef\n\tghi\n\tjkl\n\troot@Alpha:~/zhan/test# sed '1,2s/def/ddd/' abc.c   表示1到2行的所有 \"def\" 替换为 \"ddd\"\n\tabc\n\tddd\n\tghi\n\tjkl\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed '1,2s/def/-&-/' abc.c\n\tabc\n\t-def-\n\tghi\n\troot@Alpha:~/zhan/test#\n\troot@Alpha:~/zhan/test# sed -r 's/([a-z])([a-z])/-\\1-~\\2~/' abc.c == sed -E 's/([a-z])([a-z])/-\\1-~\\2~/' abc.c\n\t-a-~b~c\n\t-d-~e~f\n\t-g-~h~i\n\troot@Alpha:~/zhan/test#\n```\n\n## **awk**\nawk 是开发awk命令的三个人名字的首字母，不是单词缩写，不能读'a wa k'\nsed是以行为单位处理文件，awk比sed强，不仅以行为单位，还能以列为单位处理文件，awk缺省的行分隔符是换行，缺省的列分隔符是连续的空格和Tab，但是行分隔符和列分隔符都可以自定义\n\n\tps aux | awk '{print $0}'    取全部，相当于ps aux\n\tps aux | awk '{print $2}'    按列拆分\n`查看所有链接本机6443服务端口的客户端IP地址, 地址一致的合并, 然后连接数从高到底排序.`\n\n\t$ netstat -antp | grep :6443 | awk '{print $5}' | awk -F \":\" '{print $1}' | sort | uniq -c | sort -r -n\n\t      4 10.239.4.100\t// 表示从10.239.4.100客户端请求访问本机6443服务端口的进程数为4\n\t      3 10.239.4.80\n\t      3 10.239.141.194\n\t      3 10.239.141.145\n\t      3\n\t      2 10.40.0.6\n\t      2 10.239.140.53\n\t      2 10.239.140.133\n\t      2 10.109.19.69\n\t      1 10.40.0.9\n\t      1 10.40.0.2\n\t      1 10.40.0.1\n\t      1 10.109.19.68\nAwk option 'script' file1 file2 ……\nAwk option -f scriptfile file1 file2 ……\n和sed一样，awk处理的文件既可以由标准输入重定向得到，也可以当命令行参数传入，编辑命令可以直接当命令行参数传入，也可以用-f参数指定一个脚本文件，如果一条awk命令只有actions部分，则actions作用于待处理文件的每一行，编辑命令格式为:\n/pattern/{actions}        ps aux | awk 'print $0'  其中awk 'print $0'即为actions\ncondition{actions}\n\n如果某种产品的库存量低于75则在行末标注需要订货：\n$ awk '$2 < 75  {printf  \"%s\\t%s\\n\",  $0,  \"REORDER\"} $2 >= 75  {print $0;}'  testfile\n                  |                                |                                                 |                  |\n             pattern                    action                                       pattern        action\nproductA   30   REORDER\nproductB   76\nproductC   55   REORDER\n\n\tawk '$2 < 75 {printf \"%s %s\\n\", $0, \"reorder\";} $2 >= 75 {printf \"%s\\n\", $0;}' testfile\nProductA 70 reorder\nProductB 35 reorder\nProductC 75\n\n\tps aux | awk '$2>32000 && $2<50000 {print $2 \" recorder\";}'\n\troot@Alpha:~/zhan/test# ps aux | awk '$2>32000 && $2<50000 {print $2 \" recorder\"}'\n\t32106 recorder\n\t32146 recorder\n\t……\n\troot@Alpha:~/zhan/test#\n\tps aux | awk '$2>32000 && $2<50000 {print $2}'                           与下面命令等价\n\tps aux | awk '$2>32000 && $2<50000 {printf(\"%s\\n\", $2);}'        花括号前的\";\"分号去掉也可以\n\troot@Alpha:~/zhan/test# ps aux|awk ' $2 > 30000 && $2 < 40000 {var = var + 1} END {print var}'\n\troot@Alpha:~/zhan/test# ps aux|awk ' $2 > 30000 && $2 < 40000 {var = var + 1} END {print var}'\n\t18\n\troot@Alpha:~/zhan/test#\n\n统计文件中的空行\n```\n$ awk '/^ *$/ {x=x+1;} END {print x;}' testfile\n         以空格开始，到结束都是空格, END代表一直读到文件尾\n\troot@Alpha:~/zhan/test#  awk '/^ *$/ {x=x+1;} END {print x;}' testfile\n\t10\n\troot@Alpha:~/zhan/test#\n\n打印系统中用户账号列表\n$ awk 'BEGIN {FS=\":\"} {print $1;}' /etc/passwd   \n\tawk默认空格为列分隔符，BEGIN表示在匹配之前以\":\"冒号分隔符分割列\n$ awk -F:  '{print $1}' /etc/passwd   与上面等价\n\t-F:  设置\":\"冒号为列分隔符， -F相当于awk的option可选参数\n\t/etc/passwd 文件中把第一列用户名a对应的第二列如\"x\"删掉，则注销机器用户a下次登录则不需要再输入密码，但是sudo 等仍然需要指定密码，只是图形界面省略了登录输密码步骤\n```\n\t\n\t\n*************************************************************************************************************\n","categories":["linux"]},{"title":"cmake","url":"/2021/03/13/linux/cmake/","content":"\n## **cmake 安装**\n\n```shell\n\t$ wget https://github.com/Kitware/CMake/releases/download/v3.17.0/cmake-3.17.0-Linux-x86_64.tar.gz\n\t$ tar -zxvf cmake-3.17.0-Linux-x86_64.tar.gz\n\t$ ln -s /<PATH>/cmake-3.17.0-Linux-x86_64/bin/cmake /usr/bin/cmake\n\t$ cmake -version\n```\n\n## **函数使用说明**\n参考链接:https://www.jianshu.com/p/aaa19816f7ad\n\n## 编写 CMakeLists.txt\n### 1. 源文件只有一两个时候\n\n```shell\n\t$ touch CMakeLists.txt\n\t# CMake 最低版本号要求, 指定运行此配置文件所需的 CMake 的最低版本\n\tcmake_minimum_required(VERSION 2.8)\n\t# 项目信息, 参数值是 Demo1，该命令表示项目的名称是 Demo1\n\tproject(Demo1)\n\t# 指定生成目标, 将名为 main.cc 的源文件编译成一个名称为 Demo 的可执行文件\n\tadd_executable(Demo main.cc MathFunctions.cc)\n```\n### 2. 如果源文件很多\n把所有源文件的名字都加进add_executable将是一件烦人的工作, 更省事的方法是使用 aux_source_directory 命令，该命令会查找指定目录下的所有源文件，然后将结果存进指定变量名, 其语法如下:  \n\n```\n\taux_source_directory(<dir> <variable>)\n```\n\n可以修改 CMakeLists.txt 如下：\n\n```\n\tcmake_minimum_required(VERSION 2.8)\n\tproject(Demo2)\n\t# 查找当前目录下的所有源文件\n\t# 并将名称保存到 DIR_SRCS 变量\n\taux_source_directory(. DIR_SRCS)\n\t# 指定生成目标\n\tadd_executable(Demo ${DIR_SRCS})\n```\n\nCMake 会将当前目录所有源文件的文件名赋值给变量 DIR_SRCS ，再指示变量 DIR_SRCS 中的源文件需要编译成一个名称为 Demo 的可执行文件。  \n### 3. 多个目录，多个源文件\n\n```\n\t./Demo3\n\t    |\n\t    +--- main.cc\n\t    |\n\t    +--- math/\n\t          |\n\t          +--- MathFunctions.cc\n\t          |\n\t          +--- MathFunctions.h\n```\n\n需要分别在项目根目录 Demo3 和 math 目录里各编写一个 CMakeLists.txt 文件。为了方便，我们可以先将 math 目录里的文件编译成静态库再由 main 函数调用.  \n根目录中的 CMakeLists.tx\n\n```\n\t# CMake 最低版本号要求\n\tcmake_minimum_required (VERSION 2.8)\n\t# 项目信息\n\tproject (Demo3)\n\t# 查找当前目录下的所有源文件\n\t# 并将名称保存到 DIR_SRCS 变量\n\taux_source_directory(. DIR_SRCS)\n\t# 添加 math 子目录, 这样 math 目录下的 CMakeLists.txt 文件和源代码也会被处理\n\tadd_subdirectory(math)\n\t# 指定生成目标\n\tadd_executable(Demo main.cc)\n\t# 添加链接库, 指明可执行文件 Demo 需要连接一个名为 MathFunctions 的链接库\n\ttarget_link_libraries(Demo MathFunctions)\n```\n\n子目录math中的 CMakeLists.txt：\n\n```\n\t# 查找当前目录下的所有源文件\n\t# 并将名称保存到 DIR_LIB_SRCS 变量\n\taux_source_directory(. DIR_LIB_SRCS)\n\t# 生成链接库, 将math 目录中的源文件编译为静态链接库\n\tadd_library (MathFunctions ${DIR_LIB_SRCS})\n```\n\n## **编译**\n\n```shell\n\tcmake .\n```\n\n## **安装和测试**\n这两个功能分别可以通过在产生 Makefile 后使用 make install 和 make test 来执行.  \n首先先在 math/CMakeLists.txt 文件里添加下面两行：\n\n```\n\t# 指定 MathFunctions 库的安装路径\n\tinstall (TARGETS MathFunctions DESTINATION bin)\n\tinstall (FILES MathFunctions.h DESTINATION include)\n```\n指明 MathFunctions 库的安装路径。之后同样修改根目录的 CMakeLists 文件，在末尾添加下面几行：\n\n```\n\t# 指定安装路径\n\tinstall (TARGETS Demo DESTINATION bin)\n\tinstall (FILES \"${PROJECT_BINARY_DIR}/config.h\"\n\t         DESTINATION include)\n```\n通过上面的定制，生成的 Demo 文件和 MathFunctions 函数库 libMathFunctions.o 文件将会被复制到 /usr/local/bin 中，而 MathFunctions.h 和生成的 config.h 文件则会被复制到 /usr/local/include 中。我们可以验证一下（顺带一提的是，这里的 /usr/local/ 是默认安装到的根目录，可以通过修改 CMAKE_INSTALL_PREFIX 变量的值来指定这些文件应该拷贝到哪个根目录）.  \n\n\n## **Cmake常用命令**\n### 1. add_library\n该指令的主要作用就是将指定的源文件生成链接文件，然后添加到工程中去。该指令常用的语法如下：\n\n```\n\tadd_library(<name> [STATIC | SHARED | MODULE]\n\t            [EXCLUDE_FROM_ALL]\n\t            [source1] [source2] [...])\n```\n<name>表示库文件的名字，该库文件会根据命令里列出的源文件来创建。\nSTATIC、SHARED和MODULE的作用是指定生成的库文件的类型。\nSTATIC库是目标文件的归档文件，在链接其它目标的时候使用。\nSHARED库会被动态链接（动态链接库），在运行时会被加载。\nMODULE库是一种不会被链接到其它目标中的插件，但是可能会在运行时使用dlopen-系列的函数。\n默认状态下，库文件将会在于源文件目录树的构建目录树的位置被创建，该命令也会在这里被调用。\n而语法中的source1 source2分别表示各个源文件。\n#### **使用案例**\n\n```\n\tadd_subdirectory(sub_dir)\n\tfile(GLOB HDRS \"*.h\")\n\tfile(GLOB SRCS \"*.c\")\n\tset(TARGET_NAME pnp_module)\n\tadd_library(${TARGET_NAME} SHARED ${HDRS} ${SRCS})\n\ttarget_link_libraries(${TARGET_NAME} PRIVATE\n\t        ${CMAKE_THREAD_LIBS_INIT}\t\t\t\t// 库引用了需要使用-lpthread的.c源码文件，需要加上这句\n\t        pnp_utils_static\n\t        m)\n\t\n\t// CMAKE_CURRENT_SOURCE_DIR：这是当前处理的CMakeLists.txt所在的目录。当前正在处理的源目录的路径。这是当前正由cmake处理的源目录的完整路径。\n\t// CMAKE_CURRENT_LIST_DIR：这是当前正在处理的listfile的目录。当前正在处理的列表文件的完整目录。\n\t// 在处理sub_dir/CMakeLists.txt时，CMAKE_CURRENT_LIST_DIR将引用项目/ src，而CMAKE_CURRENT_SOURCE_DIR仍指向外部目录项目。\n\ttarget_include_directories(${TARGET_NAME} PUBLIC ${CMAKE_CURRENT_LIST_DIR})\n```\n\n### 2. target_link_libraries\n该指令的作用为将目标文件与库文件进行链接。该指令的语法如下：\n\n```\ntarget_link_libraries(<target> [item1] [item2] [...]\n                      [[debug|optimized|general] <item>] ...)\n```\n<target>是指通过add_executable()和add_library()指令生成已经创建的目标文件。\n[item]表示库文件没有后缀的名字。\n默认情况下，库依赖项是传递的。当这个目标链接到另一个目标时，链接到这个目标的库也会出现在另一个目标的连接线上。\n这个传递的接口存储在interface_link_libraries的目标属性中，可以通过设置该属性直接重写传递接口。\n\n<dl>\n<dt>cmake target_link_libraries() 中<PUBLIC|PRIVATE|INTERFACE> 的区别<dt>\n<dd>如果目标的头文件中包含了依赖的头文件(源文件间接包含)，那么这里就是PUBLIC</dd>\n<dd>如果目标仅源文件中包含了依赖的头文件，那么这里就是PRIVATE</dd>\n<dd>如果目标的头文件包含依赖，但源文件未包含，那么这里就是INTERFACE</dd>\n</dl>\n\n#### **使用案例：**\n\n * 案例1：\n\n```\n\tTARGET_LINK_LIBRARIES(myProject hello)\t\t\t// 连接libhello.so库\n\tTARGET_LINK_LIBRARIES(myProject libhello.a)\n\tTARGET_LINK_LIBRARIES(myProject libhello.so)\n\t// 库引用了需要使用-lpthread的.c源码文件，需要加上${CMAKE_THREAD_LIBS_INIT}\n\ttarget_link_libraries (${PROJECT_NAME} ${CMAKE_THREAD_LIBS_INIT})\n```\n\n * 案例2：\n\n```\n\tadd_executable(myProject main.cpp)\n\ttarget_link_libraries(myProject eng mx)     \n\t#equals to below \n\ttarget_link_libraries(myProject -leng -lmx) `\n\ttarget_link_libraries(myProject libeng.so libmx.so)`\n```\n\n### 3. target_include_directories\n为add_executable() 或者add_library() 中定义的输出目标指定编译选项\nInclude的头文件的查找目录，也就是Gcc的[-Idir...]选项\n\n```\n\ttarget_include_directories(<target> [SYSTEM] [BEFORE]\n\t\t\t\t\t\t\t\t<INTERFACE|PUBLIC|PRIVATE> [items1...]\n\t\t\t\t\t\t\t\t[<INTERFACE|PUBLIC|PRIVATE> [items2...]\n\t\t\t\t\t\t\t\t...])\n```\n\n#### **使用案例:**\n\n```\n\t# gcc头文件查找目录，相当于-I选项，e.g -I/foo/bar\n\t#CMAKE_SOURCE_DIR是cmake内置变量表示当前项目根目录\n\ttarget_include_directories(test_elf\n\t    PRIVATE\n\t    ${CMAKE_SOURCE_DIR}\n\t    ${CMAKE_SOURCE_DIR}/common\n\t    ${CMAKE_SOURCE_DIR}/syscalls\n\t)\n\t# 其他编译选项定义，e.g -fPIC\n\ttarget_compile_options(test_elf\n\t    PRIVATE\n\t    -std=c99 \n\t    -Wall \n\t    -Wextra \n\t    -Werror\n\t)\n```\n\n## **-DCMAKE_BUILD_TYPE**\n\n```shell\n\tcmake -DCMAKE_BUILD_TYPE=Debug ..\n```\n通过以上编译能够使用 **`gdb`** 进行调试, 否则无法gdb来调试生成的可执行文件.\n\n## **遇到的问题**\n\n```shell\n\t$ mkdir build\n\t$ cd build\n\t$ cmake ..\n\t出现如下错误\n\t......\n\tCMake Error at /usr/local/cmake-3.17.0-Linux-x86_64/share/cmake-3.17/Modules/FindPackageHandleStandardArgs.cmake:164 (message):\n\t  Could NOT find PythonLibs (missing: PYTHON_LIBRARIES PYTHON_INCLUDE_DIRS)\n\tCall Stack (most recent call first):\n\t  /usr/local/cmake-3.17.0-Linux-x86_64/share/cmake-3.17/Modules/FindPackageHandleStandardArgs.cmake:445 (_FPHSA_FAILURE_MESSAGE)\n\t  /usr/local/cmake-3.17.0-Linux-x86_64/share/cmake-3.17/Modules/FindPythonLibs.cmake:310 (FIND_PACKAGE_HANDLE_STANDARD_ARGS)\n\t  python/CMakeLists.txt:2 (find_package)\n```\n原因是没有安装python的开发版\n\n```shell\n\t$ yum install python-devel \n```\n网上还有这种操作的, 可以pass掉`cmake -DPYTHON_INCLUDE_DIR=/usr/include/python2.7 -DPYTHON_LIBRARY=/usr/lib/python2.7/config/libpython2.7.so ..`\n\n","categories":["linux"]},{"title":"Ubuntu下安装CUDA8.0及nvidia驱动(详细教程）","url":"/2021/03/13/linux/Ubuntu下安装CUDA8.0及nvidia驱动(详细教程)/","content":"为什么说对的系统呢，这是因为在多次尝试后发现，如果使用ubuntu14安装显卡驱动会出现无法进入系统，一直在循环在登录界面，而ubuntu16则不会出现这个问题，所以说最好升级一下吧，毕竟14有点太老了。\n\n重要的一点是不要在安装或安装后升级内核，否则cuda无法识别内核而导致安装失败！（如果已经升级并且cuda报错，那么百度或谷歌一下如何降内核吧）\n\n先安装一些依赖吧，接下来可能用的到\n\n```shell\nsudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\nsudo apt-get install --no-install-recommends libboost-all-dev\nsudo apt-get install libopenblas-dev liblapack-dev libatlas-base-dev\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\nsudo apt-get install git cmake build-essential\n```\n\n使用如下指令安装你的nvidia驱动（ubntun14会无法进入系统）\n\n```shell\nsudo apt-get update\nsudo apt-get install nvidia-367\n```\n\n安装好后重启，按super（windows）键，在你的程序中搜索nvidia就可以看到\n\n表示你已经成功安装显卡驱动\n\n2. 下载cuda8.0 （这里一定要是cuda8，后面的版本不适应与上诉的NVIDIA驱动）\n\n从这里下载 https://developer.nvidia.com/cuda-downloads\n\n下载出来的1G多的那个就好，下载好后将文件剪切到你的home下，右击文件，点击属性，打开后如图，选择权限，勾选执行\n\n3. 安装CUDA8.0\n\n配置一下环境变量\n\n```shell\nsudo gedit ~/.bashrc\n```\n\n加入这两行：\n\n```shell\nexport LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH\n\nexport LD_LIBRARY_PATH=/lib/x86_64-linux-gnu:$LD_LIBRARY_PATH\n```\n\n然后，就可以开始安装了\n\n```shell\ncd ~\nsudo sh cuda_8.0.61_375.26_linux.run\n```\n\n然后会看到(按照我下面的输入即可）\n\n```\nDo you accept the previously read EULA?\n\naccept/decline/quit: accept\n\nInstall NVIDIA Accelerated Graphics Driver for Linux-x86_64 361.62?\n\n(y)es/(n)o/(q)uit: n\n\nInstall the CUDA 8.0 Toolkit?\n\n(y)es/(n)o/(q)uit: y\n\nEnter Toolkit Location\n[ default is /usr/local/cuda-8.0 ]: 回车\n\nDo you want to install a symbolic link at /usr/local/cuda?\n(y)es/(n)o/(q)uit: y\n\nInstall the CUDA 8.0 Samples?\n\n(y)es/(n)o/(q)uit: y\n\nEnter CUDA Samples Location\n[ default is /root ]:回车\n```\n\n接下来会出现\n\n```\nInstalling the CUDA Toolkit in /usr/local/cuda-8.0 …\nInstalling the CUDA Samples in /root …\nCopying samples to /home/derek/NVIDIA_CUDA-8.0_Samples now…\nFinished copying samples.\n\n= Summary =\n\nDriver: Installed\nToolkit: Installed in /usr/local/cuda-8.0\nSamples: Installed in /home/derek\n\nPlease make sure that\n– PATH includes /usr/local/cuda-8.0/bin\n– LD_LIBRARY_PATH includes /usr/local/cuda-8.0/lib64, or, add /usr/local/cuda-8.0/lib64 to /etc/ld.so.conf and run ldconfig as root\n\nTo uninstall the CUDA Toolkit, run the uninstall script in /usr/local/cuda-8.0/bin\n\nPlease see CUDA_Installation_Guide_Linux.pdf in /usr/local/cuda-8.0/doc/pdf for detailed information on setting up CUDA.\n\nWARNING: Incomplete installation! This installation did not install the CUDA Driver. A driver of version at least 361.00 is required for CUDA 8.0 functionality to work.\n\nTo install the driver using this installer, run the following command, replacing with the name of this run filesudo.run -silent -driver\n```\n\n这样就已经安装好了，就是这么简单不要怀疑，只要不出error就可以，如果出现kernel相关错误，那一般是由于内核太高了，cuda不能识别，包括cuda9等等后来的也不能，所以还是想办法降一下内核吧\n\n4. 编辑配置文件\n\n```shell\nsudo gedit ~/.bash_profile\n```\n\n打开配置文件，加入以下几行\n\n```shell\nexport LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64\"\nexport CUDA_HOME=/usr/local/cuda-8.0\n```\n也可以在.bashrc中配置\n\n```shell\nsudo gedit ~/.bashrc\n```\n加入这两行\n\n```shell\nexport PATH=/usr/local/cuda-8.0/bin:$PATH\nexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH\nsource ~/.bashrc\n``` \n\n现在就可以验证一下cuda是否安装成功咯\n\n```shell\ncd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery\nsudo make\n./deviceQuery\n```\n","categories":["linux"]},{"title":"curl & wget","url":"/2021/03/13/linux/curl_wget/","content":"\n## Linux curl命令详解\n在Linux中curl是一个利用URL规则在命令行下工作的文件传输工具，可以说是一款很强大的http命令行工具。它支持文件的上传和下载，是综合传输工具，但按传统，习惯称url为下载工具。\n常见参数：\n```\n\t-A/--user-agent <string>              设置用户代理发送给服务器\n\t-b/--cookie <name=string/file>    cookie字符串或文件读取位置\n\t-c/--cookie-jar <file>                    操作结束后把cookie写入到这个文件中\n\t-C/--continue-at <offset>            断点续转\n\t-D/--dump-header <file>              把header信息写入到该文件中\n\t-e/--referer                                  来源网址\n\t-f/--fail                                          连接失败时不显示http错误\n\t-H/--header                           添加 HTTP 请求的标头.\n\t-o/--output                                  把输出写到该文件中\n\t-O/--remote-name                      把输出写到该文件中，保留远程文件的文件名\n\t-r/--range <range>                      检索来自HTTP/1.1或FTP服务器字节范围\n\t-s/--silent                                    静音模式。不输出任何东西\n\t-T/--upload-file <file>                  上传文件\n\t-u/--user <user[:password]>      设置服务器的用户和密码\n\t-w/--write-out [format]                什么输出完成后\n\t-x/--proxy <host[:port]>              在给定的端口上使用HTTP代理\n\t-#/--progress-bar                        进度条显示当前的传送状态\n```\nwget更像一个迅雷，是个专门用来下载文件的下载利器.\n\n**`/dev/null`** 是类 unix 系统的 空设备, **`> /del/null`** 就是把标准输出重定向到空设备，不显示也不保存, **`2>&1`** 是把标准错误重定向到标准输出，也就是前面的空设备.\n\n\n### 基本命令\n\n```shell\n$ curl -O http://man.linuxde.net/text.iso                    #O大写，不用O只是打印内容不会下载\n$ wget http://www.linuxde.net/text.iso                       #不用参数，直接下载文件\nroot@alpha:/home/test# curl -O http://man.linuxde.net/text.iso\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n\t\t\t\t\t\t\t\tDload  Upload   Total   Spent    Left  Speed\n100  6332  100  6332    0     0   441k      0 --:--:-- --:--:-- --:--:--  441k\n```\n\n**`-H`** 参数添加 HTTP 请求的标头。\n```\n// 添加 HTTP 标头Accept-Language: en-US.\n$ curl -H 'Accept-Language: en-US' https://google.com\n\n// 添加两个 HTTP 标头.\n$ curl -H 'Accept-Language: en-US' -H 'Secret-Message: xyzzy' https://google.com\n\n// 添加 HTTP 请求的标头是Content-Type: application/json，然后用 -d 参数发送 JSON 数据.\n$ curl -d '{\"login\": \"emma\", \"pass\": \"123\"}' -H 'Content-Type: application/json' https://google.com/login\n```\n\n\n### 保存网页\n\n```shell\n\t$ curl http://www.linux.com >> linux.html\n```\n\n### 下载文件\n\n```shell\n\t$ curl -O http://man.linuxde.net/text.iso                    #O大写，不用O只是打印内容不会下载\n\t$ wget http://www.linuxde.net/text.iso                       #不用参数，直接下载文件\n```\n\n### 下载文件并重命名\n\n```shell\n\t$ curl -o rename.iso http://man.linuxde.net/text.iso         #o小写\n\t$ wget -O rename.zip http://www.linuxde.net/text.iso         #O大写\n```\n\n### 断点续传\n在windows中，我们可以使用迅雷这样的软件进行断点续传。curl可以通过内置option:-C同样可以达到相同的效果\n如果在下载dodo1.JPG的过程中突然掉线了，可以使用以下的方式续传\n```shell\n\tcurl -O -C -URL http://man.linuxde.net/text.iso            #C大写\n\twget -c http://www.linuxde.net/text.iso                    #c小写\n```\n\n### 限速下载\n```shell\n\t$ curl --limit-rate 50k -O http://man.linuxde.net/text.iso\n\t$ wget --limit-rate=50k http://www.linuxde.net/text.iso\n```\n\n### 显示响应头部信息\n```shell\n\t$ curl -I http://man.linuxde.net/text.iso\n\t$ wget --server-response http://www.linuxde.net/test.iso\n```\n\n### wget利器--打包下载网站\n```shell\n\t$ wget --mirror -p --convert-links -P /var/www/html http://man.linuxde.net/\n```\n\n### -O(大写)保存网页中的文件, 要注意这里后面的url要具体到某个文件，不然抓不下来\n```shell\n\t$ curl -O http://www.linux.com/hello.sh\n```\n\n### -x来支持设置代理\n```shell\n\t$ curl -x 192.168.100.100:1080 http://www.linux.com\t\n```\n\n### 保存http的response里面的cookie信息。内置option:-c（小写）\n```shell\n\t$ curl -c cookiec.txt  http://www.linux.com\n```\n\n### 保存http的response里面的header信息。内置option: -D\n```shell\n\t$ curl -D cookied.txt http://www.linux.com\n```\n\n### 很多网站都是通过监视你的cookie信息来判断你是否按规矩访问他们的网站的，因此我们需要使用保存的cookie信息。内置option: -b\n```shell\n\t$ curl -b cookiec.txt http://www.linux.com\n```\n\n### 以服务器上的名称保存文件到本地: -O（大写)\n```shell\n\t$ curl -O http://www.linux.com/dodo1.JPG\n```\n\n### 循环下载\n```shell\n\t$ curl -O http://www.linux.com/dodo[1-5].JPG\n\t$ curl -O http://www.linux.com/{hello,bb}/dodo[1-5].JPG\n```\n下载http://www.linux.com/hello/dodo[1-5].JPG 和 http://www.linux.com/bb/dodo[1-5].JPG 文件\n\n\n### 显示进度条\n\n```shell\n\t$ curl -# -O http://www.linux.com/dodo1.JPG\n\troot@alpha:/home/zhan/test# curl -# -O http://www.linux.com/dodo1.JPG\n\t####################################################################################### 100.0%\n```\n\n### 上传文件\n```shell\n$ curl -T dodo1.JPG -u 用户名:密码 ftp://www.linux.com/img/\n```\n\n### Additional\n\n```shell\n\troot@alpha:/home/zhan/images_test# curl 10.239.140.186:8081\n\tYou have hit fef0dd414fe4\n\troot@alpha:/home/zhan/images_test# wget 10.239.140.186:8081\n\t--2020-05-11 14:59:53--  http://10.239.140.186:8081/\n\tResolving child-prc.intel.com (child-prc.intel.com)... 10.239.4.100\n\tConnecting to child-prc.intel.com (child-prc.intel.com)|10.239.4.100|:913... connected.\n\tProxy request sent, awaiting response... 200 OK\n\tLength: unspecified\n\tSaving to: ‘index.html’\n\t\n\tindex.html                              [ <=>                                                                ]      24  --.-KB/s    in 0s\n\t\n\t2020-05-11 14:59:53 (3.27 MB/s) - ‘index.html’ saved [24]\n\t\n\troot@alpha:/home/zhan/images_test# ls\n\tindex.html\n\troot@alpha:/home/zhan/images_test# cat index.html\n\tYou have hit fef0dd414fe4\n\troot@alpha:/home/zhan/images_test#\n```\n\n","categories":["linux"]},{"title":"cppc动态调频","url":"/2021/03/13/linux/cppc动态调频/","content":"CPPC的全称是Collaborative Processor Performance Control\nCPC的全称是Per cpu table called，是bios提供的一组acpi表(ACPI表示高级配置和电源管理接口（Advanced Configuration and Power Management Interface))，用于设置cpu的频率。这组acpi表如下：\n\n```\n1. /*\n2. * An example CPC table looks like the following.\n3. *\n4. *\tName(_CPC, Package()\n5. *\t\t\t{\n6. *\t\t\t17,\n7. *\t\t\tNumEntries\n8. *\t\t\t1,\n9. *\t\t\t// Revision\n10. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x120, 2)},\n11. *\t\t\t// Highest Performance\n12. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x124, 2)},\n13. *\t\t\t// Nominal Performance\n14. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x128, 2)},\n15. *\t\t\t// Lowest Nonlinear Performance\n16. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x12C, 2)},\n17. *\t\t\t// Lowest Performance\n18. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x130, 2)},\n19. *\t\t\t// Guaranteed Performance Register\n20. *\t\t\tResourceTemplate(){Register(PCC, 32, 0, 0x110, 2)},\n21. *\t\t\t// Desired Performance Register\n22. *\t\t\tResourceTemplate(){Register(SystemMemory, 0, 0, 0, 0)},\n23. *\t\t\t..\n24. *\t\t\t..\n25. *\t\t\t..\n26. *\n27. *\t\t}\n28. * Each Register() encodes how to access that specific register.\n29. * e.g. a sample PCC entry has the following encoding:\n30. *\n31. *\tRegister (\n32. *\t\tPCC,\n33. *\t\tAddressSpaceKeyword\n34. *\t\t8,\n35. *\t\t//RegisterBitWidth\n36. *\t\t8,\n37. *\t\t//RegisterBitOffset\n38. *\t\t0x30,\n39. *\t\t//RegisterAddress\n40. *\t\t9\n41. *\t\t//AccessSize (subspace ID)\n42. *\t\t0\n43. *\t\t)\n44. *\t}\n45. */\n```\n\n那cppc表具体要怎么工作呢？具体在driver/cpufreq/cppc_cpufreq.c中。\n这里的cppc_cpufreq_init是入口函数，这个函数向cpufreq的framework注册了一个可以调频的cpu driver\n\n```shell\nstatic int __init cppc_cpufreq_init(void)\n{\nret = cpufreq_register_driver(&cppc_cpufreq_driver);\nif (ret)\ngoto out;\n}\nstatic struct cpufreq_driver cppc_cpufreq_driver = {\n.flags = CPUFREQ_CONST_LOOPS,\n.verify = cppc_verify_policy,\n .target = cppc_cpufreq_set_target,\n .get = cppc_cpufreq_get_rate,\n .init = cppc_cpufreq_cpu_init,\n .stop_cpu = cppc_cpufreq_stop_cpu,\n .name = \"cppc_cpufreq\",\n };\n //cppc_cpufreq_driver 最终的函数就是target，最终cpu调频就是通过target 这个回调函数来实现\n static int cppc_cpufreq_set_target(struct cpufreq_policy *policy,\n unsigned int target_freq,\n unsigned int relation)\n {\n struct cppc_cpudata *cpu;\n struct cpufreq_freqs freqs;\n u32 desired_perf;\n int ret = 0;\n cpu = all_cpu_data[policy->cpu];\n //得到要设置的频率\n desired_perf = cppc_cpufreq_khz_to_perf(cpu, target_freq);\n /* Return if it is exactly the same perf */\n if (desired_perf == cpu->perf_ctrls.desired_perf)\n return ret;\n cpu->perf_ctrls.desired_perf = desired_perf;\n freqs.old = policy->cur;\n freqs.new = target_freq;\n cpufreq_freq_transition_begin(policy, &freqs);\n //通过acpi 提供的的接口来设置cpu 频率\n ret = cppc_set_perf(cpu->cpu, &cpu->perf_ctrls);\n cpufreq_freq_transition_end(policy, &freqs, ret != 0);\n if (ret)\n pr_debug(\"Failed to set target on CPU:%d. ret:%d\\n\",\n cpu->cpu, ret);\n return ret;\n }\n```\n\n这里的cppc_set_perf实现在driver/acpi/cppc_acpi.c中实现，通过这个接口可以通过固件来设置cpu频率。\n","categories":["linux"]},{"title":"chown改变文件所属用户和用户组","url":"/2021/03/13/linux/chown/","content":"\n## chown\nLinux chown（英文全拼：change owner）命令用于设置文件所有者和文件关联组的命令.  \n\nchown 需要超级用户 root 的权限才能执行此命令.  \n\n### 语法:\n\n```shell\n\tchown [-cfhvR] [--help] [--version] user[:group] file...\n```\n\n### 参数:\n\n```\n\tuser : 新的文件拥有者的使用者 ID\n\tgroup : 新的文件拥有者的使用者组(group)\n\t-c : 显示更改的部分的信息\n\t-f : 忽略错误信息\n\t-h :修复符号链接\n\t-v : 显示详细的处理信息\n\t-R : 处理指定目录以及其子目录下的所有文件\n\t--help : 显示辅助说明\n\t--version : 显示版本\n```\n\n### 实例\n将当前前目录下的所有文件与子目录的拥有者皆设为当前登陆用户\n\n```shell\n\tchown -R `whoami` ./\n```\n把 /var/run/httpd.pid 的所有者设置 root：\n\n```shell\n\tchown root /var/run/httpd.pid\n```\n将文件 file1.txt 的拥有者设为 runoob，群体的使用者 runoobgroup :\n\n```shell\n\tchown runoob:runoobgroup file1.txt\n```\n将当前前目录下的所有文件与子目录的拥有者皆设为 runoob，群体的使用者 runoobgroup:\n\n```shell\n\tchown -R runoob:runoobgroup *\n```\n\n把 /home/runoob 的关联组设置为 512 （关联组ID），不改变所有者：\n```shell\n\tchown :512 /home/runoob\n```\n\n\n\n\n","categories":["linux"]},{"title":"dmidecode","url":"/2021/03/13/linux/dmidecode/","content":"## dmidecode命令\n```\n查看服务器型号：dmidecode | grep \"Product Name\"\n查看主板的序列号：dmidecode | grep \"Serial Number\"\n查看系统序列号：dmidecode -s system-serial-number\n查看内存信息：dmidecode -t memory\n查看OEM信息：dmidecode -t 11\n```\n\n不带选项执行dmidecode命令通常会输出所有的硬件信息。dmidecode命令有个很有用的选项-t，可以按指定类型输出相关信息，假如要获得处理器方面的信息，则可以执行：\n\n```shell\ndmidecode -t processor\n```\n","categories":["linux"]},{"title":"docker","url":"/2021/03/13/linux/docker/","content":"\n## docker保存/lodad镜像\n复制镜像和复制容器都是通过保存为新镜像而进行的。\n具体为：\n保存镜像\n\n```shell\ndocker save ID > xxx.tar\ndocker load < xxx.tar\ndocker load image-name:new < image.tar\ndocker load -i image.tar\n```\n\n//load期间使用$df -hl可以查看最下面docker绑定的根分区大小，如果image.tar要大于根分区大小则load会报空间不足的错误，解决方法下面有解.\n保存容器\n\n```shell\ndocker export ID >xxx.tar\ndocker import xxx.tar container:v1\n```\n\n然后再`docker run -it container:v1 bash`\n\n```shell\n\tdocker run --name faiss-gcc-9.1 -it faiss-images-gcc-9.1.0:1.0 /bin/bash  //用这个\n\texit\n\tdocker start faiss-gcc-9.1\n\tdocker attach faiss-gcc-9.1\n```\n\n软件镜像（如 weChat.exe）----> 运行镜像----> 产生一个容器（正在运行的软件，运行的 微信程序）；\n操作\t命令\t说明\n运行\n```\ndocker run --name container-name -d image-name:tag /bin/bash\n\t运行一个名为 container-name的容器,并在容器里运行/bin/bash,如果不指定运行/bin/bash则docker启动后就立即退出，这跟docker机制有关，docker是后台运行，必须有前台进程，没有前台程序就退出\n\t#退出\n\texit\n\t#关闭\n\tdocker stop mycentos\n\t#重启\n\t1.docker start mycentos\n\t#重启后,在用mycentos再打开/bin/bash\n\t2.docker exec -ti mycentos /bin/bash    //1和2两步是合起来用的\n```\n如:docker run --name myredis –d redis\t /bin/bash\n--name：自定义容器名\n-d：表示后台运行\nimage-name:指定运行的镜像名称\n\ntag:镜像的版本\n\n列表\tdocker ps（查看运行中的容器）；\t加上-a；可以查看所有容器\n停止\tdocker stop container-name/container-id\t停止当前运行的指定容器\n启动\tdocker start container-name/container-id\t启动容器\n删除\tdocker rm container-id\t删除指定容器\n端口映射\t-p 6379:6379\n如:docker run  --name myredis  -d -p 6379:6379 docker.io/redis\t\n-p:主机端口映射到容器内部的端口\n\n容器日志\tdocker logs container-name/container-id\t \n官网可查看更多命令：https://docs.docker.com/engine/reference/commandline/docker/\n\n删除Images\n\n```shell\n$ docker rm      --   Remove one or more containers\n$ docker rmi     --   Remove one or more images\n```\n\n想要删除运行过的images必须首先删除它的container。继续来看刚才的例子，\n\n```shell\n$ docker ps -a\nCONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS                   NAMES\n117843ade696        ed9c93747fe1        /bin/sh -c /usr/sbin   46 hours ago        Up 46 hours         0.0.0.0:49153->22/tcp   test_sshd\n```\n\n可以看出ed9c93747fe1的image被117843ade696的container使用着，所以必须首先删除该container\n```shell\n$ docker rm 117843ade696\nError: container_delete: Impossible to remove a running container, please stop it first\n2014/03/22 16:36:44 Error: failed to remove one or more containers\n出现错误，这是因为该container正在运行中(运行docker ps查看)，先将其关闭\n```\n\n```shell\n$ docker stop 117843ade696\n117843ade696\n$ docker rm 117843ade696\n117843ade696\n$ docker rmi ed9c93747fe1\nDeleted: ed9c93747fe16627be822ad3f7feeb8b4468200e5357877d3046aa83cc44c6af\nDeleted: c8a0c19429daf73074040a14e527ad5734e70363c644f18c6815388b63eedc9b\nDeleted: 95dba4c468f0e53e5f1e5d76b8581d6740aab9f59141f783f8e263ccd7cf2a8e\nDeleted: c25dc743e40af6858c34375d450851bd606a70ace5d04e231a7fcc6d2ea23cc1\nDeleted: 20562f5714a5ce764845119399ef75e652e23135cd5c54265ff8218b61ccbd33\nDeleted: c8af1dc23af7a7aea0c25ba9b28bdee68caa8866f056e4f2aa2a5fa1bcb12693\nDeleted: 38fdb2c5432e08ec6121f8dbb17e1fde17d5db4c1f149a9b702785dbf7b0f3be\nDeleted: 79ca14274c80ac1df1333b89b2a41c0e0e3b91cd1b267b31bef852ceab3b2044\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nCentOS65            latest              e55a74a32125        2 days ago          360.6 MB\n```\n\n可以看出，image已经被删除。\nFrom <https://blog.csdn.net/flydreamzhll/article/details/80900509> \n\n## Docker 拷贝\n\n将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。\n```shell\ndocker cp /www/runoob 96f7f14e99ab:/www/\n```\n\n将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www。\n```shell\ndocker cp /www/runoob 96f7f14e99ab:/www\n```\n\n将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中。\n```shell\ndocker cp  96f7f14e99ab:/www /tmp/\n```\n\n```shell\n主机copy到docker\n$ docker cp /opt/test/file.txt mycontainer_id：/opt/testnew/\ndocker文件copy到主机\n$ docker cp mycontainer_id：/opt/testnew/file.txt /opt/test/\n```\n\nFrom <http://www.runoob.com/docker/docker-cp-command.html> \n\n## 更新images\n\ndocker commit Container-id Images-name 将在某个image a 上做改动的新的container更新为最新的image \n\n```shell\n$ docker commit a981e981ef65 faiss-images\n```\n\n## Docker run\n\n```shell\n$ which python\n/usr/bin/python\n$ env\n\n$ vim test.sh\n\t#!/bin/bash\n\texport MKLROOT=/opt/intel/compilers_and_libraries/linux/mkl/\n\texport LD_PRELOAD=/opt/intel/compilers_and_libraries/linux/mkl//lib/intel64/libmkl_core.so:/opt/intel/compilers_and_libraries/linux/mkl//lib/intel64/libmkl_sequential.so\n\n\tcd /test_faiss\n\tpython /test_faiss/test_sift1M.py\n\n$ docker run -t image-name /bin/bash /test_faiss/test.sh\n\t\t\t\t此时image已是在最新container上更新过的image\n   Vtune 绑定docker运行\n$ amplxe-cl -collect hotspots -r test_hot docker run -t faiss-images /bin/bash /test_faiss/test.sh\n\t\t     \t\t     保存目录\n$ amplxe-cl -collect hotspots -r test_hot_1 ls\n```\n\n## 设置镜像标签\n我们可以使用 docker tag 命令，为镜像添加一个新的标签。\n```shell\n$ docker tag 860c279d2fec runoob/centos:dev\n```\n\ndocker tag 镜像ID，这里是 860c279d2fec ,用户名称、镜像源名(repository name)和新的标签名(tag)。\n使用 docker images 命令可以看到，ID为860c279d2fec的镜像多一个标签。\n``` shell\n$ docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nrunoob/centos       6.7                 860c279d2fec        5 hours ago         190.6 MB\nrunoob/centos       dev                860c279d2fec        5 hours ago         190.6 MB\nrunoob/ubuntu       v2                  70bf1840fd7c        22 hours ago        158.5 MB\nubuntu              14.04               90d5884b1ee0        6 days ago          188 MB\nphp                 5.6                 f40e9e0f10c8        10 days ago         444.8 MB\nnginx               latest              6f8d099c3adc        13 days ago         182.7 MB\nmysql               5.6                 f2e8d6c772c0        3 weeks ago         324.6 MB\nhttpd               latest              02ef73cf1bc0        3 weeks ago         194.4 MB\nubuntu              15.10               4e3b13c8a266        5 weeks ago         136.3 MB\nhello-world         latest              690ed74de00f        6 months ago        960 B\ncentos              6.7                 d95b5ca17cc3        6 months ago        190.6 MB\ntraining/webapp     latest              6fae60ef3446        12 months ago       348.8 MB\n```\n\n## Docker修改空间大小\n * 第一种：\n```\n$ docker info\nContainers: 0\n Running: 0\n Paused: 0\n Stopped: 0\nImages: 0\nServer Version: 1.12.6\nStorage Driver: devicemapper\n Pool Name: docker-253:0-67125080-pool\n Pool Blocksize: 65.54 kB\n Base Device Size: 10.74 GB#这个就是限制，容器根分区的大小！\n Backing Filesystem: xfs\n Data file: /dev/loop0\n Metadata file: /dev/loop1\n Data Space Used: 11.8 MB\n Data Space Total: 107.4 GB\n Data Space Available: 36.95 GB\n Metadata Space Used: 581.6 kB\n Metadata Space Total: 2.147 GB\n Metadata Space Available: 2.147 GB\n Thin Pool Minimum Free Space: 10.74 GB\n Udev Sync Supported: true\n Deferred Removal Enabled: false\n Deferred Deletion Enabled: false\n Deferred Deleted Device Count: 0\ndocker的版本是1.12,修改容器根分区的大小即可：\n\n  dm.loopdatasize=100G是指存放数据的数据库空间为100g，默认是100g\ndm.loopmetadatasize=10G是存放Metadata数据空间为10g，默认是2g\ndm.fs=xft是指容器磁盘分区为xft\ndm.basesize=20G是指容器根分区默认为20g，默认是10g\n```\n```shell\n$ vi /etc/sysconfig/docker-storage\n```\n修改下面参数即可\n```\nDOCKER_STORAGE_OPTIONS=\"--storage-driver devicemapper --storage-opt dm.loopdatasize=200G --storage-opt dm.loopmetadatasize=10G -g /dev/docker/ --storage-opt dm.fs=xfs --storage-opt dm.basesize=30G\"\n```\n利用-g参数即可指定存储挂载路径。比如，示例中的配置将存储目录挂载在/data/docker/路径下\n重新挂载新的路径后原来路径下的images和container都找不到，-g参数去掉回到默认挂载路径再按照下面重启docker服务会发现原来的images和container都存在.\n\n最后重启容器，问题解决\n\n```shell\n// 停止docker服务的命令如下:\nsystemctl stop docker\n// 重新启动：\nsystemctl daemon-reload && systemctl start docker\n```\n\n * 第二种： //试了试不怎么灵\nDocker默认空间大小分为两个，一个是池空间大小，另一个是容器空间大小。\n池空间大小默认为：100G\n容器空间大小默认为是：10G\n所以修改空间大小也分为两个：\n这里使用centos下的yum进行安装的Docker。\n \n首先，修改空间大小，必需使Docker运行在daemon环境下，即先停止正在运行的docker服务：\nservice docker stop\n然后使用命令使用daemon环境下运行docker：\ndocker -d          //可以不需要这条\n一、修改池空间大小方法：\ndd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/data bs=1G count=0 seek=1000\ndd if=/dev/zero of=/var/lib/docker/devicemapper/devicemapper/metadata bs=1G count=0 seek=10\n上面的1000为1TB大小，即为数据池空间大小为1TB，而10则为Metadata的空间大小，10GB\n从运行完后，启动docker service\nservice docker start\n使用命令查看docker池空间大小：\ndocker info\n\n可以看到池空间已经被设置为data=1TB和metadata=10GB\n\n## 关于Docker目录挂载的总结\nDocker容器启动的时候，如果要挂载宿主机的一个目录，可以用-v参数指定。\n譬如我要启动一个centos容器，宿主机的/test目录挂载到容器的/soft目录，可通过以下方式指定：\n\n```shell\n$ docker run -it  --privileged=true -v /test:/soft  --name ContainerName ImagesName /bin/bash\n```\n\n这样在容器启动后，容器内会自动创建/soft的目录。通过这种方式，我们可以明确一点，即-v参数中，冒号\":\"前面的目录是宿主机目录，后面的目录是容器内目录。\n挂载宿主机已存在目录后，在容器内对其进行操作，报“Permission denied”。\n可通过两种方式解决：\n1> 关闭selinux。\n临时关闭：# setenforce 0\n永久关闭：修改/etc/sysconfig/selinux文件，将SELINUX的值设置为disabled。\n2> 以特权方式启动容器 \n指定--privileged参数\n如：# docker run -it --privileged=true -v /test:/soft centos /bin/bash\n\n## 容器的特权模式运行\n\n--privileged 可以启动docker的特权模式，这种模式允许我们以其宿主机具有（几乎）所有能力来运行容器，包括一些内核特性和设备访问。\n\n这是让我们可以在dokcer中运行docker的必要参数\n\n让docker运行在--privileged特权模式会有一些安全风险。这种模式下运行容器对docker宿主机拥有root访问权限。\n\n","categories":["linux"]},{"title":"dd 命令","url":"/2021/03/13/linux/dd_command/","content":"\n## \n\nLinux dd 命令用于读取、转换并输出数据。\ndd可从标准输入或文件中读取数据，根据指定的格式来转换数据，再输出到文件、设备或标准输出。\n\n参数说明:\nif=文件名：输入文件名，默认为标准输入。即指定源文件。\nof=文件名：输出文件名，默认为标准输出。即指定目的文件。\n\nbs=bytes：同时设置读入/输出的块大小为bytes个字节\n\nconv=<关键字>，关键字可以有以下11种：\n  conversion：用指定的参数转换文件。\n  ascii：转换ebcdic为ascii\n  ebcdic：转换ascii为ebcdic\n  ibm：转换ascii为alternate ebcdic\n  block：把每一行转换为长度为cbs，不足部分用空格填充\n  unblock：使每一行的长度都为cbs，不足部分用空格填充\n  lcase：把大写字符转换为小写字符\n  ucase：把小写字符转换为大写字符\n  swab：交换输入的每对字节\n  noerror：出错时不停止\n  notrunc：不截短输出文件\n  sync：将每个输入块填充到ibs个字节，不足部分用空（NUL）字符补齐。\n\n--help：显示帮助信息\n--version：显示版本信息\n\n### 实例\n\n在Linux 下制作启动盘，可使用如下命令：\n\n```shell\n\t$ dd if=boot.img of=/dev/fd0 bs=1440k\n```\n\n将testfile文件中的所有英文字母转换为大写，然后转成为testfile_1文件，在命令提示符中使用如下命令：\n\n```shell\n\t$ dd if=testfile_2 of=testfile_1 conv=ucase \n```\n\n由标准输入设备读入字符串，并将字符串转换成大写后，再输出到标准输出设备\n\n```shell\n\t$ dd conv=ucase \n```\n\n输入以上命令后按回车键，输入字符串，再按回车键，按组合键Ctrl+D 退出\n\n```shell\n\t$ dd conv=ucase \n\thello\t\t// 输入字符串后按回车键  \n\tHELLO\t\t// 按组合键Ctrl+D才会输出并退出，转换成大写结果  \n\t0+1 records in\n\t0+1 records out\n\t6 bytes (6 B) copied, 5.79589 s, 0.0 kB/s\n```\n\n\n\n","categories":["linux"]},{"title":"cut","url":"/2021/03/13/linux/cut/","content":"\n## cut\n\n**`cut`** 命令用来显示行中的指定部分，删除文件中指定字段. **`cut`** 经常用来显示文件的内容.  \n\n<!-- more -->\n\n## 语法\n```\ncut(选项)(参数)\n```\n\n## 选项\n\n```\n-b：仅显示行中指定直接范围的内容；\n-c：仅显示行中指定范围的字符；\n-d：指定字段的分隔符，默认的字段分隔符为“TAB”；\n-f：显示指定字段的内容；\n-n：与“-b”选项连用，不分割多字节字符；\n--complement：补足被选择的字节、字符或字段；\n--out-delimiter=<字段分隔符>：指定输出内容是的字段分割符；\n--help：显示指令的帮助信息；\n--version：显示指令的版本信息。\n```\n\n## 参数\n文件：指定要进行内容过滤的文件。\n\n## 实例\n\n文件:\n```\n$ vim test.txt\nNo Mark Percent\n01 69 91\n02 71 87\n03 68 98\n\n$ vim test1.txt\nNo,Name,Mark,Percent\n01,tom,69,91\n02,jack,71,87\n03,alex,68,98\n```\n\n命令\n```\n$ cut -f1 -d\" \" test.txt\n// 与上面命令一样\n$ cat test.txt | cut -f1 -d\" \"\nNo\n01\n02\n03\n\n$ cut -f2 -d\",\" test1.txt\n// 与上面命令一样\n$ cat test1.txt | cut -f2 -d\",\"\nName\ntom\njack\nalex\n```\n\n\n\n\n","categories":["linux"]},{"title":"curl 升级","url":"/2021/03/13/linux/curl升级/","content":"\n官网:\nhttps://curl.haxx.se/download.html#LinuxRedhat\nhttps://mirror.city-fan.org/ftp/contrib/sysutils/Mirroring/\nhttp://www.city-fan.org/ftp/contrib/yum-repo/rhel6/x86_64/\n\n\n## 卸载旧版本curl\n\n```shell\n\trpm -qa | grep curl\n\t  libcurl-7.29.0-57.el7_8.1.x86_64\n\t  python-pycurl-7.19.0-19.el7.x86_64\n\t  curl-7.29.0-57.el7_8.1.x86_64\n\t  libcurl-devel-7.29.0-57.el7_8.1.x86_64\n```\n\n卸载: 基本语法\n\n```shell\n\trpm -e <RPM包的名称>\n```\n增加参数 --nodeps ,就可以强制删除，但是一般不推荐这样做，因为依赖于该软件包的程序可能无法运行.  \n\n## 安装curl包\n基本语法:\n\n```shell\n\trpm -ivh  <RPM包全路径名称>\n```\n\n参数说明： i=install 安装 v=verbose 提示 h=hash  进度条\n\n## 升级curl包\n需要升级curl包根本原因在于 curl 7.19或7.29 版本不支持 TLS v1.1 以上的协议，如果需要支持 TLS v1.2 版本，至少需要升级到 curl 7.34 版本。\n\n```shell\n\trpm -Uvh  http://www.city-fan.org/ftp/contrib/yum-repo/rhel6/x86_64/city-fan.org-release-2-1.rhel6.noarch.rpm\n\tyum --showduplicates list curl --disablerepo=\"*\" --enablerepo=\"city*\"\n\t(与上面好像不一样)yum --showduplicates list curl --disablerepo=\"*\"  --enablerepo=\"fan*\"\n\t  Loaded plugins: fastestmirror, langpacks\n\t  Loading mirror speeds from cached hostfile\n\t   * city-fan.org: nervion.us.es\n\t   * city-fan.org-source: nervion.us.es\n\t  Installed Packages\n\t  curl.x86_64                                             7.29.0-57.el7_8.1                                               @updates\n\t  Available Packages\n\t  curl.x86_64                                             7.72.0-2.0.cf.rhel7                                             city-fan.org\n```\n修改该 \"city-fan.org\" repo的enable为1\n\n```shell\n\tvim /etc/yum.repos.d/city-fan.org.repo\n\t[city-fan.org]\n\tname=city-fan.org repository for Red Hat Enterprise Linux (and clones) $releasever ($basearch)\n\t#baseurl=http://mirror.city-fan.org/ftp/contrib/yum-repo/rhel$releasever/$basearch\n\tmirrorlist=http://mirror.city-fan.org/ftp/contrib/yum-repo/mirrorlist-rhel$releasever\n\tenabled=1\n\tgpgcheck=1\n\tgpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-city-fan.org\n```\n\n安装最新的curl\n\n```shell\n\tyum install curl\n\t  ......\n\t  ---> Package libcurl-devel.x86_64 0:7.29.0-57.el7_8.1 will be updated\n\t  ---> Package libcurl-devel.x86_64 0:7.72.0-2.0.cf.rhel7 will be an update\n\t  ......\n```\n\n如果提示缺少依赖 libnghttp2.so.14()(64bit)\n```shell\n\trpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/Packages/l/libnghttp2-1.6.0-1.el6.1.x86_64.rpm\n```\n\n然后重复第4步即可\n查看curl版本\n\n```shell\n\tcurl -V\n\tcurl 7.72.0 (x86_64-redhat-linux-gnu) libcurl/7.72.0 NSS/3.44 zlib/1.2.7 libpsl/0.7.0 (+libicu/50.1.2) libssh2/1.9.0 nghttp2/1.33.0\n\tRelease-Date: 2020-08-19\n\tProtocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtsp scp sftp smb smbs smtp smtps telnet tftp\n\tFeatures: AsynchDNS GSS-API HTTP2 HTTPS-proxy IPv6 Kerberos Largefile libz Metalink NTLM NTLM_WB PSL SPNEGO SSL UnixSockets\n```\n\n(疑惑地方)实践安装后在 curl 7.72 版本中，使用的密码学库并没有从 NSS 替换为 OpenSSL 了，输入以下命令可以看出：\n\n```shell\n\tcurl-config --ssl-backends\n\t  NSS\n```\n\n\n## reference\nhttps://www.cnblogs.com/kingsonfu/p/10069755.html\nhttps://bbs.huaweicloud.com/blogs/123952\n\n\n\n","categories":["linux"]},{"title":"/etc/profile, /etc/bashrc, ~/.bashrc","url":"/2021/03/13/linux/etc_profile-etc_bashrc-~.bashrc/","content":"1、Ubuntu保存环境变量的几个文件\n * /etc/profile\n在用户登录时，操作系统定制用户环境时使用的第一个文件，此文件为系统的每个用户设置环境信息，当用户第一次登录时，该文件被执行。\n\n * /etc/environment\n在用户登录时，操作系统使用的第二个文件， 系统在读取用户个人的profile前，设置环境文件的环境变量。\n\n * ~/.profile\n在用户登录时，用到的第三个文件 是.profile文件，每个用户都可使用该文件输入专用于自己使用的shell信息，当用户登录时，该文件仅仅执行一次！默认情况下，会设置一些环境变量，执行用户的.bashrc文件。\n\n * /etc/bashrc\n为每一个运行bash shell的用户执行此文件，当bash shell被打开时，该文件被读取。\n\n * ~/.bashrc\n该文件包含专用于用户的bash shell的bash信息，当登录时以及每次打开新的shell时，该该文件被读取。\n```\n…\nexport http_proxy=http://child-prc.intel.com:913/\nexport https_proxy=http://child-prc.intel.com:913/\n```\n这样修改每次yum,pip等工具下载文件时候不能再写policy\n使文件立刻生效，$ source ~/.profile\n\nNote： 以上文件可通过$ sudo gedit 文件名 或 $ sudo vim 文件名打开；建议只修改~/.profile文件，如果只修改~/.bashrc文件，后期使用go get 命令时，会提示GOPATH未设置。\n2、设置GOPATH和GOROOT\n$ sudo gedit ~/.profile\n在文件最后添加\n export GOROOT=\"/usr/lib/go-1.8\" // 引号内设置为你自己的go安装目录\n export GOBIN=$GOROOT/bin\n export GOPATH=\"/home/test/gopath\" // 引号内设置为自己的go项目的工作区间\n export PATH=$PATH:$GOPATH/bin    // 原路径后用冒号连接新路径\n\n使文件立刻生效，$ source ~/.profile\n重启系统即可\n","categories":["linux"]},{"title":"gdb","url":"/2021/03/13/linux/gdb/","content":"\n## gdb实例\n\n```shell\n\tgcc main.c a.c b.c -o app -g\n```\n * -g:会保留函数名和变量名\n\n例如一个程序名为prog 参数为 -l a -C abc\n\n则，运行gcc/g++ -g  prog.c/cpp -o prog\n\n就可以用gdb调试程序prog\n\n```shell\n\tgdb prog\n```\n\n进入gdb调试界面\n输入参数命令set args 后面加上程序所要用的参数，注意，不再带有程序名，直接加参数，如：\n\n```shell\n\tset args -l a -C abc\n```\n回车后输入\n\n```shell\n\tr\n```\n\n即可开始调试\n\n```shell\n\t(gdb) start\t 开始运行程序，只运行main函数里的第一行.\n\t(gdb) run\t\t运行程序直到断点或程序末尾\n\t(gdb) c\t\t继续运行程序\n\t(gdb) n\t\t运行一行代码\n\t(gdb) s\t\t进入函数内部运行\n\t(gdb) show listsize\t默认显示10行源代码\n\t(gdb) set listsize=20\t设置默认显示20行源代码\n\t(gdb) l 30\t显示源代码第30行附近上下文\n\t(gdb) l functionname\t显示函数名的源代码\n\t(gdb) l insert_sort.c:15\t显示源代码所引用的insert_sort.c代码文件中第15行上下文\n\t(gdb) l insert_sort.c:functioname\t显示源代码所引用的insert_sort.c代码文件中的function函数的代码\n\t(gdb) l main.c:main\n```\ngdb中的中文注释会乱码\n\n```shell\n\t(gdb) b 行号\t添加断点，//注释和\"{\", \"}\"等都是无效注释\n\t(gdb) b 函数名\n\t(gdb) b 文件名:行号\n\t(gdb) b 文件名：函数名\n\t(gdb) i(info) b\t查看设置过的所有断点\n\tNum: 断点编号\n\tEnb: enable 断点现在是否可用，y就是可用, n无效程序跑起来不会停到这个断点\n\tAddress: 断点在内存中的位置\n\tWhat: 如in functionname at main.c:12\n\t        在某个function->main.c文件->第12行\n\n\t(gdb) i(info) display\t查看display的变量行号\n\tNum Enb Expression\n\t1        y      i\n\t2        y      array[i]\n\n\t(gdb) d 编号\t删除断点编号\n\t如：d 1;  d 2 3;  d 4-5\n\t(gdb) dis 行号\t设置指定断点无效，Enb为n\n\t(gdb) ena 行号\t设置指定断点有效，Enb为y\n```\n设置条件断点\n\n```shell\n\t(gdb) b 行号 if i == 10\t在第num行打断点，当i == 10候停下来\n\t……\n\t9    for(int i = 0; i < 20; i++)\n\t10      printf(\"XXX\");\n\t……\n```\n查看变量\n\n```shell\n\t(gdb) p i\t查看i变量的值\n\t(gdb) ptype i\t查看变量类型\n\t(gdb) ptype array\ttype = int\n\t    type = int[31]\n\t(gdb) display array[i]\t设置变量的自动显示\n\t(gdb) display i\t每循环到断点处就输出array[i]的值\n\t(gdb) undisplay Num\tNub为i display看到的Enm值，取消变量的自动显示\n```\n设置变量\n\n```shell\n\t(gdb) set var 变量名=值\t设置变量值\n```\n跳出循环和退出\n\n```shell\n\t(gdb) until\t跳出for循环，此循环执行完就退出，循环里的断点去掉或设置无效\n\t(gdb) finish\t跳出函数，函数里的断点去掉或设置无效\n\t(gdb) q\t退出gdb调试\n```\n","categories":["linux"]},{"title":"helm安装","url":"/2021/03/13/linux/helm/","content":"\n## **Helm简介**\n\nReference Link:\n\nhttps://www.cnblogs.com/lori/p/12653024.html\nhttps://www.cnblogs.com/bigberg/p/13925981.html\n\nHelm 是一个由 CNCF 孵化和管理的项目, 用于对需要在Kubernetes上部署的复杂应用进行定义, 安装和更新.  \n\n在3.0版本之前, Helm 由 **HelmClient** 和 **TillerServer** 两个组件完成.\n\n**`在helm3之前的版本里，它由客户端helm和服务端tiller组成，而 helm3.0之后它去掉了tiller，而直接与k8s通讯`**，可以说在部署上更简单了，目前自己测试项目中是部署2.x版本的helm.\n\n**HelmClient** 是一个客户端.  \n**TillerServer** 负责客户端指令和 Kubernetes 集群之间的交互, 根据 Chart 定义, 生成和管理各种 Kubernetes 的资源对象.  \n\n## **Helm 3.0+ 安装**\n\n[Helm offical installation tutorial](https://helm.sh/docs/intro/install/)\nManually download and install the Helm release version: [release](https://github.com/helm/helm/releases)\n\n```shell\n$ curl -O https://get.helm.sh/helm-v3.3.1-linux-amd64.tar.gz\n$ tar -zxvf helm-v3.3.1-linux-amd64.tar.gz\n$ mv linux-amd64/helm /usr/local/bin/helm\n```\n\nFrom there, you should be able to run the client and add the stable repo: `helm help`\n\n### **命令补全**\n\n```shell\n# vim ~/.bashrc\nsource <(helm completion bash)\n\n# source ~/.bashrc\n```\n\n## **Helm 2.0+ 安装**\n\n### **Helm安装**\n\n```shell\n$ curl -O https://get.helm.sh/helm-v2.16.5-linux-amd64.tar.gz\n$ tar -zxvf helm-v2.16.5-linux-amd64.tar.gz\n$ mv linux-amd64/helm /usr/local/bin/helm\n```\n\n### **Tiller Server 安装**\nReference Link: https://github.com/fnproject/fn-helm/issues/21\n\nTiller Server 安装推荐使用helm init命令进行.  \n\n这一命令会在kubectl当前context指定集群内的 kube-system 命名空间创建一个 Deployment 和一个 Service, 运行TillerServer服务.  \n\n```shell\n$ kubectl create serviceaccount --namespace kube-system tiller\n$ kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller\n// 安装tiller\n$ helm init --service-account tiller --skip-refresh\n$ kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}'\n```\n\n","categories":["linux"]},{"title":"Linux中禁用命令历史记录","url":"/2021/03/13/linux/history/","content":"\n## 关闭history记录功能\n\n```shell\n\t$ set +o history\n```\n\n## 打开history记录功能\n\n```shell\n\t$ set -o history\n```\n\n## 清空记录\n\n```shell\n\t$ history -c 记录被清空，重新登录后恢复。\n\t$ rm -f $HOME/.bash_history 删除记录文件，清空历史。\n```\n\n","categories":["linux"]},{"title":"jq json解析工具","url":"/2021/03/13/linux/jq_json解析工具/","content":"\n## jq\n\nreference:\nhttps://stedolan.github.io/jq/download/\nhttps://blog.csdn.net/qq_38423105/article/details/89786736\n\n<!-- more -->\n\n## 安装\n\n```\n安装EPEL源\nyum install epel-release\n\n安装完EPEL源后，可以查看下jq包是否存在\nyum list jq\n \n安装jq\nyum install jq\n```\n\n\n\n\n","categories":["linux"]},{"title":"getopt,getopts解析命令行长参数","url":"/2021/03/13/linux/getopt_getopts_解析命令长参数/","content":"\n## **getopt与getopts比较**\ngetopt 与 getopts 都是 Bash 中用来获取与分析命令行参数的工具，常用在 Shell 脚本中被用来分析脚本参数。\n\n两者的比较\n\n（1）getopts 是 Shell 内建命令，getopt 是一个独立外部工具\n\n（2）getopts 使用语法简单，getopt 使用语法较复杂, getopts选项参数的格式必须是-d val，而不能是中间没有空格的-dval。\n\n（3）getopts 不支持长参数（如：--option ），getopt 支持\n\n（4）getopts 不会重排所有参数的顺序，getopt 会重排参数顺序（这里的区别下面会说明）\n\n（5）getopts 出现的目的是为了代替 getopt 较快捷的执行参数分析工作\n\n## **getopts**\n * ${OPTARG}, 用来取当前选项的值\n * ${OPTIND}, 代表当前选项在参数列表中的位移\n * shift, 把选项参数抹去\ngetopts在处理参数的时候，处理一个开关型选项，OPTIND加1，处理一个带值的选项参数，OPTIND则会加2.  \n${OPTIND} - 1对整个参数列表进行左移操作，最左边的参数就丢失了.  \n1.选项参数的格式必须是-d val，而不能是中间没有空格的-dval。\n2.所有选项参数必须写在其它参数的前面，因为getopts是从命令行前面开始处理，遇到非-开头的参数，或者选项参数结束标记--就中止了，如果中间遇到非选项的命令行参数，后面的选项参数就都取不到了。\n3.不支持长选项， 也就是--debug之类的选项\n\n### **实例1**\ntouch bash.sh\n\n```shell\n\t#!/bin/bash\n\techo 初始 OPTIND: $OPTIND\n\twhile getopts \"a:b:c\" arg #选项后面的冒号表示该选项需要参数\n\tdo\n\t    case $arg in\n\t        a)\n\t            echo \"a's arg:$OPTARG\" #参数存在$OPTARG中\n\t            ;;\n\t        b)\n\t            echo \"b's arg:$OPTARG\"\n\t            ;;\n\t        c)\n\t            echo \"c's arg:$OPTARG\"\n\t            ;;\n\t        ?)  #当有不认识的选项的时候arg为?\n\t            echo \"unkonw argument\"\n\t            exit 1\n\t        ;;\n\t    esac\n\tdone\n\techo 处理完参数后的 OPTIND：$OPTIND\n\techo 移除已处理参数个数：$((OPTIND-1))\n\tshift $((OPTIND-1))\n\techo 参数索引位置：$OPTIND\n\techo 准备处理余下的参数：\n\techo \"Other Params: $@\"\n```\n\n执行并输出:\n\n```shell\n\t$ ./bash.sh -a 1 -b 2 -c 3  test -oo xx -test\n\t初始 OPTIND: 1\n\ta's arg:1\n\tb's arg:2\n\tc's arg:\n\t处理完参数后的 OPTIND：6\n\t移除已处理参数个数：5\n\t参数索引位置：6\n\t准备处理余下的参数：\n\tOther Params: 3 test -oo xx -test\n```\n\n## **getopt**\n在getopt的较老版本中，存在一些bug，不大好用，在后来的版本中解决了这些问题，我们称之为getopt增强版。通过-T选项，我们可以检查当前的getopt是否为增强版，返回值为4，则表明是增强版的.  \n\n```shell\n\t$ getopt -T\n\t$ echo $?\n\t  4\n\t$ getopt -V\n\t  getopt from util-linux 2.23.2\n```\n\n$1 和 ${1}的效果是一样的, 但是不用花括号的话，$10 会被认为是 $1 和一个字符 0.  \n * -o 表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项\n如-carg 而不能是-c arg\n * --long表示长选项\n * \"$@\"把每个参数作为一个字符串返回，可以使用for循环来遍历\n  * ./bash.sh a b c d\n        a\n        b\n        c\n        d\n * $* 将所有参数当做一个整体来引用\n  * ./bash.sh a b c d\n        a b c d\n * $# 表示参数的个数。\n * $? 最近一个执行的命令的退出状态。\n * $_ 上一个命令的最后一个参数。使用快捷键 ESC+. 也是这个效果\n * -n:出错时的信息\n * -- ：举一个例子比较好理解：我们要创建一个名字为 \"-f\"的目录你会怎么办？mkdir -f #不成功，因为-f会被mkdir当作选项来解析，这时就可以使用, mkdir -- -f 这样-f就不会被作为选项。  \n\n### **实例1**\ntouch get_opt.sh\n\n```shell\n\t#!/bin/bash\n\techo $@\n\t#-o或--options选项后面接可接受的短选项，如ab:c::，表示可接受的短选项为-a -b -c，其中-a选项不接参数，-b选项后必须接参数，-c选项的参数为可选的\n\t#-o表示短选项，两个冒号表示该选项有一个可选参数，可选参数必须紧贴选项, 如-carg 而不能是-c arg\n\t#-l或--long选项后面接可接受的长选项，用逗号分开，冒号的意义同短选项。\n\t#-n选项后接选项解析错误时提示的脚本名字\n\t# Note that we use `\"$@\"' to let each command-line parameter expand to a separate word. The quotes around `$@' are essential!\n\t# We need TEMP as the `eval set --' would nuke the return value of getopt.\n\tARGS=`getopt -o ab:c:: --long along,blong:,clong:: -n 'get_opt.sh' -- \"$@\"`\n\tif [ $? != 0 ]; then\n\t    echo \"Terminating...\"\n\t    exit 1\n\tfi\n\t\n\techo $ARGS\n\t#将规范化后的命令行参数分配至位置参数（$1,$2,...)\n\teval set -- \"${ARGS}\"\n\t\n\twhile true\n\tdo\n\t    case \"$1\" in    # 每次循环匹配下面参数后就会删除(左移)参数, 因此每次都是匹配$1位置的变量\n\t        -a|--along)\n\t            echo \"Option a\";\n\t            shift   # 参数左移一位, 相当于删除一位参数变量\n\t            ;;\n\t        -b|--blong)\n\t            echo \"Option b, argument $2\";\n\t            shift 2 # 参数左移两位, 相当于删除参数变量和变量后面的值\n\t            ;;\n\t        -c|--clong)\n\t            case \"$2\" in    # 匹配变量后面的值\n\t                \"\")         # 变量值为空\n\t                    echo \"Option c, no argument\";\n\t                    shift 2\n\t                    ;;\n\t                *)          # 变量值为任意值, 不要加引号或双引号\n\t                    echo \"Option c, argument $2\";\n\t                    shift 2;\n\t                    ;;\n\t            esac\n\t            ;;\n\t        --)         # 在上面的eval命令会在命令行中加上--参数\n\t            shift\n\t            break\n\t            ;;\n\t        *)\t\t\t# 变量值为任意值, 不要加引号或双引号\n\t            echo \"Internal error!\"\n\t            exit 1\n\t            ;;\n\t    esac\n\tdone\n\t\n\t#处理剩余的参数\n\tfor arg in $@\n\tdo\n\t    echo \"processing $arg\"\n\tdone\n```\n\n执行:\n**Note: 对于用`::`来声明的可选参数**\n * 短参数如-c必须紧挨后面参数值,如-c456而不是-c 456.`  \n * 长参数如--clong后面必须加`=`, 否则参数值会判断为空, 如`--clong=456`, 如果写成--clong 456那么解析时候--clong后面参数为空\n\n```shell\n\t$ chmod +x get_opt.sh\n\t$ ./get_opt.sh -b 123 -a -c456 file1 file2\n\t-b 123 -a -c456 file1 file2\n\t-b '123' -a -c '456' -- 'file1' 'file2'\n\tOption b, argument 123\n\tOption a\n\tOption c, argument 456\n\tprocessing file1\n\tprocessing file2\n\t\n\t$ ./get_opt.sh --blong 123 -a --clong=456 file1 file2\n\t--blong 123 -a --clong=456 file1 file2\n\t--blong '123' -a --clong '456' -- 'file1' 'file2'\n\tOption b, argument 123\n\tOption a\n\tOption c, argument 456\n\tprocessing file1\n\tprocessing file2\n\t\n\t$ ./get_opt.sh -c -a file1 file2\n\t-c -a file1 file2\n\t-c '' -a -- 'file1' 'file2'\n\tOption c, no argument\n\tOption a\n\tprocessing file1\n\tprocessing file2\n```\n\n","categories":["linux"]},{"title":"expect 执行与终端的交互操作","url":"/2021/03/13/linux/expect执行与终端的交互操作/","content":"\n## **expect用法介绍**\n### 安装expect\n\n```shell\n\t$ yum install expect\n```\n### **1.定义脚本执行的shell**\n\n```shell\n\t$ vim expect_shell.sh\n\t#!/usr/bin/expect\n```\n\n### **2.设置超时时间**\n```shell\nset timeout 30\n```\n单位是秒, 如果设为timeout -1 意为永远不超时.\n\n### **3.spawn**\nspawn是进入expect环境后才能执行的内部命令, 不能直接在默认的shell环境中进行执行\n主要功能: 传递交互指令.\n\n### **4.expect**\n这里的expect是expect的内部命令\n主要功能是判断输出结果是否包含某项字符串, 没有则立即返回, 负责就等待一段时间后返回, 等待时间通过timeout进行设置.\n\n```shell\n\texpect { \"yes/no\" { send \"yes\\r\"} }\n\texpect \"*#\"\t\t\t// 表示匹配所有, 不管进程输出什么都能匹配成功.\n\texpect \"eof\"\t\t// 退出expect会话spawn进程, 重新退到shell上来\n```\n\n### **5.send**\n执行交互动作, 将交互要执行的动作进行输入给交互指令\n命令字符串结尾加上回车\"\\r\"\n\n```shell\n\tsend \"exit\\r\"\t\t//退出远程终端\n```\n\n### **6.interact**\n执行完成后爆出交互状态, 把控制权交给控制台, 如果不加这一项, 交互完成会自动退出.\n\n### **7.exp_continue**\n继续执行接下来的交互操作\n\n### **8.$argc, $argv**\n\nexpect脚本接受从终端bash传递过来的参数, 可以用 `[lindex $argv n]` 获得, n从0开始, 分别表示第一个,第二个,第三个...参数.\n$argc 表示传递参数总个数.\n\n\n## **实例1**\n脚本后缀名最好用 `.exp`, 这样写脚本时候容易排错.\ntouch expect_shell.exp\n\n```shell\n\t#!/usr/bin/expect\n\t\n\t# yum install expect    // 安装expect\n\t# spawn     // 开启一个会话, 也就是开启一个进程\n\t# expect    // 匹配进程执行后终端输出的信息中包含的字符串\n\t# set timeout 30    // 设置超时时间, 单位是秒, 如果设为timeout -1 意味永不超时\n\t# exp_continue:     // 如果没有匹配成功, 继续向下匹配, 如字符串中不含有\"yes/no\", 则继续向下匹配比如是否含有\"password\"\n\t# send      // 向终端输入信息\n\t# interact  // 留在会话进程中等待进行下一步操作\n\t\n\t# Open a session\n\tspawn ssh root@10.239.131.206\n\t\n\t# Match the string contained in the received message\n\texpect {\n\t    \"yes/no\" { send \"yes\\r\"; exp_continue } # 花括号'{','}'和里面内容间要有空格\n\t    \"password\" { send \"123456\\r\" };\n\t}\n\t\n\t# After the above match is successful, continue to perform the interactive operation\n\t\n\texpect \"*#\"\n\tsend \"rm -rf /mnt/minio_data1\\r\"\n\tsend \"mkdir /mnt/minio_data2\\r\"\n\t#expect \"*#\"\n\tsend \"exit\\r\"\n\t\n\t# exit expect process, exit spawn session\n\texpect \"eof\"\n```\n执行: \n\n```shell\n\t$ chmod a+x expect_shell.exp\n\t$ ./expect_shell.exp\n```\n\n## **实例2**\ntouch bash_shell.sh\n\n```shell\n\t#!/bin/bash\n\tusername=root\n\tpassword=123456\n\tnode01=10.239.131.206\n\t\n\texpect <<-EOF\n\tspawn ssh $username@$node01\n\texpect {\n\t    \"yes/no\" { send \"yes\\r\"; exp_continue }\n\t    \"password\" { send \"${password}\\r\"; exp_continue }\n\t    \"Last login\" { send \"\\r\" }\t\t# 设置免密登陆后不需要输入密码, 输出字符串含有\"Last login\"\n\t}\n\texpect \"*#\"\n\tsend \"mkdir /mnt/test3 -p \\r\"\n\texpect \"*#\"\n\tsend \"exit\\r\"\n\texpect \"eof\"\n\tEOF\n```\n\n执行：\n\n```shell\n\t$ chmod a+x bash_shell.sh\n\t$ ./bash_shell.sh\n```\n\n## **实例3, 解析命令行参数**\ntouch mkdir_node_minio.sh\n登陆选择的机器上创建文件夹后再退出\n\n```shell\n\t#!/bin/bash\n\t\n\t#usecase: ./bash_shell_commands.sh -u <username> -p <pwd>  --node01 <node01-ip> --node02 <node02-ip> --node03 <node03-ip> --node04=<node04-ip>\n\tARGS=`getopt -o u:p: --long username:,password:,node01_ip:,node02_ip:,node03_ip:,node04_ip:: -n 'mkdir_node_minio.sh' -- \"$@\"`\n\tif [ $? != 0 ]; then\n\t    echo \"Terminating...\"\n\t    exit 1\n\tfi\n\teval set -- \"${ARGS}\"\n\techo ${ARGS}\n\t\n\twhile true\n\tdo\n\t    case \"$1\" in\n\t        -u|--username)\n\t            username=$2\n\t            shift 2 ;;\n\t        -p|--password)\n\t            password=$2\n\t            shift 2 ;;\n\t        --node01_ip)\n\t            node01_ip=$2\n\t            nodes+=(${node01_ip})\n\t            shift 2 ;;\n\t        --node02_ip)\n\t            node02_ip=$2\n\t            nodes+=(${node02_ip})\n\t            shift 2 ;;\n\t        --node03_ip)\n\t            node03_ip=$2\n\t            nodes+=(${node03_ip})\n\t            shift 2 ;;\n\t        --node04_ip)\n\t            case \"$2\" in\n\t                \"\")\n\t                    shift 2 ;;\n\t                *)\n\t                    node04_ip=$2\n\t                    nodes+=(${node04_ip})\n\t                    shift 2 ;;\n\t            esac\n\t            ;;\n\t        --)\n\t            shift\n\t            break ;;\n\t        *)\n\t            echo \"Error!\"\n\t            exit 1 ;;\n\t    esac\n\tdone\n\t\n\tfor node in ${nodes[*]}\n\tdo\n\texpect <<-EOF\n\tspawn ssh $username@${node}\n\texpect {\n\t    \"yes/no\" { send \"yes\\r\"; exp_continue }\t# 如果匹配不到会卡顿一会继续往下执行, 因此一定要事先知道每一步终端会输出哪些信息\n\t    \"password\" { send \"${password}\\r\"; exp_continue }\n\t    \"Last login\" { send \"\\r\" }\t\t# 设置免密登陆后不需要输入密码, 输出字符串含有\"Last login\"\n\t}\n\texpect \"*#\"\n\tsend \"rm -rf /mnt/minio_data_1 \\r\"\n\tsend \"mkdir -p /mnt/minio_data_1 \\r\"\n\texpect \"*#\"\n\tsend \"exit\\r\"\n\texpect \"eof\"\n\tEOF\n\tdone\n```\n\n","categories":["linux"]},{"title":"linux制作U盘启动盘","url":"/2021/03/13/linux/linux制作U盘启动盘/","content":"## 1. 找到U盘:\n```shell\nsudo fdisk -l\n```\n![](fdisk.png)\n\n## 2. 卸载U盘:\n```shell\nsudo umount /dev/sdb1\n```\n![](umount.png)\n\n## 3. 格式化U盘:\n```shell\nsudo mkfs.vfat /dev/sdb -I\n```\n![](mkfs.png)\n\n##4. 制作启动盘:\n\ndd时候，of=跟上U盘的根，如sdc是U盘，分出的还有一个区(卷)sdc1, 那么of=/dev/sdc而不是sdc1\n```shell\nsudo dd if=~/Downloads/ubuntu-16.04-desktop-amd64.iso of=/dev/sdb status=progress\n```\n![](dd.png)\n\n小技巧, 加上status=progress 可以看到进度\n\n制作完成后发现32GB的U盘空间只有几百M，可以用如下命令清空U盘\n但是原来的U盘文件全部删除\n```shell\n1. # dd if=/dev/zero of=/dev/${USB}\n注:\n(1).# 为 root\n(2).${USB} = 你的USB\n2. 格式化U盘\nmkfs.vfat /dev/sdb -I\n```","categories":["linux"]},{"title":"java安装与卸载","url":"/2021/03/13/linux/java安装与卸载/","content":"\n## Centos7 yum 安装java\n<!-- more -->\n\n```shell\n\t$ yum list |grep java-11\n\t(optional)$ yum search java\n\t\n\t// openjdk*等代表开源的java, Oracle版是闭源的只有可执行java文件\n\tyum install -y java-11-openjdk.x86_64 java-11-openjdk-devel.x86_64 java-11-openjdk-headless.x86_64\n\t\n\t// 查看yum安装过的java\n\tyum list installed |grep java\n```\n\n### jdk安装位置\n\n```shell\n\tls /usr/lib/jvm/\n```\n\n## 切换java版本\n\n### (推荐)第一种, 使用update-alternatives切换java版本\n **1. 查看linux系统中的java是auto还是manual选择版本的**\n\n```shell\n\t$ update-alternatives --list | grep java\n\t$ update-alternatives --display java\n\tjava - status is auto.\n\t link currently points to /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.275.b01-0.el7_9.x86_64/jre/bin/java\n\t/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64/jre/bin/java - family java-1.7.0-openjdk.x86_64 priority 1700261\n```\n\n **2. 切换java版本**\n * 方法1:\n\n```shell\n\t$ update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-11-openjdk-11.0.9.11-2.el7_9.x86_64/bin/java 100\nusage: alternatives --install <link> <name> <path> <priority>\n```\n\n * 方法2:\n\n```shell\n\t$ update-alternatives --config java\n\tThere are 3 programs which provide 'java'.\n\t\n\t  Selection    Command\n\t-----------------------------------------------\n\t   1           java-1.7.0-openjdk.x86_64 (/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64/jre/bin/java)\n\t*+ 2           java-1.8.0-openjdk.x86_64 (/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-0.el7_8.x86_64/jre/bin/java)\n\t   3           java-11-openjdk.x86_64 (/usr/lib/jvm/java-11-openjdk-11.0.9.11-2.el7_9.x86_64/bin/java)\n\t\n\tEnter to keep the current selection[+], or type selection number: 3\n```\n\n**3. 再次查看你linux系统java版本**\n\n```shell\n\t$ update-alternatives --display java\n\tjava - status is manual.\n\t link currently points to /usr/lib/jvm/java-11-openjdk-11.0.9.11-2.el7_9.x86_64/bin/java\n\t/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.261-2.6.22.2.el7_8.x86_64/jre/bin/java - family java-1.7.0-openjdk.x86_64 priority 1700261\n```\n\n### 第二种, 添加java环境变量\n如果你定义了JAVA_HOME环境变量，根据你设置的Java版本更新变量, 如下.  \n编辑配置文件，设置java环境变量  \n\n```shell\n\t$ vim /etc/profile\n\t#set java environment\n\tJAVA_HOME=/usr/lib/jvm/java-11-openjdk-11.0.9.11-2.el7_9.x86_64\n\tJRE_HOME=$JAVA_HOME\n\tCLASS_PATH=.:$JRE_HOME/lib\n\tPATH=$PATH:$JAVA_HOME/bin\n\texport JAVA_HOME JRE_HOME CLASS_PATH PATH\n```\n让修改生效：\n\n```shell\n\tsource /etc/profile\n```\n验证：\n\n```shell\n\tjava -version\n```\n\n### 切回auto版本\n\n```shell\n\t$ update-alternatives --auto java\n```\n\n### 删除某个版本:\n\n```shell\n\t$ update-alternatives --remove java /usr/lib/jvm/java-11-openjdk-11.0.9.11-2.el7_9.x86_64/bin/java\n```\n\n## yum卸载java\n\n```shell\n\trpm -qa | grep java\n\t// 卸载java 相关的包, \n\tyum -y remove java-1.8.0-open*\n\tyum -y remove java-11-open*\n\t\n\trpm -qa | grep jdk\n\t// 卸载jdk相关的包\n\tyum -y remove copy-jdk-configs-3.3-10.el7_5.noarch\n```shell\n\n","categories":["linux"]},{"title":"Learning Links","url":"/2021/03/13/links/Learning_links/","content":"\n## 目前接触到的各种Learn视频网站  \n\nCSDN学院: https://edu.csdn.net/  \n黑马程序员: http://www.itheima.com/ or http://yun.itheima.com/?pc&hm  //提供很多免费课程  \n传智播客: http://www.itcast.cn/  \nphp中文网: https://www.php.cn/  提供很多免费课程\n慕课网: https://www.imooc.com/  \n\n## 博客or文档Learn\n\nw3school在线教程: https://www.w3school.com.cn/\n在线编写测试HTML代码: https://www.w3school.com.cn/tiy/t.asp?f=html_div_test\nC语言中文网: http://c.biancheng.net/\n菜鸟教程: https://www.runoob.com/  \n\n## MDN\n\nhttps://developer.mozilla.org/zh-CN/  \n\n## Web前端  \n### NodeJS\n\n网址: https://nodejs.org\n中文网址:http://nodejs.cn/  \nnpm说明文档网址(查各种node_modules源码和使用说明): https://www.npmjs.com/  \n模拟 NodeJS 单线程但又是异步执行情况的网址: http://latentflip.com/loupe/  \n\n### AngularJS\n\nchrome 浏览器打开: https://angularjs.org/ 网站下载免解压或压缩版  \nGihub: https://github.com/angular/angular.js/releases  \n\n## 需要加别人QQ获取资源\n\nhttp://www.itjcw123.cn/  \nhttp://19r9qt.coding-pages.com/categories/  \n\n## 免费在线作图, 实时协作\n\nXmind官网地址： https://www.xmind.cn\nProcessOn在线作图地址：https://www.processon.com\n\n","categories":["LearnLinks"]},{"title":"Centos安装chrome","url":"/2021/03/13/linux/Centos安装chrome/","content":"\n## Centos安装chrome\n\n<!-- more -->\n\n### 1.配置yum下载源：\n\n在目录 /etc/yum.repos.d/ 下新建文件 google-chrome.repo\n```\n[root@localhost ~]#  cd /ect/yum.repos.d/\n[root@localhost yum.repos.d]#  vim google-chrome.repo\n```\n\n编辑google-chrome.repo，内容如下，，编辑后保存退出(：wq)\n\n```\n[google-chrome]\nname=google-chrome\nbaseurl=http://dl.google.com/linux/chrome/rpm/stable/$basearch\nenabled=1\ngpgcheck=1\ngpgkey=https://dl-ssl.google.com/linux/linux_signing_key.pub\n```\n安装google chrome浏览器：\n\n```\n[root@localhost yum.repos.d]# yum -y install google-chrome-stable --nogpgcheck\n```\n查看浏览器版本\n\n```\ngoogle-chrome --version\n```\n\n### 2.驱动的安装\n\n下载地址\n\n```\nhttps://chromedriver.storage.googleapis.com/index.html\n```\n找到相应的驱动发到 /usr/bin/\n\n```\nmv chromedriver /usr/bin/\n```\n查看所属权限\n\n```\nls -l chromedriver\n```\n赋予执行权限并将其赋给要执行的用户\n\n```\nchmod 755 /usr/bin/chromedriver\nchown xxx:xxx /usr/bin/chromedriver\n```\n此上操作就可用程序自动化调用谷歌浏览器\n\n\n\n","categories":["linux"]},{"title":"Centos 更改kernel 默认启动项","url":"/2021/03/13/linux/Centos更改kernel 默认启动项/","content":"\n在生成grub.cfg之前，最好先备份原始的grub.cfg文件\n\n```shell\nuname -r   #查看当前内核\nrpm -qa | grep kernel   #显示已经安装的内核 \ncat /boot/grub2/grub.cfg | menuentry   #查看启动项 \n\ngrub2-set-default \"CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core)\" #配置默认内核\ngrub2-editenv list #查看默认启动项\n\ngrub2-mkconfig -o /boot/grub2/grub.cfg   #生成配置 \n```\n\nFrom <https://blog.csdn.net/c935612575/article/details/81558352?utm_source=blogxgwz4> \n","categories":["linux"]},{"title":"Devtoolset 升级gcc到8.3.1","url":"/2021/03/13/linux/Devtoolset 升级gcc到8.3.1/","content":"设置gcc到8.0再用 -mavx512f 参数\nTo install the full tools-set including gfortran on centos 7:\n\n```shell\n\tyum install centos-release-scl\n\tyum install devtoolset-8                       //yum install devtoolset-7, 升级到gcc 7.3\n\tscl enable devtoolset-8 -- bash           //scl enable devtoolset-7 -- bash, 只在当前终端生效\n```\nenable the tools:\n\n```shell\n\tsource /opt/rh/devtoolset-8/enable \n```\nyou may wish to put the command above in .bash_profile or ~/.bashrc\n\nref: https://unix.stackexchange.com/questions/477360/centos-7-gcc-8-installation\n","categories":["linux"]},{"title":"Linux kernel 更新","url":"/2021/03/13/linux/Linux_kernel_更新/","content":"\n## Linux kernel 更新\n\n### 第一种: 源码更新：\n目前系统内核目录\n/usr/src/kernels/3.10.0-693.el7.x86_64/.config\n/usr/src/kernels/3.10.0-957.21.3.el7.x86_64/.config\n上面文件夹内基本都只有头文件，缺少kernel的源码文件\nmake -j12\n先make modules_install\n然后make install\n就可以重启了\n这种kernel源码一般只有头文件的。。。我没用过yum安装的kernel，都是自己去kernel.org上下载源码自己编译的\n就算下载的kernel比目前版本新也可以，只需要make oldconfig\n就会把老的config都应用到新的内核里\n\n\n1.下载内核\nhttp://www.kernel.org下载内核代码\n * 下载\n\n```shell\nsudo wget https://cdn.kernel.org/pub/linux/kernel/v4.x/linux-4.17.3.tar.xz\n```\n * 解压 下载的文件格式是.tar.xz，需要先解压\n\n```shell\nsudo xz -d linux-4.17.3.tar.xz\n```\n * 这样只解压了一层，发现解压后是tar格式，需要用tar命令再解压\n```shell\nsudo tar -xf linux-4.17.3.tar\n```\n * 这样便可以进入到目录linux-4.17.3了\n\n2.部署内核源码\n把内核解压到/usr/src目录下 cd /usr/src tar -xvf ~/Downloads/linux-4.14.1.tar.xz #解压源码\n\n3.在内核代码目录下创建.config\n\n```shell\ncd linux-4.14.1 \ncp /boot/config-`uname -r` .config #这里`uname -r`可以求出当前的内核版本 sudo make oldconfig\n```\n\n4. 编译内核\n\n```shell\nsudo make\nsudo make modules_install\nsudo make install\n```\n\n编译时可能出现缺少openssl，apt install即可，make的时间比较长，中途如果出错再次编译前最好先sudo make clean\n5. 测试\n\n```shell\nsudo reboot #重启\nuname -r # 查看内核版本\n```\n\n第一次重启可能比较慢，耐心等待即可。\n\n### 第二种: rpm包更新:\nCentOS7 更新最新内核\n内核下载地址：https://elrepo.org/linux/kernel/el7/x86_64/RPMS/\n内核选择\nkernel-lt（lt=long-term）长期有效\nkernel-ml（ml=mainline）主流版本\n升级kernel\n安装过程\n1.下载内核\n\n```shell\nwget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-5.4.3-1.el7.elrepo.x86_64.rpm\n// 如果网站不需要证件，或文件过大需后台下载(-b)命令如下:\nwget -c -b https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-ml-5.4.3-1.el7.elrepo.x86_64.rpm --no-check-certificate\n```\n\n2.安装内核\n\n```shell\nrpm -ivh kernel-ml-5.4.3-1.el7.elrepo.x86_64.rpm\n```\n\n3.查看当前默认内核\n\n```shell\ngrub2-editenv list\nsaved_entry=CentOS Linux (3.10.0-327.28.3.el7.x86_64) 7 (Core)\n```\n\n4.查看所有内核启动 grub2\n\n```shell\nawk -F \\' '$1==\"menuentry \" {print i++ \" : \" $2}' /etc/grub2.cfg \n0 : CentOS Linux (5.4.3-1.el7.elrepo.x86_64) 7 (Core)\n1 : CentOS Linux (3.10.0-327.28.3.el7.x86_64) 7 (Core)\n2 : CentOS Linux (3.10.0-327.22.2.el7.x86_64) 7 (Core)\n3 : CentOS Linux (3.10.0-327.13.1.el7.x86_64) 7 (Core)\n4 : CentOS Linux, with Linux 0-rescue-cd8c4444947b4b0b818457f51ded6591\n```\n\n5.修改为最新的内核启动\n```shell\ngrub2-set-default 'CentOS Linux (5.4.3-1.el7.elrepo.x86_64) 7 (Core)'\n```\n\n6.再次查看内核\n```shell\ngrub2-editenv list\nsaved_entry=CentOS Linux (5.4.3-1.el7.elrepo.x86_64) 7 (Core)\n```\n\n7.重新启动\n\n```shell\nreboot\n```\n\n8.更新kernel-ml-headers\n\n```shell\nwget http://ftp.osuosl.org/pub/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-headers-5.4.3-1.el7.elrepo.x86_64.rpm\n```\n如果遇到headers版本冲突如下\n先卸载当前的kernel-headers, 会卸载一些依赖如gcc g++等，可以在卸载后安装新版kernel-headers后再安装回来\nyum remove kernel-headers\n再安装此此5.4 kernel-headers\nrpm -ivh kernel-ml-headers-5.4.3-1.el7.elrepo.x86_64.rpm\n\n9.更新kernel-ml-devel\nhttp://ftp.osuosl.org/pub/elrepo/kernel/el7/x86_64/RPMS/kernel-ml-devel-5.4.3-1.el7.elrepo.x86_64.rpm\n```shell\nrpm -ivh kernel-ml-devel-5.4.3-1.el7.elrepo.x86_64.rpm\n```\n\n10. 安装gcc等\n```shell\nyum install gcc\n```\n\n\n\n","categories":["linux"]},{"title":"Linux 修改root名称","url":"/2021/03/13/linux/Linux 修改root名称/","content":"\n## 修改root名称\n```shell\nvi /etc/passwd\n```\n按i键进入编辑状态\n修改第1行第1个root为新的用户名\n按esc键退出编辑状态，并输入:x保存并退出\n```shell\nvi /etc/shadow                                                 //root用户对应的密码文件\n```\n按i键进入编辑状态\n修改第1行第1个root为新的用户名\n按esc键退出编辑状态，并输入:x!强制保存并退出\n为了正常使用sudo，需要修改/etc/sudoers的设置，修改方法如下（来自How to add users to /etc/sudoers）：\n\t运行vi sudo\n\t找到root    ALL=(ALL)       ALL\n\t在下面添加一行：新用户名    ALL=(ALL)       ALL\n\t:x保存退出\n","categories":["linux"]},{"title":"Intel_pstate & HWP功能enable/disable","url":"/2021/03/13/linux/Intel_pstate&HWP功能enable_disable/","content":"\nlinux/drivers/cpufreq/intel_pstate.c\n\nFrom <https://github.com/torvalds/linux/blob/v5.0/drivers/cpufreq/intel_pstate.c> \n\n\nIntel_pstate & acpi-cpufreq 驱动切换\nBy default it is intel_pstate driver\n               Change intel_pstate driver to acpi-cpufreq as follow:\n\n```\n/etc/default/grub\nchange\nGRUB_CMDLINE_LINUX=\"***quiet\"\nto\nGRUB_CMDLINE_LINUX=\"***quiet intel_pstate=disable\"\nthen\nUEFI 系统上的指令是 grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg\nafter reboot, then acpi-cpufreq driver willbe used.\n```\n\n查看HWP功能是否开启：\n```shell\nrdmsr 0x770, 读取1则enable， 读取0则disable\n```\n\n关闭HWP功能：\n```shell\n$ vim /etc/default/grub\nGRUB_CMDLINE_LINUX = \"*** intel_pstate=no_hwp\"\n```\n\nintel_pstate=disable\tdisable intel_pstate 驱动\nintel_pstate=passive\tPassive mode enabled, default_driver = &intel_cpufreq\n\t机器重启后cpupower frequency-info查看驱动确实是intel_cpufreq \n\t  current CPU frequency: Unable to call hardware\n\t  current CPU frequency: 1000 MHz (asserted by call to kernel)\nintel_pstate=no_hwp\tHWP disabled, 重启后执行cpupower frequency-info命令查看\n\t  current CPU frequency: Unable to call hardware\n\t  current CPU frequency: 1000 MHz (asserted by call to kernel)\nintel_pstate=force\t\nintel_pstate=hwp_only\t\nintel_pstate=per_cpu_perf_limits\t\nintel_pstate=support_acpi_ppc\tacpi_ppc = true\n","categories":["linux"]},{"title":"linux/Linux能访问外网_外网却访问不了此Linux","url":"/2021/03/13/linux/Linux能访问外网_外网却访问不了此Linux/","content":"查看是否开启了ssh服务是否安装,使用命令：\nsudo ps -e |grep ssh\n\n安装openssh-server，使用命令：\nsudo apt-get install openssh-server\n\n查看主机的IP地址，使用命令：\nifconfig\n\n • 启动ssh命令：\n```shell\nservice sshd start\n```\n\n • 停止ssh命令：\n```shell\nservice sshd stop\n```"},{"title":"Linux下命令Is和命令dir的区别","url":"/2021/03/13/linux/Linux下命令Is和命令dir的区别/","content":"\n## Linux下命令Is和命令dir的区别\n\n<!-- more -->\n\n在Linux下命令ls和dir都有打印目录内容的功能\n## 名词解释\n\n * ls - list directy contents 是linux下的显示目录内容的命令.\n * DIR,是directory的缩写,是目录的意思.也是打开Linux目录内容的命令。\n\n## 区别:\n* ls 是Linux的原装命令，dir是原来dos的命令，Linux选择兼容了此个dos命令，所以dir和ls在功能上是一样的！只是由来有所区别！\n\n演示\n\n```\n[ds@iz1zdpxadujj9vz text]$ dir\na.out  hellon  hellon.c\n[ds@iz1zdpxadujj9vz text]$ ls\na.out  hellon  hellon.c\n[ds@iz1zdpxadujj9vz text]$ ^C\n[ds@iz1zdpxadujj9vz text]$ \n```\n\n## 结论：\n在Linux中，dir和ls有着相同的作用！","categories":["linux"]},{"title":"EOF","url":"/2021/03/13/linux/EOF/","content":"\n## EOF\n通过cat配合重定向能够生成文件并追加操作,在它之前先熟悉几个特殊符号:\n\n```\n\t< :输入重定向\n\t> :输出重定向\n\t>> :输出重定向,进行追加,不会覆盖之前内容\n\t<< :标准输入来自命令行的一对分隔号的中间内容.\n```\n\n\"<< EOF EOF\" 的作用是在命令执行过程中用户自定义输入，它类似于起到一个临时文件的作用，只是比使用文件更方便灵活\n\nEOF是END Of File的缩写,表示自定义终止符.既然自定义,那么EOF就不是固定的,可以随意设置别名,在linux按ctrl-d就代表EOF.\nEOF一般会配合cat能够多行文本输出.\n其用法如下:\n\n```\n\t<<EOF        //开始\n\t....\n\tEOF            //结束\n```\n\n还可以自定义，比如自定义：\n\n```\n\t<<BBB        //开始\n\t....\n\tBBB              //结束\n```\n\n1. 向文件test.sh里输入内容。 \n\n```shell\n\t$ cat << EOF > test.sh 或者 $ cat > test.sh << EOF\n\t> 123123123\n\t> 3452354345\n\t> asdfasdfs\n\t> EOF\n```\n\n追加内容\n\n```shell\n\t$ cat << EOF >>test.sh 或者 $ cat >> test.sh << EOF\n\t> 7777\n\t> 8888\n\t> EOF\n```\n\n覆盖\n\n```shell\n\t$ cat << EOF >test.sh 或者 $ cat > test.sh << EOF\n\t> 55555\n\t> EOF\n```\n\n2. 自定义EOF，比如自定义为wang\n\n```shell\n\t$ cat << wang > haha.txt 或者 $ cat > test.sh << wang\n\t> ggggggg\n\t> 6666666\n\t> wang\n```\n\n\n\n\n","categories":["linux"]},{"title":"Makefile","url":"/2021/03/13/linux/Makefile/","content":"\n## 内置变量（Implicit Variables）\nMake命令提供一系列内置变量，比如，$(CC) 指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。\n\n<!-- more -->\n\n```\noutput:\n\t$(CC) -o output input.c\n```\n\n## 自动变量（Automatic Variables）\n\n### $@\n$@指代当前目标，就是Make命令当前构建的那个目标。比如，**`make foo`** 的 **`$@`** 就指代 **`foo`**。\n```\na.txt b.txt: \n\ttouch $@\n```\n等同于下面的写法。\n```\na.txt:\n    touch a.txt\nb.txt:\n    touch b.txt\n```\n\n### $<\n$< 指代第一个前置条件。比如，规则为 t: p1 p2，那么$< 就指代p1。\n```\na.txt: b.txt c.txt\n\tcp $< $@\n```\n等同于下面的写法。\n```\na.txt: b.txt c.txt\n\tcp b.txt a.txt \n```\n\n### $?\n\n$? 指代比目标更新的所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，其中 p2 的时间戳比 t 新，$?就指代p2。\n\n### $^\n\n$^ 指代所有前置条件，之间以空格分隔。比如，规则为 t: p1 p2，那么 $^ 就指代 p1 p2.\n\n### $*\n\n$* 指代匹配符 % 匹配的部分， 比如% 匹配 f1.txt 中的f1 ，$* 就表示 f1.\n\n\n## 行首加上@的作用\n\nMakefile文件中放置@符号的作用是在make时不将这一行打印出来，因为在Makefile的默认中make时会打印这一行的字符串。\n\n测试如下：\n\n例如：在gcc –o test.o test.c这一行之前没有添加@时，\n\n![](01.png)\n\n执行make后, 在终端有相应的打印信息。\n\n![](02.png)\n\n当加上@时：\n\n![](03.png)\n\n执行make：\n\n![](04.png)\n\n终端上就不会有gcc –o test.o test.c的打印信息了.\n\n\n## := ?= += =的区别\n\n * = 是最基本的赋值\n * := 是覆盖之前的值\n * ?= 是如果没有被赋值过就赋予等号后面的值\n * += 是添加等号后面的值\n\n## 通配符 `*`\n通配符（wildcard）用来指定一组符合条件的文件名。Makefile 的通配符与 Bash 一致，主要有星号（*）、问号（？）和 [...] 。比如， *.o 表示所有后缀名为o的文件。\n```\nclean:\n\trm -f *.o\n```\n\n## 模式匹配 `%`\nMake命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。\n```\n%.o: %.c\n```\n等同于下面的写法。\n```\nf1.o: f1.c\nf2.o: f2.c\n```\n使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。\n\nreference: https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/#plug-in-certificates-and-key-into-the-cluster\n```\nmake -f ../tools/certs/Makefile.selfsigned.mk cluster1-cacerts\n```\n../tools/certs/Makefile.selfsigned.mk一部分内容如下\n\n```\n%/intermediate.conf: L=$(dir $@)\n%/intermediate.conf:\n\t@echo \"[ req ]\" > $@\n\t@echo \"encrypt_key = no\" >> $@\n\t......\n```\n\n\n","categories":["linux"]},{"title":"MMIO","url":"/2021/03/13/linux/MMIO/","content":"Linux kernel目录\n/usr/src/kernels/3.10.0-693.el7.x86_64/.config\n/usr/src/kernels/3.10.0-957.21.3.el7.x86_64/.config\n上面文件夹内基本都只有头文件，缺少kernel的源码文件\nyumdownloader --source kernel  安装新的kernel源码\n\n/dev/mem是linux下的一个字符设备, 源文件是kernel/drivers/char/mem.c, 这个设备文件是专门用来读写物理地址用的。里面的内容是所有物理内存的地址以及内容信息。通常只有root用户对其有读写权限。\n\n```c\n#include<sys/mman.h>\nvoid *mmap(void *start, size_t length, int prot, int flags,\n           int fd, off_t offset);\nint munmap(void *start, size_t length);\n```\n\nmmap详细用法不在此展开, 特别注意参数start(一般赋值为NULL)和offset是页(page, 一般默认大小为4096bytes)对齐的，而且一定要判断mmap函数的返回值。\n\n```c\n#define MAP_SIZE 4096UL\n#define MAP_MASK (MAP_SIZE - 1)\n\nif((fd = open(\"/dev/mem\", O_RDWR | O_SYNC)) == -1)\n{\n\tFATAL;\n}\nprintf(\"/dev/mem opened.\\n\"); \nfflush(stdout);\ntarget = strtoul(argv[1], 0, 0);\nmap_base = mmap(0, MAP_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, target & ~MAP_MASK);\n    if(map_base == (void *) -1)\n\t{\n\t\tFATAL;\n\t}\n    printf(\"Memory mapped at address %p.\\n\", map_base); \n    fflush(stdout);\n```\n\n\n在driver中通过alloc_pages申请得到的page，将page的物理地址export到user space，但是user space拿到这个物理地址后并不能mmap成功。通过perror(“mmap”)，发现总是返回错误\"Operation not permitted!\"，后来发现是由于kernel对user space访问/dev/mem是有限制的，通过编译选项：CONFIG_STRICT_DEVMEM来限制user space 对物理内存的访问，这个选项的说明在arch/x86/Kconfig.debug中有说明：\n```\nconfig STRICT_DEVMEM\n    bool \"Filter access to /dev/mem\"\n    ---help---\n      If this option is disabled, you allow userspace (root) access to all\n      of memory, including kernel and userspace memory. Accidental\n      access to this is obviously disastrous, but specific access can\n      be used by people debugging the kernel. Note that with PAT support\n      enabled, even in this case there are restrictions on /dev/mem\n      use due to the cache aliasing requirements.\n      If this option is switched on, the /dev/mem file only allows\n      userspace access to PCI space and the BIOS code and data regions.\n      This is sufficient for dosemu and X and all common users of\n      /dev/mem.\n      If in doubt, say Y.\n```\n只有在.config文件中设置CONFIG_STRICT_DEVMEM=n才能获得对整个memory的访问权限，在默认情况下，\nCONFIG_STRICT_DEVMEM=y，这也就是之前mmap总是报错：“Operation not permitted”的原因。\n设置这个选项后，编译kernel，然后运行tool，mmap还是返回错误：“Invalid argument”。后来查到还需要设置\n编译选项CONFIG_X86_PAT=n，这个选项也是默认开启的，但是要关闭这个选项还需要开启CONFIG_EXPERT，\n否则CONFIG_X86_PAT总是关不掉。\n \n设置好这三个编译选项后，重新编译kernel，然后运行tool，发现kernel已经解除了对mmap的访问限制，可以\n正确读取对应物理地址的内容了。\n最后还可以通过修改内核源代码来实现，具体的源文件时在/drivers/char/目录下的mem.c文件\nstatic inline int  range_is_allowed(unsigned long pfn, unsigned long size);\n\n\n#######################################################################################\n\nAttachment of Ref 3\n\nI think I've found the issue -- it's to do with /dev/mem memory mapping protection on the x86.\n\nPl refer to this LWN article:\"x86: introduce /dev/mem restrictions with a config option\"http://lwn.net/Articles/267427/\n\nCONFIG_NONPROMISC_DEVMEM\n\nNow (i tested this on a recent 3.2.21 kernel), the config option seems to be called CONFIG_STRICT_DEVMEM.\n\nI changed my kernel config:\n\n```shell\n$ grep DEVMEM .config\n CONFIG_STRICT_DEVMEM is not set\n```\nWhen the above prg was run with the previous kernel, with CONFIG_STRICT_DEVMEM SET:dmesg shows:\n\n[29537.565599] Program a.out tried to access /dev/mem between 1000000->1001000.\n[29537.565663] a.out[13575]: segfault at ffffffff ip 080485bd sp bfb8d640 error 4 in a.out[8048000+1000]\nThis is because of the kernel protection..\n\nWhen the kernel was rebuilt (with the CONFIG_STRICT_DEVMEM UNSET) and the above prg was run :\n\n```shell\n$ ./a.out \nmmap failed: Invalid argument\n```\nThis is because the 'offset' parameter is > 1 MB (invalid on x86) (it was 16MB).\n\nAfter making the mmap offset to be within 1 MB:\n\n```shell\n$ ./a.out\n```\naddr: 0xb7758000\n*addr: 138293760 \nIt works!See the above LWN article for details.\n\nOn x86 architectures with PAT support (Page Attribute Table), the kernel still prevents the mapping of DRAM regions. The reason for this as mentioned in the kernel source is:\n\nThis check is nedded to avoid cache aliasing when PAT is enabled\nThis check will cause a similar error to the one mentioned above. For example:\n\nProgram a.out tried to access /dev/mem between [mem 68200000-68201000].\nThis restriction can be removed by disabling PAT. PAT can be disabled by adding the \"nopat\" argument to the kernel command line at boot time.\n","categories":["linux"]},{"title":"VScode 取消python lint功能","url":"/2021/03/13/language/python/VScode取消python-lint功能/","content":"\n## 取消python lint功能\n\nVScode编写python文件保存后比如某行注释写了两个或多个 **`###`**, 想保留此格式, 但是VScode安装了autopep8之后, 保存python文件会自动删除一个 **`#`**, 只保留一个 **`#`**\n\n取消方式:\n\nFile -> Perferences -> Settings -> 搜索python -> 找到 **`Formatting:Provider`**, 选择 **`yapf`** 就可以了\n\n![](01.JPG)\n","categories":["language","python"]},{"title":"VScode_python包numpy等自动提示","url":"/2021/03/13/language/python/VScode_python包numpy自动提示/","content":"\n## VScode配置python自动提示\n\n### linux\n\n1. 查看Linux pip install 命令安装包的路径如`/usr/local/lib64/python3.6/site-packages/`.  \n如果不清楚pip安装包的路径, 可以执行两次`pip install numpy`, 第二次执行就可以看到默认pip安装包的目录.  \n\n2. 确定python安装目录如`/usr/lib64/python3.6/`\n\n3. 设置VScode\n文件 – 设置 – 首选项，搜索autoComplete，点击\"在settings.json中编辑\"，添加模块路径\n\n```\n\"python.autoComplete.extraPaths\": [\n    \"/usr/local/lib64/python3.6/site-packages/\",\n    \"/usr/lib64/python3.6/\",\n]\n```\n\n修改之后**重启VScode**, 就可以了\n\n\n### windows\n\n1. 安装Anaconda\n\n2. windows配置Anaconda安装路径如下:\n\n![](01.PNG)\n\n3. VScode 文件 – 设置 – 首选项，搜索autoComplete，点击\"在settings.json中编辑\"，添加模块路径\n\n > **Note:** 有时候设置VScode的python模块自动提示不成功, 原因是要设置Anaconda路径到上图中下面的 **`系统变量`** 的 **`Path`**, 再重启 **cmd** 端口或 **powershell** 输入 `python` 即可查看到正常输入, 重启VScode后python模块即可自动提示.\n\n```\n\"python.autoComplete.extraPaths\": [\n    \"D:\\\\Anaconda3\\\\Lib\\\\site-packages\",\n    \"D:\\\\Anaconda3\\\\Scripts\",\n],\n\"python.autoComplete.addBrackets\": true,\n\"python.jediEnabled\": false,\n```\n\n修改之后**重启VScode**, 就可以了\n\n\n## python代码保存自动格式化\nVScode创建python文件会自动提示安装autopep8, 或者通过以下命令进行安装\n\n```\npip install -U autopep8\n```\n\n","categories":["language","python"]},{"title":"希尔排序","url":"/2021/03/13/language/排序算法/希尔排序/","content":"\n## 速记方法\n插冒选希, 快堆并基\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n从平均情况看：堆排序、归并排序、快速排序胜过希尔排序。\n\n从最好情况看：冒泡排序和直接插入排序更胜一筹。\n\n从最差情况看：堆排序和归并排序强过快速排序。\n\n虽然直接插入排序和冒泡排序速度比较慢，但是当初始序列整体或局部有序是，这两种算法的效率比较高。\n\n当初始序列整体或局部有序时，快速排序算法效率会下降。\n\n当排序序列较小且不要求稳定性是，直接排序效率较好；要求稳定性时，冒泡排序法效率较好。\n\n## 希尔排序\n希尔排序的算法思想：将待排序数组按照步长gap进行分组，然后将每组的元素利用直接插入排序的方法进行排序；每次将gap折半减小，循环上述操作；当gap=1时，利用直接插入，完成排序。\n\n同样的：从上面的描述中我们可以发现：希尔排序的总体实现应该由三个循环完成：\n\n1. 第一层循环：将gap依次折半，对序列进行分组，直到gap=1\n2. 第二、三层循环：也即直接插入排序所需要的两次循环。具体描述见上。\n\n\n``` c\n\t#include <stdio.h>\n\t\n\t/**\n\t * 宏\t功能\n\t * ’#‘\t字符串化\n\t * ‘##’\t字符连接的功能\n\t * \"__VA_ARGS__’\t这个可变参数的宏是新的C99规范中新增的, 和变参函数中的...一致\n\t * ‘##__VA_ARGS__’\t宏前面加上##的作用在于，当可变参数的个数为0时，这里的##起到把前面多余的\",\"去掉的作用,否则会编译出错\n\t */\n\t#define PRINT(fmt, ...) \\\n\t    printf(fmt, ##__VA_ARGS__)\n\t\n\t#define PRINT1(...) \\\n\t    printf(__VA_ARGS__)\n\t\n\tvoid shell_sort(int array[], int length)\n\t{\n\t    int temp, i, j, h;\n\t    for (h = length / 2; h > 0; h = h / 2)\n\t    {\n\t        for (i = h; i < length; ++i)\n\t        {\n\t            temp = array[i];\n\t            for (j = i - h; j >= 0; j -= h)\n\t            {\n\t                if (temp < array[j])\n\t                {\n\t                    array[j + h] = array[j];\n\t                }\n\t                else\n\t                {\n\t                    break;\n\t                }\n\t            }\n\t            array[j + h] = temp;\n\t        }\n\t    }\n\t}\n\t\n\tint main()\n\t{\n\t    int array[] = {29, 18, 87, 56, 3, 27, 9, 63, 39};\n\t\n\t    int length = sizeof(array) / sizeof(array[0]);\n\t    shell_sort(array, length);\n\t    int i = 0;\n\t    for (i = 0; i < length; i++)\n\t    {\n\t        PRINT(\"%d \", array[i]);\n\t    }\n\t    PRINT(\"\\n\");\n\t    return 0;\n\t}\n```\n\nMakefile：\n\n```\n\tinsertion_sort: 01.insertion_sort.c\n\t\tgcc -g 01.insertion_sort.c -o 01.insertion_sort.o\n\t\n\tbubble_sort: 02.bubble_sort.c\n\t\tgcc -g 02.bubble_sort.c -o 02.bubble_sort.o\n\t\n\tselect_sort: 03.select_sort.c\n\t\tgcc -g 03.select_sort.c -o 03.select_sort.o\n\t\n\tshell_sort: 04.shell_sort.c\n\t\tgcc -g 04.shell_sort.c -o 04.shell_sort.o\n\t\n\tclean:\n\t\trm -rf *.o\n```\n\n编译：\n\n```shell\n\t// 编译\n\tmake shell_sort\n\t\n\t// 执行\n\t./04.shell_sort.o\n\t\n\t// 清理\n\tmake clean\n```\n\n\n\n","categories":["language","排序算法"]},{"title":"冒泡排序","url":"/2021/03/13/language/排序算法/冒泡排序/","content":"\n## 速记方法\n插冒选希, 快堆并基\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n从平均情况看：堆排序、归并排序、快速排序胜过希尔排序。\n\n从最好情况看：冒泡排序和直接插入排序更胜一筹。\n\n从最差情况看：堆排序和归并排序强过快速排序。\n\n虽然直接插入排序和冒泡排序速度比较慢，但是当初始序列整体或局部有序是，这两种算法的效率比较高。\n\n当初始序列整体或局部有序时，快速排序算法效率会下降。\n\n当排序序列较小且不要求稳定性是，直接排序效率较好；要求稳定性时，冒泡排序法效率较好。\n\n## 冒泡排序\n![](bubble_sort_01.gif)\n\n冒泡排序思路比较简单：\n\n1. 将序列当中的左右元素，依次比较，保证右边的元素始终大于左边的元素；\n（ 第一轮结束后，序列最后一个元素一定是当前序列的最大值；）\n2. 对序列当中剩下的n-1个元素再次执行步骤1。\n3. 对于长度为n的序列，一共需要执行n-1轮比较\n（利用while循环可以减少执行次数）\n\n\n``` c\n\t#include <stdio.h>\n\t\n\t/**\n\t * 宏\t功能\n\t * ’#‘\t字符串化\n\t * ‘##’\t字符连接的功能\n\t * \"__VA_ARGS__’\t这个可变参数的宏是新的C99规范中新增的, 和变参函数中的...一致\n\t * ‘##__VA_ARGS__’\t宏前面加上##的作用在于，当可变参数的个数为0时，这里的##起到把前面多余的\",\"去掉的作用,否则会编译出错\n\t */\n\t#define PRINT(fmt, ...) \\\n\t    printf(fmt, ##__VA_ARGS__)\n\t\n\t#define PRINT1(...) \\\n\t    printf(__VA_ARGS__)\n\t\n\t// 在C语言中是不存在引用的，也就是说C语言中&表示的不是引用，而是取地址符\n\t// 用指针来取代引用，在函数中传进来地址, 或者可以将代码扩展名保存成.cpp文件\n\tvoid swap_int(int *num1, int *num2)\n\t{\n\t    int temp = 0;\n\t    temp = *num1;\n\t    *num1 = *num2;\n\t    *num2 = temp;\n\t}\n\t\n\tvoid bubble_sort_01(int array[], int length)\n\t{\n\t    int i, j, temp;\n\t    for (i = 0; i < length - 1; ++i)        // 总共比较n - 1 轮.\n\t    {\n\t        for (j = 0; j < length - i - 1; ++j)// 每轮从第一个开始, 两两比较到剩余的元素的前一元素位置.\n\t        {\n\t            if (array[j] > array[j + 1])    // 如果前者大于后者, 则把大的前者冒泡到后边.\n\t            {\n\t                swap_int(&array[j], &array[j + 1]);\n\t            }\n\t        }\n\t    }\n\t}\n\t\n\tint main()\n\t{\n\t    int array[] = {29, 18, 87, 56, 3, 27, 9, 63, 39};\n\t\n\t    int length = sizeof(array) / sizeof(array[0]);\n\t    bubble_sort_01(array, length);\n\t    int i = 0;\n\t    PRINT(\"%s-(%d), %s \\n\",__FILE__, __LINE__, __FUNCTION__);\n\t    for (i = 0; i < length; i++)\n\t    {\n\t        PRINT(\"%d \", array[i]);\n\t    }\n\t    PRINT(\"\\n\");\n\t    return 0;\n\t}\n```\n\nMakefile：\n\n```\n\tinsertion_sort: 01.insertion_sort.c\n\t\tgcc -g 01.insertion_sort.c -o 01.insertion_sort.o\n\t\n\tbubble_sort: 02.bubble_sort.c\n\t\tgcc -g 02.bubble_sort.c -o 02.bubble_sort.o\n\t\n\tclean:\n\t\trm -rf *.o\n```\n编译：\n\n``` shell\n\t// 编译\n\tmake bubble_sort\n\t\n\t// 执行\n\t./02.bubble_sort.o\n\t\n\t// 清理\n\tmake clean\n```\n\n## 改进冒泡排序算法\n传统冒泡排序中每一趟排序操作只能找到一个最大值或最小值,我们考虑利用在每趟排序中进行正向和反向两遍冒泡的方法一次可以得到两个最终值(最大者和最小者) , 从而使排序趟数几乎减少了一半。\n\n改进后的算法实现为:\n\n```c\n\t#define PRINT(fmt, ...) \\\n\t    printf(fmt, ##__VA_ARGS__)\n\t\n\tvoid swap_int(int *num1, int *num2)\n\t{\n\t    int temp;\n\t    temp = *num1;\n\t    *num1 = *num2;\n\t    *num2 = temp;\n\t}\n\t\n\tvoid bubble_sort_02(int array[], int length)\n\t{\n\t    PRINT(\"%s-(%d), %s \\n\",__FILE__, __LINE__, __FUNCTION__);\n\t    int low = 0, high = length - 1;\n\t    int i;\n\t    while (low < high)\n\t    {\n\t        for(i = 0; i <= high; ++i)     // 正向冒泡, 找到最大值\n\t        {\n\t            if (array[i] > array[i + 1])\n\t            {\n\t                swap_int(&array[i], &array[i + 1]);\n\t            }\n\t        }\n\t        --high;    // 将最大值移动到当前要排序的最后位置, high前移一位\n\t        for (i = high; i > 0; --i)     // 反向冒泡, 找到最小值\n\t        {\n\t            if (array[i] < array[i - 1])\n\t            {\n\t                swap_int(&array[i], &array[i - 1]);\n\t            }\n\t        }\n\t        ++low;      // 将最小值移动到当前要排序的最前位置, high前移一位\n\t    }\n\t}\n```\n\n\n\n","categories":["language","排序算法"]},{"title":"比较8大算法之间的差异","url":"/2021/03/13/language/排序算法/比较算法之间的差异/","content":"\n## 速记方法\n插冒选希, 快堆并基\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n从平均情况看：堆排序、归并排序、快速排序胜过希尔排序。\n\n从最好情况看：冒泡排序和直接插入排序更胜一筹。\n\n从最差情况看：堆排序和归并排序强过快速排序。\n\n虽然直接插入排序和冒泡排序速度比较慢，但是当初始序列整体或局部有序是，这两种算法的效率比较高。\n\n当初始序列整体或局部有序时，快速排序算法效率会下降。\n\n当排序序列较小且不要求稳定性是，直接排序效率较好；要求稳定性时，冒泡排序法效率较好。\n\n## 时间比较\n\n * 1w个数据时：\n\n```\n直接插入排序:11.615608\n希尔排序:13.012008\n简单选择排序:3.645136000000001\n堆排序:0.09587900000000005\n冒泡排序:6.687218999999999\n#****************************************************\n快速排序:9.999999974752427e-07 \n#快速排序有误：实际上并未执行\n#RecursionError: maximum recursion depth exceeded in comparison\n#****************************************************\n归并排序:0.05638299999999674\n基数排序:0.08150400000000246\n```\n\n * 10w个数据时：\n\n```\n直接插入排序:1233.581131\n希尔排序:1409.8012320000003\n简单选择排序:466.66974500000015\n堆排序:1.2036720000000969\n冒泡排序:751.274449\n#****************************************************\n快速排序:1.0000003385357559e-06\n#快速排序有误：实际上并未执行\n#RecursionError: maximum recursion depth exceeded in comparison\n#****************************************************\n归并排序:0.8262230000000272\n基数排序:1.1162899999999354\n```\n\n从运行结果上来看，**`堆排序、归并排序、基数排序`** 真的快。\n\n对于快速排序迭代深度超过的问题，可以将考虑将快排通过非递归的方式进行实现。\n\n\n","categories":["language","排序算法"]},{"title":"插入排序","url":"/2021/03/13/language/排序算法/插入排序/","content":"\n## 速记方法\n插冒选希, 快堆并基\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n从平均情况看：堆排序、归并排序、快速排序胜过希尔排序。\n\n从最好情况看：冒泡排序和直接插入排序更胜一筹。\n\n从最差情况看：堆排序和归并排序强过快速排序。\n\n虽然直接插入排序和冒泡排序速度比较慢，但是当初始序列整体或局部有序是，这两种算法的效率比较高。\n\n当初始序列整体或局部有序时，快速排序算法效率会下降。\n\n当排序序列较小且不要求稳定性是，直接排序效率较好；要求稳定性时，冒泡排序法效率较好。\n\n## 插入排序\n![](insertion_sort_01.gif)\n![](insertion_sort_02.png)\n\n方法：对于给定的一组记录，初始时假定第一个记录自成一个有序的序列，其余的记录为无序序列；接着从第二个记录开始，按照记录的大小依次将当前处理的记录插入到其之前的有序序列中，直至最后一个记录插入到有序序列为止。\n\n1. 第一层循环：遍历待比较的所有数组元素\n2. 第二层循环：将本轮选择的元素(selected)与已经排好序的元素(ordered)相比较。\n如果：selected > ordered，那么将二者交换\n\n01.insertion_sort.c：\n\n```c\n\t#include <stdio.h>\n\t\n\t/**\n\t * 宏\t功能\n\t * ’#‘\t字符串化\n\t * ‘##’\t字符连接的功能\n\t * \"__VA_ARGS__’\t这个可变参数的宏是新的C99规范中新增的, 和变参函数中的...一致\n\t * ‘##__VA_ARGS__’\t宏前面加上##的作用在于，当可变参数的个数为0时，这里的##起到把前面多余的\",\"去掉的作用,否则会编译出错\n\t */\n\t#define PRINT(fmt, ...) \\\n\t    printf(fmt, ##__VA_ARGS__)\n\t\n\t#define PRINT1(...) \\\n\t    printf(__VA_ARGS__)\n\t\n\t// 在C语言中是不存在引用的，也就是说C语言中&表示的不是引用，而是取地址符\n\t// 用指针来取代引用，在函数中传进来地址, 或者可以将代码扩展名保存成.cpp文件\n\tvoid insertion_sort(int array[], int length)\n\t{\n\t    int temp, i, j;\n\t    for (i = 0; i < length; i++)\n\t    {\n\t        temp = array[i];                // 当前要插入的元素, 前面[i - 1]个元素已排序好, 拿出来第[i]位出元素, 前面元素可以后移一位覆盖此第[i]处元素\n\t        for (j = i - 1; j >= 0; j--)    // 与其它已排序过的元素从大到小比较, 原因是数组元素后移方便\n\t        {\n\t            if (temp < array[j])        // 如果当前要插入的元素小于已排序好的元素.\n\t            {\n\t                array[j + 1] = array[j];// 将已排序好的元素后移.\n\t            }\n\t            else\n\t            {\n\t                break;                  // 如果当前要插入的元素大于已排序好的元素array[j]则跳出.\n\t            }\n\t        }\n\t        array[j + 1] = temp;            // 将要插入的元素放到当前比较到的位置处array[j + 1].\n\t    }\n\t}\n\t\n\tint main()\n\t{\n\t    int array[] = {29, 18, 87, 56, 3, 27, 9, 63, 39};\n\t\n\t    int length = sizeof(array) / sizeof(array[0]);\n\t    insertion_sort(array, length);\n\t    int i = 0;\n\t    PRINT(\"%s-(%d), %s \\n\",__FILE__, __LINE__, __FUNCTION__);\n\t    for (i = 0; i < length; i++)\n\t    {\n\t        PRINT(\"%d \", array[i]);\n\t    }\n\t    PRINT(\"\\n\");\n\t\n\t    return 0;\n\t}\n```\n\nMakefile：\n\n```\n\tinsertion_sort: 01.insertion_sort.c\n\t\tgcc -g 01.insertion_sort.c -o 01.insertion_sort.o\n\t\n\tclean:\n\t\trm -rf *.o\n```\n编译：\n\n```shell\n\t// 编译\n\tmake insertion_sort\n\t\n\t// 执行\n\t./01.insertion_sort.o\n\t\n\t// 清理\n\tmake clean\n```\n\n\n\n\n","categories":["language","排序算法"]},{"title":"选择排序","url":"/2021/03/13/language/排序算法/选择排序/","content":"\n## 速记方法\n插冒选希, 快堆并基\n\n| 排序方法 | 最好时间 | 平均时间 | 最坏时间 | 辅助存储 | 稳定性 | 备注 |\n| :-----: | :----- | :------: | :-----:| :-----: | :------: | :------: |\n| 插入排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 冒泡排序 | O(n) | O(n^2) | O(n^2) | O(1) | 稳定 |\n| 选择排序 | O(n^2) | O(n^2) | O(n^2) | O(1) | 不稳定 |\n| 希尔排序 | O(n^1.3) | O(nlogn) | O(n^2) | O(1) | 不稳定 |\n| 快速排序 | O(nlogn) | O(nlogn) | O(n^2) | O(logn) | 不稳定 |\n| 堆排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(1) | 不稳定 |\n| 归并排序 | O(nlogn) | O(nlogn) | O(nlogn) | O(n) | 稳定 |\n| 基数排序 | O(kn) | O(kn) | O(kn) | O(n) | 稳定 |\n\n从平均情况看：堆排序、归并排序、快速排序胜过希尔排序。\n\n从最好情况看：冒泡排序和直接插入排序更胜一筹。\n\n从最差情况看：堆排序和归并排序强过快速排序。\n\n虽然直接插入排序和冒泡排序速度比较慢，但是当初始序列整体或局部有序是，这两种算法的效率比较高。\n\n当初始序列整体或局部有序时，快速排序算法效率会下降。\n\n当排序序列较小且不要求稳定性是，直接排序效率较好；要求稳定性时，冒泡排序法效率较好。\n\n## 选择排序\n![](select_sort.gif)\n\n简单选择排序的基本思想：比较+交换。\n\n1. 从待排序序列中，找到关键字最小的元素；\n2. 如果最小元素不是待排序序列的第一个元素，将其和第一个元素互换；\n3. 从余下的 N - 1 个元素中，找出关键字最小的元素，重复(1)、(2)步，直到排序结束。\n  因此我们可以发现，简单选择排序也是通过两层循环实现。  \n  第一层循环：依次遍历序列当中的每一个元素  \n  第二层循环：将遍历得到的当前元素依次与余下的元素进行比较，符合最小元素的条件，则交换。\n![](select_sort_02.png)\n\n对于给定的一组记录，经过第一轮比较后得到最小的记录，然后将记录与第一个记录的位置进行交换；接着对不包括第一个记录以外的其他记录进行第二轮排序，得到最小的记录并与第二个记录进行位置交换；重复该过程，直到进行比较的记录只有一个为止。\n\n``` c\n\t#include <stdio.h>\n\t\n\t/**\n\t * 宏\t功能\n\t * ’#‘\t字符串化\n\t * ‘##’\t字符连接的功能\n\t * \"__VA_ARGS__’\t这个可变参数的宏是新的C99规范中新增的, 和变参函数中的...一致\n\t * ‘##__VA_ARGS__’\t宏前面加上##的作用在于，当可变参数的个数为0时，这里的##起到把前面多余的\",\"去掉的作用,否则会编译出错\n\t */\n\t#define PRINT(fmt, ...) \\\n\t    printf(fmt, ##__VA_ARGS__)\n\t\n\t#define PRINT1(...) \\\n\t    printf(__VA_ARGS__)\n\t\n\tvoid swap_int(int *num1, int *num2)\n\t{\n\t    int temp;\n\t    temp = *num1;\n\t    *num1 = *num2;\n\t    *num2 = temp;\n\t}\n\t\n\tvoid select_sort(int array[], int length)\n\t{\n\t    int i, j, temp = 0;\n\t    for (i = 0; i < length; ++i)\t\t// 从第一个开始, 比较然后确定最小的进行交换.\n\t    {\n\t        for (j = i + 1; j < length; ++j)\n\t        {\n\t            if (array[i] > array[j])\n\t            {\n\t                swap_int(&array[i], &array[j]);\n\t            }\n\t        }\n\t    }\n\t}\n\t\n\tint main()\n\t{\n\t    int array[] = {29, 18, 87, 56, 3, 27, 9, 63, 39};\n\t\n\t    int length = sizeof(array) / sizeof(array[0]);\n\t    select_sort(array, length);\n\t    int i = 0;\n\t    for (i = 0; i < length; i++)\n\t    {\n\t        PRINT(\"%d \", array[i]);\n\t    }\n\t    PRINT(\"\\n\");\n\t    return 0;\n\t}\n```\nMakefile：\n\n```\n\tinsertion_sort: 01.insertion_sort.c\n\t\tgcc -g 01.insertion_sort.c -o 01.insertion_sort.o\n\t\n\tbubble_sort: 02.bubble_sort.c\n\t\tgcc -g 02.bubble_sort.c -o 02.bubble_sort.o\n\t\n\tselect_sort: 03.select_sort.c\n\t\tgcc -g 03.select_sort.c -o 03.select_sort.o\n\t\n\tclean:\n\t\trm -rf *.o\n```\n编译：\n\n```shell\n\t// 编译\n\tmake select_sort\n\t\n\t// 执行\n\t./03.select_sort.o\n\t\n\t// 清理\n\tmake clean\n```\n\n\n\n\n","categories":["language","排序算法"]},{"title":"entertainment","url":"/2021/03/13/entertainment/entertainment/","content":"\n## 电视剧\n\n《有翡》第二集\n\n<!-- more -->\n\n## 美剧\n\n《闪电侠》第二季\n《艾米莉在巴黎》 第一季\n《绯闻女孩》第一季  第09集 23:26\n\n\n已看完:\n《曼达洛人》第二季    √\n\n\n## 动漫\n\n《海贼王》第901集\n\n\n## 喜马拉雅\n\n《斗罗大陆4》第992集\n\n\n\n","categories":["entertainment"]},{"title":"hexo将jupyter *.ipynb文件加入博客方法","url":"/2021/03/13/blogs/blog搭建/hexo将jupyter_加入博客方法/","content":"\nReference: \n1. https://www.freesion.com/article/68531420207/\n2. http://wxnacy.com/2020/06/13/hexo-use-jupyter-notebook/\n3. https://libinruan.github.io/2018/10/27/Blog-with-Hexo/\n<!-- more -->\n","categories":["blogs","blog搭建"]},{"title":"01 const new&delete inline overload externC","url":"/2021/03/13/language/cpp/01_const_newDelete/","content":"\n## c++ base\n头文件, const_newDelete_inline_overload_externC.h\n\n``` cpp\n\t#ifndef CONST_NEWDELETE_INLINE_H\n\t#define CONST_NEWDELETE_INLINE_H\n\t#include <iostream>\n\t\n\t/**\n\t* extern \"C\" 既可以修饰一句 C++ 代码，也可以修饰一段 C++ 代码.\n\t* 在程序中加上extern \"C\"后，相当于告诉编译器这部分代码是C语言写的，\n\t* 因此要按照C语言进行编译，而不是C++\n\t**/\n\t#ifdef __cplusplus\n\textern \"C\" void display();\n\t#else\n\tvoid display();\n\t#endif\n\t\n\t#ifdef __cplusplus\n\textern \"C\"\n\t{\n\t#endif\n\t    void func11();\n\t    void func22();\n\t#ifdef __cplusplus\n\t}\n\t#endif\n```\n\n源文件 const_newDelete_inline_overload_externC.cpp\n\n``` cpp\n\t#include <iostream>\n\t#include <const_newDelete_inline_overload_externC.h>\n\tusing namespace std;\n\t\n\tinline int SQ(int y) { return y * y; }\n\t\n\t/*\n\t* 默认参数\n\t* 默认参数只能放在形参列表的最后，而且一旦为某个形参指定了默认值，那么它后面的所有形参都必须有默认值.\n\t* 在给定的作用域中只能指定一次默认参数\n\t*/\n\tfloat d = 10.8;\n\tvoid func1(int n, float b = d + 1.1, char c = '@')\n\t{\n\t    cout << n << \" \" << b << \" \" << c << endl;\n\t}\n\t\n\t/*函数的重载的规则：\n\t* 函数名称必须相同。\n\t* 参数列表必须不同（1.个数不同、2.类型不同、3.参数排列顺序不同等）。\n\t* 函数的返回类型可以相同也可以不相同。\n\t* 仅仅返回类型不同不足以成为函数的重载。\n\t* C++代码在编译时会根据参数列表对函数进行重命名，例如void Swap(int a, int b)会被重命名为_Swap_int_int，void Swap(float x, float y)会被重命名为_Swap_float_float.\n\t* 从这个角度讲，函数重载仅仅是语法层面的，本质上它们还是不同的函数，占用不同的内存，入口地址也不一样.\n\t*/\n\t\n\tint main()\n\t{\n\t    /* const\n\t     * 全局 const 变量的可见范围是当前文件\n\t     * C++ 规定，全局 const 变量的作用域仍然是当前文件，但是它在其他文件中是不可见的，这和添加了static关键字的效果类似\n\t     * 对比 const 和 #define 的优缺点时提到，#define 定义的常量仅仅是字符串的替换，不会进行类型检查，而 const 定义的常量是有类型的，编译器会进行类型检查，相对来说比 #define 更安全，所以鼓励大家使用 const 代替 #define\n\t     * \n\t     */\n\t    const int n = 10; // C++ 对 const 的处理更像是编译时期的#define，是一个值替换的过程,\n\t    int *p = (int *)&n;\n\t    *p = 99;\n\t    printf(\"%d\\n\", n); // printf(\"%d\\n\", n);语句在编译时就将 n 的值替换成了 10，效果和printf(\"%d\\n\", 10)一样; 输出10\n\t\n\t    /* \n\t     * new delete\n\t     */\n\t    int *int_p1 = (int *)malloc(sizeof(int) * 10); //c语言中, 在堆区分配10个int型的内存空间\n\t    free(int_p1);                                  //释放内存\n\t    int *int_p2 = new int[10];                     //c++语言中, 分配10个int型的内存空间\n\t    delete[] int_p2;                               //释放内存\n\t    int *int_p3 = new int;\n\t    delete int_p3;\n\t\n\t    /** inline 内联函数在编译时会将函数调用处用函数体替换，编译完成后函数就不存在了，所以在链接时不会引发重复定义错误。\n\t     *  这一点和宏很像，宏在预处理时被展开，编译时就不存在了。从这个角度讲，内联函数更像是编译期间的宏.\n\t     *  和宏一样，内联函数可以定义在头文件中（不用加 static 关键字），并且头文件被多次#include后也不会引发重复定义错误.\n\t     *  宏替换为内联函数\n\t     *  #define SQ(y) ((y)*(y)) \n\t     **/\n\t    int num = 9;\n\t    int sq = 200 / SQ(num + 1);\n\t    cout << sq << endl;\n\t\n\t    /*\n\t     * 默认参数\n\t     **/\n\t    func1(1);\n\t    func1(1, 1.2);\n\t    func1(1, 1.3, '!');\n\t\n\t    return 0;\n\t}\n\t\n\t#endif\n```\n\n编译：\n\n``` shell\n\tg++ const_newDelete_inline_overload_externC.cpp -I ./\n```\n\n执行\n\n``` shell\n\t$ ./a.out\n\t10\n\t2\n\t1 11.9 @\n\t1 1.2 @\n\t1 1.3 !\n```\n\n## string类\n在C语言中，有两种方式表示字符串：\n * 一种是用字符数组来容纳字符串，例如char str[10] = \"abc\"，这样的字符串是可读写的；\n * 一种是使用字符串常量，例如char *str = \"abc\"，这样的字符串只能读，不能写。\n两种形式总是以\\0作为结束标志.\n\n### copy-on-write\n只有当字符串被修改的时候才创建各自的拷贝，这种实现方式称为写时复制（copy-on-write）策略.  \n当字符串只是作为值参数（value parameter）或在其他只读情形下使用，这种方法能够节省时间和空间.  \n\n``` cpp\n\tstring s1(\"12345\");\n\tstring s2 = s1;\n\tcout << (s1 == s2) << endl;\n\ts1[0] = '6';\n\tcout << \"s1 = \" << s1 << endl;  //62345\n\tcout << \"s2 = \" << s2 << endl;  //12345\n\tcout << (s1 == s2) << endl;\n```\n\n在 GCC 下的运行结果:\n\n```\n\t1\n\ts1 = 62345\n\ts2 = 12345\n\t0\n```\n\n### length\nstring 是 C++ 中常用的一个类.  \n与C风格的字符串不同，string 的结尾没有结束标志'\\0'.  \n\n``` cpp\n\tstring s = \"http://c.biancheng.net\";\n\tint len = s.length();\n\tcout<< len <<endl; // 输出 8\n```\n\n### c_str()\n在实际编程中，有时候必须要使用C风格的字符串（例如打开文件时的路径），为此，string 类为我们提供了一个转换函数 c_str()\nc_str()能够将 string 字符串转换为C风格的字符串，并返回该字符串的 const 指针（const char*）\n\n``` cpp\n\tstring path = \"D:\\\\demo.txt\";\n\tFILE *fp = fopen(path.c_str(), \"rt\");\n```\n\n### cin >> 输入字符串 string\n可以像对待普通变量那样对待 string 变量，也就是用>>进行输入，用<<进行输出.  \n``` cpp\n\tstring s;\n\tcin >> s;  //输入字符串\n\tcout << s << endl;  //输出字符串\n```\n运行结果：\n```\n\thttp://c.biancheng.net  http://vip.biancheng.net↙\n\thttp://c.biancheng.net\n```\n虽然我们输入了两个由空格隔开的网址，但是只输出了一个，这是因为输入运算符>>默认会忽略空格，遇到空格就认为输入结束，所以最后输入的http://vip.biancheng.net没有被存储到变量 s。\n\n### 访问字符串中的字符\n```cpp\n\tstring s = \"1234567890\";\n\tfor(int i=0,len=s.length(); i<len; i++){\n\t    cout<<s[i]<<\" \";\n\t}\n\tcout<<endl;\n\ts[5] = '5';\n\tcout<<s<<endl;\n```\n运行结果：\n```\n\t1 2 3 4 5 6 7 8 9 0\n\t1234557890\n```\n### 字符串的拼接\n可以使用+或+=运算符来直接拼接字符串, 再也不需要使用C语言中的 strcat()、strcpy()、malloc() 等函数来拼接字符串了，再也不用担心空间不够会溢出了.  \n用+来拼接字符串时，运算符的两边可以都是 string 字符串，也可以是一个 string 字符串和一个C风格的字符串，还可以是一个 string 字符串和一个字符数组，或者是一个 string 字符串和一个单独的字符.  \n``` cpp\n\tstring s1 = \"first \";\n\tstring s2 = \"second \";\n\tchar *s3 = \"third \";\n\tchar s4[] = \"fourth \";\n\tchar ch = '@';\n\tstring s5 = s1 + s2;\n\tstring s6 = s1 + s3;\n\tstring s7 = s1 + s4;\n\tstring s8 = s1 + ch;\n\t\n\tcout<<s5<<endl<<s6<<endl<<s7<<endl<<s8<<endl;\n```\n运行结果：\n```\n\tfirst second\n\tfirst third\n\tfirst fourth\n\tfirst @\n```\n### string 字符串的增删改查\n\n#### insert()\ninsert() 函数可以在 string 字符串中指定的位置插入另一个字符串，它的一种原型为：\n``` cpp\n\tstring& insert (size_t pos, const string& str);\n```\npos 表示要插入的位置，也就是下标；str 表示要插入的字符串，它可以是 string 字符串，也可以是C风格的字符串。  \n实例:\n``` cpp\n\tstring s1 = \"1234567890\";\n\tstring s2 = \"aaa\";\n\ts1.insert(5, s2);\n```\n运行结果：\n```\n\t12345aaa67890\n```\n#### erase()\nerase() 函数可以删除 string 中的一个子字符串。它的一种原型为：\n``` cpp\n\tstring& erase (size_t pos = 0, size_t len = npos);\n```\npos 表示要删除的子字符串的起始下标，len 表示要删除子字符串的长度。如果不指明 len 的话，那么直接删除从 pos 到字符串结束处的所有字符（此时 len = str.length - pos）。\n\n#### substr()\nsubstr() 函数用于从 string 字符串中提取子字符串，它的原型为：\n``` cpp\n\tstring substr (size_t pos = 0, size_t len = npos) const;\n```\npos 为要提取的子字符串的起始下标，len 为要提取的子字符串的长度。\n\n#### find() 函数\nfind() 函数用于在 string 字符串中查找子字符串出现的位置，它其中的两种原型为：\n``` cpp\n\tsize_t find (const string& str, size_t pos = 0) const;\n\tsize_t find (const char* s, size_t pos = 0) const;\n```\n第一个参数为待查找的子字符串，它可以是 string 字符串，也可以是C风格的字符串。第二个参数为开始查找的位置（下标）；如果不指明，则从第0个字符开始查找.  \n\n#### rfind() 函数\nrfind() 和 find() 很类似，同样是在字符串中查找子字符串，不同的是 find() 函数从第二个参数开始往后查找，而 rfind() 函数则最多查找到第二个参数处，如果到了第二个参数所指定的下标还没有找到子字符串，则返回一个无穷大值4294967295.  \n\n#### find_first_of() 函数\nfind_first_of() 函数用于查找子字符串和字符串共同具有的字符在字符串中首次出现的位置\n\n\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"03 class","url":"/2021/03/13/language/cpp/03_class/","content":"\n## 类class\n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n```\n\n### 普通类\n\n``` cpp\n\t/**\n\t * 类只是一个模板（Template），编译后不占用内存空间，不存在于编译后的可执行文件中；\n\t * 所以在定义类时不能对成员变量进行初始化，因为没有地方存储数据。\n\t * 只有在创建对象以后会在栈区或者堆区分配内存，这个时候就可以赋值了。\n\t *\n\t */\n\tclass Student\n\t{\n\t    //C++ 中的 public、private、protected 只能修饰类的成员，不能修饰类，C++中的类没有共有私有之分.\n\tprivate:\n\t    // name、age、score 在内存地址分布按照声明的顺序由低到高依次排列，和结构体非常类似，也会有内存对齐的问题.\n\t    char *name;\n\t    int age;\n\t    float score;\n\t\n\tpublic:\n\t    // 在类体内部对成员函数作声明，而在类体外部进行定义，这是一种良好的编程习惯，实际开发中大家也是这样做的.\n\t    void say();\n\t    void set_variable(char *name, int age, float score);\n\t}; // 类定义的最后有一个分号;，它是类定义的一部分，表示类定义结束了，不能省略。\n\t\n\tvoid Student::say()\n\t{\n\t    cout << this->name << \"的年龄是: \" << this->age << \", 成绩是: \" << this->score << endl;\n\t}\n\t\n\tvoid Student::set_variable(char *name, int age, float score)\n\t{\n\t    this->name = name;\n\t    this->age = age;\n\t    this->score = score;\n\t}\n```\n\n### **封闭类**\n\n``` cpp\n\t/**\n\t *  一个类的成员变量如果是另一个类的对象，就称之为“成员对象”。包含成员对象的类叫封闭类（enclosed class）\n\t * 创建封闭类的对象时，它包含的成员对象也需要被创建，这就会引发成员对象构造函数的调用。\n\t * 如何让编译器知道，成员对象到底是用哪个构造函数初始化的呢？这就需要借助封闭类构造函数的初始化列表\n\t */\n\tclass Tyre // 轮胎类\n\t{\n\tpublic:\n\t    Tyre(int radius, int width);\n\t    void show() const;\n\t\n\tprivate:\n\t    int m_radius;\n\t    int m_width;\n\t};\n\tTyre::Tyre(int radius, int width) : m_radius(radius), m_width(width)\n\t{\n\t    cout << \"Tyre constructor\" << endl;\n\t}\n\tTyre::~Tyre()\n\t{\n\t    cout << \"Tyre destructor\" << endl;\n\t}\n\tvoid Tyre::show() const\n\t{\n\t    cout << \"轮胎半径:\" << this->m_radius << \", 轮胎宽度:\" << this->m_width << endl;\n\t}\n\t\n\tclass Engine // 引擎类\n\t{\n\tpublic:\n\t    // 设置了默认参数的构造函数可以看作无参构造函数\n\t    Engine(float displacement = 2.0);\n\t    void show() const;\n\t\n\tprivate:\n\t    float m_displacement;\n\t};\n\tEngine::Engine(float displacement) : m_displacement(displacement)\n\t{\n\t    cout << \"Engine constructor\" << endl;\n\t}\n\tEngine::~Engine()\n\t{\n\t    cout << \"Engine destructor\" << endl;\n\t}\n\tvoid Engine::show() const\n\t{\n\t    cout << \"排量: \" << this->m_displacement << \"L\" << endl;\n\t}\n\t\n\tclass Car\n\t{\n\tpublic:\n\t    Car(int price, int radius, int width);\n\t    void show() const;\n\t\n\tprivate:\n\t    int m_price;\n\t    Tyre m_tyre;\n\t    Engine m_engine;\n\t};\n\t// m_engine 默认用 Engine 类的无参构造函数初始化\n\tCar::Car(int price, int radius, int width) : m_price(price), m_tyre(radius, width)\n\t{\n\t    cout << \"Car constructor\" << endl;\n\t}\n\tCar::~Car()\n\t{\n\t    cout << \"Car destructor\" << endl;\n\t}\n\tvoid Car::show() const\n\t{\n\t    cout << \"价格: \" << this->m_price << endl;\n\t    this->m_tyre.show();\n\t    this->m_engine.show();\n\t}\n```\n\n### **对象数组**\n\n``` cpp\n\t/**\n\t * 对象数组\n\t */\n\tclass CTest\n\t{\n\tpublic:\n\t    CTest(); //不带花括号是声明, 带花括号则是声明+定义就不能在类外重新定义构造函数, 其它函数也一样.\n\t    CTest(int n);\n\t    CTest(int n, int m);\n\t};\n\t\n\tCTest::CTest()\n\t{\n\t    cout << \"Constructor 0 Called!\" << endl;\n\t}\n\t\n\tCTest::CTest(int n)\n\t{\n\t    cout << \"Constructor 1 Called!\" << endl;\n\t}\n\t\n\tCTest::CTest(int n, int m)\n\t{\n\t    cout << \"Constructor 2 Called!\" << endl;\n\t}\n```\n\n### **静态成员变量**\n使用静态成员变量来实现多个对象共享数据的目标。静态成员变量是一种特殊的成员变量，它被关键字static修饰.  \n共享数据的典型使用场景是计数.  \n * static 成员变量属于类，不属于某个具体的对象，即使创建多个对象，也只为static变量分配一份内存，所有对象使用的都是这份内存中的数据.  \n * static 成员变量必须在类声明的外部初始化.  \n * 静态成员变量在初始化时不能再加 static，但必须要有数据类型。被 private、protected、public 修饰的静态成员变量都可以用这种方式初始化.  \n * static 成员变量不占用对象的内存，而是在所有对象之外开辟内存，即使不创建对象也可以访问.  \n**注意：** static 成员变量的内存既不是在声明类时分配，也不是在创建对象时分配，而是在（类外）初始化时分配。反过来说，没有在类外初始化的 static 成员变量不能使用\n\n\n``` cpp\n\tclass Student1\n\t{\n\tpublic:\n\t    Student1(char *name, int age, float score);\n\t    void show();\n\t\n\tpublic:\n\t    static int m_total; //静态成员变量\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\tint Student1::m_total = 0;\n\tStudent1::Student1(char *name, int age, float score) : m_name(name), m_age(age), m_score(score)\n\t{\n\t    m_total++; //操作静态成员变量\n\t}\n\tvoid Student1::show()\n\t{\n\t    cout << m_name << \"的年龄是\" << m_age << \"，成绩是\" << m_score << \"（当前共有\" << m_total << \"名学生）\" << endl;\n\t}\n```\n\n### **静态成员函数**\n普通成员变量占用对象的内存，静态成员函数没有 this 指针，不知道指向哪个对象，无法访问对象的成员变量，也就是说静态成员函数不能访问普通成员变量，只能访问静态成员变量.  \n普通成员函数必须通过对象才能调用，而静态成员函数没有 this 指针，无法在函数体内部访问某个对象，所以不能调用普通成员函数，只能调用静态成员函数。  \n`静态成员函数与普通成员函数的根本区别在于：普通成员函数有 this 指针，可以访问类中的任意成员；而静态成员函数没有 this 指针，只能访问静态成员（包括静态成员变量和静态成员函数）.`  \n\n``` cpp\n\tclass Student2\n\t{\n\tpublic:\n\t    Student2(char *name, int age, float score);\n\t    void show();\n\t\n\tpublic: //声明静态成员函数\n\t    static int getTotal();\n\t    static float getPoints();\n\t\n\tprivate:\n\t    static int m_total;    //总人数\n\t    static float m_points; //总成绩\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\t\n\tint Student2::m_total = 0;\n\tfloat Student2::m_points = 0.0;\n\tStudent2::Student2(char *name, int age, float score) : m_name(name), m_age(age), m_score(score)\n\t{\n\t    m_total++;\n\t    m_points += score;\n\t}\n\tvoid Student2::show()\n\t{\n\t    cout << m_name << \"的年龄是\" << m_age << \"，成绩是\" << m_score << endl;\n\t}\n\t//定义静态成员函数\n\tint Student2::getTotal()\n\t{\n\t    return m_total;\n\t}\n\tfloat Student2::getPoints()\n\t{\n\t    return m_points;\n\t}\n```\n\n### *const 成员函数,变量 和 const 对象*\nconst 成员函数可以使用类中的所有成员变量，但是不能修改它们的值，这种措施主要还是为了保护数据而设置的。const 成员函数也称为常成员函数.  \n常成员函数需要在声明和定义的时候在函数头部的结尾加上 const 关键字.  \n`需要强调的是，必须在成员函数的声明和定义处同时加上 const 关键字`\n * 函数开头的 const 用来修饰函数的返回值，表示返回值是 const 类型，也就是不能被修改，例如const char * getname()。\n * 函数头部的结尾加上 const 表示常成员函数，这种函数只能读取成员变量的值，而不能修改成员变量的值，例如char * getname() const。\n\nconst 也可以用来修饰对象，称为常对象。一旦将对象定义为常对象之后，就只能调用类的 const 成员（包括 const 成员变量和 const 成员函数）.  \n\n\n``` cpp\n\tclass Student3{\n\tpublic:\n\t    Student3(char *name, int age, float score);\n\t    void show();\n\t    //声明常成员函数\n\t    char *getname() const;\n\t    int getage() const;\n\t    float getscore() const;\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\tStudent3::Student3(char *name, int age, float score): m_name(name), m_age(age), m_score(score){ }\n\tvoid Student3::show(){\n\t    cout<<m_name<<\"的年龄是\"<<m_age<<\"，成绩是\"<<m_score<<endl;\n\t}\n\t//定义常成员函数\n\tchar * Student3::getname() const{\n\t    return m_name;\n\t}\n\tint Student3::getage() const{\n\t    return m_age;\n\t}\n\tfloat Student3::getscore() const{\n\t    return m_score;\n\t}\n```\n### **友元函数**\n借助友元（friend），可以使得其他类中的成员函数以及全局范围内的函数访问当前类的 private 成员.  \n * 在当前类以外定义的、不属于当前类的函数也可以在类中声明，但要在前面加 friend 关键字，这样就构成了友元函数.  \n * 友元函数可以是不属于任何类的非成员函数，也可以是其他类的成员函数。\n * 友元函数不同于类的成员函数，在友元函数中不能直接访问类的成员，必须要借助对象\n\n#### **非成员友元函数**\n\n``` cpp\n\tclass Student{\n\tpublic:\n\t    Student(char *name, int age, float score);\n\tpublic:\n\t    friend void show(Student *pstu);  //将show()声明为友元函数\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\tStudent::Student(char *name, int age, float score): m_name(name), m_age(age), m_score(score){ }\n\t//非成员函数\n\tvoid show(Student *pstu){\n\t    cout<<pstu->m_name<<\"的年龄是 \"<<pstu->m_age<<\"，成绩是 \"<<pstu->m_score<<endl;\n\t}\n\tint main(){\n\t    Student stu(\"小明\", 15, 90.6);\n\t    show(&stu);  //调用友元函数\n\t    Student *pstu = new Student(\"李磊\", 16, 80.5);\n\t    show(pstu);  //调用友元函数\n\t    return 0;\n\t}\n```\n#### **将其他类的成员函数声明为友元函数**\n\n``` cpp\n\t// 提前声明Address类\n\t// 因为在 Address 类定义之前、在 Student 类中使用到了它，如果不提前声明，编译器会报错\n\tclass Address;\n\t//声明Student类\n\tclass Student{\n\tpublic:\n\t    Student(char *name, int age, float score);\n\tpublic:\n\t    void show(Address *addr);\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\t//声明Address类\n\tclass Address{\n\tprivate:\n\t    char *m_province;  //省份\n\t    char *m_city;  //城市\n\t    char *m_district;  //区（市区）\n\tpublic:\n\t    Address(char *province, char *city, char *district);\n\t    // 将Student类中的成员函数show()声明为友元函数\n\t\t// 使得show()利用Address对象来访问Address类的private成员变量\n\t    friend void Student::show(Address *addr);\n\t};\n\t//实现Student类\n\tStudent::Student(char *name, int age, float score): m_name(name), m_age(age), m_score(score){ }\n\tvoid Student::show(Address *addr){\n\t    cout<<m_name<<\"的年龄是 \"<<m_age<<\"，成绩是 \"<<m_score<<endl;\n\t    cout<<\"家庭住址：\"<<addr->m_province<<\"省\"<<addr->m_city<<\"市\"<<addr->m_district<<\"区\"<<endl;\n\t}\n\t//实现Address类\n\tAddress::Address(char *province, char *city, char *district){\n\t    m_province = province;\n\t    m_city = city;\n\t    m_district = district;\n\t}\n\tint main(){\n\t    Student stu(\"小明\", 16, 95.5f);\n\t    Address addr(\"陕西\", \"西安\", \"雁塔\");\n\t    stu.show(&addr);\n\t   \n\t    Student *pstu = new Student(\"李磊\", 16, 80.5);\n\t    Address *paddr = new Address(\"河北\", \"衡水\", \"桃城\");\n\t    pstu -> show(paddr);\n\t    return 0;\n\t}\n```\n### **友元类**\n不仅可以将一个函数声明为一个类的“朋友”，还可以将整个类声明为另一个类的“朋友”，这就是友元类。友元类中的所有成员函数都是另外一个类的友元函数.  \n * 友元的关系是单向的而不是双向的。如果声明了类 B 是类 A 的友元类，不等于类 A 是类 B 的友元类，类 A 中的成员函数不能访问类 B 中的 private 成员。\n * 友元的关系不能传递。如果类 B 是类 A 的友元类，类 C 是类 B 的友元类，不等于类 C 是类 A 的友元类。\n`除非有必要，一般不建议把整个类声明为友元类，而只将某些成员函数声明为友元函数，这样更安全一些`  \n\n\n``` cpp\n\tclass Address;  //提前声明Address类\n\t//声明Student类\n\tclass Student{\n\tpublic:\n\t    Student(char *name, int age, float score);\n\tpublic:\n\t    void show(Address *addr);\n\tprivate:\n\t    char *m_name;\n\t    int m_age;\n\t    float m_score;\n\t};\n\t//声明Address类\n\tclass Address{\n\tpublic:\n\t    Address(char *province, char *city, char *district);\n\tpublic:\n\t    //将Student类声明为Address类的友元类\n\t    friend class Student;\n\tprivate:\n\t    char *m_province;  //省份\n\t    char *m_city;  //城市\n\t    char *m_district;  //区（市区）\n\t};\n\t//实现Student类\n\tStudent::Student(char *name, int age, float score): m_name(name), m_age(age), m_score(score){ }\n\tvoid Student::show(Address *addr){\n\t    cout<<m_name<<\"的年龄是 \"<<m_age<<\"，成绩是 \"<<m_score<<endl;\n\t    cout<<\"家庭住址：\"<<addr->m_province<<\"省\"<<addr->m_city<<\"市\"<<addr->m_district<<\"区\"<<endl;\n\t}\n\t//实现Address类\n\tAddress::Address(char *province, char *city, char *district){\n\t    m_province = province;\n\t    m_city = city;\n\t    m_district = district;\n\t}\n\tint main(){\n\t    Student stu(\"小明\", 16, 95.5f);\n\t    Address addr(\"陕西\", \"西安\", \"雁塔\");\n\t    stu.show(&addr);\n\t   \n\t    Student *pstu = new Student(\"李磊\", 16, 80.5);\n\t    Address *paddr = new Address(\"河北\", \"衡水\", \"桃城\");\n\t    pstu -> show(paddr);\n\t    return 0;\n\t}\n```\n\n### **C++ class和struct区别**\nC++ 中保留了C语言的 struct 关键字，并且加以扩充。在C语言中，struct 只能包含成员变量，不能包含成员函数.  \n而在C++中，struct 类似于 class，既可以包含成员变量，又可以包含成员函数.  \nC++中的 struct 和 class 基本是通用的，唯有几个细节不同：\n * 使用 class 时，类中的成员默认都是 private 属性的；而使用 struct 时，结构体中的成员默认都是 public 属性的。\n * class 继承默认是 private 继承，而 struct 继承默认是 public 继承.\n * class 可以使用模板，而 struct 不能.\n在编写C++代码时，强烈建议使用 class 来定义类，而使用 struct 来定义结构体，这样做语义更加明确\n\n## **调用**\n\n``` cpp\n\tint main()\n\t{\n```\n### **普通类**\n``` cpp\n\t    /**\n\t     * 栈内存是程序自动管理的，不能使用 delete 删除在栈上创建的对象；\n\t     */\n\t    Student stu; // 对象stu在栈上分配内存，需要使用&获取它的地址\n\t    Student *pstu = &stu;\n\t    //在栈上创建对象, 16\n\t    //对象的大小只受成员变量的影响，和成员函数没有关系.\n\t    cout << sizeof(stu) << endl;\n\t\n\t    /**\n\t     * 通过 new 创建出来的对象就不一样了，它在堆上分配内存，没有名字，只能得到一个指向它的指针，\n\t     * 所以必须使用一个指针变量来接收这个指针，否则以后再也无法找到这个对象了，更没有办法使用它.\n\t     * 堆内存由程序员管理，对象使用完毕后可以通过 delete 删除.\n\t     */\n\t    Student *pstu1 = new Student; // 在堆上创建对象\n\t    pstu1->set_variable(\"小明\", 10, 100.0);\n\t    pstu1->say();\n\t    //在堆上创建对象, 16\n\t    cout << sizeof(*pstu1) << endl;\n\t    //类的大小, 16\n\t    cout << sizeof(Student) << endl;\n\t\n\t    delete pstu1;\n```\n#### **输出**\n```\n\t16\n\t小明的年龄是: 10, 成绩是: 100\n\t16\n\t16\n\tlength: 12\n```\n### **封闭类**\n``` cpp\n\t    // m_engine 默认用 Engine 类的无参构造函数初始化\n\t    Car car(200000, 19, 245);\n\t    car.show();\n```\n### **构造函数和析构函数执行顺序**\n封闭类对象生成时，先执行所有成员对象的构造函数，然后才执行封闭类自己的构造函数。成员对象构造函数的执行次序和成员对象在类定义中的次序一致，与它们在构造函数初始化列表中出现的次序无关.  \n当封闭类对象消亡时，先执行封闭类的析构函数，然后再执行成员对象的析构函数，成员对象析构函数的执行次序和构造函数的执行次序相反，即先构造的后析构.  \n\n#### **输出**\n```\n\tEngine constructor\n\tTyre constructor\n\tCar constructor\n\t价格: 200000\n\t轮胎半径:19, 轮胎宽度:245\n\t排量: 2L\n\tCar destructor\n\tTyre destructor\n\tEngine destructor\n```\n### **对象数组**\n``` cpp\n\t// 三个元素分别用构造函数 1、2、0 初始化\n\tCTest array1[3] = {1, CTest(1, 2)};\n\tcout << \"-----------------\" << endl;\n\t// 三个元素分别用构造函数 2、2、1 初始化\n\tCTest array2[3] = {CTest(1, 2), CTest(1, 2), 1};\n\tcout << \"-----------------\" << endl;\n\t// 两个元素指向的对象分别用构造函数 1、2 初始化\n\t// pArray[2] 没有初始化，其值是随机的，不知道指向哪里\n\tCTest *pArray[3] = {new CTest(4), new CTest(1, 2)};\n```\n#### **输出**\n```\n\tConstructor 1 Called!\n\tConstructor 2 Called!\n\tConstructor 0 Called!\n\t-----------------\n\tConstructor 2 Called!\n\tConstructor 2 Called!\n\tConstructor 1 Called!\n\t-----------------\n\tConstructor 1 Called!\n\tConstructor 2 Called!\n```\n### **静态成员变量**\n``` cpp\n\t    // 下面这三种方式是等效的\n\t    //通过类类访问 static 成员变量\n\t    Student1::m_total = 10;\n\t    //通过对象来访问 static 成员变量\n\t    Student1 stu1(\"小明\", 15, 92.5f);\n\t    stu1.m_total = 20;\n\t    //通过对象指针来访问 static 成员变量\n\t    Student1 *pstu1 = new Student1(\"李华\", 16, 96);\n\t    pstu1->m_total = 20;\n\t    Student1::m_total = 0;\n\t    (new Student1(\"小明\", 15, 90))->show();\n\t    (new Student1(\"李磊\", 16, 80))->show();\n\t    (new Student1(\"张华\", 16, 99))->show();\n```\n#### **输出**\n```\n\t小明的年龄是15，成绩是90（当前共有1名学生）\n\t李磊的年龄是16，成绩是80（当前共有2名学生）\n\t张华的年龄是16，成绩是99（当前共有3名学生）\n```\n### **静态成员函数**\n``` cpp\n\t    (new Student2(\"小明\", 15, 90.6))->show();\n\t    (new Student2(\"李磊\", 16, 80.5))->show();\n\t    (new Student2(\"张华\", 16, 99.0))->show();\n\t    (new Student2(\"王康\", 14, 60.8))->show();\n\t    int total = Student2::getTotal();\n\t    float points = Student2::getPoints();\n\t    cout << \"当前共有\" << total << \"名学生，总成绩是\" << points << \"，平均分是\" << points / total << endl;\n\t    return 0;\n```\n#### **输出**\n```\n\t小明的年龄是15，成绩是90.6\n\t李磊的年龄是16，成绩是80.5\n\t张华的年龄是16，成绩是99\n\t王康的年龄是14，成绩是60.8\n\t前共有4名学生，总成绩是330.9，平均分是82.725\n```\n### **const 成员变量,函数 和 const 对象**\n\n``` cpp\n\t    const Student *pstu = new Student(\"李磊\", 16, 80.5);\n\t    //pstu -> show();  //error\n\t    cout<<pstu->getname()<<\"的年龄是\"<<pstu->getage()<<\"，成绩是\"<<pstu->getscore()<<endl;\n\n\t    return 0;\n\t}\n```\n\n\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"05 Reference & pointer","url":"/2021/03/13/language/cpp/05_reference_pointer/","content":"\n## Reference\n对于像 char、bool、int、float 等基本类型的数据，它们占用的内存往往只有几个字节，对它们进行内存拷贝非常快速.  \n而数组、结构体、对象是一系列数据的集合，数据的数量没有限制，可能很少，也可能成千上万，对它们进行频繁的内存拷贝可能会消耗很多时间，拖慢程序的执行效率.  \nC/C++ 禁止在函数调用时直接传递数组的内容，而是强制传递数组指针.  \n而对于结构体和对象没有这种限制，调用函数时既可以传递指针，也可以直接传递内容；为了提高效率，建议传递指针.  \n但是在 C++ 中，我们有了一种比指针更加便捷的传递聚合类型数据的方式，那就是引用（Reference）.  \n引用类似于 Windows 中的快捷方式，一个可执行程序可以有多个快捷方式，通过这些快捷方式和可执行程序本身都能够运行程序；引用还类似于人的绰号（笔名），使用绰号（笔名）和本名都能表示一个人。.  \n引用的定义方式类似于指针，只是用&取代了*，语法格式为：\n``` cpp\n\ttype &name = data;\n```\n引用必须在定义的同时初始化，并且以后也要从一而终，不能再引用其它数据，这有点类似于常量（const 变量）.`  \n``` cpp\n\tint main()\n\t{\n\t    int a = 99;\n\t    int &b = a;  // 变量赋值给引用变量\n\t    int &c = b;  // 引用便量赋值给引用变量, a, b, c 指向同一块内存地址.\n\t    b = 66;      // 引用也可以修改原始变量中所存储的数据\n\t    cout << a << \", \" << b << \", \" << c << endl;\n\t    cout << &a << \", \" << &b << \", \" << &c << endl; \n\t    return 0;\n\t}\n```\n输出:  \n```\n\t66, 66, 66\n\t0x7ffe710a246c, 0x7ffe710a246c, 0x7ffe710a246c\n```\n从输出结果可以看出，a 和 r 的地址一样，都是0x7ffe7cc6fe64；或者说地址为0x7ffe7cc6fe64的内存有两个名字，a 和 r，想要访问该内存上的数据时，使用哪个名字都行。  \n\n如果读者不希望通过引用来修改原始的数据，那么可以在定义时添加 const 限制，形式为：  \n``` cpp\n\tconst type &name = value;\n```\n也可以是：  \n``` cpp\n\ttype const &name = value;\n```\n这种引用方式为常引用\n\n### C++引用作为函数参数\n在定义或声明函数时，我们可以将函数的形参指定为引用的形式，这样在调用函数时就会将实参和形参绑定在一起，让它们都指代同一份数据。如此一来，如果在函数体中修改了形参的数据，那么实参的数据也会被修改，从而拥有“在函数内部影响函数外部数据”的效果。  \n\n按引用传参在使用形式上比指针更加直观。在以后的 C++ 编程中，鼓励读者大量使用引用，它一般可以代替指针（当然指针在C++中也不可或缺）.  \n\n**指针传递与引用传递区别如下：**\n``` cpp\n\tvoid swap1(int *m, int *n)\n\t{\n\t    int temp = *m;\n\t    *m = *n;\n\t    *n = temp;\n\t}\n\t\n\tvoid swap2(int &m, int &n)\n\t{\n\t    int temp = m;\n\t    m = n;\n\t    n = temp;\n\t    cout << \"Reference address: \" << &m << \" , \" << &n << endl;\n\t}\n\n\tint num1 = 1, num2 = 2;\n\tswap1(&num1, &num2);\n\tcout << \"num1: \" << num1 << \", num2: \" << num2 << endl;\n\tnum1 = 1;\n\tnum2 = 2;\n\tcout << \"Address: \" << &num1 << \" , \" << &num2 << endl;\n\tswap2(num1, num2);\n\tcout << \"num1: \" << num1 << \", num2: \" << num2 << endl;\n```\n输出:  \n```\n\tnum1: 2, num2: 1\n\tAddress: 0x7fff31e04090 , 0x7fff31e0408c\n\tReference address: 0x7fff31e04090 , 0x7fff31e0408c\n\tnum1: 2, num2: 1\n```\n可以看到引用参数在内存中地址和实参变量地址相同.  \n\n\n### C++引用作为函数返回值\n``` cpp\n\tint &plus10(int &num)\n\t{\n\t    num += 10;\n\t    return num;\n\t}\n\n\tint num = 10;\n\tplus10(num);\n\tcout << \"num: \" << num << endl;\n```\n输出:\n```\n\tnum: 20\n```\n在将引用作为函数返回值时应该注意一个小问题，就是不能返回局部数据（例如局部变量、局部对象、局部数组等）的引用，因为当函数调用完成后局部数据就会被销毁，有可能在下次使用时数据就不存在了，C++ 编译器检测到该行为时也会给出警告.  \n但可以返回用非局部变量赋值的引用变量如下面的&n.  \n\n``` cpp\n\tint &plus10(int &num)\n\t{\n\t    int &n = num;\n\t    n += 10;\n\t    return n;\n\t    // int m = r + 10;  \n\t    // return m;        //错误用法, 禁止返回局部数据的引用\n\t}\n```\n### 引用实质\n其实引用只是对指针进行了简单的封装，它的底层依然是通过指针实现的，引用占用的内存和指针占用的内存长度一样，在 32 位环境下是 4 个字节，在 64 位环境下是 8 个字节，之所以不能获取引用的地址，是因为编译器进行了内部转换.  \n\n``` cpp\n\tint a = 99;\n\tint &r = a;\n\tr = 18;\n\tcout<<&r<<endl;\n```\n编译时会被转换成如下的形式：\n\n``` cpp\n\tint a = 99;\n\tint *r = &a;\n\t*r = 18;\n\tcout<<r<<endl;\n```\n\n## \"引用\" 与 \"指针\" 区别\n1. 引用必须在定义时初始化，并且以后也要从一而终，不能再指向其他数据；而指针没有这个限制，指针在定义时不必赋值，以后也能指向任意数据。\n\n2. 可以有 const 指针，但是没有 const 引用。也就是说，引用变量不能定义为下面的形式：\n\n\n``` cpp\n\tint a = 20;\n\tint & const r = a;\n```\n因为 r 本来就不能改变指向，加上 const 是多此一举。\n\n3. 指针可以有多级，但是引用只能有一级，例如，int **p是合法的，而int &&r是不合法的。如果希望定义一个引用变量来指代另外一个引用变量，那么也只需要加一个&，如下所示：\n\n\n``` cpp\n\tint a = 10;\n\tint &r = a;\n\tint &rr = r;\n```\n4. 指针和引用的自增（++）自减（--）运算意义不一样。对指针使用 ++ 表示指向下一份数据，对引用使用 ++ 表示它所指代的数据本身加 1；自减（--）也是类似的道理.  \n\n\n``` cpp\n\tint a = 10;\n\tint &r = a;\n\tr++;\n\tcout << r << endl;\n\t\n\tint arr[2] = {27, 84};\n\tint *p = arr;\n\tp++;\n\tcout << *p << endl;\n```\n输出:\n\n```\n\t11\n\t84\n```\n## 指针\n指针就是数据或代码在内存中的地址，指针变量指向的就是内存中的数据或代码。这里有一个关键词需要强调，就是内存，指针只能指向内存，不能指向寄存器或者硬盘，因为寄存器和硬盘没法寻址。  \n\n### 指针无法指向的变量等\n例如表达式的结果、函数的返回值等，它们可能会放在内存中，也可能会放在寄存器中。一旦它们被放到了寄存器中，就没法用&获取它们的地址了，也就没法用指针指向它们了。  \n\n`寄存器离 CPU 近，并且速度比内存快，将临时数据放到寄存器是为了加快程序运行.`  \n\nint、double、bool、char 等基本类型的数据往往不超过 8 个字节，用一两个寄存器就能存储，所以这些类型的临时数据通常会放到寄存器中；而对象、结构体变量是自定义类型的数据，大小不可预测，所以这些类型的临时数据通常会放到内存中.  \n\n1. 下面的代码演示了表达式所产生的临时结果, 这些表达式的结果都会被放到寄存器中，尝试用&获取它们的地址都是错误的。\n\n\n``` cpp\n\tint n = 100, m = 200;\n\tint *p1 = &(m + n);    //m + n 的结果为 300\n\tint *p2 = &(n + 100);  //n + 100 的结果为 200\n\tbool *p4 = &(m < n);   //m < n 的结果为 false\n```\n2. 下面的代码演示了函数返回值所产生的临时结果, func() 的返回值 100 也会被放到寄存器中，也没法用&获取它的地址.  \n\n\n``` cpp\n\tint func(){\n\t    int n = 100;\n\t    return n;\n\t}\n\tint *p = &(func());\n```\n3. 常量表达式由于不包含变量，没有不稳定因素，所以在编译阶段就能求值。编译器不会分配单独的内存来存储常量表达式的值，而是将常量表达式的值和代码合并到一起，放到虚拟地址空间中的代码区.  \n诸如 100、200+34、34.5*23、3+7/3 等不包含变量的表达式称为常量表达式（Constant expression).  \n\n### 引用错误使用临时变量\n引用作为函数参数时，有时候很容易给它传递临时数据, 这是不允许的, 因为引用和指针在本质上是一样的，引用仅仅是对指针进行了简单的封装.  \n\n\n``` cpp\n\tbool isOdd(int &n){\n\t    if(n%2 == 0){\n\t        return false;\n\t    }else{\n\t        return true;\n\t    }\n\t}\n\t\n\tint a = 100;\n\tisOdd(a);      // 正确\n\tisOdd(a + 9);  // 错误\n\tisOdd(27);     // 错误\n```\n### 引用正确使用临时变量\nconst 引用和普通引用不一样，我们只能通过 const 引用读取数据的值，而不能修改它的值，所以不用考虑同步更新的问题，也不会产生两份不同的数据，为 const 引用创建临时变量反而会使得引用更加灵活和通用.  \n\n``` cpp\n\tbool isOdd(const int &n){  //改为常引用\n\t    if(n/2 == 0){\n\t        return false;\n\t    }else{\n\t        return true;\n\t    }\n\t}\n```\n由于在函数体中不会修改 n 的值，所以可以用 const 限制 n，这样一来，下面的函数调用就都是正确的了：\n\n``` cpp\n\tint a = 100;\n\tisOdd(a);        //正确\n\tisOdd(a + 9);    //正确\n\tisOdd(27);       //正确\n\tisOdd(23 + 55);  //正确\n```\n对于第 2 行代码，编译器不会创建临时变量，会直接绑定到变量 a；对于第 3~5 行代码，编译器会创建临时变量来存储临时数据。也就是说，编译器只有在必要时才会创建临时变量\n\n## 引用类型的函数形参请尽可能的使用 const\n当引用的类型和数据的类型不一致时，如果它们的类型是相近的，并且遵守「数据类型的自动转换」规则，那么编译器就会创建一个临时变量，并将数据赋值给这个临时变量（这时候会发生自动类型转换），然后再将引用绑定到这个临时的变量，这与「将 const 引用绑定到临时数据时」采用的方案是一样的.  \n\n引用作为函数参数时，如果在函数体内部不会修改引用所绑定的数据，那么请尽量为该引用添加 const 限制.  \n\n``` cpp\n\t#include <cstdio>\n\tusing namespace std;\n\tdouble volume(const double &len, const double &width, const double &hei)\n\t{\n\t    return len * width * 2 + len * hei * 2 + width * hei * 2;\n\t}\n\tint main()\n\t{\n\t    int a = 12, b = 3, c = 20;\n\t    double v1 = volume(a, b, c);\n\t    double v2 = volume(10, 20, 30);\n\t    double v3 = volume(89.4, 32.7, 19);\n\t    double v4 = volume(a + 12.5, b + 23.4, 16.78);\n\t    double v5 = volume(a + b, a + c, b + c);\n\t    printf(\"%lf, %lf, %lf, %lf, %lf\\n\", v1, v2, v3, v4, v5);\n\t    return 0;\n\t}\n```\n运行结果：\n\n```\n\t672.000000, 2200.000000, 10486.560000, 3001.804000, 3122.000000\n```\n概括起来说，将引用类型的形参添加 const 限制的理由有三个：\n * 使用 const 可以避免无意中修改数据的编程错误；\n * 使用 const 能让函数接收 const 和非 const 类型的实参，否则将只能接收非 const 类型的实参；\n * 使用 const 引用能够让函数正确生成并使用临时变量。\n\n\n","categories":["language","cpp"]},{"title":"06 继承与派生","url":"/2021/03/13/language/cpp/06_继承与派生/","content":"\n## **继承与派生**\n**`继承（Inheritance）`** 可以理解为一个类从另一个类获取成员变量和成员函数的过程。例如类 B 继承于类 A，那么 B 就拥有 A 的成员变量和成员函数。\n\n在C++中，**`派生（Derive）`** 和 **`继承`** 是一个概念，只是站的角度不同。继承是儿子接收父亲的产业，派生是父亲把产业传承给儿子。\n\n被继承的类称为父类或基类，继承的类称为子类或派生类。“子类”和“父类”通常放在一起称呼，“基类”和“派生类”通常放在一起称呼。\n\n派生类除了拥有基类的成员，还可以定义自己的新成员，以增强类的功能。  \n\n以下是两种典型的使用继承的场景：\n1. 当你创建的新类与现有的类相似，只是多出若干成员变量或成员函数时，可以使用继承，这样不但会减少代码量，而且新类会拥有基类的所有功能。\n\n2. 当你需要创建多个类，它们拥有很多相似的成员变量或成员函数时，也可以使用继承。可以将这些类的共同成员提取出来，定义为基类，然后从基类继承，既可以节省代码，也方便后续修改成员。\n\n继承方式包括 public（公有的）、private（私有的）和 protected（受保护的），此项是可选的，如果不写，那么默认为 private.  \n\n## **public、protected、private**\n**public、protected、private 修饰类的成员**  \n类成员的访问权限由高到低依次为 public --> protected --> private，public 成员可以通过对象来访问，private 成员不能通过对象访问。\n\n`protected 成员和 private 成员类似，也不能通过对象访问`。但是当存在继承关系时，protected 和 private 就不一样了：基类中的 protected 成员可以在派生类中使用，而基类中的 private 成员不能在派生类中使用，下面是详细讲解。\n\n**public、protected、private 指定继承方式**  \n不同的继承方式会影响基类成员在派生类中的访问权限。\n\n1. public继承方式\n * 基类中所有 public 成员在派生类中为 public 属性；\n * 基类中所有 protected 成员在派生类中为 protected 属性；\n * 基类中所有 private 成员在派生类中不能使用。\n\n2. protected继承方式\n * 基类中的所有 public 成员在派生类中为 protected 属性；\n * 基类中的所有 protected 成员在派生类中为 protected 属性；\n * 基类中的所有 private 成员在派生类中不能使用。\n\n3. private继承方式\n * 基类中的所有 public 成员在派生类中均为 private 属性；\n * 基类中的所有 protected 成员在派生类中均为 private 属性；\n * 基类中的所有 private 成员在派生类中不能使用\n`基类成员在派生类中的访问权限不得高于继承方式中指定的权限.`\n`也就是说，继承方式中的 public、protected、private 是用来指明基类成员在派生类中的最高访问权限的。`\n如果希望基类的成员既不向外暴露（不能通过对象访问），还能在派生类中使用，那么只能声明为 protected。  \n**注意:** 我们这里说的是基类的 private 成员不能在派生类中使用，并没有说基类的 private 成员不能被继承。实际上，基类的 private 成员是能够被继承的，并且（成员变量）会占用派生类对象的内存，它只是在派生类中不可见，导致无法使用罢了.  \n\n如果派生类中的成员（包括成员变量和成员函数）和基类中的成员重名，那么就会遮蔽从基类继承过来的成员, 对于成员函数要引起注意，不管函数的参数如何，只要名字一样就会造成遮蔽.  \n\n## **基类派生类构造函数**\n基类的成员函数可以被继承，可以通过派生类的对象访问，但这仅仅指的是普通的成员函数，`类的构造函数不能被继承`。构造函数不能被继承是有道理的，因为即使继承了，它的名字和派生类的名字也不一样，不能成为派生类的构造函数，当然更不能成为普通的成员函数。\n\n在设计派生类时，对继承过来的成员变量的初始化工作也要由派生类的构造函数完成，但是大部分基类都有 private 属性的成员变量，它们在派生类中无法访问，更不能使用派生类的构造函数来初始化。\n\n这种矛盾在C++继承中是普遍存在的，解决这个问题的思路是：在派生类的构造函数中调用基类的构造函数.  \n\n通过派生类创建对象时必须要调用基类的构造函数，这是语法规定。换句话说，定义派生类构造函数时最好指明基类构造函数；如果不指明，就调用基类的默认构造函数（不带参数的构造函数）；如果没有默认构造函数，那么编译失败.  \n\n和构造函数类似，析构函数也不能被继承。与构造函数不同的是，在派生类的析构函数中不用显式地调用基类的析构函数，因为每个类只有一个析构函数，编译器知道如何选择，无需程序员干涉.  \n\n另外析构函数的执行顺序和构造函数的执行顺序也刚好相反：\n * 创建派生类对象时，构造函数的执行顺序和继承顺序相同，即先执行基类构造函数，再执行派生类构造函数。\n * 而销毁派生类对象时，析构函数的执行顺序和继承顺序相反，即先执行派生类析构函数，再执行基类析构函数\n\n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\tnamespace BaseDerived\n\t{\n\t    class People\n\t    {\n\t    protected:\n\t        char *m_name;\n\t        int m_age;\n\t\n\t    public:\n\t        People();\n\t        People(char *, int);\n\t        ~People();\n\t    };\n\t    People::People() : m_name(\"People\"), m_age(0)\n\t    {\n\t        cout << \"People constructor\" << endl;\n\t    }\n\t    People::People(char *name, int age) : m_name(name), m_age(age)\n\t    {\n\t        cout << \"People constructor\" << endl;\n\t    }\n\t    People::~People() { cout << \"People destructor\" << endl; }\n\t\n\t    class Student : public People\n\t    {\n\t    private:\n\t        float m_score;\n\t\n\t    public:\n\t        Student();\n\t        Student(char *name, int age, float score);\n\t        ~Student();\n\t        void display();\n\t    };\n\t    // 派生类构造函数中只能调用直接基类的构造函数，不能调用间接基类的\n\t    Student::Student() : m_score(0.0)\n\t    {\n\t        cout << \"Student constructor\" << endl;\n\t    }\n\t    Student::Student(char *name, int age, float score) : People(name, age), m_score(score)\n\t    {\n\t        cout << \"Student constructor\" << endl;\n\t    }\n\t    Student::~Student()\n\t    {\n\t        cout << \"Student destructor\" << endl;\n\t    }\n\t    void Student::display()\n\t    {\n\t        cout << m_name << \"的年龄是:\" << m_age << \", 成绩是:\" << m_score << endl;\n\t    }\n\t} // namespace BaseDerived\n\t\n\tusing namespace BaseDerived;\n\tint main()\n\t{\n\t    Student stu1;\n\t    stu1.display();\n\t    Student stu2(\"小明\", 16, 90.5);\n\t    stu2.display();\n\t\n\t    return 0;\n\t}\n```\n运行结果:\n\n```\n\tPeople constructor\n\tStudent constructor\n\tPeople的年龄是:0, 成绩是:0\n\tPeople constructor\n\tStudent constructor\n\t小明的年龄是:16, 成绩是:90.5\n\tStudent destructor\n\tPeople destructor\n\tStudent destructor\n\tPeople destructor\n```\n## **多重继承**\n\n在前面的例子中，派生类都只有一个基类，称为单继承（Single Inheritance）。除此之外，C++也支持多继承（Multiple Inheritance），即一个派生类可以有两个或多个基类。  \n\n> 多继承容易让代码逻辑复杂、思路混乱，一直备受争议，中小型项目中较少使用，后来的 Java、C#、PHP 等干脆取消了多继承。  \n\n多继承的语法也很简单，将多个基类用逗号隔开即可。例如已声明了类A、类B和类C，那么可以这样来声明派生类D：\n\n``` cpp\n\tclass D: public A, private B, protected C{\n\t    //类D新增加的成员\n\t}\n```\n**多继承下的构造函数**\n\n多继承形式下的构造函数和单继承形式基本相同，只是要在派生类的构造函数中调用多个基类的构造函数。以上面的 A、B、C、D 类为例，D 类构造函数的写法为：\n\n``` cpp\n\tD(形参列表): A(实参列表), B(实参列表), C(实参列表){\n\t    //其他操作\n\t}\n```\n基类构造函数的调用顺序和和它们在派生类构造函数中出现的顺序无关，而是和声明派生类时基类出现的顺序相同。  \n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t//基类\n\tclass BaseA\n\t{\n\tpublic:\n\t    BaseA(int a, int b);\n\t    ~BaseA();\n\t\n\tpublic:\n\t    void show();\n\t\n\tprotected:\n\t    int m_a;\n\t    int m_b;\n\t};\n\tBaseA::BaseA(int a, int b) : m_a(a), m_b(b)\n\t{\n\t    cout << \"BaseA constructor\" << endl;\n\t}\n\tBaseA::~BaseA()\n\t{\n\t    cout << \"BaseA destructor\" << endl;\n\t}\n\tvoid BaseA::show()\n\t{\n\t    cout << \"m_a = \" << m_a << endl;\n\t    cout << \"m_b = \" << m_b << endl;\n\t}\n\t\n\t//基类\n\tclass BaseB\n\t{\n\tpublic:\n\t    BaseB(int c, int d);\n\t    ~BaseB();\n\t    void show();\n\t\n\tprotected:\n\t    int m_c;\n\t    int m_d;\n\t};\n\tBaseB::BaseB(int c, int d) : m_c(c), m_d(d)\n\t{\n\t    cout << \"BaseB constructor\" << endl;\n\t}\n\tBaseB::~BaseB()\n\t{\n\t    cout << \"BaseB destructor\" << endl;\n\t}\n\tvoid BaseB::show()\n\t{\n\t    cout << \"m_c = \" << m_c << endl;\n\t    cout << \"m_d = \" << m_d << endl;\n\t}\n\t\n\t//派生类\n\tclass Derived : public BaseA, public BaseB\n\t{\n\tpublic:\n\t    Derived(int a, int b, int c, int d, int e);\n\t    ~Derived();\n\t\n\tpublic:\n\t    void display();\n\t\n\tprivate:\n\t    int m_e;\n\t};\n\tDerived::Derived(int a, int b, int c, int d, int e) : BaseA(a, b), BaseB(c, d), m_e(e)\n\t{\n\t    cout << \"Derived constructor\" << endl;\n\t}\n\tDerived::~Derived()\n\t{\n\t    cout << \"Derived destructor\" << endl;\n\t}\n\tvoid Derived::display()\n\t{\n\t    BaseA::show(); //调用BaseA类的show()函数\n\t    BaseB::show(); //调用BaseB类的show()函数\n\t    cout << \"m_e = \" << m_e << endl;\n\t}\n\t\n\tint main()\n\t{\n\t    Derived obj(1, 2, 3, 4, 5);\n\t    obj.display();\n\t    return 0;\n\t}\n```\n运行结果:  \n```\n\tBaseA constructor\n\tBaseB constructor\n\tDerived constructor\n\tm_a = 1\n\tm_b = 2\n\tm_c = 3\n\tm_d = 4\n\tm_e = 5\n\tDerived destructor\n\tBaseB destructor\n\tBaseA destructor\n```\n## **向上转型**\n类其实也是一种数据类型，也可以发生数据类型转换，不过这种转换只有在基类和派生类之间才有意义，并且只能将派生类赋值给基类，包括将派生类对象赋值给基类对象、将派生类指针赋值给基类指针、将派生类引用赋值给基类引用，这在 C++ 中称为向上转型（Upcasting）。相应地，将基类赋值给派生类称为向下转型（Downcasting）.  \n\n向上转型非常安全，可以由编译器自动完成；向下转型有风险，需要程序员手动干预.  \n\n`赋值的本质是将现有的数据写入已分配好的内存中，对象的内存只包含了成员变量，所以 **对象之间的赋值** 是 **成员变量的赋值**，成员函数不存在赋值问题.`  \n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t//基类\n\tclass A\n\t{\n\tpublic:\n\t    A(int a);\n\t\n\tpublic:\n\t    void display();\n\t\n\tpublic:\n\t    int m_a;\n\t};\n\tA::A(int a) : m_a(a) {}\n\tvoid A::display()\n\t{\n\t    cout << \"Class A: m_a=\" << m_a << endl;\n\t}\n\t\n\t//派生类\n\tclass B : public A\n\t{\n\tpublic:\n\t    B(int a, int b);\n\t\n\tpublic:\n\t    void display();\n\t\n\tpublic:\n\t    int m_b;\n\t};\n\tB::B(int a, int b) : A(a), m_b(b) {}\n\tvoid B::display()\n\t{\n\t    cout << \"Class B: m_a=\" << m_a << \", m_b=\" << m_b << endl;\n\t}\n\t\n\tint main()\n\t{\n\t    A a(10);\n\t    B b(66, 99);\n\t    //赋值前\n\t    a.display();\n\t    b.display();\n\t    cout << \"--------------\" << endl;\n\t    //赋值后\n\t    a = b;\n\t    a.display();\n\t    b.display();\n\t\n\t    return 0;\n\t}\n```\n运行结果:\n```\n\tClass A: m_a=10\n\tClass B: m_a=66, m_b=99\n\t--------------\n\tClass A: m_a=66  // m_a 赋值为66.\n\tClass B: m_a=66, m_b=99\n```\n`将派生类对象赋值给基类对象时，会舍弃派生类新增的成员，也就是“大材小用”`.\n\n![](01.png)\n可以发现，即使将派生类对象赋值给基类对象，基类对象也不会包含派生类的成员，所以依然不同通过基类对象来访问派生类的成员.  \n对于上面的例子，a.m_a 是正确的，但 a.m_b 就是错误的，因为 a 不包含成员 m_b.  \n\n这种转换关系是不可逆的，`只能用派生类对象给基类对象赋值，而不能用基类对象给派生类对象赋值`。理由很简单，基类不包含派生类的成员变量，无法对派生类的成员变量赋值。同理，同一基类的不同派生类对象之间也不能赋值.  \n\n`编译器通过指针来访问成员变量，指针指向哪个对象就使用哪个对象的数据；编译器通过指针的类型来访问成员函数，指针属于哪个类的类型就使用哪个类的函数.`  \n\n`基类的引用也可以指向派生类的对象，并且它的表现和指针是类似的.`  \n\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"07 多态与虚函数","url":"/2021/03/13/language/cpp/07_多态与虚函数/","content":"\n## **多态**\nC++ 虚函数对于多态具有决定性的作用，有虚函数才能构成多态.  \n * 1. 只需要在虚函数的声明处加上 virtual 关键字，函数定义处可以加也可以不加.\n * 2. 可以只将基类中的函数声明为虚函数，这样所有派生类中具有遮蔽关系的同名函数都将自动成为虚函数, 派生类可以不添加virtual关键字.\n * 3. 当在基类中定义了虚函数时，如果派生类没有定义新的函数来遮蔽此函数，那么将使用基类的虚函数.\n * 4. 只有派生类的虚函数覆盖基类的虚函数（函数原型相同）才能构成多态.\n      如基类虚函数的原型为virtual void func();，派生类虚函数的原型为virtual void func(int);，那么当基类指针 p 指向派生类对象时，语句p -> func(100);将会出错，而语句p -> func();将调用基类的函数.  \n * 5. 构造函数不能是虚函数。对于基类的构造函数，它仅仅是在派生类构造函数中被调用，这种机制不同于继承。也就是说，派生类不继承基类的构造函数，将构造函数声明为虚函数没有什么意义.\n * 6. 析构函数可以声明为虚函数，而且有时候必须要声明为虚函数, 将基类的析构函数声明为虚函数后，派生类的析构函数也会自动成为虚函数.\n * 7. 存在基类的指针，通过该指针调用虚函数.\n\n如果不使用多态，那么就需要定义多个指针变量，很容易造成混乱；而有了多态，只需要一个指针变量 p 就可以调用所有派生类的虚函数。\n\n从这个例子中也可以发现，对于具有复杂继承关系的大中型程序，多态可以增加其灵活性，让代码更具有表现力。\n\n### **什么时候声明虚函数**\n首先看成员函数所在的类是否会作为基类。然后看成员函数在类的继承后有无可能被更改功能，如果希望更改其功能的，一般应该将它声明为虚函数。如果成员函数在类被继承后功能不需修改，或派生类用不到该函数，则不要把它声明为虚函数。  \n\n### **指针实现多态**\n有了虚函数，基类指针指向基类对象时就使用基类的成员（包括成员函数和成员变量），指向派生类对象时就使用派生类的成员。换句话说，基类指针可以按照基类的方式来做事，也可以按照派生类的方式来做事，它有多种形态，或者说有多种表现方式，我们将这种现象称为多态（Polymorphism）.  \n\nC++提供多态的目的是：可以通过基类指针对所有派生类（包括直接派生和间接派生）的成员变量和成员函数进行“全方位”的访问，尤其是成员函数。如果没有多态，我们只能访问成员变量.  \n\n```cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t//基类People\n\tclass People\n\t{\n\tpublic:\n\t    People(char *name, int age);\n\t    virtual void display(); //声明为虚函数\n\tprotected:\n\t    char *m_name;\n\t    int m_age;\n\t};\n\tPeople::People(char *name, int age) : m_name(name), m_age(age) {}\n\tvoid People::display()\n\t{\n\t    cout << m_name << \"今年\" << m_age << \"岁了，是个无业游民。\" << endl;\n\t}\n\t\n\t//派生类Teacher\n\tclass Teacher : public People\n\t{\n\tpublic:\n\t    Teacher(char *name, int age, int salary);\n\t    virtual void display(); //声明为虚函数\n\tprivate:\n\t    int m_salary;\n\t};\n\tTeacher::Teacher(char *name, int age, int salary) : People(name, age), m_salary(salary) {}\n\tvoid Teacher::display()\n\t{\n\t    cout << m_name << \"今年\" << m_age << \"岁了，是一名教师，每月有\" << m_salary << \"元的收入。\" << endl;\n\t}\n\t\n\tint main()\n\t{\n\t    People *p = new People(\"王志刚\", 23);\n\t    p->display();\n\t\n\t    p = new Teacher(\"赵宏佳\", 45, 8200);\n\t    p->display();\n\t\n\t    return 0;\n\t}\n```\n运行结果：\n```\n\t王志刚今年23岁了，是个无业游民。\n\t赵宏佳今年45岁了，是一名教师，每月有8200元的收入。\n```\n\n\n### 引用实现多态\n\n引用在本质上是通过指针的方式实现的, 借助引用也可以实现多态.  \n\n不过引用不像指针灵活，指针可以随时改变指向，而引用只能指代固定的对象，在多态性方面缺乏表现力，所以以后我们再谈及多态时一般是说指针.  \n\n```cpp\n\tint main(){\n\t    People p(\"王志刚\", 23);\n\t    Teacher t(\"赵宏佳\", 45, 8200);\n\t   \n\t    People &rp = p;\n\t    People &rt = t;\n\t   \n\t    rp.display();\n\t    rt.display();\n\t    return 0;\n\t}\n```\n运行结果：\n\n```\n\t王志刚今年23岁了，是个无业游民。\n\t赵宏佳今年45岁了，是一名教师，每月有8200元的收入。\n```\n## **虚析构函数**\n如果基类析构函数不定义为virtual, 如:\n\n```cpp\n\tBase *pb = new Derived();\n\tdelete pb;\n```\n只调用了基类的析构函数，没有调用派生类的析构函数.  \n不调用派生类的析构函数会导致 name 指向的 100 个 char 类型的内存空间得不到释放；除非程序运行结束由操作系统回收，否则就再也没有机会释放这些内存。这是典型的内存泄露.  \n * 析构函数是非虚函数，通过指针访问非虚函数时，编译器会根据指针的类型来确定要调用的函数；也就是说，指针指向哪个类就调用哪个类的函数.  \n * 将基类的析构函数声明为虚函数后，派生类的析构函数也会自动成为虚函数。这个时候编译器会忽略指针的类型，而根据指针的指向来选择函数；也就是说，指针指向哪个类的对象就调用哪个类的函数.  \n **`在执行派生类的析构函数的过程中，又会调用基类的析构函数。派生类析构函数始终会调用基类的析构函数，并且这个过程是隐式完成的.`**\n \n在实际开发中，一旦我们自己定义了析构函数，就是希望在对象销毁时用它来进行清理工作，比如释放内存、关闭文件等，如果这个类又是一个基类，那么我们就必须将该析构函数声明为虚函数，否则就有内存泄露的风险。也就是说，大部分情况下都应该将基类的析构函数声明为虚函数.  \n\n```cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t//基类\n\tclass Base\n\t{\n\tpublic:\n\t    Base();\n\t    virtual ~Base();\n\t\n\tprotected:\n\t    char *str;\n\t};\n\tBase::Base()\n\t{\n\t    str = new char[100];\n\t    cout << \"Base constructor\" << endl;\n\t}\n\tBase::~Base()\n\t{\n\t    delete[] str;\n\t    cout << \"Base destructor\" << endl;\n\t}\n\t\n\t//派生类\n\tclass Derived : public Base\n\t{\n\tpublic:\n\t    Derived();\n\t    ~Derived();\n\t\n\tprivate:\n\t    char *name;\n\t};\n\tDerived::Derived()\n\t{\n\t    name = new char[100];\n\t    cout << \"Derived constructor\" << endl;\n\t}\n\tDerived::~Derived()\n\t{\n\t    delete[] name;\n\t    cout << \"Derived destructor\" << endl;\n\t}\n\t\n\tint main()\n\t{\n\t    Base *pb = new Derived();\n\t    delete pb;\n\t\n\t    cout << \"-------------------\" << endl;\n\t\n\t    Derived *pd = new Derived();\n\t    delete pd;\n\t\n\t    return 0;\n\t}\n```\n运行结果：\n\n```\n\tBase constructor\n\tDerived constructor\n\tDerived destructor\n\tBase destructor\n\t-------------------\n\tBase constructor\n\tDerived constructor\n\tDerived destructor\n\tBase destructor\n```\n## **纯虚函数 & 抽象类**\n可以将虚函数声明为纯虚函数，语法格式为：\n\n```cpp\n\tvirtual 返回值类型 函数名 (函数参数) = 0;\n```\n纯虚函数没有函数体，只有函数声明，在虚函数声明的结尾加上=0，表明此函数为纯虚函数。\n> 最后的=0并不表示函数返回值为0，它只起形式上的作用，告诉编译系统“这是纯虚函数”。\n**`包含纯虚函数的类称为抽象类（Abstract Class）`**。之所以说它抽象，是因为它无法实例化，也就是无法创建对象。原因很明显，纯虚函数没有函数体，不是完整的函数，无法调用，也无法为其分配内存空间。  \n\n抽象类通常是作为基类，让派生类去实现纯虚函数。派生类必须实现纯虚函数才能被实例化.\n\n1. 一个纯虚函数就可以使类成为抽象基类，但是抽象基类中除了包含纯虚函数外，还可以包含其它的成员函数（虚函数或普通函数）和成员变量。\n\n2. 只有类中的虚函数才能被声明为纯虚函数，普通成员函数和顶层函数均不能声明为纯虚函数。如下例所示：\n\n\n```cpp\n\t//顶层函数不能被声明为纯虚函数\n\tvoid fun() = 0;   //compile error\n\tclass base{\n\tpublic :\n\t    //普通成员函数不能被声明为纯虚函数\n\t    void display() = 0;  //compile error\n\t};\n```\n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t//线\n\tclass Line    // 最顶层的基类，在 Line 类中定义了两个纯虚函数 area() 和 volume()\n\t{\n\tpublic:\n\t    Line(float len);\n\t    virtual float area() = 0;\n\t    virtual float volume() = 0;\n\t\n\tprotected:\n\t    float m_len;\n\t};\n\tLine::Line(float len) : m_len(len) {}\n\t\n\t//矩形\n\tclass Rec : public Line    // Rec 也仍然是抽象类, 没有实现继承来的 volume() 函数，volume() 仍然是纯虚函数\n\t{\n\tpublic:\n\t    Rec(float len, float width);\n\t    float area();\n\t\n\tprotected:\n\t    float m_width;\n\t};\n\tRec::Rec(float len, float width) : Line(len), m_width(width) {}\n\tfloat Rec::area() { return m_len * m_width; }\n\t\n\t//长方体\n\tclass Cuboid : public Rec\n\t{\n\tpublic:\n\t    Cuboid(float len, float width, float height);\n\t    float area();\n\t    float volume();\n\t\n\tprotected:\n\t    float m_height;\n\t};\n\tCuboid::Cuboid(float len, float width, float height) : Rec(len, width), m_height(height) {}\n\tfloat Cuboid::area() { return 2 * (m_len * m_width + m_len * m_height + m_width * m_height); }\n\tfloat Cuboid::volume() { return m_len * m_width * m_height; }\n\t\n\t//正方体\n\tclass Cube : public Cuboid\n\t{\n\tpublic:\n\t    Cube(float len);\n\t    float area();\n\t    float volume();\n\t};\n\tCube::Cube(float len) : Cuboid(len, len, len) {}\n\tfloat Cube::area() { return 6 * m_len * m_len; }\n\tfloat Cube::volume() { return m_len * m_len * m_len; }\n\t\n\tint main()\n\t{\n\t    Line *p = new Cuboid(10, 20, 30);\n\t    cout << \"The area of Cuboid is \" << p->area() << endl;\n\t    cout << \"The volume of Cuboid is \" << p->volume() << endl;\n\t\n\t    p = new Cube(15);\n\t    cout << \"The area of Cube is \" << p->area() << endl;\n\t    cout << \"The volume of Cube is \" << p->volume() << endl;\n\t\n\t    return 0;\n\t}\n```\n运行结果：\n```\n\tThe area of Cuboid is 2200\n\tThe volume of Cuboid is 6000\n\tThe area of Cube is 1350\n\tThe volume of Cube is 3375\n```\n本例中定义了四个类，它们的继承关系为：Line --> Rec --> Cuboid --> Cube。\n\n在 Rec 类中，实现了 area() 函数；所谓实现，就是定义了纯虚函数的函数体。但这时 Rec 仍不能被实例化，因为它没有实现继承来的 volume() 函数，volume() 仍然是纯虚函数，所以 Rec 也仍然是抽象类.  \n\n抽象基类除了约束派生类的功能，还可以实现多态。请注意第 51 行代码，指针 p 的类型是 Line，但是它却可以访问派生类中的 area() 和 volume() 函数，正是由于在 Line 类中将这两个函数定义为纯虚函数；如果不这样做，51 行后面的代码都是错误的.  \n\n\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"Hexo部署博客遇到的问题","url":"/2021/03/13/blogs/blog搭建/hexo部署博客遇到的问题/","content":"\n## spawn failed错误\n\nhexo发生error：spawn failed错误的解决方法\n\n<!-- more -->\n\n问题描述\n先是出现错误：\n```\nerror：spawn failed...\n```\n\n然后经过一些博客的操作会出现以下问题：\n```\nfatal: cannot lock ref 'HEAD': unable to resolve reference HEAD: Invalid argument error: src refspec\n```\n\n或者：\n```\nerror: src refspec HEAD does not match any.等等\n```\n\n总结一下：\n问题大多是因为 `git` 进行 `push` 或者 `hexo d` 的时候改变了一些 `deploy_git` 文件下的内容。\n\n解决办法\n删除 `.deploy_git` 文件夹;\n输入 `git config --global core.autocrlf false`\n然后，依次执行：\n```\nhexo clean\nhexo g\nhexo d\n```\n问题解决。暴力直接，有效。\n\n","categories":["blogs","blog搭建"]},{"title":"04 Destructor","url":"/2021/03/13/language/cpp/04_destructor/","content":"\n### destructor\n\n### **构造函数和析构函数执行顺序**\n类对象生成时，先执行所有成员对象的构造函数，然后才执行类自己的构造函数。成员对象构造函数的执行次序和成员对象在类定义中的次序一致，与它们在构造函数初始化列表中出现的次序无关.  \n当类对象消亡时，先执行类的析构函数，然后再执行成员对象的析构函数，成员对象析构函数的执行次序和构造函数的执行次序相反，即先构造的后析构.  \n\n``` cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t/**\n\t * 析构函数（Destructor）也是一种特殊的成员函数，没有返回值.\n\t * 不需要程序员显式调用（程序员也没法显式调用）而是在销毁对象时自动执行.\n\t * 构造函数的名字和类名相同，而析构函数的名字是在类名前面加一个~符号.\n\t * 析构函数没有参数，不能被重载，因此一个类只能有一个析构函数.\n\t * 如果用户没有定义，编译器会自动生成一个默认的析构函数。\n\t */\n\t\n\tnamespace Destructor\n\t{\n\t    class VLA\n\t    {\n\t    public:\n\t        // 用 new 分配内存时会调用构造函数\n\t        VLA(int len);\n\t        // 用 delete 释放内存时会调用析构函数\n\t        ~VLA();\n\t\n\t    private:\n\t        const int m_len; // m_len 变量不允许修改, 只能通过初始化列表赋值.\n\t        int *m_arr;\n\t        int *m_p;\n\t\n\t    public:\n\t        void input();\n\t        void show();\n\t\n\t    private:\n\t        int *at(int i); // at() 函数只在类的内部使用\n\t    };\n\t\n\t    VLA::VLA(int len) : m_len(len)\n\t    {\n\t        if (len > 0)\n\t        {\n\t            // new 创建的对象位于堆区，通过 delete 删除时才会调用析构函数\n\t            // 如果没有 delete，析构函数就不会被执行.\n\t            m_arr = new int[len];\n\t        }\n\t        else\n\t        {\n\t            m_arr = NULL;\n\t        }\n\t    }\n\t\n\t    VLA::~VLA()\n\t    {\n\t        delete[] m_arr;\n\t        cout << \"Delete m_arr\" << endl;\n\t    }\n\t\n\t    void VLA::input()\n\t    {\n\t        for (int i = 0; m_p = at(i); i++)\n\t        {\n\t            cin >> *at(i);\n\t            cout << \"&mp: \" << m_p << endl;\n\t        }\n\t    }\n\t\n\t    void VLA::show()\n\t    {\n\t        for (int i = 0; m_p = at(i); i++)\n\t        {\n\t            if (i == m_len - 1)\n\t            {\n\t                cout << *at(i) << endl;\n\t            }\n\t            else\n\t            {\n\t                cout << *at(i) << \", \";\n\t            }\n\t        }\n\t    }\n\t\n\t    int *VLA::at(int i)\n\t    {\n\t        if (!m_arr || i < 0 || i >= m_len)\n\t        {\n\t            return NULL;\n\t        }\n\t        else\n\t        {\n\t            return m_arr + i;\n\t        }\n\t    }\n\t} // namespace Destructor\n\t\n\tusing namespace Destructor;\n\tint main()\n\t{\n\t    int n;\n\t    cout << \"Input array length: \";\n\t    cin >> n;\n\t    VLA *parr = new VLA(n);\n\t    parr->input();\n\t    parr->show();\n\t    delete parr;\n\t    return 0;\n\t}\n```\n## **输出**\n```\n\tInput array length: 3 \n\t1 2 3\n\t&mp: 0x11f5030\n\t&mp: 0x11f5034\n\t&mp: 0x11f5038\n\t1, 2, 3\n\tDelete m_arr\n```\n\n","categories":["language","cpp"]},{"title":"new char[size]()用法","url":"/2021/03/13/language/cpp/new_char[size]()用法/","content":"\n## new char[size]()会初始化并清零字符数组\nReference Link: \nhttps://stackoverflow.com/questions/41082924/c-difference-between-new-charsize-and-new-charsize\nhttps://blog.csdn.net/huangshanchun/article/details/50008959\n\n```cpp\n\tchar * text = new char[size];\n```\nvs.\n\n```cpp\n\tchar * text = new char[size]();\n```\nnew char[size]() would zero-initialize the array. new char[size] would leave it uninitialized, containing random garbage.\n\n\n在C++ 中new char[]() 编译器默认将其初始化为0，new char[]则不会初始化, 有时候输出会看到垃圾数据。\n\n```cpp\n\t#include<iostream>\n\tusing namespace std;\n\t \n\tint main(int argc,char *argv[])\n\t{\n\t\tchar *p=new char[10];// vs 编译器则不进行初始化\n\t\tchar *q=new char[10]();//vs 编译器将其初始化为0\n\t \n\t\tcout<<\"p:\"<<p<<endl;\n\t\tcout<<\"q:\"<<q<<endl;\n\t\tcin.get();\n\t}\n```\n","categories":["language","cpp"]},{"title":"02 constructor","url":"/2021/03/13/language/cpp/02_constructor/","content":"\n## constructor\n``` cpp\n\t#include <iostream>\n\t\n\tusing namespace std;\n\t\n\tnamespace Constructor\n\t{\n\t\n\t    class Student\n\t    {\n\t    private:\n\t        const char *m_name; // 初始化 const 成员变量的唯一方法就是使用初始化列表\n\t        int m_age;\n\t        float m_score;\n\t\n\t    public:\n\t        /**\n\t     * 一个类必须有构造函数，要么用户自己定义，要么编译器自动生成。\n\t     * 一旦用户自己定义了构造函数，不管有几个，也不管形参如何，编译器都不再自动生成。\n\t     */\n\t        Student(char *name, int age, float score);\n\t        void show();\n\t    };\n\t\n\t    Student::Student(char *name, int age, float score) : m_name(name), m_age(age) // 初始化列表\n\t    {\n\t        this->m_score = score;\n\t    }\n\t\n\t    void Student::show()\n\t    {\n\t        cout << this->m_name << \"的年龄是: \" << this->m_age << \", 成绩是: \" << this->m_score << endl;\n\t    }\n\t} // namespace Constructor\n\t\n\tusing namespace Constructor;\n\t// main函数不能再namespace里面\n\tint main()\n\t{\n\t    // 有了上面的using namespace constructor, 接下来就可以加上constructor::也可以不加上constructor::\n\t    Constructor::Student *stu = new Student(\"李华\", 10, 100);\n\t    stu->show();\n\t\n\t    system(\"pause\");\n\t    return 0;\n\t}\n```\n\n\n","categories":["language","cpp"]},{"title":"2.Linux Node.js 安装","url":"/2021/03/13/language/nodejs/2.Linux_Node.js_安装/","content":"<!-- date: 2020-01-06 13:50:12    //可以添加到上面-->\n\n## Linux安装:\ncurl: https://curl.haxx.se/download.html\n\n```shell\n\t curl-7.69.1.tar.gz\n\t ./configure --prefix=/usr/local/curl\n\t make -j12\n\t make install\n\t ln -s /usr/local/curl/bin/curl /usr/bin\n\t vim ~/.bashrc 添加 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/curl/lib\n\t source ~/.bashrc\n\t curl --version \t\t// 查看curl版本和支持的协议如http, https\n```\n","categories":["language","nodejs"]},{"title":"09 异常","url":"/2021/03/13/language/cpp/09_异常/","content":"\n\n## **异常**\n\n```cpp\n\ttry{\n\t    // 可能抛出异常的语句\n\t}catch(exceptionType variable){\n\t    // 处理异常的语句\n\t}\n```\n * **`exceptionType`** 是异常类型，它指明了当前的 catch 可以处理什么类型的异常.\n * **`variable`** 是一个变量，用来接收异常信息.\n如果不希望 catch 处理异常数据，也可以将 variable 省略掉，也即写作：\n\n\n```cpp\n\ttry{\n\t    // 可能抛出异常的语句\n\t}catch(exceptionType){\n\t    // 处理异常的语句\n\t}\n```\n\n检测到异常后程序的执行流会发生跳转，从异常点跳转到 catch 所在的位置，位于异常点之后的、并且在当前 try 块内的语句就都不会再执行了；即使 catch 语句成功地处理了错误，程序的执行流也不会再回退到异常点，所以这些语句永远都没有执行的机会了.  \n\n异常既然是一份数据，那么就应该有数据类型。C++ 规定，**`异常类型`**可以是 int、char、float、bool 等基本类型，也可以是指针、数组、字符串、结构体、类等聚合类型。  \n\n**`exceptionType variable`** 和函数的形参非常类似，当异常发生后，会将异常数据传递给 variable 这个变量，这和函数传参的过程类似。当然，只有跟 exceptionType 类型匹配的异常数据才会被传递给 variable，否则 catch 不会接收这份异常数据，也不会执行 catch 块中的语句.  \n\nC++ 语言本身以及标准库中的函数抛出的异常，都是 exception 类或其子类的异常。也就是说，抛出异常时，会创建一个 exception 类或其子类的对象。  \n\n\texception 类位于 <exception> 头文件.  \n\n```cpp\n\t#include <iostream>\n\t#include <string>\n\t#include <exception>\n\tusing namespace std;\n\t\n\tclass Base\n\t{\n\t};\n\tclass Derived : public Base\n\t{\n\t};\n\t\n\tvoid func()\n\t{\n\t    throw \"Unknown Exception\"; //抛出异常\n\t    cout << \"[1]This statement will not be executed.\" << endl;\n\t}\n\t\n\tint main()\n\t{\n\t    string str = \"http://c.biancheng.net\";\n\t    try\n\t    {\n\t        char ch1 = str[100];\n\t        cout << ch1 << endl;\n\t    }\n\t    catch (exception e)\n\t    {\n\t        cout << \"[1]out of bound!\" << endl;\n\t    }\n\t\n\t    try\n\t    {\n\t        char ch2 = str.at(100);\n\t        cout << ch2 << endl;\n\t    }\n\t    catch (exception &e)  // 之所以使用引用，是为了提高效率。如果不使用引用，就要经历一次对象拷贝（要调用拷贝构造函数）的过程。\n\t    { //exception类位于<exception>头文件中\n\t        cout << \"[2]out of bound!\" << endl;\n\t    }\n\t\n\t    try\n\t    {\n\t        func();\n\t    }\n\t    catch (const char *&e)\n\t    {\n\t        cout << e << endl;\n\t    }\n\t\n\t    try\n\t    {\n\t        throw Derived(); //抛出自己的异常类型，实际上是创建一个Derived类型的匿名对象\n\t        cout << \"This statement will not be executed.\" << endl;\n\t    }\n\t    catch (int)\n\t    {\n\t        cout << \"Exception type: int\" << endl;\n\t    }\n\t    catch (char *)\n\t    {\n\t        cout << \"Exception type: cahr *\" << endl;\n\t    }\n\t    catch (Base)\n\t    { //匹配成功（向上转型）\n\t        cout << \"Exception type: Base\" << endl;\n\t    }\n\t    catch (Derived)\n\t    {\n\t        cout << \"Exception type: Derived\" << endl;\n\t    }\n\t\n\t    int nums[] = {1, 2, 3};\n\t    try\n\t    {\n\t        throw nums;   // 数组类型\n\t        throw str;    // 数组类型\n\t        throw 100;    // int 类型\n\t        cout << \"This statement will not be executed.\" << endl;\n\t    }\n\t    catch (const int *)\n\t    {\n\t        cout << \"Exception type: const int *\" << endl;\n\t    }\n\t\n\t    return 0;\n\t}\n```\n运行结果；\n```\n\t[2]out of bound!\n\tUnknown Exception\n\tException type: Base\n\tException type: const int *\n```\n * 期望的是，异常被catch(Derived)捕获，但是从输出结果可以看出，异常提前被catch(Base)捕获了，这说明 catch 在匹配异常类型时发生了向上转型（Upcasting）.  \n * nums 本来的类型是int [3]，但是 catch 中没有严格匹配的类型，所以先转换为int *，再转换为const int *.  \n\n## exception类\n\n\texception 类位于 <exception> 头文件.  \n\nC++语言本身或者标准库抛出的异常都是 exception 的子类，称为标准异常（Standard Exception）。你可以通过下面的语句来捕获所有的标准异常：\n\n```cpp\n\ttry{\n\t    //可能抛出异常的语句\n\t}catch(exception &e){\n\t    //处理异常的语句\n\t}\n```\n之所以使用引用，是为了提高效率。如果不使用引用，就要经历一次对象拷贝（要调用拷贝构造函数）的过程。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"08 模板","url":"/2021/03/13/language/cpp/08_模板/","content":"\n## 函数模板\n\n在C++中，数据的类型也可以通过参数来传递，在函数定义时可以不指明具体的数据类型，当发生函数调用时，编译器可以根据传入的实参自动推断数据类型。这就是类型的参数化.  \n\n值（Value）和类型（Type）是数据的两个主要特征，它们在C++中都可以被参数化.  \n\n所谓函数模板，实际上是建立一个通用函数，它所用到的数据的类型（包括返回值类型、形参类型、局部变量类型）可以不具体指定，而是用一个虚拟的类型来代替（实际上是用一个标识符来占位），等发生函数调用时再根据传入的实参来逆推出真正的类型。这个通用函数就称为**`函数模板（Function Template）.`**\n\n`一但定义了函数模板，就可以将类型参数用于函数定义和函数声明了。说得直白一点，原来使用 int、float、char 等内置类型的地方，都可以用类型参数来代替.`\n\n**typename关键字也可以使用class关键字替代，它们没有任何区别**。C++ 早期对模板的支持并不严谨，没有引入新的关键字，而是用 class 来指明类型参数.  \n\n```cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\ttemplate <typename T>\n\tvoid Swap(T *a, T *b)\n\t{\n\t    T temp = *a;\n\t    *a = *b;\n\t    *b = temp;\n\t}\n\t\n\ttemplate <typename T>\n\tvoid Swap1(T &a, T &b)\n\t{\n\t    T temp = a;\n\t    a = b;\n\t    b = temp;\n\t}\n\t\n\ttemplate <typename T> //模板头，这里不能有分号\n\tT max(T a, T b, T c)\n\t{ //函数头\n\t    T max_num = a;\n\t    if (b > max_num)\n\t        max_num = b;\n\t    if (c > max_num)\n\t        max_num = c;\n\t    return max_num;\n\t}\n\t\n\tint main()\n\t{\n\t    //交换 int 变量的值\n\t    int n1 = 100, n2 = 200;\n\t    Swap(&n1, &n2);\n\t    cout << n1 << \", \" << n2 << endl;\n\t\n\t    //交换 float 变量的值\n\t    float f1 = 12.5, f2 = 56.93;\n\t    Swap(&f1, &f2);\n\t    cout << f1 << \", \" << f2 << endl;\n\t\n\t    //交换 char 变量的值\n\t    char c1 = 'A', c2 = 'B';\n\t    Swap1(c1, c2);\n\t    cout << c1 << \", \" << c2 << endl;\n\t\n\t    //交换 bool 变量的值\n\t    bool b1 = true, b2 = false;\n\t    Swap1(b1, b2);\n\t    cout << b1 << \", \" << b2 << endl;\n\t\n\t    //求三个浮点数的最大值\n\t    double d1 = 1.0, d2 = 2.0, d3 = 3.0;\n\t    double d_max = max(d1, d2, d3);\n\t    cout << \"d_max=\" << d_max << endl;\n\t\n\t    return 0;\n\t}\n```\n运行结果：\n```\n\t200, 100\n\t56.93, 12.5\n\tB, A\n\t0, 1\n\td_max=3\n```\n## 类模板\n\nC++ 除了支持函数模板，还支持类模板（Class Template）。函数模板中定义的类型参数可以用在函数声明和函数定义中，类模板中定义的类型参数可以用在类声明和类实现中。类模板的目的同样是将数据的类型参数化。  \n\n声明类模板的语法为：\n\n```cpp\n\ttemplate<typename 类型参数1 , typename 类型参数2 , …> class 类名{\n\t    //TODO:\n\t};\n```\n一但声明了类模板，就可以将类型参数用于类的成员函数和成员变量了。换句话说，原来使用 int、float、char 等内置类型的地方，都可以用类型参数来代替。  \n\n```cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\ttemplate <class T1, class T2> //这里不能有分号\n\tclass Point\n\t{\n\tpublic:\n\t    Point(T1 x, T2 y) : m_x(x), m_y(y) {}\n\t\n\tpublic:\n\t    T1 getX() const; //获取x坐标\n\t    void setX(T1 x); //设置x坐标\n\t    T2 getY() const; //获取y坐标\n\t    void setY(T2 y); //设置y坐标\n\tprivate:\n\t    T1 m_x; //x坐标\n\t    T2 m_y; //y坐标\n\t};\n\t\n\ttemplate <class T1, class T2>  //模板头\n\tT1 Point<T1, T2>::getX() const /*函数头*/\n\t{\n\t    return m_x;\n\t}\n\t\n\ttemplate <class T1, class T2>\n\tvoid Point<T1, T2>::setX(T1 x)\n\t{\n\t    m_x = x;\n\t}\n\t\n\ttemplate <class T1, class T2>\n\tT2 Point<T1, T2>::getY() const\n\t{\n\t    return m_y;\n\t}\n\t\n\ttemplate <class T1, class T2>\n\tvoid Point<T1, T2>::setY(T2 y)\n\t{\n\t    m_y = y;\n\t}\n\t\n\tint main()\n\t{\n\t    Point<int, int> p1(10, 20);\n\t    cout << \"x=\" << p1.getX() << \", y=\" << p1.getY() << endl;\n\t\n\t    Point<int, char *> p2(10, \"东经180度\");\n\t    cout << \"x=\" << p2.getX() << \", y=\" << p2.getY() << endl;\n\t\n\t    Point<char *, char *> *p3 = new Point<char *, char *>(\"东经180度\", \"北纬210度\");\n\t    cout << \"x=\" << p3->getX() << \", y=\" << p3->getY() << endl;\n\t\n\t    return 0;\n\t}\n```\n运行结果:\n```\n\tx=10, y=20\n\tx=10, y=东经180度\n\tx=东经180度, y=北纬210度\n```\n\n\n","categories":["language","cpp"]},{"title":"2.Windows10 Node.js 安装","url":"/2021/03/13/language/nodejs/2.Windows10_Node.js_安装/","content":"\n<!-- date: 2020-01-06 13:50:12    //可以添加到上面-->\n\n### 1. 下载node安装包(LTS版本) https://nodejs.org/en/  \n运行msi文件默认下一步即可\n\n### 2. cd C:\\Program Files\\nodejs\n\n``` shell\nmkdir node_cache\nmkdir node_global\n```\n\n### 3. <font color='red'>管理员</font>方式打开cmd\n``` shell\n\tnpm -v\n\tnpm config list\n\tnpm config set proxy http://child-prc.intel.com:913\n\tnpm config set https-proxy http://child-prc.intel.com:913\n\tnpm config set prefix \"C:\\Program Files\\nodejs\\node_global\"\n\tnpm config set cache \"C:\\Program Files\\nodejs\\node_cache\"\n\t\n\tnpm install npm -g    //将npm安装一份到刚迁移的新目录(方便后续统一管理)。\n```\n\n### 4. 配置环境变量(注意结合自己的node安装位置)\n • 新建NODE_PATH环境变量，值为 C:\\Program Files\\nodejs\\node_global\\node_modules\n • 在path环境变量中加入 C:\\Program Files\\nodejs\\node_global\n  NODE_PATH环境变量是指向新的模块安装位置(node默认使用该环境变量，建立就行)  \n  path中加入的是node模块的启动方式目录  \n![](NODE_PATH.png)\n可以把安装nodejs时候自动配置的PATH变量C:\\Users\\yazhanma\\AppData\\Roaming\\npm删掉\n![](PATH_nodejs_intall.png)\n\n### 5. 到此node安装完毕，介绍下node的模块安装方式(关闭cmd命令行并重新以管理员身份打开,让更新的环境变量生效)。\n\n``` shell\nnpm install 模块名 –g\n如 #npm install -g hexo-cli， 稍等一会\n```\n会在如下路径找到hexo\nC:\\Program Files\\nodejs\\node_global\nC:\\Program Files\\nodejs\\node_global\\node_modules\n\nAppendix:  \n设置git代理  \n\n``` shell\n    git config --global http.proxy http://127.0.0.1:1080  \n    git config --global https.proxy https://127.0.0.1:1080  \n```\n\n如果所用网络不需要代理，则要把npm代理和git代理去掉  \n1、去掉npm代理\n``` shell\nnpm config delete proxy\nnpm config delete https-proxy\n```\n\n2、去掉git代理\n``` shell\ngit config --global --unset http.proxy\ngit config --global --unset https.proxy\n```\n\n### FAQ(Frequently asked question)\n遇到设置完D:\\nodejs\\node_global环境变量后CMD窗口打开输入 hexo 可用, 但是powershell打开无法运行hexo(D:\\nodejs\\node_global\\hexo)\n提示原因是安全security问题，禁止运行脚本, 修改方法:\npowershell窗口打开运行如下命令:\n\n``` shell\n\tSet-ExecutionPolicy -Scope CurrentUser\n\t再输入remotesigned 即可\n```\n\n\n\n\n","categories":["language","nodejs"]},{"title":"go language","url":"/2021/03/13/language/go/goLang/","content":"\n## **go追本溯源**\nGo语言由Google公司的Ken Thompson(肯·汤普森), Rob Pike(罗布·派克)和Robert Griesemer(罗伯特·格瑞史莫)三位大师合作创造, 2017年9月开始设计, 2009年11月使用BSD授权许可正式对外公布.  \n\nKen Thompson(肯·汤普森): 1983年图灵奖得主, UNIX和C语言的发明人之一, 操作系统Plan 9的主要作者, 和Rob Pike还合作创造了UTF-8编码格式.  \n\nRob Pike(罗布·派克): UNIX核心成员之一\n\nRobert Griesemer(罗伯特·格瑞史莫): 参与设计开发了谷歌javascript V8引擎和java HotSpot虚拟机.  \n\nGo = C + Python\n\n## **Go环境安装**\ngo: https://golang.google.cn/dl/\n\n``` shell\n\t go1.14.1.linux-amd64.tar.gz\t\n\t tar -C /usr/local -xzf go1.12.17.linux-amd64.tar.gz\n\t vim ~/.bashrc 添加 export PATH=$PATH:/usr/local/go/bin\n\t source ~/.bashrc\n```\n\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### 1. go环境安装\nGo 语言支持以下系统：\n- Linux\n- FreeBSD\n- Mac OS X（也称为 Darwin）\n- Windows\n安装包下载地址为：https://golang.org/dl/。\n如果打不开可以使用这个地址：https://golang.google.cn/dl/。\nWindows下.msi 文件会安装在 c:\\Go 目录下。你可以将 c:\\Go\\bin 目录添加到 Path 环境变量中。添加后你需要重启命令窗口才能生效。\n\ntest.go 文件代码：\n``` go\npackage main\n\nimport \"fmt\"\n\nfunc main() {\n   fmt.Println(\"Hello, World!\")\n}\n```\n运行命令\n``` shell\n第一种方式:可以使用 go run 命令\nC:\\Go_WorkSpace>go run test.go\n\tHello, World!\n\n第二种方式:还可以使用 go build 命令来生成二进制文件\n$ C:\\Go_WorkSpace>go build hello.go \n$ dir\nhello    hello.go\n$ hello.exe\nHello, World!\n```\n※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### 2. 语言结构\nGo 语言的基础组成有以下几个部分：\n+ 包声明\n+ 引入包\n+ 函数\n+ 变量\n+ 语句 & 表达式\n+ 注释\n\n``` go\n// 当标识符（包括常量、变量、类型、函数名、结构字段等等）以一个大写字母开头，如：Group1, 标识符的对象就可以被外部包的代码所使用（客户端程序需要先导入这个包）,（像面向对象语言中的 public）\n// 标识符如果以小写字母开头，则对包外是不可见的，但是他们在整个包的内部是可见并且可用的（像面向对象语言中的 protected ）\n\t\t\t\t\t// 文件名与包名没有直接关系，不一定要将文件名与包名定成同一个。\n\t\t\t\t\t// 同一个文件夹下的文件只能有一个包名，否则编译报错。\npackage main\t\t// 定义了包名。你必须在源文件中非注释的第一行指明这个文件属于哪个包\n\t\t\t\t\t// package main表示一个可独立执行的程序，每个 Go 应用程序都包含一个名为 main 的包。\n\nimport \"fmt\"\t\t// 告诉 Go 编译器这个程序需要使用 fmt 包, fmt 包实现了格式化 IO（输入/输出）的函数\n\nfunc main() {\t\t// main 函数是每一个可执行程序所必须包含的, \"{\" 不能单独放在一行\n   /* 这是我的第一个简单的程序 */\n   fmt.Println(\"Hello, World!\")\t\t// fmt.Print(\"hello, world\\n\") 可以得到相同的结果\n}\n```\n","categories":["language","go"]},{"title":"4.node_modules","url":"/2021/03/13/language/nodejs/4.node_modules/","content":"\n### 设置淘宝镜像\n\n``` shell\nnpm install -g cnpm --registry=https://registry.npm.taobao.org\n```\n之后就可以用$cnpm install 来安装包\n![](taobao_mirror.PNG)\n\n### 先执行如下命令create package.json\npackage.json和package-lock.json可以在把项目发送给别人后便于别人查看和下载node_modules\n当接到新的NodeJS平台项目后，直接cd到项目目录然后$npm install就可以安装项目所需的所有node modules\n\n``` shell\n\t$cd 项目目录\n\t$npm init\t\t// create package.json文件\n\t\tinit过程中默认回车就可以了\n\t\tPress ^C at any time to quit.\n\t\tpackage name: (workspace)\t//默认入口项目名\n\t\tversion: (1.0.0)\n\t\tdescription:\n\t\tentry point: (index.js)\n\t\ttest command:\n\t\tgit repository:\n\t\tkeywords:\n\t\tauthor:\n\t\tlicense: (ISC)\n\t\tAbout to write to F:\n\t$npm install 包\t// 会自动创建package-lock.json文件\n```\n\n### supervisor\n这个包用来启动如app.js等服务文件，当源码里有代码改变时，supervisor会自动重启服务，浏览器直接刷新就可看到更改\n``` shell\n$ npm -g install supervisor\n$ supervisor app.js\n```\n\n\n","categories":["language","nodejs"]},{"title":"3.匿名函数,自执行函数","url":"/2021/03/13/language/nodejs/3.JSP匿名函数,自执行函数/","content":"\n### 1.1 函数声明\njavascript具有“函数声明提升”的特性，即执行代码之前，先读取函数声明，意味着函数声明可以放在调用它的语句之后。如下代码可以正常执行：\n```js\nsum(1, 2);\nfunction sum(x, y) {\n    alert(x+y);\n}\n```\n\n### 1.2 函数表达式\n函数表达式在使用前必须先赋值\n函数表达式中，创建的函数叫做匿名函数，因为function关键字后面没有标识符。\n```js\nvar ss = function(x, y) {\n    alert(x+y);\n}\nss(1, 2);\n```\n\n### 2 匿名函数的调用方式\n匿名函数，顾名思义就是没有名字的函数\n上面的函数表达式中的创建，即创建一个匿名函数，并将匿名函数赋值给变量ss，用ss来进行函数的调用，\n调用的方式就是在变量ss后面加上一对括号()，如果有参数传入的话就是ss(1,2)，这就是匿名函数的一种调用方式。\n再看一下以下例子：\n1) 将匿名函数用()括起来；\n2) 然后在后面加一对小括号(包含参数列表)\n紧随其后的另一个圆括号会立即调用这个函数。\n\n```js\n(function(x, y){return x+y;})(2, 3)  // 定义并立即调用了一个匿名函数\n\n如a = function(){}()，\"a=\"这个告诉了编译器这是一个函数表达式，而不是函数声明，因为函数表达式后面可以跟()\n\n因此下面两段代码是等价的。\nvar aa = function(x) {\n    alert(x);\n}(5); // 5\n(function(x){alert(x);})(5);\n```\n\n### 3 自执行函数\n我们创建了一个匿名的函数，并立即执行它，由于外部无法引用它内部的变量，因此在执行完后很快就会被释放，关键是这种机制不会污染全局对象。\n自执行函数，即定义和调用合为一体。\n自执行函数的一些表达方式：\n\n```js\n// 下面2个括号()都会立即执行  \n(function () { /* code */ } ()); // 推荐使用这个  \n(function () { /* code */ })(); // 但是这个也是可以用的\n```\n\n\n\n","categories":["language","nodejs"]},{"title":"NodeJs简介","url":"/2021/03/13/language/nodejs/NodeJs简介/","content":"\n### bilibili网址:[https://www.bilibili.com/video/BV1FJ411Q7fi](https://www.bilibili.com/video/BV1FJ411Q7fi)\n\n## node.js 网站\n\n> 1. [node.js官方网站](https://nodejs.org/)\n> 2. [node.js中文网](http://nodejs.cn/)\n> 3. [node.js 中文社区](https://cnodejs.org/)\n\n## NodeJs简介\n * 开发工具: WebStorm, VScode.\n * Node.js是一个javascript运行环境，它让JavaScript可以开发后端程序.\n * Nodejs是基于V8 JS引擎，V8 JS引擎是Google发布的开源JavaScript引擎,本身就是用于Chrome浏览器的JS解析部分.\n * Ryan Dahl把V8 JS引擎搬到了服务器上，用来做服务器的软件.\n * 基于 node.js 可以开发控制台程序（命令行程序、CLI程序）、桌面应用程序（GUI）（借助 node-webkit、electron 等框架实现）、Web 应用程序（网站）\n\n npm 官网: [https://www.npmjs.com/](https://www.npmjs.com/)\n\n * NodeJs语法完全是JS语法，打破了过去JavaScript只能在浏览器中运行的局面，前后端编程环境统一.\n\n * node.js 全栈开发技术栈: MEAN - MongoDB Express Angular Node.js\n\n### node.js 有哪些特点？\n\n> 1. 事件驱动(当事件被触发时，执行传递过去的回调函数)\n> 2. 非阻塞 I/O 模型（当执行I/O操作时，不会阻塞线程）磁盘I/O(文件I/O)，网络I/O(当向网络发送一些数据或接受一些数据也称为I/O)\n> 3. 单线程(V8引擎只有堆deep和一个调用栈stack, 之所以单线程又是非阻塞, 原因是在NodeJs底层开辟一个异步操作来做如写文件等,JS代码依然是单线程, Nodejs底层帮我们把文件写完后，把回调函数放到队列里, NodeJs主线程执行完栈里面函数后，发现队列里有回调函数，就取出来到栈里再执行)\n\t在线动画演示：http://latentflip.com/loupe\n> 4. 拥有世界最大的开源库生态系统 —— npm。\n\n### NodeJs超强的高并发能力\nJava、PHP、.net 等服务器端语言中，会为每个客户端创建一个新线程，每个线程需要耗费约2MB内存.  \n\n理论上，一个8GB内存的服务器可以同时连接的最大用户数为4000个左右，如果要实现进一步的高并发就需要增加服务器的数量，硬件成本就会上升.\n\nNode.js不为每个客户的连接创建一个新的线程，而仅仅使用一个线程，当用户连接就触发一个内部事件，通过非阻塞I/O、事件驱动机制\n\n让Node.js程序宏观上也是并行的. 使用Node.js, 一个8GB内存的服务器，可以同时处理超过4万用户的连接.\n\n总结: 同样的服务器配置，Node.js的并发量将近是传统的后端语言的10倍.\n\n### 实现高性能服务器\nV8 JavaScript并不局限于在浏览器中运行，Node.js将其用在了服务器中.\n\n该引擎使用C++语言开发的一种高性能JavaScript引擎，引擎内部使用一种全新的编译技术, 解析并执行JavaScript脚本语言\n\n意味着用JavaScript编写的语言可以有与低端c语言非常相近的执行效率,所以NodeJs是可以实现高性能服务器.\n\n\n#### 非阻塞\n\n``` js\nvar fs = require('fs');\nconsole.log('1');\nfs.readFile('my.json', function(err, data){\n\tconsole.log('2');\n})\nconsole.log('3');\n输出：\n1\n3\n2\n```\n\n#### 回调函数\n\n``` js\nvar fs=require('fs');\nfunction getmsg(callback){\n\tfs.readFile('my.json', function(err, data){\n\t\tcallback(data);\n\t})\n})\n\ngetmsg(function(data_result){\n\tconsole.log(data_result.toString());\n})\n\n```\n\n\n\n\n","categories":["language","nodejs"]},{"title":"Aanconda conda配置proxy下载软件","url":"/2021/03/13/language/python/Anaconda使用conda安装软件/","content":"\n## conda 下载软件\n\n配置公司proxy:\n```\nconda config --set proxy_servers.http http://<proxy>:port\nconda config --set proxy_servers.https http://<proxy>:port\n```\n<!-- more -->\n\n### pandoc 安装\n\n#### windows msi安装包进行安装\nThe simplest way to get the latest pandoc release is to use the installer.\n[Download the latest installer](https://github.com/jgm/pandoc/releases)\n\n#### 使用conda进行安装\npandoc 可以使用 conda 安装，关于 conda 的安装和使用可以参考前一篇文档 conda_pip_info。 使用如下命令安装 pandoc\n```\n$ conda install pandoc\n```\n会等待一段时间solving environment, 之后就提示下载了.\n\n安装完毕后，还需要安装 pandoc-xnos 插件，主要功能是图片、表格、公式等编号的索引，此软件包采用 python 编写，使用 pip 命令安装。\n\n```\n$ pip install pandoc-xnos\n```\n由于 pandoc 生成 PDF 文件需要使用 latex 工具，因此还需要安装 texlive 软件，推荐在 linux 平台直接使用 yum 或者 apt 命令安装。\n\n","categories":["language","python"]},{"title":"浏览器工作原理","url":"/2021/03/13/language/nodejs/浏览器工作原理/","content":"\n## 浏览器的组成\n- 人机交互部分（UI）\n- 网络请求部分（Socket）\n- JavaScript引擎部分（解析执行JavaScript）\n- 渲染引擎部分（渲染HTML、CSS等）\n- 数据存储部分（cookie、HTML5中的本地存储LocalStorage、SessionStorage）\n\nsqlite\n\n\n## 主流渲染引擎\n\n### 介绍\n1. 渲染引擎 又叫 排版引擎 或 浏览器内核。\n\n2. 主流的 渲染引擎 有\n  - **Chrome浏览器**: Blink引擎（WebKit的一个分支）。\n  - **Safari浏览器**: WebKit引擎，windows版本2008年3月18日推出正式版，但苹果已于2012年7月25日停止开发Windows版的Safari。\n  - **FireFox浏览器**: Gecko引擎。\n  - **Opera浏览器**: Blink引擎(早期版使用Presto引擎）。\n  - **Internet Explorer浏览器**: Trident引擎。\n  - **Microsoft Edge浏览器**: EdgeHTML引擎（Trident的一个分支）。\n\n\n### 工作原理\n1. 解析HTML构建Dom树（Document Object Model，文档对象模型），DOM 是W3C组织推荐的处理可扩展置标语言的标准编程接口。\n\n2. 构建*渲染树*，*渲染树*并不等同于*Dom树*，因为像`head标签 或 display: none`这样的元素就没有必要放到*渲染树*中了，但是它们在*Dom树*中。\n\n3. 对*渲染树*进行布局，定位坐标和大小、确定是否换行、确定position、overflow、z-index等等，这个过程叫`\"layout\" 或 \"reflow\"`。\n\n4. 绘制*渲染树*，调用操作系统底层API进行绘图操作。\n\n\n\n### 渲染引擎工作原理示意图\n\n**渲染引擎工作原理示意图**\n\n![渲染引擎工作原理](flow.png)\n\n\n**WebKit工作原理（Chrome、Safari、Opera）**\n\n![Blink渲染引擎工作原理](webkitflow.png)\n\n\n**Gecko工作原理（FireFox）**\n\n![Gecko渲染引擎工作原理](gecko.jpg)\n\n\n\n### 浏览器的 reflow 或 layout 过程\n\nhttps://www.youtube.com/watch?v=ZTnIxIA5KGw\n\n\n### 打开 Chrome 的 Rendering 功能\n\n第一步：\n\n![第一步](chrome_rendering1.png)\n\n第二步：\n\n![第二步](chrome_rendering2.png)\n\n## 浏览器访问网站过程\n\n> 1. 在浏览器地址栏中输入网址。\n\n![淘宝网址](taobao_url.png)\n\n> 2. 浏览器通过用户在地址栏中输入的URL构建HTTP请求报文。\n\n``` js\nGET / HTTP/1.1\nHost: www.taobao.com\nConnection: keep-alive\nUpgrade-Insecure-Requests: 1\nUser-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.133 Safari/537.36\nAccept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\nAccept-Encoding: gzip, deflate, sdch, br\nAccept-Language: zh-CN,zh;q=0.8,en;q=0.6\nCookie: l=Ag0NWp9E8X4hgaGEtIBhOmKxnSOH6kG8; isg=AkZGLTL-Yr9tHDZbgd5bsn4Rlzwg5IphaK-1BzBvMmlEM-ZNmDfacSyDfdgF; thw=cn\n```\n\n> 3. 浏览器发起DNS解析请求，将域名转换为IP地址。\n\n![淘宝网址](taobao_ip.png)\n\n> 4. 浏览器将请求报文发送给服务器。\n\n> 5. 服务器接收请求报文，并解析。\n\n> 6. 服务器处理用户请求，并将处理结果封装成HTTP响应报文。\n\n```js\nHTTP/1.1 200 OK\nServer: Tengine\nDate: Thu, 13 Apr 2017 02:24:25 GMT\nContent-Type: text/html; charset=utf-8\nTransfer-Encoding: chunked\nConnection: keep-alive\nVary: Accept-Encoding\nVary: Ali-Detector-Type, X-CIP-PT\nCache-Control: max-age=0, s-maxage=300\nVia: cache8.l2cm10-1[172,200-0,C], cache13.l2cm10-1[122,0], cache3.cn206[0,200-0,H], cache6.cn206[0,0]\nAge: 293\nX-Cache: HIT TCP_MEM_HIT dirn:-2:-2\nX-Swift-SaveTime: Thu, 13 Apr 2017 02:19:32 GMT\nX-Swift-CacheTime: 300\nTiming-Allow-Origin: *\nEagleId: 9903e7e514920502659594264e\nStrict-Transport-Security: max-age=31536000\nContent-Encoding: gzip\n\n<!DOCTYPE html>\n<html lang=\"zh-CN\">\n<head>\n<meta charset=\"utf-8\" />\n<meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\" />\n<meta name=\"renderer\" content=\"webkit\" />\n<title>淘宝网 - 淘！我喜欢</title>\n<meta name=\"spm-id\" content=\"a21bo\" />\n<meta name=\"description\" content=\"淘宝网 - 亚洲较大的网上交易平台，提供各类服饰、美容、家居、数码、话费/点卡充值… 数亿优质商品，同时提供担保交易(先收货后付款)等安全交易保障服务，并由商家提供退货承诺、破损补寄等消费者保障服务，让你安心享受网上购物乐趣！\" />\n<meta name=\"aplus-xplug\" content=\"NONE\">\n<meta name=\"keyword\" content=\"淘宝,掏宝,网上购物,C2C,在线交易,交易市场,网上交易,交易市场,网上买,网上卖,购物网站,团购,网上贸易,安全购物,电子商务,放心买,供应,买卖信息,网店,一口价,拍卖,网上开店,网络购物,打折,免费开店,网购,频道,店铺\" />\n</head>\n<body>\n......\n</body>\n</html>\n```\n\n> 7. 服务器将HTTP响应报文发送给浏览器。\n\n> 8. 浏览器接收服务器响应的HTTP报文，并解析。\n\n> 9. 浏览器解析 HTML 页面并展示，在解析HTML页面时遇到新的资源需要再次发起请求。\n\n> 10. 最终浏览器展示出了页面\n\n## HTTP请求报文和响应报文格式\n\n![http请求报文和响应报文](HTTPMsgStructure2.png)\n\n\n\n## DNS 解析过程\n\n![DNS解析过程](DNS.gif)\n\n\n### windows 下 hosts 文件位置\n\nC:\\Windows\\System32\\drivers\\etc\\hosts\n\n\n\n\n\n## DOM 解析\n\n参考代码:\n\n```html\n<html>\n  <body>\n    <p>Hello World</p>\n    <div> <img src=\"example.png\" alt=\"example\"/></div>\n  </body>\n</html>\n```\n\n![Dom 解析工作原理](dom.png)\n\n\n\n## Webkit CSS 解析\n\n![CSS 解析工作原理](css_parser.png)\n\n\n\n## How Browsers work - 浏览器是如何工作的\n\n[How Browsers work](http://taligarsiel.com/Projects/howbrowserswork1.htm#The_browsers_we_will_talk_about)\nhttps://www.html5rocks.com/zh/tutorials/internals/howbrowserswork/\n\n\n\n\n","categories":["language","nodejs"]},{"title":"Express简介","url":"/2021/03/13/language/nodejs/Express简介/","content":"\n### Express 是一个自身功能极简，完全由路由和中间件构成的一个web开发框架\n中间件: 就是匹配路由之前和匹配路由之后做的一系列操作.\n中间件是一个函数，可以访问请求对象，响应对象，和web应用中处理请求响应循环流程中的中间件，一般被命名为next的变量\n\n\n中间件功能包括:\n * 执行任何代码\n * 修改请求和响应对象\n * 终结请求，响应循环\n * 调用堆栈中的下一个中间件\n \n如果get，post回调函数中，没有next参数，那么就匹配第一个路由，就不会往下匹配. 如果往下匹配必须写next.\n\nExpress 应用可使用如下几种中间件:\n * 1. 应用级中间件\n * 2. 内置中间件\n * 3. 路由级中间件\n * 4. 错误处理中间件\n * 5. 第三方中间件\n\n### 例子:\n\n``` js\nvar express=require('express'); // 引入\nvar app=new express();\t\t\t// 实例化\n/* 1. 应用级中间件app.use(), 匹配所有路由,就是在匹配其它路由之前都需要执行的操作，多用在权限判断, 没登陆之前是否可登陆 或 是否有权限访问某些网页\n* 下面中间件: 表示匹配任何路由, 如再匹配其它路由之前打印下时间\n*/\napp.use(function(req, res, next){\n\tconsole.log(new Date());\n\t// next();\t\t// 缺少next, 匹配到此路由并打印出时间后浏览器头部观察到转圈圈不再往下匹配.\n\tif(req.session != Null){\n\t\tnext();\n\t}else\n\t{\n\t\tres.redir(\"/login\");\n\t}\n})\n\n/* 2. 内置中间件，托管静态页面, 匹配路由之前, 看public下面有没有匹配的文件, 如果有则返回，没有就继续向下匹配*/\n// 项目部分目录结构: project_name/public/css/style.css\n// 第一种访问方式: http://127.0.0.1:3000/css/style.css\napp.use(express.static('public'));\n// 第二种访问方式: http://127.0.0.1:3000/static/css/style.css\napp.use(\"/static\", express.static('public'));\n\n/* 3. 路由中间件app.get(): 匹配某个路由 */\napp.get(\"/\", function(req, res, next){\n\tconsole.log(\"您好express\");\n\tnext();\t\t\t\t\t\t// 缺少next(), 浏览器会一直转圈圈卡着等待服务器继续返回.\n});\napp.get(\"/\", function(req, res){\n\tres.send(\"您好express\");\t// 匹配到此路由后, res.send()后不需要再next, 浏览器会结束响应.\n});\n\n/* 1. 应用级中间件app.use()*/\napp.use(\"/news\", function(req, res, next){\n\tconsole.log(\"应用级中间件匹配news\");\n\tnext();\n})\napp.get(\"/news\", function(req, res){\n\tres.send(\"news\");\n});\n\n/* 4. 错误处理中间件, 如果上面路由都没有匹配到，404*/\napp.use(function(req, res){\n\tres.status(404).send(\"这是404，路由没有匹配到\");\n})\n\n/* 5. 第三方中间件就是官方或者其他人好的中间件，访问https://npmjs.com搜索下载 */ \n/* \n* 不用中间件获取post提交数据: req.on(); 用中间件body-parser获取post提交的数据.\n* 一: 安装: npm install body-parser --save\n* 二: 引用: var bodyParser = require('body-parser');\n* 三：设置中间件\n*\t// parse application/x-www-form-urlencoded\n*\tapp.use(bodyParser.urlencoded({extended: false}))\n*\t//parse application/json\n*\tapp.use(bodyParser.json())\n* 四: req.body 获取post提交的数据\n*/\n\n// 二：引入body-parser中间件\nvar bodyParser = require('body-parser');\n// parse application/x-www-form-urlencoded\n// 三：配置body-parser中间件\napp.use(bodyParser.urlencoded({extended: false}))\n//parse application/json\napp.use(bodyParser.json())\n\n// ejs, 1.安装(package.json)，2.如下配置 ejs引擎\n// 默认ejs加载的是views下面的视图, 如login.ejs\napp.set('view engine', 'ejs');\napp.get('/login', function(req, res){\n\tres.render('login');\n})\n/*\tlogin.ejs文件路径: project_name/views/login.ejs, 内容含有如下表单\n* <form action=\"doLogin\" method=\"post\">\n*\t用户名: <input type=\"text\" name=\"username\"/><br/>\n*\t密  码: <input type=\"password\" name=\"password\"><br/>\n*\t<input type=\"submit\" value=\"登陆\"/>\n* </form>\n*/\n// 四：使用req.body 获取post提交的数据 \napp.post(\"/doLogin\", function(req, res){\n\tconsole.log(req.body);\n})\n```","categories":["language","nodejs"]},{"title":"STL 02 泛型编程","url":"/2021/03/13/language/cpp/STL/02.泛型编程/","content":"\n### **max**\nC++标准库里有max, 所以定义函数时候最好要与max不一样.\n\n```cpp\n\t#include <iostream>\n\tusing namespace std;\n\t\n\t/**\n\t * T 是一个占位符，更确切的说是一个类型占位符。也就是说，将来在 T 这个位置上的是一个真实、具体的数据类型，至于到底是哪个类型，完全取决于用户的需求.\n\t * 如果硬要给 T 这种类型占位符也叫做一种数据类型，提供这种想法的发明者称它为泛型（generic type），而使用这种类型占位符的编程方式就被称为泛型编程.\n\t * 值得一提的是，既然泛型并不是真实的数据类型，那么使用泛型编写的代码也就不是真正的程序实体，只能算是一个程序实体的样.\n\t * 故此，通常形象的将这种使用了泛型的代码称为模板，由模板生成实际代码的过程称为模板的具体实现.\n\t */\n\ttemplate <typename T>\n\tT my_max(T x, T y)\n\t{\n\t    return (x > y) ? x : y;\n\t}\n\t\n\tint main()\n\t{\n\t    double n = 3.3, m = 6.6;\n\t    double result = my_max(n, m);\n\t    cout << result << endl;\n\t    system(\"pause\");\n\t    return 0;\n\t}\n```\n","categories":["language","cpp"]},{"title":"STL 03 shared_ptr","url":"/2021/03/13/language/cpp/STL/03.shared_ptr/","content":"\n## shared_ptr\n\nReference Link: https://jingyan.baidu.com/article/adc81513d2c217f723bf7327.html\n\n### 实例1\n\n```cpp\n\t#include <iostream>\n\t#include <memory>\n\t\n\tusing namespace std;\n\t\n\tint main(int argc, char *argv[])\n\t{\n\t\n\t    /**\n\t     * shared_ptr智能指针使用方法:\n\t     * std::shared_ptr<Entity> e1(new Entity());\n\t     * std::shared_ptr<Entity> e1 = std::make_shared<Entity>();\n\t     */\n\t\n\t    auto ptr = std::make_shared<char *>();\n\t    std::shared_ptr<char *> ptr1 = std::make_shared<char *>();\n\t    *ptr1 = \"Hello world 1\";\n\t    cout << *ptr1 << endl;\n\t\n\t    std::shared_ptr<char *> ptr2(new char *());\n\t    *ptr2 = \"Hello world 2\";\n\t    cout << *ptr2 << endl;\n\t\n\t    { // 离开此作用域后ptr3自动销毁\n\t        auto ptr3 = ptr2;\n\t        cout << *ptr3 << endl;\n\t        cout << ptr2.use_count() << endl;\n\t    }\n\t    cout << ptr2.use_count() << endl;\n\t\n\t    return 0;\n\t}\n```\n\n### 实例2\n\n```cpp\n\t#include <iostream>\n\t#include <cstring>\n\t#include <memory>\n\t\n\tusing namespace std;\n\t\n\tint main()\n\t{\n\t    char* str = (char *)\"Hello\";\n\t    cout << \"strlen of str:\" << strlen(str) << endl;\n\t    cout << \"size   of str:\" << sizeof(str) << endl;\n\t    char* s = new char[strlen(str) + 1];\n\t    s = str;\n\t    cout << s << endl;\n\t\n\t    std::shared_ptr<char> ptr(new char[strlen(str) + 1](), [](char *p) { delete [] p; });\n\t\n\t    // 第一种, 字符串给shared_ptr赋值\n\t    for (int i = 0; i < strlen(str); i++)\n\t    {\n\t        *(ptr.get() + i) = str[i];\n\t    }\n\t\n\t    // 第二种, 字符串给shared_ptr赋值\n\t    strcpy(ptr.get(), str);\n\t\n\t    // 第一种, 输出字符串\n\t    cout << ptr << endl;\n\t\n\t    // 第二种, 输出字符串\n\t    cout << ptr.get() << endl;\n\t\n\t    // 第三种, 输出字符串\n\t    for (int i = 0; i < strlen(str); i++)\n\t    {\n\t        cout << *(ptr.get() + i) << \" \";\n\t    }\n\t    cout << endl;\n\t\n\t    // 输出一个字符\n\t    cout << *str << \",\" << *ptr << \",\" << *ptr.get() << endl;\n\t\n\t    std::shared_ptr<int> p1 = std::make_shared<int>();\n\t    *p1 = 78;\n\t    cout << \"p1=\" << *p1 << endl;\n\t\n\t    return 0;\n\t}\n```\n\n**编译**\n\n```shell\n\tg++ tt.cpp\n```\n\n**输出**\n\n```shell\n\t[root@laboratory test]# ./a.out\n\tstrlen of str:5\n\tsize   of str:8\n\tHello\n\tHello\n\tHello\n\tH e l l o\n\tH,H,H\n\tp1=78\n```\n","categories":["language","cpp"]},{"title":"The thread pool of c","url":"/2021/03/13/language/c/thread-pool/","content":"\n## 线程池\n\n线程池(thread pool)技术是指能够保证所创建的任一线程都处于繁忙状态，而不需要频繁地为了某一任务而创建和销毁线程，因为系统在创建和销毁线程时所耗费的cpu资源很大。如果任务很多，频率很高，为了单一一个任务而起线程而后销线程，那么这种情况效率相当低下的。线程池技术就是用于解决这样一种应用场景而应运而生的。\n<!-- more -->\n\n**线程池技术的工作原理：**在起先就创建一定数量的线程以队列形式存在，并为其分配一个工作队列，当工作队列为空时，表示没有任务，此时所有线程挂起等待新的工作到来。当新的工作到来时，线程队列头开始执行这个任务，然后依次是第二、第三个线程执行新到来的任务，当其中某个线程处理完任务后，那么该线程立马开始接受任务分派，从而让所有线程都处于忙碌的状态，提高并行处理效率。\n\n**线程池技术是一种典型的生产者-消费者模型。**因此，无论用哪种语言实现，只要遵循其原理本身就能够很好的工作了。那么实现线程池技术我们需要考虑到哪些技术性的问题？\n\n**C语言线程池技术的实现：**\n\n### 需要考虑的技术问题一，线程池应该包含哪些成员变量。\n\n1. 既然要开一定数量的线程，那么这个**`“一定数量(max_thread_num)”`**必定是线程池的一个成员。\n\n2. 如何表示一个线程池是否已经关闭？如果关闭那么必需要立马释放资源。所以**`“是否关闭(shutdown)”`**也是一个成员。\n\n3. 创建线程需要有**`id`**，必需要为每个线程准备一个id，所以需要一个id数组，其长度就是max_thread_num。\n\n4. 线程锁，用以保证对线程操作时的互斥性。所以需要一个锁,**`queue_lock`**。\n\n5. **`条件变量(condition_variable)`**，这里使用条件变量主要是为了广播任务到来的消息给所有线程。当有处于空闲的线程，则由此线程\n\n6. 接受任务分派。所以需要一个条件变量**`queue_ready`**。\n\n7. 最为重要的就是任务本身，也就是工作。那么工作本身又需要哪几个成员变量？首先肯定是任务入口，**`routine`**函数；\n\n8. 其次是routine函数的参数**`args`**；再次任务是以队列存在着的，所以任务本身应该包含一个**`next`**。\n\n\n\n### 需要考虑的技术问题二，线程池应该包含哪些api。\n\n一、创建线程池，create_tpool\n\n二、销毁线程池，destroy_tpool\n\n三、分派任务，add_task_2_tpool\n\n基于上述分析，我们可以先构造头文件。\n\ntpool.h\n\n``` c\n#ifndef T_POOL\n#define T_POOL\n \n#include <pthread.h>\n#include <ctype.h>\n \ntypedef struct tpool_work{\n   void* (*work_routine)(void*); //function to be called\n   void* args;                   //arguments \n   struct tool_work* next;\n}tpool_work_t;\n \ntypedef struct tpool{\n   size_t               shutdown;       //is tpool shutdown or not, 1 ---> yes; 0 ---> no\n   size_t               maxnum_thread; // maximum of threads\n   pthread_t            *thread_id;     // a array of threads\n   tpool_work_t*        tpool_head;     // tpool_work queue\n   pthread_cond_t       queue_ready;    // condition varaible\n   pthread_mutex_t      queue_lock;     // queue lock\n}tpool_t;\n \n \n/***************************************************\n*@brief:\n*       create thread pool\n*@args:   \n*       max_thread_num ---> maximum of threads\n*       pool           ---> address of thread pool\n*@return value: \n*       0       ---> create thread pool successfully\n*       othres  ---> create thread pool failed\n***************************************************/\n \nint create_tpool(tpool_t** pool,size_t max_thread_num);\n \n/***************************************************\n*@brief:\n*       destroy thread pool\n*@args:\n*        pool  --->  address of pool\n***************************************************/\nvoid destroy_tpool(tpool_t* pool);\n \n/**************************************************\n*@brief:\n*       add tasks to thread pool\n*@args:\n*       pool     ---> thread pool\n*       routine  ---> entry function of each thread\n*       args     ---> arguments\n*@return value:\n*       0        ---> add ok\n*       others   ---> add failed        \n**************************************************/\nint add_task_2_tpool(tpool_t* pool,void* (*routine)(void*),void* args);\n \n#endif//tpool.h\n```\n### 需要考虑的技术问题三，线程池的所有权应该交予谁。\n\n这里我们需要考虑到，将线程池封装成一个so库是比较好的想法，那么，线程池的所有权就应该交予调用它的函数。所以我这里采取的就是这个方法。\n\ntpool.c\n\n``` c\n#include \"tpool.h\"\n#include <unistd.h>\n#include <errno.h>\n#include <string.h>\n#include <stdlib.h>\n#include <stdio.h>\n \n \nstatic void* work_routine(void* args)\n{\n   tpool_t* pool = (tpool_t*)args;\n   tpool_work_t* work = NULL;\n \n   while(1){\n        pthread_mutex_lock(&pool->queue_lock);\n        while(!pool->tpool_head && !pool->shutdown){ // if there is no works and pool is not shutdown, it should be suspended for being awake\n            pthread_cond_wait(&pool->queue_ready,&pool->queue_lock);\n        }\n \n        if(pool->shutdown){\n           pthread_mutex_unlock(&pool->queue_lock);//pool shutdown,release the mutex and exit\n           pthread_exit(NULL);\n        }\n \n        /* tweak a work*/\n        work = pool->tpool_head;\n        pool->tpool_head = (tpool_work_t*)pool->tpool_head->next;\n        pthread_mutex_unlock(&pool->queue_lock);\n \n        work->work_routine(work->args);\n \n        free(work);\n   }\nreturn NULL;\n}\n \nint create_tpool(tpool_t** pool,size_t max_thread_num)\n{\n   (*pool) = (tpool_t*)malloc(sizeof(tpool_t));\n   if(NULL == *pool){\n        printf(\"in %s,malloc tpool_t failed!,errno = %d,explain:%s\\n\",__func__,errno,strerror(errno));\n        exit(-1);\n   }\n   (*pool)->shutdown = 0;\n   (*pool)->maxnum_thread = max_thread_num;\n   (*pool)->thread_id = (pthread_t*)malloc(sizeof(pthread_t)*max_thread_num);\n   if((*pool)->thread_id == NULL){\n        printf(\"in %s,init thread id failed,errno = %d,explain:%s\",__func__,errno,strerror(errno));\n        exit(-1);\n   }\n   (*pool)->tpool_head = NULL;\n   if(pthread_mutex_init(&((*pool)->queue_lock),NULL) != 0){\n        printf(\"in %s,initial mutex failed,errno = %d,explain:%s\",__func__,errno,strerror(errno));\n        exit(-1);\n   }\n \n   if(pthread_cond_init(&((*pool)->queue_ready),NULL) != 0){\n        printf(\"in %s,initial condition variable failed,errno = %d,explain:%s\",__func__,errno,strerror(errno));\n        exit(-1);\n   }\n \n   for(int i = 0; i < max_thread_num; i++){\n        if(pthread_create(&((*pool)->thread_id[i]),NULL,work_routine,(void*)(*pool)) != 0){\n           printf(\"pthread_create failed!\\n\");\n           exit(-1);\n        }\n   }\nreturn 0;\n}\n \nvoid destroy_tpool(tpool_t* pool)\n{\n   tpool_work_t* tmp_work;\n \n   if(pool->shutdown){\n        return;\n   }\n   pool->shutdown = 1;\n \n   pthread_mutex_lock(&pool->queue_lock);\n   pthread_cond_broadcast(&pool->queue_ready);\n   pthread_mutex_unlock(&pool->queue_lock);\n \n   for(int i = 0; i < pool->maxnum_thread; i++){\n        pthread_join(pool->thread_id[i],NULL);\n   }\n   free(pool->thread_id);\n   while(pool->tpool_head){\n        tmp_work = pool->tpool_head;\n        pool->tpool_head = (tpool_work_t*)pool->tpool_head->next;\n        free(tmp_work);\n   }\n \n   pthread_mutex_destroy(&pool->queue_lock);\n   pthread_cond_destroy(&pool->queue_ready);\n   free(pool);\n}\n \nint add_task_2_tpool(tpool_t* pool,void* (*routine)(void*),void* args)\n{\n   tpool_work_t* work,*member;\n \n   if(!routine){\n        printf(\"rontine is null!\\n\");\n        return -1;\n   }\n \n   work = (tpool_work_t*)malloc(sizeof(tpool_work_t));\n   if(!work){\n        printf(\"in %s,malloc work error!,errno = %d,explain:%s\\n\",__func__,errno,strerror(errno));\n        return -1;\n   }\n \n   work->work_routine = routine;\n   work->args = args;\n   work->next = NULL;\n \n   pthread_mutex_lock(&pool->queue_lock);\n   member = pool->tpool_head;\n   if(!member){\n        pool->tpool_head = work;\n   }\n   else{\n        while(member->next){\n           member = (tpool_work_t*)member->next;\n        }\n        member->next = work;\n   }\n \n   //notify the pool that new task arrived!\n   pthread_cond_signal(&pool->queue_ready);\n   pthread_mutex_unlock(&pool->queue_lock);\nreturn 0;\n}\n```\ndemo.c\n\n``` c\n#include \"tpool.h\"\n#include <stdio.h>\n#include <unistd.h>\n#include <time.h>\n \nvoid* fun(void* args)\n{\n   int thread = (int)args;\n   printf(\"running the thread of %d\\n\",thread);\nreturn NULL;\n}\n \nint main(int argc, char* args[])\n{\n   tpool_t* pool = NULL;\n   if(0 != create_tpool(&pool,5)){\n        printf(\"create_tpool failed!\\n\");\n        return -1;\n   }\n \n   for(int i = 0; i < 1000; i++){\n        add_task_2_tpool(pool,fun,(void*)i);\n   }\n   sleep(2);\n   destroy_tpool(pool);\nreturn 0;\n}\n```\n\nMakefile\n\n``` \npool: tpool.c demo.c\n        gcc tpool.c demo.c -o pool -lpthread -std=c99 \n \nclean:\n        rm ./pool\n```\n\n\n","categories":["language","c"]},{"title":"numpy 基本操作","url":"/2021/03/13/MLearning/numpy/numpy/","content":"\n## numpy基本操作\n\n## 属性与赋值\n\n<!-- more -->\n\n```python\nimport os\nimport numpy as np\n\narray = [1, 2, 3, 4, 5]\n\n# ndarray类型的数组是numpy最底层最基本的数据结构\narray1 = np.array(array)\nprint(type(array1))  # 输出: <class 'numpy.ndarray'>\n\nprint(array1.dtype)  # 输出: int64\n\nprint(array1.size)\n# 或者print(np.size(array1)) # 输出: 5, 一共有5个元素\n\nprint(array1.itemsize)  # 输出: 8, 每个元素占8个字节\n\nprint(array1.ndim)  # 输出: 1, 数组维度\n\narray1.fill(0)\nprint(array1)  # 输出: [0 0 0 0 0], 所有元素填充0\n```\n\n## numpy数组类型\n\n```python\n# ***************************同一类型*******************************\n# 元素必须是同一类型, 如果不是会自动向下转换\narray0 = np.array([1, 2, 3.1, 4, 5])\nprint(array0)  # 输出: [1.  2.  3.1 4.  5. ]\n\narray0 = np.array([1, 2, 3, 4, \"5\"])\nprint(array0)  # 输出: ['1' '2' '3' '4' '5']\n\narray0 = np.array([1, 2, 3, 4, '5'])\nprint(array0)  # 输出: ['1' '2' '3' '4' '5']\n\n# ***************************不同类型*******************************\n\narray0 = np.array([1, 2, 3, 4, 5, \"str\"], dtype=np.object)\nprint(array0)  # [1 2 3 4 5 'str']\nprint(array0 * 2)  # [2 4 6 8 10 'strstr']\n\n\n# ***************************类型转换*******************************\narray0 = np.array([1, 2, 3, 4, 5])\nprint(array0.astype(np.float32))  # [1. 2. 3. 4. 5.], 不会改变原来数组类型\nprint(array0)  # [1 2 3 4 5]\n\n# ***************************索引和切片截取*******************************\narray1 = np.array([1, 2, 3, 4, 5])\nprint(array1[1:3])  # 输出: [2 3], 包括左边索引值, 不包括右边索引值\nprint(array1[-2:])  # 输出: [4 5], \":\"后边不写表示取后边所有索引值\n```\n\n## 数组的生成\n\n```\narray01 = np.arange(10)\nprint(array01)  # [0 1 2 3 4 5 6 7 8 9]\narray01 = np.arange(2, 20, 2)  # 从2开始到20结束(不包含20), [2, 20), 每个元素数值间隔2的大小\nprint(array01)  # [ 2  4  6  8 10 12 14 16 18]\n\narray01 = np.arange(2, 20, 2, dtype=np.float32)\nprint(array01)  # [ 2.  4.  6.  8. 10. 12. 14. 16. 18.]\n\narray01 = np.linspace(0, 10, 10)  # 从0开始到10结束(包含10), [0,10], 等间隔包含10个元素\nprint(array01)\n\"\"\"\n[ 0.          1.11111111  2.22222222  3.33333333  4.44444444  5.55555556\n  6.66666667  7.77777778  8.88888889 10.        ]\n\"\"\"\n\narray01 = np.logspace(0, 1, 5)  # 从0开始到1结束(包含1), [0,1], 以10为底, 等间隔5个元素\nprint(array01)\n\"\"\"\n[ 1.          1.77827941  3.16227766  5.62341325 10.        ]\n\"\"\"\n\narray01 = np.zeros(3)\nprint(array01)  # [0. 0. 0.]\narray01 = np.zeros((3, 3))\nprint(array01)\n\"\"\"\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\"\"\"\narray01 = np.ones((3, 3))\nprint(array01)\n\"\"\"\n[[1. 1. 1.]\n [1. 1. 1.]\n [1. 1. 1.]]\n\"\"\"\narray01 = np.ones((3, 3)) * 9\nprint(array01)\n\"\"\"\n[[9. 9. 9.]\n [9. 9. 9.]\n [9. 9. 9.]]\n\"\"\"\n\n# 构建与原多维数组一样维度的数组\na = np.ones((3, 3))\nb = np.ones_like(a)\nb = np.zeros_like(a)\nprint(b)\n\"\"\"\n[[0. 0. 0.]\n [0. 0. 0.]\n [0. 0. 0.]]\n\"\"\"\n\n# 构建对角矩阵\n\narray01 = np.identity(5)\nprint(array01)\n\"\"\"\n[[1. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 1. 0. 0.]\n [0. 0. 0. 1. 0.]\n [0. 0. 0. 0. 1.]]\n\"\"\"\n```\n\n\n\n## numpy 数学运算\n\n### 加减乘法运算\n\n```python\n# ***************************数学运算*******************************\narray1 = np.array(array)\narray2 = array1 + 1\nprint(array2)  # 输出: [2 3 4 5 6]\n\n# 数组相加, 位数必须相同, 相同位置的元素进行相加\narray3 = array2 + array1\nprint(array3)  # 输出: [ 3  5  7  9 11]\n\n# 数组相乘, 位数必须相同, 对应位置元素进行乘法运算\narray4 = array2 * array3 # 与下面等价\narray4 = np.multiply(array2, array3)\nprint(array4)  # 输出: [ 6 15 28 45 66]\n\nprint(array4[0])  # 输出: 6\n\nprint(array4.shape)  # 输出: (5,)\n\narray5 = np.array([[1, 2, 3], [4, 5, 6]])\nprint(array5)\nprint(array5.shape)\n\"\"\"输出:\n[[1 2 3]\n [4 5 6]]\n(2, 3)  --> 表示2行, 3列\n\"\"\"\narray6 = np.array([[1, 2, 3],\n                   [3, 4, 6],\n                   [7, 8, 9]])\nprint(array6.shape)  # 输出: (3, 3), 3行, 3列\nprint(array6.ndim)  # 输出: 2, 2维数组\nprint(array6.size)  # 输出: 9, 总共有9个元素\nprint(array6)\n\"\"\"输出\n[[1 2 3]\n [3 4 6]\n [7 8 9]]\n\"\"\"\nprint(array6[1, 1])  # 4\narray6[1, 1] = 10\nprint(array6[1, 1])  # 10\nprint(array6[1])  # [ 3 10  6]\n\nprint(array6[:, 1])  # [ 2 10  8]\n```\n\n### 内积运算\n各个元素想乘再求和\n\n```\nx = np.array([2, 2])\ny = np.array([5, 5])\nprint(np.dot(x, y)) # 20\n\nx.shape = 2, 1\ny.shape = 1, 2\nprint(np.dot(x, y))\n\"\"\"\n[[10 10]\n [10 10]]\n\"\"\"\n```\n\n\n### 与或非运算\n\n#### 逻辑与\n```\nx = np.array([1, 0, 1, 4])\ny = np.array([1, 1, 1, 2])\nprint(x == y) # [ True False  True False]\n\nprint(np.logical_and(x, y))  # [ True False  True  True]\n```\n\n#### 逻辑或\n```\nprint(np.logical_or(x, y))  # [ True  True  True  True]\n```\n\n#### 逻辑非\n\n```\nprint(np.logical_not(x, y))  # [0 1 0 0]\n```\n\n\n## 浅拷贝与深拷贝\n\n```python\n# ***************************浅拷贝与深拷贝*******************************\n\"\"\"\n基本数据类型的特点：直接存储在栈(stack)中的数据.  \n引用数据类型的特点：存储的是该对象在栈中引用，真实的数据存放在堆内存里.  \n引用数据类型在栈中存储了指针，该指针指向堆中该实体的起始地址。当解释器寻找引用值时，会首先检索其在栈中的地址，取得地址后从堆中获得实体.  \n浅拷贝只复制指向某个对象的指针，而不复制对象本身，新旧对象还是共享同一块内存。但深拷贝会另外创造一个一模一样的对象，新对象跟原对象不共享内存，修改新对象不会改到原对象.  \n\"\"\"\n\n\"\"\"\n深拷贝, 堆中创建一模一样对象, 栈中创建新的引用\n\"\"\"\narray7 = array.copy()\narray7[0] = 100\nprint(array7)  # [100, 2, 3, 4, 5], 改变新对象的值\nprint(array)  # [1, 2, 3, 4, 5], 原对象没有改变\n\n\"\"\"\n浅拷贝, 栈中创建新的引用指向堆中原对象\n\"\"\"\narray8 = array\narray8[0] = 100\nprint(array8)  # [100, 2, 3, 4, 5], 改变原对象的值\nprint(array)  # [100, 2, 3, 4, 5]\n```\n\n## 数据索引\n\n```python\n# ***************************数据索引*******************************\n\"\"\"\nnp.arange(firstIndex, lastIndex(not included), interval)\n\"\"\"\ntang_array = np.arange(0, 100, 10)\nprint(tang_array)  # [ 0 10 20 30 40 50 60 70 80 90]\n\nmask = np.array([0, 0, 1, 1, 1, 0, 0, 1, 1, 1], dtype=bool)\nprint(mask)  # [False False  True  True  True]\n\nprint(tang_array[mask])  # [20 30 40 70 80 90]\n\nprint(os.path.abspath(\".\"))\n\nrandom_array = np.random.rand(10)\n# [0.90343471 0.40121791 0.3557191  0.84104087 0.1185995  0.64309389 0.49834734 0.19281528 0.54356076 0.85265315]\nprint(random_array)\nmask = random_array > 0.5\n# [ True False False  True False  True False False  True  True]\nprint(mask)\n# [0.90343471 0.84104087 0.64309389 0.54356076 0.85265315]\nprint(random_array[mask])\n\nmask = np.where(random_array > 0.5)\nprint(mask)  # (array([0, 3, 5, 8, 9]),)\n# [0.90343471 0.84104087 0.64309389 0.54356076 0.85265315]\nprint(random_array[mask])\n```\n\n## 数值计算\n\n```python\n# ***************************数值计算*******************************\n# *******相加*********\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\nprint(np.sum(tang_array))  # 21, 所有数值相加\nprint(np.sum(tang_array, axis=0))  # [5 7 9], 竖着相加\n# print(tang_array.sum(axis=0))\nprint(np.sum(tang_array, axis=1))  # [ 6 15], 横着相加\n# print(tang_array.sum(axis=1))\nprint(tang_array.ndim)  # 2, 2维数组\n\n# *******相乘*********\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\nprint(np.prod(tang_array, axis=0))  # [ 4 10 18], 竖着相乘\nprint(np.prod(tang_array, axis=1))  # [  6 120], 横着相乘\n\n# *******取最小值, 最大值, 平均值, 标准差, 方差*********\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\nprint(np.min(tang_array))  # 1\nprint(np.min(tang_array, axis=0))  # [1 2 3], 竖着最小值\nprint(np.min(tang_array, axis=1))  # [1 4], 横着最小值\n\nprint(np.max(tang_array))  # 6\nprint(np.mean(tang_array))  # 3.5\n\nprint(np.std(tang_array))  # 标准差, 1.707825127659933\nprint(np.std(tang_array, axis=0))  # 标准差, 竖着算, [1.5 1.5 1.5]\nprint(np.var(tang_array))  # 方差, 2.9166666666666665\n\n# *******取最小值索引位置*********\nprint(np.argmin(tang_array))\nprint(np.argmin(tang_array, axis=0))  # [0 0 0]\nprint(np.argmin(tang_array, axis=1))  # [0 0]\n\n\n# ********截取某个范围数据********\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\n# 截取2~4之间的数值\nprint(np.clip(tang_array, 2, 4))\n# print(tang_array.clip(2, 4))\n\"\"\"输出\n[[2 2 3]\n [4 4 4]]\n\"\"\"\n\n# ********四舍五入, 保留1位有效值********\ntang_array = np.array([1.2, 3.56, 6.4])\nprint(tang_array.round())  # [1. 4. 6.]\nprint(tang_array.round(1))  # [1.2 3.6 6.4]\nprint(tang_array.round(decimals=1))  # [1.2 3.6 6.4]\n```\n## 排序\n\n```python\n# ********排序********\ntang_array = np.array([[1.5, 1.3, 7.5],\n                       [5.6, 7.8, 1.2]])\nprint(np.sort(tang_array))\nprint(np.sort(tang_array, axis=1))\n\"\"\"输出\n[[1.3 1.5 7.5]\n [1.2 5.6 7.8]]\n\"\"\"\nprint(np.argsort(tang_array))\n\"\"\"排序完后的索引\n[[1 0 2]\n [2 0 1]]\n\"\"\"\n\n# 从[0~10]之间等间隔取10个数据, 包括0和10\ntang_array = np.linspace(0, 10, 10)\n# [ 0. 1.11111111  2.22222222  3.33333333  4.44444444  5.55555556  6.66666667  7.77777778  8.88888889 10. ]\nprint(tang_array)\nvalues = np.array([2.5, 6.5, 9.5])\n# 只返回搜索插入的位置, 原数组不变\nprint(np.searchsorted(tang_array, values))  # [3 6 9]\n\n# ********不同行或列同时进行排序********\ntang_array = np.array([[1, 0, 6],\n                       [1, 7, 0],\n                       [2, 3, 1],\n                       [2, 4, 0]])\nprint(tang_array)\n# 数组第一列按照从大到小排列, 之后第三列再按照从小到大排列, 总共排列了两次\nindex = np.lexsort([-1*tang_array[:, 0], tang_array[:, 2]])\nprint(index)  # [3 1 2 0], 返回lexsort第一个参数排序后index值\nprint(tang_array[index])\n\"\"\"输出\n[[2 4 0]\n [1 7 0]\n [2 3 1]\n [1 0 6]]\n\"\"\"\n```\n\n## 数组形状\n```\ntang_array = np.arange(10)\nprint(tang_array)  # [0 1 2 3 4 5 6 7 8 9]\nprint(tang_array.shape)  # (10,), 一维数组, 上面一维数组输出可以与下面的二维数组输出形式对比下\ntang_array.shape = 2, 5  # 元素个数要保持一致.\nprint(tang_array)\n\"\"\" 输出:\n[[0 1 2 3 4]\n [5 6 7 8 9]]\n\"\"\"\ntang_array = np.arange(10)\ntang_array = tang_array.reshape(2, 5)\nprint(tang_array)  # reshape 原数组没有改变\n\"\"\"\n[[0 1 2 3 4]\n [5 6 7 8 9]]\n\"\"\"\n```\n\n### 给数组加轴和去掉轴\n\n```\ntang_array = np.arange(10)\ntang_array = tang_array[np.newaxis, :]\nprint(tang_array)  # [[0 1 2 3 4 5 6 7 8 9]]\nprint(tang_array.shape)  # (1, 10), 二维数组, 表示一行十列, 一个样本, 十个元素特征值\n\ntang_array = np.arange(10)\ntang_array = tang_array[:, np.newaxis]\nprint(tang_array)\n\"\"\" 输出\n[[0]\n [1]\n [2]\n [3]\n [4]\n [5]\n [6]\n [7]\n [8]\n [9]]\n\"\"\"\nprint(tang_array.shape)  # (10, 1), 二维数组, 表示十行一列, 十个样本, 每个样本一个元素特征值\n\ntang_array = tang_array[:, np.newaxis, np.newaxis]\nprint(tang_array.shape)  # (10, 1, 1, 1)\ntang_array = tang_array.squeeze()  # 压缩多余的轴\nprint(tang_array.shape)  # (10,)\n```\n\n### 矩阵的转置\n```\ntang_array = tang_array.reshape(2, 5)\n#tang_array = tang_array.transpose(), 下面方式更简单\ntang_array = tang_array.T\nprint(tang_array)\n\"\"\" 输出：\n[[0 5]\n [1 6]\n [2 7]\n [3 8]\n [4 9]]\n\"\"\"\n```\n\n### 多维数组的连接和拉长\n\n```\na = np.array([[1, 2], [3, 4]])\nb = np.array([[5, 6], [7, 8]])\nc = np.concatenate((a, b))\nc = np.concatenate((a, b), axis=0)  # b作为其它样本拼接在a样本下面\nc = np.vstack((a, b))  # 等价于上面\nprint(c)\n\"\"\"输出：\n[[1 2]\n [3 4]\n [5 6]\n [7 8]]\n\"\"\"\n\n\nc = np.concatenate((a, b), axis=1)  # b作为a每个样本其它元素特征拼接在每个样本后面\nc = np.hstack((a, b))  # 等价于上面\nprint(c)\n\"\"\"输出：\n[[1 2 5 6]\n [3 4 7 8]]\n\"\"\"\n\nd = c.flatten()  # 把多维数组拉长成一维数组\nprint(d)  # [1 2 5 6 3 4 7 8]\n```\n\n\n## 随机模块\n\n```\n1. print(np.random.rand())  # 0.9300429551853078\n   print(np.random.random_sample()) # 0.6256746035165891\n\n2. rand_result = np.random.rand(3, 2)\nprint(rand_result)\n\"\"\" 输出：\n[[0.100757   0.38940102]\n [0.82948206 0.72059258]\n [0.8995502  0.33566056]]\n\"\"\"\n\n3. rand_result = np.random.randint(10, size=(2, 3), dtype=int)\nprint(rand_result)\n\"\"\" 输出：\n[[2 5 6]\n [1 3 3]]\n\"\"\"\n\nrand_result = np.random.randint(0, 10, 3) # 从0到10取三个随机数\nprint(rand_result) # [0 1 5]\n```\n\n### 高斯分布\n\n```\nnp.set_printoptions(precision=2) # 设置打印数据保留两位有效数据\nmu, sigma = 0, 0.1  # 平均值和sigma\ngaosifenbu = np.random.normal(mu, sigma, 10)\nprint(gaosifenbu)\n\"\"\" 输出：\n[ 0.11 -0.1   0.13  0.04 -0.04  0.07  0.1  -0.09 -0.13  0.02]\n\"\"\"\n```\n\n### 洗牌操作\n\n```\ntang_array = np.arange(10)\nprint(tang_array) # [0 1 2 3 4 5 6 7 8 9]\nnp.random.shuffle(tang_array) # 洗牌操作, 变为乱序\nprint(tang_array) # [6 4 7 9 1 0 2 5 8 3]\n```\n\n### 随机种子\n\n使得每次随机得到的数据都相同\n\n```\nnp.random.seed(100)\nnp.set_printoptions(precision=2)\nmu, sigma = 0, 0.1\nrand_result = np.random.normal(mu, sigma, 10)\nprint(rand_result) # [-0.17  0.03  0.12 -0.03  0.1   0.05  0.02 -0.11 -0.02  0.03]\ntang_array = np.arange(10)\nnp.random.shuffle(tang_array)\nprint(tang_array) # [9 7 1 8 5 4 3 2 6 0]\n```\n\n## 读写模块\n\n改变当前目录\n```\nimport os\nos.chdir(\"/home/***/learn/python/numpy\")\nprint(os.getcwd()) # /home/***/learn/python/numpy\n```\n\n创建tang.txt\n```\n$ cat tang.txt\n1 2 3 4 5 6\n2 3 5 7 8 9\n```\n\n### python读取文件写法\n```\ndata = []\n\nwith open('tang.txt') as f:\n    for line in f.readlines():\n        fields = line.split()\n        cur_data = [float(x) for x in fields]\n        data.append(cur_data)\ndata = np.array(data)\nprint(data)\n\"\"\" 输出：\n[[1. 2. 3. 4. 5. 6.]\n [2. 3. 5. 7. 8. 9.]]\n\"\"\"\n```\n\n### numpy读取文件\n\n#### txt文件中数据以空格方式隔开\n```\ndata = []\ndata = np.loadtxt(\"tang.txt\")\nprint(data)\n\"\"\" 输出:\n[[1. 2. 3. 4. 5. 6.]\n [2. 3. 5. 7. 8. 9.]]\n\"\"\"\n```\n\n#### txt文件中数据以‘,’隔开\ntang2.txt文件中数据:\n```\n$ cat tang2.txt\nx,y,z,w,a,b\n1,2,3,4,5,6\n2,3,5,7,8,9\n```\nnumpy读取文件\n * skiprows = 3:去掉前三行\n * delimiter = \",\": 分隔符\n * usecols = (0,1,5): 指定使用哪几列数据\n\n```\ndata = np.loadtxt(\"tang2.txt\",\n                  delimiter=\",\",\n                  skiprows=1,\n                  usecols=(0, 1, 3))\nprint(data)\n\"\"\"输出:\n[[1. 2. 4.]\n [2. 3. 7.]]\n\"\"\"\n```\n\n### numpy保存数据到文件\n```\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\nnp.savetxt(\"tang3.txt\", tang_array, fmt=\"%d\")\nnp.savetxt(\"tang4.txt\", tang_array, fmt=\"%.2f\")\nnp.savetxt(\"tang5.txt\", tang_array, fmt=\"%d\", delimiter=\",\")\n```\ntang3.txt\n```\n1 2 3\n4 5 6\n```\ntang4.txt\n```\n1.00 2.00 3.00\n4.00 5.00 6.00\n```\ntang5.txt\n```\n1,2,3\n4,5,6\n```\n\n\n## 读写array结构\n从文件load数据直接是numpy.ndarray格式\n\n### npy格式文件\n\n```\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\nnp.save(\"tang_array.npy\", tang_array)\ntang = np.load(\"tang_array.npy\")\nprint(tang)\n```\n\n\n### npz格式文件\n相当于是把数据以某种方式压缩存储, 目前推荐使用上面 **`npy`** 方式存储\n\n```\ntang_array = np.array([[1, 2, 3], [4, 5, 6]])\ntang_array2 = np.arange(10)\n\nnp.savez(\"tang_arrayz.npz\", a=tang_array, b=tang_array2)\ndata = np.load(\"tang_arrayz.npz\")\n\nprint(data.keys())  # KeysView(<numpy.lib.npyio.NpzFile object at 0x7fe509f96a58>)\n\nprint(data[\"a\"])\n\"\"\" 输出:\n[[1 2 3]\n [4 5 6]]\n\"\"\"\n\nprint(data[\"b\"])  # [0 1 2 3 4 5 6 7 8 9]\n\n```\n\n\n","tags":["MLearning"],"categories":["MLearning","numpy"]},{"title":"HTTP, TCP, Socket, grpc","url":"/2021/03/13/blogs/blogs/HTTP_TCP_Socket_grpc/","content":"\n## iso七层模型\n有应用层 表示层 会话层 传输层 网络层 数据链路层 物理层,其中http就属于应用层,tcp与udp是属于传输层\n<!-- more -->\n\n![](ISO.png)\n![](ISO-TCP.IP.png)\n> 实体层：连接网络的硬件设备，就是将电脑连接起来的物理手段. 如光缆/电缆/无线电波\n> 数据链路层 (Link)：建立逻辑连接、进行硬件地址寻址、差错校验等功能，如32位和64位计算机，他们的解码方式是不一样的，数据链路层就规定个二进制数据的解读方式。\n> 网络层 (Network)：进行逻辑地址寻址，实现不同网络之间的路径选择。网络层建立了主机之间的通信，它在网络层引入了一套地址机制:网络地址.简称网址（Ip地址），我们可以通过Ip地址,可以找到唯一的一台计算机，通过主机MAC地址来接收和发送信息.\n> 传输层 (Transport)：定义传输数据的协议端口号，以及流控和差错效验，定义了端口和端口之间的通信，帮助我们使不同的应用程序能够接收到自己所需要的的数据。\n> 会话层 (Session Layer)：包括建立、管理、终止会话，用来建立和管理应用程序之间的通信，实现自动寻址，自动收发数据。\n> 表示层 (Presentation Layer)：数据的表示、安全、压缩。比如我们要用基于Unix系统的mac电脑给pc机发送数据，表示层为我们解决了通信间语法的问题。\n> 应用层 (Application)：网络服务与最终用户的一个接口。比如不同的文件类型要用不同的应用程序打开，应用层中就规定了不同应用程序的数据格式.\n\n## http和tcp的联系\n> * http是基于tcp，就相当于生活中的吃饭时候你都会用到碗，这个碗就是tcp，吃饭这件事情就相当于http，因为我们http发送数据之前，会先进行tcp三次握手，记住这时候只是发送一些状态码的确认等，并没有对http的数据进行发送。\n\n> * http长连接和短连接，其实就是tcp长连接与短连接，在HTTP/1.0中默认使用短连接。也就是说，客户端和服务器每进行一次HTTP操作，就建立一次连接，请求结束就中断连接，HTTP1.1就使用长连接，\n用长连接的HTTP协议，会在响应头加入这行代码：\n\n\n\t Connection:keep-alive\n使用长连接每次打开一个网页除了第一次需要三次握手连接，接下来请求服务器就不用再握手了，就一直使用这个连接，这个keep-alive不会永久保持，这个可以在服务器端设置\n\n> * 长连接和短连接简短概括\n短连接就相当于每次一碗饭就去换一个碗，长连接就是每次吃饭都使用这个碗\n\n## tcp和udp的区别\n> tcp是面向连接的，udp不是面向连接的，怎么说呢？就相当于我们生活中打电话或者微信聊天，要先嘟嘟嘟连接，之后才可以打电话聊天，这个就是面向连接的，也是比较可靠的，udp是不用先去做连接的，所以叫做面向非连接，类似生活中发短信，直接发送过去\n\n有人会问了，为什么http是基于tcp而不是udp呢？\n这个tcp协议比较可靠，不过，目前，有人正在研究基于TCP+UDP混合的HTTP协议。\n\n![](socket.jpg)\n\n## IP：\n网络层协议；（高速公路）\n为计算机网络相互连接进行通信而设计的协议。\n> IP协议对应于网络层，TCP协议对应于传输层，而HTTP协议对应于应用层。注意TPC/IP位于传输层，它主要用来解决数据如何在网络中传输，与IP协议要区分开\n\n## TCP和UDP：\n传输层协议；（卡车）\n> TCP和UDP使用IP协议从一个网络传送数据包到另一个网络。把IP想像成一种高速公路，它允许其它协议在上面行驶并找到到其它电脑的出口。TCP和UDP是高速公路上的“卡车”，它们携带的货物就是像HTTP，文件传输协议FTP这样的协议等。\n\n## **HTTP**\n### **HTTP/1.x**\n> 应用层协议；（货物）。HTTP(超文本传输协议)是利用TCP在两台电脑(通常是Web服务器和客户端)之间传输信息的协议。客户端使用Web浏览器发起HTTP请求给Web服务器，Web服务器发送被请求的信息给客户端。\nHTTP是应用层协议，主要用于包装数据\nHTTP/1.x 的协议是 文本协议，是给人看的，对机器不友好，如果要对机器友好，二进制协议才是更好的选择.  \n\nHTTP/1.x 另一个问题就在于它的交互模式，一个连接每次只能一问一答，也就是client 发送了 request 之后，必须等到 response，才能继续发送下一次请求.  \n\n### **HTTP/2.x**\nHTTP/2 是一个二进制协议，这也就意味着它的可读性几乎为 0，但幸运的是，我们还是有很多工具，譬如 Wireshark， 能够将其解析出来  \n * Stream： 一个双向流，一条连接可以有多个 streams。\n * Message： 也就是逻辑上面的 request，response。\n * Frame:：数据传输的最小单位。每个 Frame 都属于一个特定的 stream 或者整个连接。一个 message 可能有多个 frame 组成\nHTTP/2 通过 stream 支持了连接的多路复用，提高了连接的利用率.  \n\n## **gPRC**\ngRPC 是 Google 基于 HTTP/2 以及 protobuf 的，要了解 gRPC 协议，只需要知道 gRPC 是如何在 HTTP/2 上面传输就可以了.  \ngRPC 的 service 接口是基于 protobuf 定义的，我们可以非常方便的将 service 与 HTTP/2 关联起来.  \n * Path : /Service-Name/{method name}\n * Service-Name : ?( {proto package name} \".\" ) {service name}\n * Message-Type : {fully qualified proto message name}\n * Content-Type : \"application/grpc+proto\"\n\n## SOCKET：\n> 套接字，TCP/IP网络的API。(港口码头/车站)Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。socket是在应用层和传输层之间的一个抽象层，它把TCP/IP层复杂的操作抽象为几个简单的接口供应用层调用已实现进程在网络中通信。\nSocket相当于调用接口(API)，用来调取TCP/IP协议, Socket接口定义了许多函数或例程，用以开发TCP/IP网络上的应用程序。\n\n## TCP/IP：\n> 代表传输控制协议/网际协议，指的是一系列协议，TCP/IP 模型在 OSI 模型的基础上进行了简化，变成了四层，从下到上分别为：网络接口层、网络层、传输层、应用层。\n\n###TCP/UDP区别\n> TCP: （传输控制协议，Transmission Control Protocol）：(类似打电话) 面向连接、传输可靠（保证数据正确性）、有序（保证数据顺序）、传输大量数据（流模式）、速度慢、对系统资源的要求多，程序结构较复杂，每一条TCP连接只能是点到点的，TCP首部开销20字节。\n\n> UDP: (用户数据报协议，User Data Protocol)：（类似发短信） 面向非连接 、传输不可靠（可能丢包）、无序、传输少量数据（数据报模式）、速度快，对系统资源的要求少，程序结构较简单 ，UDP支持一对一，一对多，多对一和多对多的交互通信，UDP的首部开销小，只有8个字节。\n\n## tcp三次握手建立连接\n![](tcp-ip-handshark.png)\n> 第一次握手：客户端发送syn包(seq=x)到服务器，并进入SYN_SEND状态，等待服务器确认；\n> 第二次握手：服务器收到syn包，必须确认客户的SYN（ack=x+1），同时自己也发送一个SYN包（seq=y），即SYN+ACK包，此时服务器进入SYN_RECV状态；\n> 第三次握手：客户端收到服务器的SYN＋ACK包，向服务器发送确认包ACK(ack=y+1)，此包发送完毕，客户端和服务器进入ESTABLISHED状态，完成三次握手。\n握手过程中传送的包里不包含数据，三次握手完毕后，客户端与服务器才正式开始传送数据。理想状态下，TCP连接一旦建立，在通信双方中的任何一方主动关闭连接之前，TCP 连接都将被一直保持下去。\n主机A向主机B发出连接请求数据包：“我想给你发数据，可以吗？”，这是第一次对话；\n主机B向主机A发送同意连接和要求同步（同步就是两台主机一个在发送，一个在接收，协调工作）的数据包：“可以，你什么时候发？”，这是第二次对话；\n主机A再发出一个数据包确认主机B的要求同步：“我现在就发，你接着吧！”，这是第三次对话。\n三次“对话”的目的是使数据包的发送和接收同步，经过三次“对话”之后，主机A才向主机B正式发送数据。\n\n## Websocket\nWebsocket协议解决了服务器与客户端全双工通信的问题。\n\n注:什么是单工、半双工、全工通信？\n信息只能单向传送为单工；\n信息能双向传送但不能同时双向传送称为半双工；\n信息能够同时双向传送则称为全双工。\n\n## WebSocket和Socket区别\n可以把WebSocket想象成HTTP(应用层)，HTTP和Socket什么关系，WebSocket和Socket就是什么关系。\nHTTP 协议有一个缺陷：通信只能由客户端发起，做不到服务器主动向客户端推送信息。\nWebSocket 协议在2008年诞生，2011年成为国际标准。所有浏览器都已经支持了。\n它的最大特点就是，服务器可以主动向客户端推送信息，客户端也可以主动向服务器发送信息，是真正的双向平等对话，属于服务器推送技术的一种。\n\n\n## 使用Socket建立网络\n> 网络上两个程序通过双向通信实现数据交换，socket又叫套接字，每个应用程序开启后，都会在传输层端口上绑定一个socket，不同应用程序之间通过寻找端口找到socket实现数据通信。\nSocket连接过程分为三个步骤：服务器监听，客户端请求，连接确认。\n\n> 1、服务器监听：服务器端套接字并不定位具体的客户端套接字，而是处于等待连接的状态，实时监控网络状态，等待客户端的连接请求。\n\n> 2、客户端请求：指客户端的套接字提出连接请求，要连接的目标是服务器端的套接字。为此，客户端的套接字必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址和端口号，然后就向服务器端套接字提出连接请求。\n\n> 3、连接确认：当服务器端套接字监听到或者说接收到客户端套接字的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发给客户端，一旦客户端确认了此描述，双方就正式建立连接。\n\n","categories":["blogs","blogs"]},{"title":"01 pandas 数据读取与展示","url":"/2021/03/13/MLearning/pandas/pandas_01/","content":"\npandas是把numpy包里的很多命令整合一下使其处理数据更方便\n\n## pandas读取csv\n\n<!-- more -->\n\n``` python\nimport pandas as pd\nimport os\nos.chdir(\"/home/***/learn/python/pandas\")\nprint(os.getcwd()) # /home/***/learn/python/pandas\n\ndf = pd.read_csv(\"data/titanic.csv\")\n\nprint(help(pd.read_csv))\n\n\nprint(df.head(6)) # 输出前6行\n\"\"\" 输出:\n   PassengerId  Survived  Pclass                                               Name  ...            Ticket     Fare  Cabin  Embarked\n0            1         0       3                            Braund, Mr. Owen Harris  ...         A/5 21171   7.2500    NaN         S\n1            2         1       1  Cumings, Mrs. John Bradley (Florence Briggs Th...  ...          PC 17599  71.2833    C85         C\n2            3         1       3                             Heikkinen, Miss. Laina  ...  STON/O2. 3101282   7.9250    NaN         S\n3            4         1       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)  ...            113803  53.1000   C123         S\n4            5         0       3                           Allen, Mr. William Henry  ...            373450   8.0500    NaN         S\n5            6         0       3                                   Moran, Mr. James  ...            330877   8.4583    NaN         Q\n\n[6 rows x 12 columns]\n\"\"\"\n\n\nprint(df.tail(6)) # 输出后6行\n\n\"\"\"输出:\n     PassengerId  Survived  Pclass                                      Name     Sex   Age  SibSp  Parch      Ticket    Fare Cabin Embarked\n885          886         0       3      Rice, Mrs. William (Margaret Norton)  female  39.0      0      5      382652  29.125   NaN        Q\n886          887         0       2                     Montvila, Rev. Juozas    male  27.0      0      0      211536  13.000   NaN        S\n887          888         1       1              Graham, Miss. Margaret Edith  female  19.0      0      0      112053  30.000   B42        S\n888          889         0       3  Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1      2  W./C. 6607  23.450   NaN        S\n889          890         1       1                     Behr, Mr. Karl Howell    male  26.0      0      0      111369  30.000  C148        C\n890          891         0       3                       Dooley, Mr. Patrick    male  32.0      0      0      370376   7.750   NaN        Q\n\"\"\"\n\n\nprint(df.info())\n\"\"\"输出:\n<class 'pandas.core.frame.DataFrame'>      // 读取csv返回DataFrame类型数据\nRangeIndex: 891 entries, 0 to 890          // csv总共有891行数据\nData columns (total 12 columns):           // csv总共有12列数据\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64  // 只有714行数据, 需要做数据填充\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object   // 含有字符串, 成为object类型\n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\"\"\"\n\n\nprint(df.index)  # 输出: RangeIndex(start=0, stop=891, step=1)\nprint(df.columns)  # 默认第一列是列名字, 如果其它列是列明需要指定\n\"\"\" 输出:\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')\n\"\"\"\n\n\nprint(df.dtypes)\n\"\"\" 输出:\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex             object\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin           object\nEmbarked        object\ndtype: object\n\"\"\"\n\n\nprint(df.values)\n\"\"\" 输出:\n[[1 0 3 ... 7.25 nan 'S']\n [2 1 1 ... 71.2833 'C85' 'C']\n [3 1 3 ... 7.925 nan 'S']\n ...\n [889 0 3 ... 23.45 nan 'S']\n [890 1 1 ... 30.0 'C148' 'C']\n [891 0 3 ... 7.75 nan 'Q']]\n\"\"\"\n```\n\n## 创建自己的dataframe结构\n### 创建dataframe结构\n```\nimport numpy as np\nimport pandas as pd\n\n// 数据为空的需要设置成 np.nan\ndata = {\n    \"country\": [\"aaa\", \"bbb\", \"ccc\"],\n    \"population\": [10, 12, np.nan],\n    \"name\": [\"Shanghai\", \"Beijing\", \"Guangzhou\"],\n    \"location\": [np.nan, \"North\", \"Sourth\"]\n}\ndf_data = pd.DataFrame(data)\nprint(df_data)\n// 输出:\n  country  population       name location\n0     aaa        10.0   Shanghai      NaN\n1     bbb        12.0    Beijing    North\n2     ccc         NaN  Guangzhou   Sourth\n```\n\n### 取指定数据\n```\ndf_data_age = df_data[\"population\"]\nprint(df_data_age[:2])\n// 输出\n0    10\n1    12\n\nprint(df_data_age.values[:2])\n// 输出\n[10 12]\n```\n\n### 指定自己的索引\n```\ndf_data = df_data.set_index(\"name\")\nprint(df_data.head())\n// 输出\n          country  population location\nname                                  \nShanghai      aaa        10.0      NaN\nBeijing       bbb        12.0    North\nGuangzhou     ccc         NaN   Sourth\n\ndf_data_population = df_data[\"population\"]\nprint(df_data_population[\"Beijing\"])\n// 输出\n12.0\n```\n\n### 获取数据基本统计特性\n```\n// 只能获取数值类型的数据的基本统计结果, 而对于其它列数据不能获取基本统计特性.\nprint(df_data.describe())\n// 输出\n       population\ncount    2.000000\t\t// 表示population这一列有2个有效数据\nmean    11.000000\t\t// 均值\nstd      1.414214\t\t// 标准差\nmin     10.000000\t\t// 最小值\n25%     10.500000\t\t// 四分之一位数值\n50%     11.000000\t\t// 二分之一位数值\n75%     11.500000\t\t// 四分之三位数值\nmax     12.000000\t\t// 最大值\n```\n\n\n\n\n\n","tags":["MLearning"],"categories":["MLearning","pandas"]},{"title":"Restful API","url":"/2021/03/13/blogs/blogs/Restful_API/","content":"\n## Rest\n> REST，即Representational State Transfer的缩写。直接翻译的意思是\"表现层状态转化\"。\n它是一种互联网应用程序的API设计理念：URL定位资源，用HTTP动词（GET,POST,DELETE,DETC）描述操作。\n> REST描述了一个架构样式的网络系统，比如 web 应用程序。它首次出现在 2000 年 Roy Fielding 的博士论文中，Roy Fielding是 HTTP 规范的主要编写者之一。在目前主流的三种Web服务交互方案中，REST相比于SOAP（Simple Object Access protocol，简单对象访问协议）以及XML-RPC更加简单明了，无论是对URL的处理还是对Payload的编码，REST都倾向于用更加简单轻量的方法设计和实现。值得注意的是REST并没有一个明确的标准，而更像是一种设计的风格。\n<!-- more -->\n\n### 产生背景\n> 近年来移动互联网的发展，前端设备层出不穷（手机、平板、桌面电脑、其他专用设备......），因此，必须有一种统一的机制，方便不同的前端设备与后端进行通信,于是RESTful诞生了，它可以通过一套统一的接口为 Web，iOS和Android提供服务。\n![](Restful.png)\n\n### URI\n> 即统一资源标识符，服务器上每一种资源，比如文档、图像、视频片段、程序 都由一个通用资源标识符（Uniform Resource Identifier, 简称\"URI\"）进行定位。\n\n### HTTP动词\n常用的HTTP动词有下面五个\n * GET（SELECT）：从服务器取出资源（一项或多项）。\n * POST（CREATE）：在服务器新建一个资源。\n * PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。\n * PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。\n * DELETE（DELETE）：从服务器删除资源。\n\n### RESTful架构\n> RESTful架构是对MVC架构改进后所形成的一种架构，通过使用事先定义好的接口与不同的服务联系起来。在RESTful架构中，浏览器使用POST，DELETE，PUT和GET四种请求方式分别对指定的URL资源进行增删改查操作。因此，RESTful是通过URI实现对资源的管理及访问，具有扩展性强、结构清晰的特点。\n\n> 服务器上每一种资源，比如一个文件，一张图片，一部电影，都有对应的url地址，如果我们的客户端需要对服务器上的这个资源进行操作，就需要通过http协议执行相应的动作来操作它，比如进行获取，更新，删除。\n简单来说就是url地址中只包含名词表示资源，使用http动词表示动作进行操作资源\n举个例子：左边是错误的设计，而右边是正确的\n\n``` \n\t GET /blog/getArticles --> GET /blog/Articles  获取所有文章\n\t GET /blog/addArticles --> POST /blog/Articles  添加一篇文章\n\t GET /blog/editArticles --> PUT /blog/Articles  修改一篇文章 \n\t GET /rest/api/deleteArticles?id=1 --> DELETE /blog/Articles/1  删除一篇文章\n```\n\n### 各大平台API为什么不使用restful的风格\n> 这就好象是中文已经用了上千年，突然有几位教授大谈“回字的14种写法”，这只能算是新瓶装旧酒、学究式的文章。一旦媒体不忽悠了，也就过气了。而媒体关注了几个月之后，关注点已经放到云、大数据等等明显是更长久、更工程化的概念上了，懒得对小伎俩去宣传了。\n> 那个东西除了平白无故地额外多出来规定，没有什么发明技术含量的东西（不能被大企业用来衍生出自己的专利技术），因此不可能成为工业标准，充其量是叫兽用用。\n> 其实所谓的 restful 更适合那些总想着沽名钓誉的硕士研究生们去堆砌辞藻，用来体现出自己比其他众多工程技术人员“高人一等”，以避免其它具有更丰富的工程技术的开发人员瞧不起这些人。除此以外，它没有什么实际的技术含量，（正如所看到的）所有实际大项目的工程技术人员都在使用轻量级的web服务方式（从10几年前的电信级的、基于http的远程传输应用就是，人家根本不用webservice）。基本上都是初学者或者大学老师在纠结于繁琐的web服务里边“如何规范名词儿”的问题。\n\n> 核心问题在于RESTful API设计，容易思考，但是不适合实际使用。\n> 这个，就和设计网络协议一样，七层网络模型，理解很给力，实际还是采用四层网络模型一样。\n> 基于RESTful API理解很好理解，但是会严重增加不必要的网络传输消耗。实际使用，基于较少网络消耗较少数据连接的考虑，会更加贴近实际业务场景的API设计，至于DELETE，PUT之类的，不是不愿意使用，是需要对现有业务进行大量不必要的改造，而这种改造，完成是没有必要的。\n\n### 原则\n> REST 指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。\n\n> Web 应用程序最重要的 REST 原则是，客户端和服务器之间的交互在请求之间是无状态的。从客户端到服务器的每个请求都必须包含理解请求所必需的信息。如果服务器在请求之间的任何时间点重启，客户端不会得到通知。此外，无状态请求可以由任何可用服务器回答，这十分适合云计算之类的环境。客户端可以缓存数据以改进性能。\n\n> 在服务器端，应用程序状态和功能可以分为各种资源。资源是一个有趣的概念实体，它向客户端公开。资源的例子有：应用程序对象、数据库记录、算法等等。每个资源都使用 URI (Universal Resource Identifier) 得到一个唯一的地址。所有资源都共享统一的接口，以便在客户端和服务器之间传输状态。使用的是标准的 HTTP 方法，比如 GET、PUT、POST 和 DELETE。Hypermedia 是应用程序状态的引擎，资源表示通过超链接互联。\n\n### 特点\n1、每一个URI代表1种资源；\n2、客户端使用GET、POST、PUT、DELETE4个表示操作方式的动词对服务端资源进行操作：GET用来获取资源，POST用来新建资源（也可以用于更新资源），PUT用来更新资源，DELETE用来删除资源；\n3、通过操作资源的表现形式来操作资源；\n4、资源的表现形式是XML或者HTML；\n5、客户端与服务端之间的交互在请求之间是无状态的，从客户端到服务端的每个请求都必须包含理解请求所必需的信息。\n\n\n","categories":["blogs","blogs"]},{"title":"01 matplotlib","url":"/2021/03/13/MLearning/matplotlib/matplotlib/","content":"\npython中有很多画图库, 基本上都是用matplotlib进行封装\n\n<!-- more -->\n\n## matplotlib\n\n```\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n// 查看matplotlib包所在位置\nprint(mpl.__file__)\n// 输出: /usr/local/lib64/python3.6/site-packages/matplotlib/__init__.py\n\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"-.\", color=\"r\")\n#plt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], linestyle=\"-.\", color=\"r\")\n\nplt.xlabel(\"xlabel\", fontsize=16)\nplt.ylabel(\"ylabel\")\nplt.show()\n#plt.savefig(\"matplotlib/test01.png\")\n\n```\n\n## 线条\n\n### 线条形状\n更多线条参考reference: \nhttps://matplotlib.org/tutorials/index.html\nhttps://matplotlib.org/tutorials/introductory/sample_plots.html\nhttps://matplotlib.org/tutorials/introductory/pyplot.html\nhttps://matplotlib.org/api/_as_gen/matplotlib.lines.Line2D.html#matplotlib.lines.Line2D.set_linestyle\nhttps://matplotlib.org/examples/pylab_examples/multicolored_line.html\n\n| 字符 | 类型 |\n| :--------: | :--------: |\n| '-' or 'solid' | 实线solid hline |\n| '_' | 横线点 |\n| '|' | vline |\n| '+' | plus |\n| '--' or 'dashed' | 虚线dashed line |\n| '-.' or 'dashdot' | 虚点线dash-dotted line |\n| ':' or 'dotted' | 点线dotted line |\n| 'None' or ' ' or '' | draw nothing |\n| '.' | 点 |\n| ',' | 像素点 |\n| 'o' | 圆点 |\n| 's' | 正方点 |\n| 'p' | 五角点 |\n| '*' | star |\n| 'h' | 六边形点1 |\n| 'H' | 六边形点2 |\n| 'D' | 实心菱形点 |\n| 'd' | 瘦菱形点 |\n| 'x' | filled |\n| 'v' | 下三角点 |\n| '^' | 上三角点 |\n| '<' | 左三角点 |\n| '>' | 右三角点 |\n| '1' | 下三叉点 |\n| '2' | 上三叉点 |\n| '3' | 左三叉点 |\n| '4' | 右三叉点 |\n\n```\nimport matplotlib.pylab as plt\nmarkers = [\n    '.', ',', 'o', 'v', '^', '<', '>', '1', '2', '3', '4', '8', 's', 'p', 'P',\n    '*', 'h', 'H', '+', 'x', 'X', 'D', 'd', '|', '_'\n]\ndescriptions = [\n    'point', 'pixel', 'circle', 'triangle_down', 'triangle_up',\n    'triangle_left', 'triangle_right', 'tri_down', 'tri_up', 'tri_left',\n    'tri_right', 'octagon', 'square', 'pentagon', 'plus (filled)', 'star',\n    'hexagon1', 'hexagon2', 'plus', 'x', 'x (filled)', 'diamond',\n    'thin_diamond', 'vline', 'hline'\n]\nx = []\ny = []\nfor i in range(5):\n    for j in range(5):\n        x.append(i)\n        y.append(j)\nplt.figure()\nfor i, j, m, l in zip(x, y, markers, descriptions):\n    plt.scatter(i, j, marker=m)\n    plt.text(i - 0.15, j + 0.15, s=m + ' : ' + l)\nplt.axis([-0.1, 4.8, -0.1, 4.5])\nplt.tight_layout()\nplt.axis('off')\nplt.show()\n```\n![](line_style.png)\n\n### 线条颜色\nreference:\nhttps://matplotlib.org/tutorials/index.html#colors\n\n```\n// 线条与颜色分开写\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"-.\", color=\"r\")\n// 线条与颜色连在一块写\nplt.plot([1, 2, 3, 4, 5], [1, 4, 9, 16, 25], \"r-.\")\n```\n\n## 绘制多条线\n\n```\nplt.plot(tang_numpy, tang_numpy, \"r--\")\nplt.plot(tang_numpy, tang_numpy**2, \"bs\")\n// 上面两行等同于以下一行python代码\nplt.plot(tang_numpy, tang_numpy, \"r--\", tang_numpy, tang_numpy**2, \"bs\")\n```\n\n指定线条宽度, 关键点标志\n```\nx = np.linspace(-10, 10)\ny = np.sin(x)\n\nplt.plot(x,\n         y,\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\t\t\t# 指定x轴值所在位置标志图形\n         markerfacecolor=\"r\",\t# 标志颜色\n         markersize=10,\t\t\t# 标志大小\n         alpha=0.5)\t\t\t\t# 透明程度\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n```\n\n先画图, 再设置参数\n```\nx = np.linspace(-10, 10)\ny = np.sin(x)\nline = plt.plot(x, y)\nplt.setp(line,\n         color=\"r\",\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\n         markerfacecolor=\"r\",\n         markersize=10,\n         alpha=0.5)\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n```\n![](line01.png)\n\n\n## 子图\n\n![](subpic01.png)\n\n```\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pylab as plt\n\ntang_numpy = np.arange(0, 10, 0.5)\nplt.plot(tang_numpy, tang_numpy, \"r--\")\nplt.plot(tang_numpy, tang_numpy**2, \"bs\")\n# 上面两行等价于下面一行\n#plt.plot(tang_numpy, tang_numpy, \"r--\", tang_numpy, tang_numpy**2, \"bs\")\nplt.show()\n\n#====================================================================================\n\nx = np.linspace(-10, 10)\ny = np.sin(x)\nline = plt.plot(x, y)\nplt.setp(line,\n         color=\"r\",\n         linestyle=\":\",\n         linewidth=3.0,\n         marker=\"o\",\n         markerfacecolor=\"r\",\n         markersize=10,\n         alpha=0.5)\nplt.savefig(\"matplotlib/line01.png\")\nplt.show()\n\n#====================================================================================\n\nx = np.linspace(-10, 10)\ny = np.sin(x)\n# 211 表示一会要画的图是3行2列的，最后一个1表示子图当中第1个图\nplt.subplot(321)\nplt.plot(x, y, color=\"r\")\nplt.xlabel(\"01XXX\")\nplt.ylabel(\"01YYY\")\nplt.title(\"Test 01\")\n# 在图中坐标(5, 0)处添加信息\nplt.text(5, 0, \"Info\")\n# 添加注释箭头\nplt.annotate(\n    \"zhushi001\",\n    xy=(-5, 0),\n    xytext=(-1, 0.3),\n    arrowprops=dict(\n        facecolor=\"black\",  # 箭头颜色\n        shrink=0.01,  # 箭头大小\n        headlength=20,  # 箭头长度\n        headwidth=10))  # 箭头宽度\n# 添加格子\nplt.grid(True)\n\nplt.subplot(324)\nplt.plot(x, x**2, color=\"b\")\nplt.xlabel(\"01XXX\")\nplt.ylabel(\"01YYY\")\nplt.title(\"Test 03\")\n\nplt.savefig(\"matplotlib\\matplotlib.png\")\nplt.show()\n\n```\n\n\n","tags":["MLearning"],"categories":["MLearning","matplotlib"]},{"title":"markdown支持高亮显示的语言","url":"/2021/03/13/blogs/blogs/markdown支持高亮显示的语言/","content":"\n## markdown支持高亮显示的语言\nreference: https://www.cnblogs.com/qyf404/p/5019631.html\n<!-- more -->\n\n![](01.PNG)","categories":["blogs","blogs"]},{"title":"telnet","url":"/2021/03/13/blogs/blogs/telnet/","content":"\n## telnet 命令\n开启Windows telent服务\n\n点击键盘`windows`键, 选择`设置`, 在`Windows Settings`页面搜索`telnet`, 勾选`Telnet Client`服务, 点击`OK`.\n<!-- more -->\n\n打开windows上的cmd或powershell执行如下命令\n\n``` \n\t// 测试本机是否能与服务器某个端口连通\n\t$ telnet 10.67.108.211 22\n```","categories":["blogs","blogs"]},{"title":"windows音箱找不到, 没有声音","url":"/2021/03/13/blogs/blogs/windows音箱找不到/","content":"##\n<!-- more -->\n\n![](1.PNG)\n\n","categories":["blogs","blogs"]},{"title":"Hexo 01 部署github blog","url":"/2021/03/13/blogs/blog搭建/Hexo_01_部署github_blog/","content":"\n## **1. 确认电脑安装过git, nodeJS**\n<!-- more -->\n\n### git install \n\n鼠标右击有`git GUI`和`git Bash`选项，随便新建一个文件夹，进入。\n\n``` shell\ngit config --global user.email \"<your-email-address>@qq.com\"\ngit config --global user.name \"<your-user-name>\"\n```\n\n\n### nodeJS install \n\n1. 在使用之前，先类掌握3个东西，明白它们是用来干什么的：\nnpm:  nodejs 下的包管理器。\n\nwebpack: 它主要用途是通过CommonJS 的语法把所有浏览器端需要发布的静态资源作相应的准备，比如资源的合并和打包。\n\nvue-cli: 用户生成Vue工程模板。（帮你快速开始一个vue的项目，也就是给你一套vue的结构，包含基础的依赖库，只需要npm install 就可以安装。\n\n2. nodejs下载网址：https://nodejs.org/en/     【如果嫌下载的慢，可以下载其他网站上的，别人有现成的，下载的比较快】\n  部署此博客用的nodeJS版本连接: https://nodejs.org/dist/latest-v13.x/\n  node-v13.14.0-x64.msi  \n\n3. 下载好后，双击安装\n4. 中途有个选择安装路径\n5. 接下去一路“next”，最后点击finish\n\n#### npm 配置proxy和镜像源\n\n1. 首先查看下目前配置: `npm config list` 查看是否已经配置了\n\n2. 设置网络代理的命令如下: `npm config set proxy=\"http://<proxy>:<port>\"`\n经过上面设置使用了http开头的源，因此不需要设https_proxy了，否则还要增加一句:\n```shell\nnpm config set https-proxy http://<proxy>:<port>\n```\n\n取消代理:\n```shell\nnpm config delete proxy\nnpm config delete https-proxy\n```\n\n3. 国外源速度不稳定，可设置国内淘宝源。查看现有源: `npm config get registry`\n**Note:** 外企配置上面相应proxy后可以不用配置国内淘宝源, 配置淘宝源后反而更慢.\n```shell\nhttps://registry.npmjs.org/  --> https://registry.npm.taobao.org/\n```\n\n4.设置淘宝源: `npm config set registry https://registry.npm.taobao.org`\n\n5.再次查看即可确认源已修改。用新源更新一波: `npm update`\n\n#### npm配置包下载包保存路径\n\n  * 首先你可以使用cmd命令进行查看当前电脑的npm 安装路径。\n```shell\nnpm config ls\n```\n\n  * 在要保存的路径下创建两个文件夹：\n```shell\nD:\\***\\node_global_modules\\\nD:\\***\\node_cache\\\n```\n\n  * 添加好目录后 执行以下代码：\n```shell\nnpm config set prefix \"D:\\***\\node_global_modules\\\"\nnpm config set cache \"D:\\***\\node_cache\\\"\n```\n\n  * 修改环境变量\n\n在path中追加修改\n```shell\nD:\\***\\node_cache\\\nD:\\***\\node_global_modules\\\n```\n\n  * 测试，重新打开一个cmd命令行，安装一个插件试试，执行\n```shell\nnpm install cordova -g // -g意思是安装到全局目录下\n```\n安装完毕后打开设置的安装路径看下是否成功\n在你设置的目录node_global_modules 中出现要下载的文件夹则表示设置成功\n\n#### 升级hexo\n\nnpm i hexo-cli -g\n\n## **2. 以管理员身份打开cmd， cd到当前目录**\n\n``` shell\nnpm install -g hexo-cli\nnpm install hexo-deployer-git --save\nhexo init\n```\n在public目录下可看到我们自己书写的博客文档（.md文件）\n\n## **3. 生成网页浏览**\n\n``` shell\nhexo g\n```\n默认地址是 localhost:4000, 浏览器输入此链接即可看到默认样式\n## **4. 选择主题，Hexo可随时使用、更换博客主题**\nHexo官方主题网页  \n其中一个主题  \n\n``` shell\ngit clone https://github.com/yelog/hexo-theme-3-hexo.git themes/3-hexo\n```\n下载项目至博客项目下的themes目录中，文件夹命名为3-hexo，并在博客配置文件_config.yml中指定使用该主题：  \n修改hexo根目录的_config.yml文件，如下  \n```\ntheme: 3-hexo\n```\n更新:\n``` shell\ncd themes/3-hexo\ngit pull\n```\n## **5. 配置SSH密钥**\n```shell\nssh-keygen -t rsa -C \"1041618918@qq.com\"\ncat ~/.ssh/id_rsa.pub                    //C:\\Users\\yazhanma\\.ssh\\id_rsa.pub\n```\n复制内容添加到github ->settings->SSH and GPG keys-> add SSH key, 名字随便取.  \n此时安装网上说的测试命令#ssh -T git@github.com是否能连接上，自己试了试不行仍然time out，但是不影响估计是得等一段时间才行，仍然继续下面步骤部署成功.  \n修改博客配置文件_config.yml  \n```\ndeploy:\n  type: git\n  repo: https://github.com/Kung-Fu-Master/Kung-Fu-Master.github.io\n  // 如果执行$hexo d后出现一个错误但还是部署成功, 可以将路径改成以下ssh格式就不会执行$hexo d命令后出错, 但是部署速度超级慢\n  // repo: git@github.com:Kung-Fu-Master/Kung-Fu-Master.github.io.git\n  branch: master\n```\n## **6. 发布到GitHub**\n```shell\nhexo g\nhexo d\n```\n完成后，就有属于自己的博客了，在github的setting中 可以看到自己的博客地址。\n\n**`https://kung-fu-master.github.io`**\n\n如果文章没更新, 对于chrome浏览器  \n第一种方法: 可以选择等待一会时间再查看  \n第二种方法: 点击地址栏右侧的菜单按钮->更多工具->清除浏览数据->清除数据 再刷新就可看到文章已更新  \n\n## **7. 配置评论系统**\n目前 3-hexo 已经集成了评论系统有 gitalk 、gitment、 disqus 、来必力、utteranc  \n\n7.1 登录 github ，注册应用\n[点击进行注册](https://github.com/settings/applications/new)\n![](registry_gitalk.PNG)\n注册完后，可得到 `Client ID` 和 `Client Secret`.  \n\n7.2 因为 gitalk 是基于 Github 的 Issue 的，所以需要指定一个仓库，用来承接 gitalk 的评论，我们一般使用 Github Page 来做我们博客的评论，所以，新建仓库名为 xxx.github.io，其中 xxx 为你的 Github 用户名  \ngitalk官网: [https://gitalk.github.io/](https://gitalk.github.io/)\n\n7.3 配置主题\n在主题下 _config.yml 中找到如下配置，启用评论，并使用 gitalk.  \n\n```\n########## 评论设置 #############\ncomment:\n  on: true\n  type: gitalk\n```\n在主题下 _config.yml 中找到 gitalk 配置，将 第 1 步 得到的 Client ID 和 Client Secret 复制到如下位置\n\n```\ngitalk:\n  githubID:    # 填你的 github 用户名\n  repo:  xxx.github.io     # 承载评论的仓库，一般使用 Github Page 仓库\n  ClientID:   # 第 7.1 步获得 Client ID\n  ClientSecret:  # 第 7.1 步获得 Client Secret\n  adminUser:     # Github 用户名\n  distractionFreeMode: true\n  language: zh-CN\n  perPage: 10\n```\n## **8. 开启字数统计**\n开启此功能需先安装插件，在 hexo 根目录(博客的目录,没有package.json文件的话会自动生成), 执行 `$ npm i hexo-wordcount --save`\n修改 _config.yml\n```\nword_count: true\n```\n## **写博客、发布文章**\n新建一篇博客，执行下面的命令, 或者自己手动创建*.md文件, 后缀名一定要是\".md\".\n``` shell\nhexo new post \"article title\"\n```\n![](deploy.jpg)\n\\source\\ _posts 将会看到 article title.md 文件\n\n用MarDown编辑器打开就可以编辑文章了。文章编辑好之后，运行生成、部署命令:  \n``` shell\nhexo g   // 生成\nhexo d   // 部署\n```\n当然你也可以执行下面的命令，相当于上面两条命令的效果\n``` shell\nhexo d -g #在部署前先生成\n```\n部署成功后访问 你的地址，https://yourName.github.io (这里输入我的地址:https://Kung-Fu-Master.github.io), 将可以看到生成的文章。\n\n## **踩坑提醒**\n1. 注意需要提前安装一个扩展：\n\n``` shell\nnpm install hexo-deployer-git --save\n```\n如果没有执行者行命令，将会提醒: deloyer not found:git\n\n2. 如果出现下面这样的错误\n\n\nPermission denied (publickey).\nfatal: Could not read from remote repository.\nPlease make sure you have the correct access rights\nand the repository exists.\n\n则是因为没有设置好public key所致。  \n在本机生成public key,不懂的可以参考我的这一篇博客Git ssh 配置及使用  \n\n","categories":["blogs","blog搭建"]},{"title":"Hexo 02 git clone blog后如何运行","url":"/2021/03/13/blogs/blog搭建/Hexo_02_git_clone_blog后如何运行/","content":"\n## **1. 确保系统已安装node.js, git工具**\n<!-- more -->\n\n1.1 删除 `blogs\\.deploy_git` 文件夹;\n\n1.2 输入\n```\ngit config --global core.autocrlf false\n```\n\n1.3 然后，依次执行：\n```\nhexo clean\nhexo g\nhexo d\n```\n\n## **2. cmd管理员身份进入git clone下来的文件夹**\n```shell\ncd ...\\blogs_backup\\blogs\nnpm install -g hexo-cli\nnpm install hexo-deployer-git --save -g\n```\n## **3. 生成部署博客**\n```shell\n//一定要到blogs目录再操作hexo g等命令\nhexo g    //generate，md生成html\nhexo s    //start，开启服务, 网页上输入http://localhost:4000 即可查看\n```\n添加***.md文件后\n```shell\n\thexo d        //deploy部署，中途需要输入账号密码, 稍等一会即可chrome网页上输入https://Kung-Fu-Master.github.io 查看\n```\n但是此时新添加的.md文件没有上传到blogs_backup git仓库中，需要打开git bash, git add这个.md文件再git commit -m \"add new .md\", git push, 完成blogs_backup仓库的更新.\n\n## **免密 $hexo d 方法**\n\n### **第一种, 亲测**\ngit bash终端 cd 到博客副本文件夹, 输入：\n```\n$git config --global credential.helper store  \n```\n接下来 `$git push` 需要输入密码，以后 `$git push` 就不用了, 然后 `$hexo d` 也就不需要了  \n\n原因是执行 $git config --global credential.helper store 后会创建文件 `C:\\Users\\hp\\.git-credentials(Windows系统下)`  \n里面内容: https://git账户名:登陆git密码@github.com  \n\n### **第二种, 网上**\nreference: https://blog.csdn.net/qq_38082304/article/details/100030976?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-6.control\n\n创建文件 C:\\Users\\hp\\.git-credentials 并添加内容 https://{username}:{password}@github.com\n\n添加git config内容\n```shell\n\tgit config --global credential.helper store\n```\n执行此命令后，用户主目录下(C:\\Users\\hp\\)的.gitconfig文件会多了一项：\n```\n[credential]\n\thelper = store\n重新git push就不需要用户名密码了\n\n## **git设置忽略文件(夹)和目录**\n\n博客clone下来后执行`hexo clean; hexo g;`后, 再查看git状态`git status`, 会看到blogs/public/* 和 db.json处于modified状态，如果想每次生成博客后忽略 blogs/public/等下的文件修改情况, 参考以下方式.  \n1. gitbash 命令进入本地git库目录\n2. 博客根目录创建.gitignore文件\n3. 修改文件, 添加如下的忽略正则内容\n\n```\n\t$ vim .gitignore\n\t.idea\t\t\t// 忽略.idea文件夹及文件夹下的文件的修改\n\t*.html\t\t\t// 忽略以.html结尾的文件的修改\n\tblogs/public/\t// 忽略blogs/public/下的所有文件的修改\n\tdb.json\t\t\t// 忽略db.json文件的修改\n\tblogs/.deploy_git/*\n```\n\n**Note:** .gitignore只能忽略那些原来没有被track的文件，如果某些文件已经被纳入了版本管理也就是已经执行了`git add <文件(夹)A>;`，则修改.gitignore, 忽略`文件(夹)A`是无效的。  \n**正确的做法**是在clone下来的仓库中手动执行 `git rm <文件(夹)A>;`如`git rm blogs/public/*`,然后再`git commit \"*\"; git push origin master;`, 远程仓库的此文件(夹)也删除后，再在本地生成的`文件(夹)A`添加到.gitignore文件后, git status等就可以忽略它的修改.  \n","categories":["blogs","blog搭建"]},{"title":"Hexo 03 写博客语法","url":"/2021/03/13/blogs/blog搭建/Hexo_03_写博客语法/","content":"\n## **表格**\n\n| 左对齐 | 右对齐 | 居中对齐 |\n| :------| -----: | :------: |\n| 单元格 | 单元格 | 单元格 |\n| 单元格 | 单元格 | 单元格 |\n<!-- more -->\n\n## 内容\n\n\t添加内容\n","categories":["blogs","blog搭建"]},{"title":"chrome浏览器一些操作","url":"/2021/03/13/blogs/blogs/chrome/","content":"## **chrome插件**\n\n### **Volume Control 音量控制**\n> volume Control-音量控制插件是一个通过声音增强器增加音量，并且使用音频控件分别设置每个标签的音量级别的Chrome插件。可以对有音频的输出的标签进行快速切换和进行音量管理，最大可以将音量增加到6倍.  \n<!-- more -->\n\nVolume Control - 音量控制 Chrome插件的优点:  \n1、将音量增加到6倍(高达600％);\n2、显示播放音频的所有选项卡;\n3、快速导航带有声音的标签;\n4、立即静音标签音量(只需点击弹出菜单中的扬声器图标);\n5、在工具栏菜单中的应用程序图标上显示音量级别;\n6、简约的设计。\n\n### Proxy SwitchyOmega\n\nProxy SwitchyOmega 可以设置浏览器访问某些网站所用的proxy策略.\n\n![](001.JPG)\n\n\n\n## **chrome浏览器添加插件遇到问题**\n安装从往上下载的后缀名为.crx的chrome插件出现\"程序包无效: CRX_HEADER_INVALID\"的错误解决方法  \n**修改文件格式，加载扩展程序(亲测有效)**\n有时候要在 chrome安装本地插件时，会报错，这时候将插件的后缀名 .crx 改为  .zip或者 .rar，然后将改好后缀名的文件解压到本地文件夹中，然后在 chrome  的设置 -> 更多工具 -> 扩展程序：\n![](add_extension_solution_3.png)\n在上图中的加载已解压的扩展程序，找到刚才的解压的扩展程序即可.  \n如下载的chrome插件名为plugin.crx  \n将原先的plugin.crx改为plugin.rar，然后将plugin.rar文件解压成文件夹plugin  \n地址栏输入chrome://extensions/,  -> 将右上开发者模式按钮打开  -> 点击【加载已解压的扩展程序】-> 选择我们刚才解压的plugin目录\n\n**开启开发者模式**\nchrome  的设置 -> 更多工具 -> 扩展程序，开启开发者模式\n![](add_extension_solution_1.png)\n\n**修改参数**\n首先打开下面地址：chrome://flags/#extensions-on-chrome-urls\n将 \"Extensions on chrome:// URLs\" 的 disabled 改为 enable 重启\n![](add_extension_solution_2.png)\n\n\n\n\n\n","categories":["blogs","blogs"]},{"title":"VScode 配置设置","url":"/2021/03/13/blogs/blogs/VScode_配置设置/","content":"\n## VScode代码保存自动格式化\n\n一、实现vs code中代码格式化快捷键：【Shift】+【Alt】+F\n<!-- more -->\n\n二、实现保存时自动代码格式化：\n\n1）文件 ------.>【首选项】---------->【设置】；\n\n2）搜索emmet.include;\n\n3）在settings.json下的【工作区设置】中添加以下语句：\n\n```\n\"editor.formatOnType\": true,\n\"editor.formatOnSave\": true\n```\n\n4）随便写代码进行测试即可。\n\n\n## python代码保存自动格式化\nVScode创建python文件会自动提示安装autopep8, 或者通过以下命令进行安装\n\n```shell\n$ pip install -U autopep8\n```\n\n## VScode 配置中文\n\n![](vscode_chinese.png)\n\n\n\n","categories":["blogs","blogs"]},{"title":"开发matplotlib遇到的问题","url":"/2021/03/13/MLearning/matplotlib/开发matplotlib遇到的问题/","content":"\n\n## VScode显示远程绘图\n\nVScode远程连接linux开发, 但是在VScode执行plt.show()不显示图片报如下错误, 而在linux远程终端是可以正常显示的.\n\n<!-- more -->\n\n```\nmatplotlib01.py:7: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n  plt.show()\n```\n解决方法：\n\n### 新的解决方法\nreference: \nhttps://blog.csdn.net/zb12138/article/details/107160825\nhttps://blog.csdn.net/qq_34846662/article/details/99051947\n\n#### windows机器\n```\n到C:\\Users\\XXX\\.ssh目录, 没有的话创建.\n打开git bash, 输入以下命令生成公私匙, 直接回车就可以了\n$ ssh-keygen -t rsa -C \"这里任意输入\" \n```\n会在C:\\Users\\XXX\\.ssh目录查看到 **`id_rsa`**和 **`id_rsa.pub`**\n\n打开VScode, 选择扩展插件, 安装插件 **`Remote X11`**, 也会连带安装 **`Remote X11(SSH)`** 等插件.\n\n#### linux远程终端\n\n输入以下命令生成公私匙, 直接回车就可以了\n```\nssh-keygen -t rsa -C \"这里任意输入\"\ncd /root/.ssh/\ncp id_rsa.pub authorized_keys\n```\n之后把 windows生成的id_rsa.pub内容追加到authorized_keys文件\n\n#### 回到windows机器\n\n1. 重启VScode, 可以发现远程登陆到linux不需要再输入密码\n2. 启动MobaXterm或Xshell(Xshell没测试过)软件, 实践中不需要打开终端窗口, 仅仅启动软件就行了.\n3. 测试matplotlib\n```\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\nplt.show()\nplt.savefig(\"matplotlib/test01.png\")\n```\n点击键盘F5或VScode打开的终端输入**`python3 matplotlib.py`**运行, 能够正常显示图片, 关闭图片能够正常保存图片到matplotlib目录\n\n### 旧的解决方法\n#### Linux服务器端\n确保ssh服务配置中开启了X11转发：\n```\n$ vim /etc/ssh/sshd_config\n包含类似如下内容：\nX11Forwarding yes\nX11DisplayOffset 10\nPrintMotd no\nPrintLastLog yes\nTCPKeepAlive yes\n```\n\n * Centos\n```\n// 安装依赖包\nyum -y install python3-tkinter\n\n// 如果用的是VScode而不是终端运行matplotlib画图, 则需要安装如下包\nyum -y install tk-devel tkcvs tkinter tklib\n```\n * Ubuntu\n```\napt-get install python3-tk\napt-get install tk-dev\n\n// 后续方法参考上面Centos设置\n```\n\n#### Windows开发机\n确保windows机器开启了OpenSSH CLient, 查看方法如下:\n\n按下键盘Windows键或点击桌面左下角 -->> 点击设置 -->> 点击应用  -->> 点击管理可选功能(Optional features) --> 查看OpenSSH Client是否已开启.\n\n如果没有开启OpenSSH需要点击上面的添加功能(Add a feature)找到OpenSSH来开启\n\n用MobaXter或Xshell打开一个linux终端 **`A`**, 输入如下命令查看此终端的DISPLAY变量值\n```\n$ echo $DISPLAY\n\tlocalhost:11.0\n```\n![](01.JPG)\n\nVScode在要执行matplotlib画图的Terminal **`B`** 设置与刚才用打开的终端 **`A`** 相同的DISPLAY变量和值, 如下\n```\nexport DISPLAY=:11.0\n```\n![](02.JPG)\n之后用VScode开发matplotlib画图后要用plt.show()显示图片时候就会借用MobaXter打开linux终端 **`A`** 来显示图片\n\n\npython代码里引入matplotlib库之后要加上引入**`TKAgg`**\n```\nimport matplotlib as mpl\nmpl.use('TkAgg')\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3, 4, 5], [1, 2, 3, 4, 5])\nplt.show()\nplt.savefig(\"test01.png\")\n```\n\n如果还是不行，看看你的matplotlib的版本，如果是最新的话，好像是存在一点bug的，推荐卸载然后重装\n```\n// 查看matplotlib包版本\npip list | grep matplotlib\n\n// 卸载matplotlib\npip uninstall matplotlib\n\n//这个版本比较稳定\npip install matplotlib=2.2.4\n```\n\n\n","tags":["MLearning"],"categories":["MLearning","matplotlib"]},{"title":"deeplearning 01 getting started","url":"/2021/03/13/DLearning/deeplearning_01_get_started/","content":"\nReferenece: [csdn 深度学习入门视频课程](https://edu.csdn.net/course/detail/3921)\n\n## **机器学习流程**\n![](01.PNG)\n数据获取，特征工程，建立模型，评估与应用  \n深度学习是一个黑盒子但是也要了解里面细节.  \n<!-- more -->\n\n## **数据集**\n**01. 大数据集**\n2009年之前机器学习耗费资源多, 速度慢, 交流关注的少.  \n2009年 李飞飞号召全美多所高校标注图片中物体特征位置, 组成一个含有22K个种类和14M张图片的库.  \nhttp://www.image-net.org/  \n图片库非常大, 做大型科研等可以参考.  \n\n2012年Alex在图像分类比赛中使用深度学习算法获得冠军，精度比第二名用传统人工智能得到的精度高出十几个百分点, 由此让人们意识到深度学习在计算机视觉方面的重要作用.\n\n2017年李飞飞宣布图像分类比赛结束, 因为机器识别效果远超人类, 比赛再做下去没有多大意义.\n\n![](02.PNG)\n在数据规模比较小是可以不用深度学习.  \n当数据规模达到上万或几十万量时候深度学习效果比传统人工智能好.  \n\n**02 小数据集**\n`CIFAR-10` 百度,google等搜索下载\n\n## **K近邻算法**\n![](03.PNG)\n![](04.PNG)\nK近邻算法对测试的用的image A和图片集中的images B,c,d..., 像素点的值对应位相减, 再求和得到两张图片总像素点的差值和.  \n再取图片集前10个差值小的图片, 会发现跟测试用的image A内容可能相差很大，只是背景或前景色相类似.  \n因此K近邻算法不能用来图像分类, 需要再画出图片中感兴趣的区域再识别.  \n\n\n## **线性函数**\n![](05.PNG)\n![](06.PNG)\n![](07.PNG)\n\n## **损失函数**\n![](08.PNG)\n错误得分 - 正确得分, 再 +1或者+10(表示正确得分要比错误得分高1或者10才算没有损失), 再跟0比大小取大值, 小于等于0表示没有损失.  \n\n\n\n\n\n","tags":["DLearning"],"categories":["DLearning"]},{"title":"工具推荐","url":"/2021/03/13/windows/工具推荐/","content":"\n## typora文档编辑软件\n\n## **IDM**(Internet Download Manager)\n\n能够帮助下载网页视频","categories":["windows"]},{"title":"播放器推荐","url":"/2021/03/13/windows/播放器推荐/","content":"\n## **potplayer**\n\n画质不用说，能调的不比kmplayer少多少还更方便它的zune皮肤就秒掉一大批播放器<img src=\"https://pic2.zhimg.com/50/8411cbfb25b642fb641db53763b6c1ee_hd.jpg?source=1940ef5c\" data-rawwidth=\"741\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic1.zhimg.com/8411cbfb25b642fb641db53763b6c1ee_r.jpg?source=1940ef5c\"/>\n\n简洁大方\n进度条等自动隐藏，鼠标放在那个位置才出现，鼠标不动立马淡化消失\n\n[Potplayer下载地址](https://link.zhihu.com/?target=https%3A//daumpotplayer.com/download/)\n\nPotplayer 是一款韩国人开发的视频播放工具，优点是启动快、占用资源少、颜值高。\n\n这款软件初始的界面可能只是比较简洁，但是我们可以通过自定义设置将其变成自己想要的样子，比如你可以设置为目前很流行的无边框模式，或者安装一些皮肤，挑选适合自己使用习惯的播放器界面。\n\n关于定制，Potoplayer 有非常多的设置项，比如快捷键、鼠标和触摸设置、播放效果设置、字幕设置、滤镜、声音控制等，非常适合那些对视频有专业需求的人。\n\n然其实很多人包括博主自己大部分功能都用不到，其实也不用担心，我们将其保持默认项就行了，因为本来设置项就藏得很深，不认真研究软件几乎都注意不到。\n\nPotplayer 是一款非常好用的Windows视频播放软件，尤其是你对颜值很注重，强烈推荐它。\n\n我喜欢的功能模块是可以快速设置跳过音频的片头和片尾, 国产的爱奇艺万能播放器只能跳过视频和部分音频的片头和片尾.  \n\n鼠标在播放器中间右击按照如图所示操作即可设置跳略播放\n![](1.PNG)\n设置播放速度:\n![](2.PNG)\n![](3.PNG)\n\n\n## **射手影音(SPlayer)**\n\n![](https://pic4.zhimg.com/80/v2-ddd05d867ad7cc5aafa3258955d54c18_720w.jpg?source=1940ef5c)\n\n[SPlayer下载地址](https://link.zhihu.com/?target=http%3A//www.splayer.org/)\n射手影音是一款简洁无边框的现代视频播放器，支持Windows和Mac，软件设计非常简洁，默认 就是无边框的播放模式，你可以无干扰的观看本地视频。\n\n另外SPlayer还有两个非常独特的功能，AI翻译以及播放网站视频。通过AI翻译，你可以为视频生 成英文或者中文的字幕，让视频观看更容易;播放网站视频功能允许你添加各大视频网站入口， 直接在播放器中观看视频，还支持小窗口播放，让你可以观看视频的同时做其他事情 。\n\n\n\n\n\n\n","categories":["windows"]},{"title":"U盘分区之后如何恢复","url":"/2021/03/13/windows/U盘分区之后如何恢复/","content":"\n1. 插入U盘。\n2. 按windows键，搜索cmd 选择以管理员身份运行。\n3. 输入diskpart,按enter。\n![](1.png)\n4. 输入list disk,按enter。\n![](2.png)\n5. 之后会看到\ndisk 0\ndisk 1\n如果你给你电脑磁盘分过区的话可能还有disk 2  、  disk 3等\n![](3.png)\n6. 输入select disk X(X代表磁盘后面的数字0、1，可磁盘的大小来判断数字是多少，一般是1),按enter\n![](4.png)\n7. 输入clean，按enter\n![](5.png)\n8. \n以上完成之后，到“磁盘管理”选择U盘，新建简单卷，并重命名你的U盘\n\n打开“磁盘管理”方法:\n任务栏\"Search\"输入\"disk management\", 选择运行\"Create and format hard disk partitions\"\n\"新建简单卷\"方法:\n打开磁盘管理后，选中U盘所在Disk 如Disk1， 右击鼠标，选中第一个新建卷，默认下一步，之后需要格式化, \"Volume lable\"输入盘volume名字如OS就可以了.\n\n","categories":["windows"]},{"title":"vagrant","url":"/2021/03/13/technologies/virtual_machine/vagrant/","categories":["technologies","虚拟机"]},{"title":"virtualbox","url":"/2021/03/13/technologies/virtual_machine/virtualbox/","categories":["technologies","虚拟机"]},{"title":"web前端datatables框架","url":"/2021/03/13/technologies/web/datatables/","content":"\n框架网站:\nhttps://datatables.net/\n\n使用样例:\nhttps://datatables.net/examples/data_sources/\n\n\n","categories":["technologies","web前端"]},{"title":"bootstrap","url":"/2021/03/13/technologies/web/bootstrap/","content":"\n#### bootstrap介绍\nBootstrap 是最流行的开发响应式 web 的 HTML, CSS, 和 JS 框架。  \nBootstrap 帮助您开发在任何尺寸都外观出众的站点：显示器、笔记本电脑、平板电脑或手机  \n\n\n\n","categories":["technologies","web前端"]},{"title":"postman模拟网页发送http请求","url":"/2021/03/13/technologies/web/postman/","content":"\n## postman\n\npostman是一个网页调试与发送HTTP请求的工具，在程序员中广泛使用，在开发调试网络程序时跟踪一些网络请求\n\n\n\n\n\n\n\n\n","categories":["technologies","web前端"]},{"title":"JavaScript","url":"/2021/03/13/technologies/web/JavaScript/","content":"\n### ES6 新特性\n\n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### ES6 let 块作用域\n``` javascript\nif(true){\n    let a = 123;  // let 是块作用域，只能再if作用域使用\n    console.log(a);\n}\n``` \n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### ES6 模板语法\n``` javascript\nvar name = \"张三\";\nvar age = 20;\nconsole.log(`${name}的年龄是${age}`);  // 不是引号''，而是键盘左上角的`\n```\n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### ES6 属性的简写\n``` javascript\nvar name = \"zhangsan\";\n//1.\nvar app1 = {\n    name:name\n}\n//2.\nvar app2 = {\n    \"name\":name\n}\n//3. 简写\nvar app3 = {\n    name\n}\nconsole.log(app1.name);\nconsole.log(app2.name);\nconsole.log(app3.name);\n``` \n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### ES6 buffer简写\n``` javascript\nvar name = \"李四\";\nvar app4 = {\n    name,\n    run:function(){\n        console.log(`${this.name}在跑步`);\n    }\n}\nvar app5 = {\n    name,\n    run(){\n        console.log(`${this.name}在跑步`);\n    }\n}\napp5.run()\n```\n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### 箭头函数\n``` javascript\nsetTimeout(function(){\n    console.log(\"执行\");\n},1000);\nsetTimeout(()=>{\n    console.log(\"执行\");\n}, 1000);\n\n// 回调函数获取异步函数里数据\n//1.\nfunction getData1(){\n    //ajax, 异步\n    setTimeout(()=>{\n        var name1 = \"张三1\";\n    },1000);\n    return name1;   // 执行会出错，找不到name1变量\n}\n//console.log(getData1());  // 出错这是获取不到name1的\n//2.\nfunction getData2(callback){\n    setTimeout(()=>{\n       var name2 = \"张三2\";\n       callback(name2);\n    });\n}\ngetData2((aaa)=>{\n    console.log(aaa);\n});\n//3. Promise来处理异步  resolve 成功的回调函数   reject失败的回调函数\nvar p1 = new Promise((resolve, reject)=>{\n    setTimeout(()=>{\n        var name3 = \"张三3\";\n        resolve(name3);\n    },1000);\n});\np1.then((data)=>{\n   console.log(data);\n});\n//4. 提取一下Promise里参数\nfunction getData3(resolve, reject){\n    setTimeout(()=>{\n        var name4 = \"张三4\";\n        resolve(name4);\n    }, 1000);\n}\nvar p2 = new Promise(getData3);\np2.then((data)=>{\n    console.log(data);\n})\n```\n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n### async声明异步方法, await等待异步方法执行完成\n``` javascript\n//1.\nasync function test1(){     // 函数定义为async，返回Promise { '您好！' }\n    return \"您好！\";\n}\n// console.log(await test());   // 出错， await必须用在async函数方法中\n//2.\nasync function test2(){\n    return new Promise((resolve, reject)=>{\n        setTimeout(()=>{\n            var name5 = \"张三5\";\n            resolve(name5)\n        }, 1000);\n    });\n}\nasync function output(){\n    var data = await test2();    // 如果不加await，返回是 Promise { '您好！' }\n    console.log(data);          // 返回 您好！\n}\noutput();\n\n// 判断一个资源到底是目录还是文件，此方法里有异步函数，需要加上async标签，再用Promise来获取异步返回，最后再用await来调用等待此异步方法执行完\nasync function isDir(path){\n    return new Promise((resolve, reject)=>{\n        fs.stat(path, (err, states)=>{\n            if(err){\n                console.log(err);\n                reject(err);\n                return ;\n            }\n            if(states.isDirectory()){\n                resolve(true);\n            }else{\n                resolve(false);\n            }\n        });\n    });\n}\n// 获取所有资源\nfunction getDir(){          // 此方法不需要加async, await只在它所在的函数上加上async就可以了，更外层的函数不需要加上asycn\n    var path = \"./testfs\";\n    var dirArr = [];\n    fs.readdir(path, async (err, data)=>{   //回调函数需要加async，因为此函数里需要用await\n        if(err){\n            console.log(err);\n            return ;\n        }\n        for(var i = 0; i < data.length; i++){\n            if(await isDir(path + \"/\" +data[i])){\n                dirArr.push(data[i]);\n            }\n        }\n        console.log(dirArr);    // 输出 [ 'fs1', 'fs2' ]\n    });\n}\ngetDir();\n//※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※※\n```\n\n\n### JSON.parse(text[, reviver])\n\t参数说明：\n\ttext:\n\t\t必需， 一个有效的 JSON 字符串。\n\treviver: \n\t\t可选，一个转换结果的函数， 将为对象的每个成员调用此函数。\n\t返回值：\n\t\t返回给定 JSON 字符串转换后的对象。\n\n``` html\n<!DOCTYPE html>\n<html>\n<head>\n<meta charset=\"utf-8\">\n<title>title)</title>\n</head>\n<body>\n<p id=\"demo\">测试JSP函数库JSON.parse()</p>\n<script>\nconst json_str = '{\"result\":true, \"count\":42}';\nconst json = { \"name\":\"MYZ\" };\n\n// 用来解析JSON字符串，构造由字符串描述的JavaScript值或对象\nconst obj = JSON.parse(json_str);\nconsole.log(obj.count);\t\t// expected output: 42\nconsole.log(obj.result);\t// expected output: true\n\nconsole.log(obj);\t\t// > Object { result: true, count: 42 }\nconsole.log(json);\t\t// > Object { age: 23 }\nconsole.log(typeof(json_str));\t\t\t// > \"string\"\nconsole.log(typeof(json));\t\t\t\t// > \"object\"\nconsole.log(typeof(obj) === typeof(json));\t// > true\n\nfor(x in json){\n  console.log(x);\t\t// > \"name\"\n  console.log(json[x]);\t\t// > \"MYZ\"\n}\n\nJSON.parse('{\"result\":true, \"count\":42}', function(k, v) {\n  console.log( k );\t\t// 输出当前三个属性，最后一个为 \"\", > \"result\", > \"count\", > \"\"\n  return v;\t\t\t// 返回修改的值\n});\n\nJSON.parse('{\"1\": 7, \"2\": 2, \"3\": {\"4\": 4, \"5\": {\"6\": 6}}}', function(k, v) {\n  console.log( k );\t\t// 输出当前属性，最后一个为 \"\" > \"1\", > \"2\", > \"4\", > \"6\", > \"5\", > \"3\", > \"\"\n  return v;\t\t\t// 返回修改的值\n});\n\n</script>\n</body>\n</html>\n```\n\n### JSON.stringify(value[, replacer [, space]])\n\tvalue:\n\t\t将要序列化成 一个 JSON 字符串的值。\n\treplacer 可选:\n\t\t如果该参数是一个函数，则在序列化过程中，被序列化的值的每个属性都会经过该函数的转换和处理；\n\t\t如果该参数是一个数组，则只有包含在这个数组中的属性名才会被序列化到最终的 JSON 字符串中；\n\t\t如果该参数为 null 或者未提供，则对象所有的属性都会被序列化；\n\t\t关于该参数更详细的解释和示例，请参考使用原生的 JSON 对象一文。\n\tspace 可选:\n\t\t指定缩进用的空白字符串，用于美化输出（pretty-print）；\n\t\t如果参数是个数字，它代表有多少的空格；上限为10。该值若小于1，则意味着没有空格；\n\t\t如果该参数为字符串（当字符串长度超过10个字母，取其前10个字母），该字符串将被作为空格；\n\t\t如果该参数没有提供（或者为 null），将没有空格。\n\t返回值:\n\t一个表示给定值的JSON字符串。\n\n``` javascript\nconst json_str = '{\"result\":true, \"count\":42}';\nconst json = { \"name\":\"MYZ\" };\nconsole.log(typeof(json_str));\t// > \"string\"\nconsole.log(json_str);\t\t\t// > \"{\"result\":true, \"count\":42}\"\nconsole.log(typeof(json));\t\t// > \"object\"\nconsole.log(json);\t\t\t\t// > Object { name: \"MYZ\" }\n\n// JSON.stringify() 将值转换为相应的JSON格式, 也就是将JSON对象转换为字符串：\n// JSON对象可以进行如json[\"name\"]来取值，JSON字符串就行如‘{ \"name\":\"MYZ\" }’\njson_ify = JSON.stringify(json);\nconsole.log(typeof(json_ify));\t// > \"string\"\nconsole.log(json_ify);\t\t\t// > \"{\"name\":\"MYZ\"}\"\n\njson_parse = JSON.parse(json_ify);\nconsole.log(typeof(json_parse));\t// > \"object\"\nconsole.log(json_parse);\t\t\t// > Object { name: \"MYZ\" }\n\nconsole.log(json[\"name\"]);\t\t\t// > \"MYZ\"\nconsole.log(json_parse[\"name\"]);\t// > \"MYZ\"\nconsole.log(json[\"name\"] === json_parse[\"name\"]);\t// > true\n```\n\n``` javascript\nconst json_str = '{\"result\":true,\"count\":42}';\nconst json = { \"name\":\"MYZ\" };\n\nvar obj1 = JSON.parse(json_str);\nvar obj2 = JSON.stringify(obj1);\nconsole.log(json_str);\t\t\t> \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2);\t\t\t\t> \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2 === json_str);\t> true\n```\n\n``` javascript\nconst json_str = '{\"result\":true, \"count\":42}';\t//字符串中间有一个空格\nconst json = { \"name\":\"MYZ\" };\n\nvar obj1 = JSON.parse(json_str);\nvar obj2 = JSON.stringify(obj1);\t// 转化的字符串中间没有空格\nconsole.log(json_str);\t\t\t// > \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2);\t\t\t\t// > \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2 === json_str);\t// > false // 就因为多了空格因此两个字符串不相等\n```\n\n``` javascript\nconst json_str = '{\"result\": true,\"count\":42}';\nconst json = { \"name\":\"MYZ\" };\n\nvar obj1 = JSON.parse(json_str);\nvar obj2 = JSON.stringify(obj1);\nconsole.log(json_str);\t\t\t// > \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2);\t\t\t\t// > \"{\"result\":true,\"count\":42}\"\nconsole.log(obj2 === json_str);\t// > false\n```\n\n### html+JS 实现循环编辑HTML标签\n![](pic.JPG)\n\n``` html\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Employee</title>\n</head>\n<body>\n    <div class=\"znfz-cont\">\n        <ul id=\"data\" class=\"znfz-list\">\n        </ul>\n    </div>\n    \n    <div id=\"myDIV\"> \n        <p class=\"child\"> div 元素中 class=\"child\" 的 p </p>\n        <p class=\"child\"> div 元素中 class=\"child\" 的另外一个 p </p>\n        <p>在 div 中的 p 元素插入 <span class=\"child\">class=\"child\" 的 span 元素</span> 。</p>\n    </div>\n    <p>点击按钮为 id=\"myDIV\" 的 div 元素中所有 class=\"child\" 元素添加背景颜色。 </p>\n    <button onclick=\"myFunction()\">点我</button>\n    <p><strong>注意:</strong> Internet Explorer 8 及更早 IE 版本不支持 getElementsByClassName() 方法。</p>\n\n    <script>\n        function myFunction() {\n            var x = document.getElementById(\"myDIV\");\n            var y = x.getElementsByClassName(\"child\");\n            var i;\n            for (i = 0; i < y.length; i++) {\n                y[i].style.backgroundColor = \"red\";\n            }\n        }\n    </script>\n\n</body>\n<script>\nvar lists =[\n    {\n        \"content\":\"尾矿库风险单元划分?\",\n        \"read_num\":256\n    },\n    {\n        \"content\":\"尾矿库今年发生的事故?\",\n        \"read_num\":65\n    }\n]\nvar str=\"\";\nfor (var i = 0; i < lists.length; i++) {\n    str += \"<li class='znfz-num'>\" + i + \"</li>\";\n    str += \"<router-link class='znfz-txt' to=''>\" + lists[i].content + \"</router-link>\"\n    str += \"<span class='read-num'>\" + lists[i].read_num + \"</span>\"\n}\n\n// 下面两种方法都可以，只不过getElementsByClassName返回的是数组元素，需要加上索引值如[0]\n//document.getElementById(\"data\").innerHTML=str;\ndocument.getElementsByClassName(\"znfz-list\")[0].innerHTML=str;\n\nvar name = \"张三\";\nvar age = 20;\nconsole.log(`${name}的年龄是${age}`);\n\n</script>\n</html>\n```\n\n\n\n\n\n\n","categories":["technologies","web前端"]},{"title":"HTML","url":"/2021/03/13/technologies/web/HTML/","content":"\n### 1.HTML 链接&锚语法\n链接的 HTML 代码很简单。它类似这样：\n``` HTML  \n\t<a href=\"https://www.baidu.com\">Link text</a>\n```\n命名锚的语法：\n``` HTML\n\t<a name=\"label\">锚（显示在页面上的文本）</a>\n\t您可以使用 id 属性来替代 name 属性，命名锚同样有效。\n\t<a name=\"tips\">基本的注意事项 - 有用的提示</a>\n\t<a href=\"#tips\">有用的提示</a>\n\t您也可以在其他页面中创建指向该锚的链接：\n\t<a href=\"http://www.w3school.com.cn/html/html_links.asp#tips\">有用的提示</a>\n\t将 # 符号和锚名称添加到 URL 的末端，就可以直接链接到 tips 这个命名锚了。\n```\n\n### 2.img\n替换文本属性（Alt）  \nalt 属性用来为图像定义一串预备的可替换的文本。替换文本属性的值是用户定义的。  \n在浏览器无法载入图像时，替换文本属性告诉读者她们失去的信息。此时，浏览器将显示这个替代性的文本而不是图像。  \n为页面上的图像都加上替换文本属性是个好习惯，这样有助于更好的显示信息，并且对于那些使用纯文本浏览器的人来说是非常有用的。  \n``` HTML\n\t// 图像的 align 属性设置为 \"right\"。图像将浮动到文本的右侧。\n\t<img src=\"boat.gif\" alt=\"Big Boat\" width=\"50\" height=\"50\" align =\"right\"> \n\t您也可以把图像作为链接来使用：\n\t<a href=\"/example/html/lastpage.html\"><img border=\"0\" src=\"/i/eg_buttonnext.gif\" /></a>\n```\n\n### 3.table\n#### 3.1\n![](1.png)\n``` HTML\n\t// background 向表格添加背景图片\n\t<table border=\"1\" background=\"/i/eg_bg_07.gif\">\n\t\t<tr>\t\t\t\t\t\t\t\t\t\t\t// <tr> 表行\n\t\t\t<th bgcolor=\"red\">姓名</th>\t\t\t\t\t// <th> 表头, 表头设置特定背景色\n\t\t\t<td>Bill Gates</td>\t\t\t\t\t\t\t// <td> 表列\n\t\t</tr>\n\t\t<tr>\n\t\t\t<th rowspan=\"2\">电话</th>\n\t\t\t<td>这个单元包含一个列表：\n\t\t\t<ul>\n\t\t\t\t<li><a href=\"https://www.baidu.com\">苹果</a></li>\n\t\t\t\t<li>香蕉</li>\n\t\t\t\t<li>菠萝</li>\n\t\t\t</ul>\n\t\t\t</td>\n\t\t</tr>\n\t\t<tr>\n\t\t<td>\t\t\t\t\n\t\t\t// Cell padding 来创建单元格内容与其边框之间的空白。\n\t\t\t// Cell spacing 增加单元格之间的距离\n\t\t\t// bgcolor 向表格添加背景色\n\t\t\t<table border=\"1\" cellpadding=\"10\" cellspacing=\"3\" bgcolor=\"blue\">\t\t\n\t\t\t\t<tr>\n\t\t\t\t\t<td>A</td>\n\t\t\t\t\t<td>B</td>\n\t\t\t\t</tr>\n\t\t\t\t<tr>\n\t\t\t\t\t<td>C</td>\n\t\t\t\t\t<td>D</td>\n\t\t\t\t</tr>\n\t\t\t</table>\n\t\t</td>\n\t\t</tr>\n\t</table>\n```\n#### 3.2\n![](3.png)\n``` HTML\n\t<p>Table with frame=\"box\":</p>\n\t<table frame=\"box\">\n\t<tr>\n\t\t<th>Month</th>\n\t\t<th>Savings</th>\n\t</tr>\n\t<tr>\n\t\t<td>January</td>\n\t\t<td>$100</td>\n\t</tr>\n\t</table>\n\t\n\t<p>Table with frame=\"above\":</p>\n\t<table frame=\"above\">\n\t<tr>\n\t\t<th>Month</th>\n\t\t<th>Savings</th>\n\t</tr>\n\t<tr>\n\t\t<td>January</td>\n\t\t<td>$100</td>\n\t</tr>\n\t</table>\n\t<p>Table with frame=\"below\":</p>\n\t<table frame=\"below\">\n\t<tr>\n\t\t<th>Month</th>\n\t\t<th>Savings</th>\n\t</tr>\n\t<tr>\n\t\t<td>January</td>\n\t\t<td>$100</td>\n\t</tr>\n\t</table>\n\t\n\t<p>Table with frame=\"hsides\":</p>\n\t<table frame=\"hsides\">\n\t<tr>\n\t\t<th>Month</th>\n\t\t<th>Savings</th>\n\t</tr>\n\t<tr>\n\t\t<td>January</td>\n\t\t<td>$100</td>\n\t</tr>\n\t</table>\n\t\n\t<p>Table with frame=\"vsides\":</p>\n\t<table frame=\"vsides\">\n\t<tr>\n\t\t<th>Month</th>\n\t\t<th>Savings</th>\n\t</tr>\n\t<tr>\n\t\t<td>January</td>\n\t\t<td>$100</td>\n\t</tr>\n</table>\n```\n#### 3.3\n单元格的边框没有被显示出来。为了避免这种情况，在空单元格中添加一个空格占位符，就可以将边框显示出来。 \n![](table_td_empty.gif)\n``` HTML\n\t<table border=\"1\">\t\t\t// 带边框\n\t<caption>我的标题</caption>\t// table 标题\n\t<tr>\n\t<td>row 1, cell 1</td>\n\t<td>row 1, cell 2</td>\n\t</tr>\n\t<tr>\n\t<td>&nbsp;</td>\t\t\t\t// 空格由和号开始 (\"&\")，然后是字符\"nbsp\"，并以分号结尾(\";\")\n\t<td>row 2, cell 2</td>\n\t</tr>\n\t</table>\n```\n#### 3.4\n横跨两列的单元格，横跨两行的单元格  \n![](2.png)\n``` HTML\n\t<th colspan=\"2\">电话</th>\n\t<th rowspan=\"2\">电话</th>\n```\n\n### 4.无序ul,有序ol,自定义列表项dl\n![](4.png)\n``` html\n<h2>一个定义列表：</h2>\n<ul type=\"square\">\t\t// type=\"circle\" type=\"disc\"\n  <li>咖啡</li>\n  <li>茶</li>\n  <li>牛奶</li>\n</ul>\n\n<ol start=\"50\">\n  <li>咖啡</li>\n  <li>牛奶</li>\n  <li>茶</li>\n</ol>\n<ol type=\"A\">\t// type=\"a\", type=\"I\", type=\"i\"\n  <li>咖啡</li>\n  <li>牛奶</li>\n  <li>茶</li>\n  <ul>\n    <li>红茶</li>\n    <li>绿茶\n      <ul>\n      <li>中国茶</li>\n      <li>非洲茶</li>\n      </ul>\n  </li>\n  </ul>\n</ol>\n\n<dl>\n   <dt>计算机</dt>\n   <dd>用来计算的仪器 ... ...</dd>\n   <dt>显示器</dt>\n   <dd>以视觉方式显示信息的装置 ... ...</dd>\n</dl>\n```\n\n### 5. div, span\n``` html\n\t<!DOCTYPE html>\n\t<html>\n\t<head>\n\t<link rel=\"stylesheet\" type=\"text/css\" href=\"mystyle.css\">\n\t<style type=\"text/css\">\n\tbody {\n\t\tbackground-color: pink;\n\t\t}\n\t#header {\n\t\tbackground-color:black;\n\t\tcolor:white;\n\t\ttext-align:center;\n\t\tpadding:5px;\n\t}\n\t.nav {\n\t\tline-height:30px;\n\t\tbackground-color:#eeeeee;\n\t\theight:300px;\n\t\twidth:100px;\n\t\tfloat:left;\n\t\tpadding:5px;\t      \n\t}\n\t#section {\n\t\twidth:350px;\n\t\tfloat:left;\n\t\tpadding:10px;\t \t \n\t}\n\t#footer {\n\t\tbackground-color:black;\n\t\tcolor:white;\n\t\tclear:both;\n\t\ttext-align:center;\n\t\tpadding:5px;\t \t \n\t}\n\t</style>\n\t</head>\n\t<body>\n\t<div id=\"header\">\n\t<h1>City Gallery</h1>\n\t</div>\n\t<div class=\"nav\">\n\tLondon<br>\n\tParis<br>\n\tTokyo<br>\n\t</div>\n\t<div id=\"section\">\n\t<h2>London</h2>\n\t<p>\n\tLondon is the <span style=\"color:red\">capital city</span> of England. It is the most populous city in the United Kingdom,with a metropolitan area of over 13 million inhabitants.\n\t</p>\n\t</div>\n\t<div id=\"footer\">\n\tCopyright ? W3Schools.com\n\t</div>\n\t</body>\n\t</html>\n```\n### 6. 框架 frameset, frame\n![](6.png)\n不能将 \"body\" 标签与 \"frameset\" 标签同时使用！\n假如一个框架有可见边框，用户可以拖动边框来改变它的大小。为了避免这种情况发生，可以在 <frame> 标签中加入：noresize=\"noresize\"。\n``` html\n<html>\n\t<frameset rows=\"25%,50%,25%\">\t\t\t\t\t// 水平框架\n\t\t<frame src=\"/example/html/frame_a.html\">\n\t\t<frameset cols=\"25%,50%,25%\">\t\t\t\t// 垂直框架\n\t\t\t<frame src=\"https://www.sina.com.cn/\">\n\t\t\t<frame src=\"https://www.baidu.com\">\n\t\t\t<frame src=\"/example/html/frame_c.html\">\n\t\t</frameset>\n\t\t<frame src=\"/example/html/frame_c.html\">\n\t</frameset>\n</html>\n```\n![](6_1.png)\n```\n// // view-source:https://www.w3school.com.cn\n<html>\n<frameset cols=\"120,*\">\n  <frame src=\"/example/html/html_contents.html\">\n  <frame src=\"/example/html/frame_a.html\" name=\"showframe\">\n</frameset>\n</html>\n\n// view-source:https://www.w3school.com.cn/example/html/html_contents.html\n<html>\n<body>\n<a href =\"/example/html/frame_a.html\" target =\"showframe\">Frame a</a><br />\n<a href =\"/example/html/frame_b.html\" target =\"showframe\">Frame b</a><br />\n<a href =\"/example/html/frame_c.html\" target =\"showframe\">Frame c</a>\n</body>\n</html>\n\n// view-source:https://www.w3school.com.cn/example/html/frame_a.html   , frame_b.html,  frame_c.html\n<html>\n<body bgcolor=\"#8F8FBD\">\n<h3>Frame A</h3>\n</body>\n</html>\n\n```\n\n### 7. 内联框架 iframe\n![](7.png)\nheight 和 width 属性用于规定 iframe 的高度和宽度，属性值的默认单位是像素，但也可以用百分比来设定（比如 \"80%\"）\nframeborder 属性规定是否显示 iframe 周围的边框，设置属性值为 \"0\" 就可以移除边框\n注释：由于链接的目标匹配 iframe 的名称，所以点击\"百度新闻\"链接, 会在 iframe 中打开百度新闻网页。\n``` html\n<iframe src=\"https://www.baidu.com\" frameborder=\"0\" width=\"80%\" height=\"200\" name=\"iframe_a\"></iframe>\n<p><a href=\"http://news.baidu.com/\" target=\"iframe_a\">百度新闻</a></p>\n```\n<iframe src=\"https://www.baidu.com\" frameborder=\"0\" width=\"80%\" height=\"200\" name=\"iframe_a\"></iframe>\n<p><a href=\"http://news.baidu.com/\" target=\"iframe_a\">百度新闻</a></p>\n\n### 8. HTML 头部元素\n```\n标签\t描述  \n<head>\t定义关于文档的信息。  \n<title>\t定义文档标题。  \n<base>\t定义页面上所有链接的默认地址或默认目标。  \n<link>\t定义文档与外部资源之间的关系。\n\t<link> 标签最常用于连接样式表：\n<meta>\t定义关于 HTML 文档的元数据。 \n\t<meta> 标签提供关于 HTML 文档的元数据。元数据不会显示在页面上，但是对于机器是可读的。\n\t典型的情况是，meta 元素被用于规定页面的描述、关键词、文档的作者、最后修改时间以及其他元数据。\n\t<meta> 标签始终位于 head 元素中。\n\t元数据可用于浏览器（如何显示内容或重新加载页面），搜索引擎（关键词），或其他 web 服务。\n\t下面的 meta 元素定义页面的描述：\n\t<meta name=\"description\" content=\"Free Web tutorials on HTML, CSS, XML\" />\n\t下面的 meta 元素定义页面的关键词：\n\t<meta name=\"keywords\" content=\"HTML, CSS, XML\" />\n\tname 和 content 属性的作用是描述页面的内容。\n<script>\t定义客户端脚本。  \n<style>\t定义文档的样式信息。\n\t<style> 标签用于为 HTML 文档定义样式信息。\n\t您可以在 style 元素内规定 HTML 元素在浏览器中呈现的样式\n```\n\n### 9. HTML 字符实体\n```\n注释：实体名称对大小写敏感！\n\n显示结果\t描述\t\t实体名称\t\t实体编号\n\t\t空格\t\t&nbsp;\t\t\t&#160;\n<\t\t小于号\t\t&lt;\t\t\t&#60;\n>\t\t大于号\t\t&gt;\t\t\t&#62;\n&\t\t和号\t\t&amp;\t\t\t&#38;\n\"\t\t引号\t\t&quot;\t\t\t&#34;\n'\t\t撇号 \t\t&apos;IE不支持\t&#39;\n￠\t\t分（cent）\t&cent;\t\t\t&#162;\n£\t\t镑（pound）\t&pound;\t\t\t&#163;\n¥\t\t元（yen）\t&yen;\t\t\t&#165;\n€\t\t欧元（euro）\t&euro;\t\t\t&#8364;\n§\t\t小节\t\t&sect;\t\t\t&#167;\n©\t\t版权(copyright)\t&copy;\t\t\t&#169;\n®\t\t注册商标\t&reg;\t\t\t&#174;\n™\t\t商标\t\t&trade;\t\t\t&#8482;\n×\t\t乘号\t\t&times;\t\t\t&#215;\n÷\t\t除号\t\t&divide;\t\t&#247;\n```\n\n### 10. HTML 颜色名\n提示：仅有 16 种颜色名被 W3C 的 HTML 4.0 标准支持  \n它们是：aqua、black、blue、fuchsia、gray、green、lime、maroon、navy、olive、purple、red、silver、teal、white、yellow。  \n如果使用其它颜色的话，就应该使用十六进制的颜色值。   \n\n### 11. HTML <!DOCTYPE>\n```\n<!DOCTYPE> 声明帮助浏览器正确地显示网页\nWeb 世界中存在许多不同的文档。只有了解文档的类型，浏览器才能正确地显示文档。\nHTML 也有多个不同的版本，只有完全明白页面中使用的确切 HTML 版本，浏览器才能完全正确地显示出 HTML 页面。这就是 <!DOCTYPE> 的用处。\n<!DOCTYPE> 不是 HTML 标签。它为浏览器提供一项信息（声明），即 HTML 是用什么版本编写的。\n常用的声明\nHTML5\n<!DOCTYPE html>\n\nHTML 4.01\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\n\"http://www.w3.org/TR/html4/loose.dtd\">\n\nXHTML 1.0\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n\"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n```\n\n### 12. form表单\nHTML 表单用于收集用户输入。```<form>``` 元素定义 HTML 表单\n\n下面是 ```<form>``` 属性的列表：\n属性\t描述\naccept-charset\t规定在被提交表单中使用的字符集（默认：页面字符集）。\naction\t规定向何处提交表单的地址（URL）（提交页面）。\nautocomplete\t规定浏览器应该自动完成表单(默认:开启)\nenctype\t规定被提交数据的编码（默认：url-encoded）。\nid\t表单唯一id\tid=\"form1\"\nmethod\t规定在提交表单时所用的 HTTP 方法（默认：GET）。\nname\t规定识别表单的名称（对于 DOM 使用：document.forms.name）。\nnovalidate\t规定在提交表单时不对表单数据进行验证。\ntarget\t规定 action 属性中地址的目标（默认：_self）。\n\n列出了一些常用的输入限制\nautocomplete\t当自动完成开启，浏览器会基于用户之前的输入值自动填写值\tautocomplete=\"off\"\nautofocus\t规定当页面加载时 ```<input>``` 元素应该自动获得焦点\tautofocus\ndisabled\t规定输入字段应该被禁用, 不可用和不可点击, 被禁用的元素不会被提交\nmax\t规定输入字段的最大值。\nmaxlength\t规定输入字段的最大字符数。\nmin\t规定输入字段的最小值。\npattern\t规定通过其检查输入值的正则表达式。 只能包含三个字母的输入字段:pattern=\"[A-Za-z]{3}\"\nplaceholder 属性规定用以描述输入字段预期值的提示（样本值或有关格式的简短描述）\nreadonly\t规定输入字段为只读(无法修改).\nrequired\t如果设置，则规定在提交表单之前必须填写输入字段。\nsize\t规定输入字段的宽度（以字符计）。\nstep\t规定输入字段的合法数字间隔。如果 step=\"3\"，则合法数字应该是 -3、0、3、6、等等。\nvalue\t规定输入字段的默认值。\n\n下列属性适用于 type=\"submit\" 以及 type=\"image\"。\nformaction 属性覆盖 ```<form>``` 元素的 action 属性.\tformaction=\"demo_admin.asp\"\nformenctype 属性规定当把表单数据（form-data）提交至服务器时如何对其进行编码(仅针对 method=\"post\" 的表单).\tformenctype=\"multipart/form-data\"\nformmethod 属性定义用以向 action URL 发送表单数据(form-data)的HTTP方法.\tformmethod=\"post\"\nformtarget 属性规定的名称或关键词指示提交表单后在何处显示接收到的响应。\tformtarget=\"_blank\"会提交到新窗口/选项卡\n\n下列属性适用于 type=\"submit\"\n如果设置，则规定在提交表单时不对 ```<input>``` 元素进行验证。\nformnovalidate 属性覆盖 ```<form>``` 元素的 novalidate 属性。\tformnovalidate \n\n下列属性适用于 type=\"image\"\nheight 和 width 属性规定 ```<input type=\"image\">```元素的高度和宽度。\n![](12.png)\n``` HTML\n// action 属性定义在提交表单时执行的动作,指定了某个服务器脚本来处理被提交表单, 通常表单会被提交到 web 服务器上的网页\n// method 属性规定在提交表单时所用的 HTTP 方法（GET 或 POST）\n// 如果表单提交是被动的(比如搜索引擎查询),并且没有敏感信息,当您使用 GET 时，表单数据在页面地址栏中是可见的:action_page.php?firstname=Mickey&lastname=Mouse\n// GET 最适合少量数据的提交。浏览器会设定容量限制。\n// POST 的安全性更加，因为在页面地址栏中被提交的数据是不可见的. 如果表单正在更新数据，或者包含敏感信息(例如密码)\n<form action=\"action_page.php\" method=\"GET\" id=\"form1\">\n First name:<br>\n\n<fieldset>\n\n// 定义用于文本输入的单行输入字段\n<input type=\"text\" name=\"firstname\"  value=\"jone\" disabled>\n<br>\n\n// password 字段中的字符会被做掩码处理（显示为星号或实心圆）\n<input type=\"password\" name=\"psw\" autocomplete=\"off\" autofocus>\n<br>\n\n// <datalist> 元素为 <input> 元素规定预定义选项列表。\n<input list=\"browsers\" name=\"browser\" placeholder=\"Firefox\">\n<datalist id=\"browsers\">\n  <option value=\"Internet Explorer\">\n  <option value=\"Firefox\">\n  <option value=\"Chrome\">\n  <option value=\"Opera\">\n  <option value=\"Safari\">\n</datalist>\n\n// 单选按钮允许用户在有限数量的选项中选择其中之一\n<input type=\"radio\" name=\"sex\" value=\"male\" checked>Male\n<br>\n<input type=\"radio\" name=\"sex\" value=\"female\">Female\n<br>\n\n// 复选框允许用户在有限数量的选项中选择零个或多个选项\n<input type=\"checkbox\" name=\"vehicle\" value=\"Bike\">I have a bike\n<br>\n<input type=\"checkbox\" name=\"vehicle\" value=\"Car\">I have a car\n<br>\n\n// 下拉列表\n<select name=\"cars\">\n<option value=\"volvo\">Volvo</option>\n<option value=\"saab\" selected>Saab</option>\t\t// 通过添加 selected 属性来定义预定义选项。\n<option value=\"fiat\">Fiat</option>\n<option value=\"audi\">Audi</option>\n</select>\n\n// 定义多行输入字段(文本域)\n<textarea name=\"message\" rows=\"10\" cols=\"30\" readonly>\nThe cat was playing in the garden.\n</textarea>\n\n// 定义可点击的按钮\n<button type=\"button\" onclick=\"alert('Hello World!')\">Click Me!</button>\n\n</fieldset>\n\nQuantity (between 1 and 5):<br>\n<input type=\"number\" name=\"quantity\" min=\"1\" max=\"5\">\n<br>\n\nQuantity (between 1 and 5):<br>\n<input type=\"number\" name=\"quantity\" min=\"1\" max=\"5\">\n<br>\n\n// 日期\nBirthday:\n  <input type=\"date\" name=\"bday1\"><br>\nEnter a date before 1980-01-01:\n  <input type=\"date\" name=\"bday2\" max=\"1979-12-31\"><br>\n  Enter a date after 2000-01-01:\n  <input type=\"date\" name=\"bday3\" min=\"2000-01-02\"><br>\n\nSelect your favorite color:\n  <input type=\"color\" name=\"favcolor\"><br>\n\n// 用于应该包含一定范围内的值的输入字段, 根据浏览器支持，输入字段能够显示为滑块控件\n<input type=\"range\" name=\"points\" min=\"0\" max=\"10\"><br>\n\nBirthday (month and year):\n  <input type=\"month\" name=\"bdaymonth\"><br>\nSelect a week:\n  <input type=\"week\" name=\"week_year\"><br>\nSelect a time:\n  <input type=\"time\" name=\"usr_time\"><br>\nBirthday (date and time):\n  <input type=\"datetime\" name=\"bdaytime\"><br>\nBirthday (date and time):\n  <input type=\"datetime-local\" name=\"bdaytime\"><br>\nE-mail:\n  <input type=\"email\" name=\"email\"><br>\nSearch Google:\n  <input type=\"search\" name=\"googlesearch\"><br>\nTelephone:\n  <input type=\"tel\" name=\"usrtel\"><br>\nAdd your homepage 某些智能手机识别 url 类型，并向键盘添加 \".com\" 以匹配 url 输入。:\n  <input type=\"url\" name=\"homepage\"><br>\n\n// multiple 属性是布尔属性。如果设置，则规定允许用户在 <input> 元素中输入一个以上的值。multiple 属性适用于以下输入类型：email 和 file。\nSelect images: <input type=\"file\" name=\"img\" multiple><br>\n\n\n\n\n// 如果要正确地被提交，每个输入字段必须设置一个 name 属性。\n// 定义用于向表单处理程序（form-handler）提交表单的按钮\n// 表单处理程序在表单的 action 属性中指定\n<input type=\"submit\" value=\"Submit\"> \n\n// formaction 属性覆盖 <form> 元素的 action 属性\n<input type=\"submit\" formaction=\"demo_admin.asp\" value=\"Submit as admin\">\n   \n</form> \n\n下面的 \"Last name\" 字段位于 form 元素之外，但仍然是表单的一部分。<br>\nLast name: <input type=\"text\" name=\"lname\" form=\"form1\">\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["technologies","web前端"]},{"title":"PKI 体系概述","url":"/2021/03/13/technologies/security/pki/","content":"\nReference: [简述 PKI体系概述](https://www.jianshu.com/p/46a911bd49a7)\n\n# **Public Key Infrastructure(PKI)**\nCA 中心管理并运营 CA 系统，CA 系统负责颁发数字证书。  \n专门负责颁发数字证书的系统称为 CA 系统，负责管理并运营 CA 系统的机构称为 CA 中心。所有与数字证书相关的各种概念和技术，统称为 **PKI（Public Key Infrastructure）**  \n\n# **加密基元**\n加密基元就是一些基础的密码学算法，通过它们才能够构建更多的密码学算法、协议、应用程序。\n![](base_encryption.png)\n说明：\n * 散列函数（散列（hash）、指纹、消息摘要、摘要算法、杂凑函数）：把任意长度的输入消息数据转化成固定长度的输出数据的一种密码算法。\n * 消息验证代码：验证数据完整性，即数据没有被篡改。\n * 数字签名：RSA私钥加密，公钥解密，结合散列函数。验证消息真实性。\n * 伪随机函数（PRF）：生成任意数量的伪随机数据。\n * RSA：可以同时用于密钥交换和身份验证（数字签名）。\n * DHE_RSA：DHE 算法：密钥协商，RSA 算法：身份验证（数字签名）。\n * ECDHE_RSA： ECDHE 算法：密钥协商，RSA 算法：身份验证（数字签名）。\n * ECDHE_ECDSA ：ECDHE 算法：密钥协商，ECDSA 算法：身份验证（数字签名）。\n\n# **密钥管理**\n![](key_save.png)\n密钥管理模式：\n * 无中心模式；\n * 有中心模式；\n\n# **PKI 模式**\nPKI 是 Public Key Infrastructure 的缩写，其主要功能是绑定证书持有者的身份和相关的密钥对（通过为公钥及相关的用户身份信息签发数字证书），为用户提供方便的证书申请、证书作废、证书获取、证书状态查询的途径，并利用数字证书及相关的各种服务（证书发布，黑名单发布，时间戳服务等）实现通信中各实体的身份认证、完整性、抗抵赖性和保密性。  \n![](pki_mode.png)\n * 数字证书：解决公钥与用户映射关系问题；\n * CA：解决数字证书签发问题；\n * KMC：解决私钥的备份与恢复问题；\n * 双证书机制：「签名证书及私钥」只用于签名验签，「加密证书及私钥」只用于加密解密。\n * LDAP：解决数字证书查询和下载的性能问题，避免 CA 中心成为性能瓶颈。\n * CRL（证书作废列表） 和 OSCP（在线证书状态协议）：方便用户快速获得证书状态。\n * RA：方便证书业务远程办理、方便证书管理流程与应用系统结合。\n * 电子认证服务机构：保证 CA 系统在数字证书管理方面的规范性、合规性和安全性。\n\n## 基于数字证书可以实现四种基本安全功能:\n 1. 身份认证；\n 2. 保密性；\n 3. 完整性；\n 4. 抗抵赖性；\n\n# **PKI 基本系统组件**\n完整的 PKI 系统必须具有数字证书、认证中心（CA）、证书资料库、证书吊销系统、密钥备份及恢复系统、PKI 应用接口系统等构成部分。  \n**组件描述:**\n * **数字证书:** 包含了用于签名和加密数据的公钥的电子凭证，是PKI的核心元素\n * **认证中心（CA）:** 数字证书的申请及签发机关，CA必须具备权威性\n * **证书资料库:** 存储已签发的数字证书和公钥，以及相关证书目录，用户可由此获得所需的其他用户的证书及公钥\n * **证书吊销列表（CRL）/OCSP:** 在有效期内吊销的证书列表，在线证书状态协议OCSP是获得证书状态的国际协议\n * **密钥备份及恢复:** 为避免因用户丢失解密密钥而无法解密合法数据的情况，PKI提供备份与恢复密钥的机制。必须由可信的机构来完成。并且，密钥备份与恢复只能针对解密密钥，签名私钥不能够作备份\n * **PKI应用接口（API）:** 为各种各样的应用提供安全、一致、 可信的方式与PKI交互，确保建立起来的网络环境安全可靠，并降低管理成本\n\n# **TLS**\n![](tls_deployment.jpg)\n**密码套件**决定了本次连接采用哪一种加密算法、密钥协商算法、HMAC 算法，即各个密码学算法的组合。\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"TLS/SSL/HTTP/HTTPS protocol","url":"/2021/03/13/technologies/security/tls_ssl_https_http_protocol/","content":"\n## What is protocol\n网络协议通俗的讲是网络上两台计算机之间通信所要共同遵守的标准。协议规定了一种信息交流的格式和规范。协议本身并不是一种软件，只是一种通信的标准，协议最终需要由软件来实现，网络协议的实现就是在不同的软件和硬件环境下，执行可运行于开中环境的“协议”翻译程序。说白了就是软件要实现这个协议翻译程序，从而使双方遵守这某一协议。不同的网络交互软件的功能可能不同，但是都会翻译同一种网络协议。实现网络协议，就像是给所有接入网络的设备配备了一个“通用语言翻译器”，这些翻译都懂通用语言：例如国际上的英语，同时它也懂得本国的语言。这样就能实现不同国家不同环境的人接入同一个网络并进行交流。  \n\n**协议分层**：我用英语说：“How are you.” 不一定表示“你好！”，我们可以事先约定，这句话表示“再看一遍”的意思。这就象是所谓的江湖“黑话”，或叫“专业术语”。实际上，这时我们自己制定了一个新的通信标准，一个新的“高层协议”己经诞生了。这个协议在“英语”的基础上，再制定自己的通信标准，这种新的通信标准就是基于“英语”这种“底层协议”的“高层协议”，我们可以把这种协议取名为“讲课协议”。说白了协议的分层就是在一个既定的协议之上再添加某些限定条件。创造一个新的协议。比如果规定双方用英语交流，那么在使用英语交流的过程中，对英语中的“Hello”单词，定义他的意思是”帅哥“。那么它在双方交流的时候就被理解为帅哥的意思。这就是又制定了一个新的协议。协议除了分层之外，还可以组合，比如将IP协议，TCP协议以及UDP协议组合在一起称为TCP/IP协议。  \n\n所以说下面的SSL协议就是在TCP协议的基础上，又对信息传输做了某些规定，从而产生的一种新的协议。\n\n## HTTPS、TLS、SSL\nHTTPS，也称作HTTP over TLS。TLS的前身是SSL，TLS 1.0通常被标示为SSL 3.1，TLS 1.1为SSL 3.2，TLS 1.2为SSL 3.3。下图描述了在TCP/IP协议栈中TLS(各子协议)和HTTP的关系。  \n![](http_tls_ssl.png)\n\n## HTTPS protocol\n![](https.png)\n由于http明文传输，很不安全所以出现了https.\n\nhttps是由http“调用”SSL/TLS中的加密算法和协议逻辑，实现保密传输。\n\nhttps不能说是一个协议，只能说是一种应用。SSL加密http中的内容。然后默认使用433端口进行传输，SSL还可以加密Email，默认使用955，465端口。任何一个应用层协议都可以调用TLS/SSL来加密其明文数据。\n\n所以简单总结就是 : https = http + SSL/TLS  \nHTTPS在RFC2818被标准化, HTTPS工作在443端口，而HTTP默认工作在80端口。  \nHTTP的连接很简单,是无状态的。  \nHTTPS协议是由SSL+HTTP协议构建的可进行加密传输、身份认证的网络协议，要比HTTP协议安全。  \n\n从上面可看出，HTTPS和HTTP协议相比提供了  \n· 数据完整性：内容传输经过完整性校验  \n\n· 数据隐私性：内容经过对称加密，每个连接生成一个唯一的加密密钥  \n\n· 身份认证：第三方无法伪造服务端(客户端)身份  \n\n其中，数据完整性和隐私性由TLS Record Protocol保证，身份认证由TLS Handshaking Protocols实现。  \n\n## SSL protocol\nSSL协议（Secure Socket Layer安全套接层）是Netscape研发的，保障在Internet上数据传输安全的一个协议，它利用数据加密技术，可以确保数据在网络上的传输过程不被截取或者窃听。被广泛应用于Web浏览器和服务器之间的身份认证和加密数据传输。\n\nSSL协议位于TCP/IP协议与各种应用层协议之间。为数据通讯提供安全支持。\n\nSSL协议可以分为两层：\n\n**SSL记录协议**：建立在可靠的传输协议（如TCP）上，为高层协议提供数据封装，压缩，加密，解密基本功能。\n**SSL握手协议**：建立在SSL记录协议止尚，用于在实际的数据传输开始之前，通讯双方进行身份验证，协商加密算法，交换加密密钥等。\n\nEmail over SSL: \n类似于HTTP over SSL，邮件协议例如：  \n * SMTP，POP3、IMAP也能支持SSL。  \n * SMTP over TLS的标准文档在RFC2487  \n * POP3和IMAP over TLS的标准化文档在RFC2595.  \n\n## TLS protocol\nTLS协议Transport Layer Security 传输层安全）是SSL协议经历了SSL1.0,2.0,3.0版本后发展成为的一种新的安全标准协议。TLS有1.0，1.1，1.2，1.3这几个版本.  \n\nTLS全称是安全传输层协议，用于在两个通信应用程序之间提供保密性和数据完整性。\n\n该协议由两层组成：\n\n**TLS记录协议**，**TLS握手协议**，较底层的是TLS记录协议。位于某个可靠的传输协议（如TCP）之上。\n\n一次加密通信需要实现3个任务：机密性，完整性，身份认证。\n\n版本号：TLS记录合适与SSL记录格式相同，但是版本号值不同，TLS的版本1.0使用的版本号为SSLV3.1.\n\n## TLS/SSL在网络通信模型中的位置\n * 在OSI协议层中\n![](osi.png)\n7\t应用层  \napplication layer\t例如HTTP、SMTP、SNMP、FTP、Telnet、SIP、SSH、NFS、RTSP、XMPP、Whois、ENRP  \n6\t表示层  \npresentation layer\t例如XDR、ASN.1、SMB、AFP、NCP  \n5\t会话层  \nsession layer\t例如ASAP、SSH、ISO 8327 / CCITT X.225、RPC、NetBIOS、ASP、IGMP、Winsock、BSD sockets  \n4\t传输层  \ntransport layer\t例如TCP、UDP、TLS、RTP、SCTP、SPX、ATP、IL  \n3\t网络层  \nnetwork layer\t例如IP、ICMP、IPX、BGP、OSPF、RIP、IGRP、EIGRP、ARP、RARP、X.25  \n2\t数据链路层  \ndata link layer\t例如以太网、令牌环、HDLC、帧中继、ISDN、ATM、IEEE 802.11、FDDI、PPP  \n1\t物理层  \nphysical layer\t例如线路、无线电、光纤  \n\n * TCP/IP协议中\n![](tcp_ip.png)\n4\t应用层  \napplication layer\t例如HTTP、FTP、DNS  \n（如BGP和RIP这样的路由协议，尽管由于各种各样的原因它们分别运行在TCP和UDP上，仍然可以将它们看作网络层的一部分）  \n3\t传输层  \ntransport layer\t例如TCP、UDP、RTP、SCTP  \n（如OSPF这样的路由协议，尽管运行在IP上也可以看作是网络层的一部分）  \n2\t网络互连层  \ninternet layer\t对于TCP/IP来说这是因特网协议（IP）  \n（如ICMP和IGMP这样的必须协议尽管运行在IP上，也仍然可以看作是网络互连层的一部分；ARP不运行在IP上）  \n1\t网络接口层  \nlink layer\t例如以太网、Wi-Fi、MPLS等。  \n\n\n\n\n\n\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"IntelliJ idea","url":"/2021/03/13/technologies/maven/IntelliJ_idea/","content":"\n## 下载IntelliJ idea\n\n## 配置Maven\n\n## 选择自安装的JDK\n\n## 配置Proxy\n\n## 安装plugins\n\n## 安装thrift插件\n\n## Install shellchek to verify shell scripts\n","categories":["technologies","maven"]},{"title":"navicat","url":"/2021/03/13/technologies/maven/navicat_premium/","content":"\n## Navicat简介\n\tNavicat是可以管理多种数据库Mysql, redis, MongoDB等等的软件，收费\n\n\n\n","categories":["technologies","maven"]},{"title":"thrift","url":"/2021/03/13/technologies/maven/thrift/","content":"\n## Thrift 简介\nThrift可以定义多种语言的函数接然后并生成源码文件.\n\n## Ubuntu18.04 安装 Thrift\n * 1. sudo apt-get install automake bison flex g++ git libboost-all-dev libevent-dev libssl-dev libtool make pkg-config\n * 2. tar -zxvf thrift-0.9.3.tar.gz\n * 3. cd thrift-0.9.3\n * 4.  ./configure CXXFLAGS='-ggdb3' --with-java --with-python --without-cpp --without-boost --without-csharp --without-erlang --without-perl --without-php --without-php_extension --without-ruby --without-haskell --without-go\n * 5. make\n * 6. make install\n * 7. thrift -version\n * 8. cd thrift-0.9.3/lib/py\n * 9. python setup.py install\n\n### 遇到的问题:\n#### 问题1: from thrift.Thrift import TType, TMessageType, TFrozenDict, TException, TApplicationException ImportError: cannot import name TFrozenDict\n#### 问题2: 安装sasl的时候报错：sasl/saslwrapper.h:22:23: fatal error: sasl/sasl.h: No such file or directory\n * 久经折腾，最终发现缺少包：https://pypi.python.org/pypi/sasl/0.1.3\n * Debian/Ubuntu:$ apt-get install python-dev libsasl2-dev gcc\n * CentOS/RHEL:$ yum install gcc-c++ python-devel.x86_64 cyrus-sasl-devel.x86_64\n * 最后安装: 某些包没有关联上，装包时加上[hive]后缀 $ pip install pyhive[hive]\n\n\n","categories":["technologies","maven"]},{"title":"grpc 01 conception","url":"/2021/03/13/technologies/grpc/grpc_01_conception/","content":"\n## **Conception**\n所谓RPC(remote procedure call 远程过程调用)框架实际是提供了一套机制，使得应用程序之间可以进行通信，而且也遵从server/client模型。使用的时候客户端调用server端提供的接口就像是调用本地的函数一样。如下图所示就是一个典型的RPC结构图\n![](1.png)\ngRPC其实就是google在rpc基础上定义了一套自己的使用方式, 也可说成是google的prc方式开发.  \n学习gPRC得学习Protocol Buffers, 它可以用来定义消息和服务.  \n然后只需要实现服务即可, 剩余的gPRC代码将会自动为你生成.  \n.proto这个文件可以使用于十几种开发语言(包括服务端和客户端), 并且允许你使用同一个框架来支持每秒百万级以上的PRC调用.  \n\n### 开发模式\ngPRC使用合约优先的API开发模式, 默认使用Protocol buffers(protobuf)作为接口设计语言(IDL), 这个.proto文件包括两部分:\n * gPRC服务的定义\n * 服务端和客户端之间传递的消息\n\n## **开发环境**\nVSCode 扩展插件: vscode-proto3, Clang-Format\nWindows还需要安装Clang, Widnows安装地址：[Clang download for windows](http://www.llvm.org/releases/download.html) Windows (64-bit) (.sig)\n\n下载后安装过程中可以选择Add LLVM to the system PATH for all users\n\n## VSCode 一些常用操作\nCtrl+, 打开设置面板\nCtrl+shift+p 搜索theme主题样式 -> File icon theme -> Install Additional File Icon Themes... -> 可以选择Material Icon Theme， 装完主题之后点击右下角蹦出的Activate激活  \n\n\n\n\n\n","categories":["technologies","grpc"]},{"title":"certificates, public/private key, signature digest","url":"/2021/03/13/technologies/security/key_digest_signature_certificates/","content":"\n## **openssl概述**\nOpenSSL 是一个强大的安全套接字层密码库，囊括主要的密码算法、常用的密钥和证书封装管理功能及SSL协议，并提供丰富的应用程序供测试或其它目的使用。\n\n## **公钥/私钥/签名/验证签名/加密/解密/非对称加密**\n我们一般的加密是用一个密码加密文件,然后解密也用同样的密码.这很好理解,这个是对称加密.而有些加密时,加密用的一个密码,而解密用另外一组密码,这个叫非对称加密,意思就是加密解密的密码不一样.初次接触的人恐怕无论如何都理解不了.其实这是数学上的一个素数积求因子的原理的应用,如果你一定要搞懂,百度有大把大把的资料可以看,其结果就是用这一组密钥中的一个来加密数据,可以用另一个解开.是的没错,公钥和私钥都可以用来加密数据,相反用另一个解开,公钥加密数据,然后私钥解密的情况被称为加密解密,私钥加密数据,公钥解密一般被称为签名和验证签名.\n\n因为公钥加密的数据只有它相对应的私钥可以解开,所以你可以把公钥给人和人,让他加密他想要传送给你的数据,这个数据只有到了有私钥的你这里,才可以解开成有用的数据,其他人就是得到了,也看懂内容.同理,如果你用你的私钥对数据进行签名,那这个数据就只有配对的公钥可以解开,有这个私钥的只有你,所以如果配对的公钥解开了数据,就说明这数据是你发的,相反,则不是.这个被称为签名.\n\n实际应用中,一般都是和对方交换公钥,然后你要发给对方的数据,用他的公钥加密,他得到后用他的私钥解密,他要发给你的数据,用你的公钥加密,你得到后用你的私钥解密,这样最大程度保证了安全性.\n\n## **RSA/AES/DSA/SHA/MD5**\n非对称加密的算法有很多,比较著名的有RSA/DSA ,不同的是RSA可以用于加/解密,也可以用于签名验签,DSA则只能用于签名.  \n\n至于<font face=\"fantasy\" color=#0099ff>**SHA**</font>则是一种和md5相同的算法,它不是用于加密解密或者签名的,它被称为<font face=\"fantasy\" color=#0099ff>**摘要算法**</font>.就是通过一种算法,依据数据内容生成一种固定长度的摘要,这串摘要值与原数据存在对应关系,就是原数据会生成这个摘要,但是,这个摘要是不能还原成原数据的,嗯….,正常情况下是这样的,这个算法起的作用就是,如果你把原数据修改一点点,那么生成的摘要都会不同,传输过程中把原数据给你再给你一个摘要,你把得到的原数据同样做一次摘要算法,与给你的摘要相比较就可以知道这个数据有没有在传输过程中被修改了.  \nHash算法特别的地方在于它是一种单向算法，用户可以通过Hash算法对目标信息生成一段特定长度的唯一的Hash值，却不能通过这个Hash值重新获得目标信息。因此Hash算法常用在不可还原的密码存储、信息完整性校验等。\n\n常见的Hash算法有MD2、MD4、MD5、HAVAL、SHA.\n\n加密算法的效能通常可以按照算法本身的复杂程度、密钥长度（密钥越长越安全）、加解密速度等来衡量。上述的算法中，除了DES密钥长度不够、MD2速度较慢已逐渐被淘汰外，其他算法仍在目前的加密系统产品中使用。\n\n实际应用过程中,因为需要加密的数据可能会很大,进行加密费时费力,所以一般都会把原数据先进行摘要,然后对这个摘要值进行加密,将原数据的明文和加密后的摘要值一起传给你.这样你解开加密后的摘要值,再和你得到的数据进行的摘要值对应一下就可以知道数据有没有被修改了,而且,因为私钥只有你有,只有你能解密摘要值,所以别人就算把原数据做了修改,然后生成一个假的摘要给你也是不行的,你这边用密钥也根本解不开.  \n\n**RSA: **\n**非对称加密**，公钥加密，私钥解密，反之亦然。由于需要大数的乘幂求模等算法，运行速度慢，不易于硬件实现。\n\n通常私钥长度有512bit，1024bit，2048bit，4096bit，长度越长，越安全，但是生成密钥越慢，加解密也越耗时。\n\n既然是加密，那肯定是不希望别人知道我的消息，所以只有我才能解密，所以可得出公钥负责加密，私钥负责解密；\n\n同理，既然是签名，那肯定是不希望有人冒充我发消息，只有我才能发布这个签名，所以可得出私钥负责签名，公钥负责验证。\n\n**AES: **\n**对称加密**，密钥最长只有256个bit，执行速度快，易于硬件实现。由于是对称加密，密钥需要在传输前通讯双方获知。\n\n基于以上特点，通常使用RSA来首先传输AES的密钥给对方，然后再使用AES来进行加密通讯。\n\n## **CA/PEM/DER/X509/PKCS**\n一般的公钥不会用明文传输给别人的,正常情况下都会生成一个文件,这个文件就是公钥文件,然后这个文件可以交给其他人用于加密,但是传输过程中如果有人恶意破坏,将你的公钥换成了他的公钥,然后得到公钥的一方加密数据,不是他就可以用他自己的密钥解密看到数据了吗,为了解决这个问题,需要一个公证方来做这个事,任何人都可以找它来确认公钥是谁发的.这就是CA,CA确认公钥的原理也很简单,它将它自己的公钥发布给所有人,然后一个想要发布自己公钥的人可以将自己的公钥和一些身份信息发给CA,**CA用自己的密钥进行加密,这里也可以称为<font face=\"fantasy\" color=#0099ff>签名</font>.** 然后这个**包含了你的公钥和你的信息的文件就可以称为<font face=\"fantasy\" color=#0099ff>证书文件</font>了.**这样一来所有得到一些公钥文件的人,通过CA的公钥解密了文件,如果正常解密那么机密后里面的信息一定是真的,因为加密方只可能是CA,其他人没它的密钥啊.这样你解开公钥文件,看看里面的信息就知道这个是不是那个你需要用来加密的公钥了.\n\n实际应用中,一般人都不会找CA去签名,因为那是收钱的,所以可以自己做一个自签名的证书文件,就是自己生成一对密钥,然后再用自己生成的另外一对密钥对这对密钥进行签名,这个只用于真正需要签名证书的人,普通的加密解密数据,直接用公钥和私钥来做就可以了.\n\n密钥文件的格式用OpenSSL生成的就只有PEM和DER两种格式,PEM的是将密钥用base64编码表示出来的,直接打开你能看到一串的英文字母,DER格式是二进制的密钥文件,直接打开,你可以看到……..你什么也看不懂!  \n\n.X509是通用的证书文件格式定义.  \n\npkcs的一系列标准是指定的存放密钥的文件标准。  \n\n你只要知道PEM DER X509 PKCS这几种格式是可以互相转化的.  \n\n\n下面内容转自: [csdn certificate key blog](https://blog.csdn.net/qq_38684504/article/details/89083200?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-3.nonecase)\n\n## **一、公钥加密**\n\n假设一下，我找了两个数字，一个是1，一个是2。我喜欢2这个数字，就保留起来，不告诉你们(私钥），然后我告诉大家，1是我的公钥。\n\n我有一个文件，不能让别人看，我就用1加密了。别人找到了这个文件，但是他不知道2就是解密的私钥啊，所以他解不开，只有我可以用数字2，就是我的私钥，来解密。这样我就可以保护数据了。\n\n我的好朋友x用我的公钥1加密了字符a，加密后成了b，放在网上。别人偷到了这个文件，但是别人解不开，因为别人不知道2就是我的私钥，只有我才能解密，解密后就得到a。这样，我们就可以传送加密的数据了。\n\n## **二、私钥签名**\n\n如果我用私钥加密一段数据（当然只有我可以用私钥加密，因为只有我知道2是我的私钥），结果所有的人都看到我的内容了，因为他们都知道我的公钥是1，那么这种加密有什么用处呢？\n\n但是我的好朋友x说有人冒充我给他发信。怎么办呢？我把我要发的信，内容是c，用我的私钥2，加密，加密后的内容是d，发给x，再告诉他解密看是不是c。他用我的公钥1解密，发现果然是c。这个时候，他会想到，能够用我的公钥解密的数据，必然是用我的私钥加的密。只有我知道我的私钥，因此他就可以确认确实是我发的东西。\n\n这样我们就能确认发送方身份了。这个过程叫做数字签名。当然具体的过程要稍微复杂一些。<table><tr><td bgcolor=#54FF9F>用私钥来加密数据，用途就是数字签名.</td></tr></table>\n\n**总结：**\n\n     **公钥和私钥是成对的，它们互相解密。**\n\n     **公钥加密，私钥解密。**\n\n     **私钥数字签名，公钥验证。**\n\n> **数字签名**是指将摘要信息使用接收者的公钥进行加密，与密文一起发送给接收者。接收者使用自己的私钥对摘要信息进行解密，然后使用Hash函数对收到的密文产生一个摘要信息，然后将摘要信息与发送着传输过来解密后的摘要信息对比是否一致。如果一致，则表明数据信息没有被篡改。\n\n## **三、举例**\n\n比如有两个用户Alice和Bob，Alice想把一段明文通过双钥加密的技术发送给Bob，Bob有一对公钥和私钥，那么加密解密的过程如下：\n\n 1. Bob将他的公开密钥传送给Alice。\n 2. Alice用Bob的公开密钥加密她的消息，然后传送给Bob。\n 3. Bob用他的私人密钥解密Alice的消息。\n上面的过程可以用下图表示，Alice使用Bob的公钥进行加密，Bob用自己的私钥进行解密。\n![](0.PNG)\n例子和图出自《网络安全基础 应用与标准第二版》\n\n## **四、图解**\n\n今天，我读到一篇好文章。\n\n它用图片通俗易懂地解释了，\"数字签名\"（digital signature）和\"数字证书\"（digital certificate）到底是什么。\n\n我对这些问题的理解，一直是模模糊糊的，很多细节搞不清楚。读完这篇文章后，发现思路一下子就理清了。为了加深记忆，我把文字和图片都翻译出来了。\n\n数字签名是什么？\n\n作者：David Youd\n\n翻译：阮一峰\n\n原文网址：http://www.youdzone.com/signature.html\n\n1.\n\n![](1.png)\n\n鲍勃（服务器）有两把钥匙，一把是公钥，另一把是私钥。\n\n2.\n\n![](2.png)\n\n鲍勃把公钥送给他的朋友们----帕蒂（客户端1）、道格（客户端2）、苏珊（客户端3）----每人一把。\n\n3.\n\n![](3.png)\n\n苏珊（客户端3）给鲍勃（服务器）写信，写完后用鲍勃的公钥加密，达到保密的效果。\n\n4.\n\n![](4.png)\n\n鲍勃收信后，用私钥解密，看到信件内容。\n\n5.\n\n![](5.png)\n\n鲍勃给苏珊回信，写完后用Hash函数，生成信件的摘要（digest）。\n\n6.\n\n![](6.png)\n\n然后，<font face=\"fantasy\" color=#0099ff>**鲍勃使用私钥，对这个摘要加密，生成\"数字签名\"(signature).**</font>\n\n7.\n\n![](7.png)\n\n鲍勃将这个签名，附在信件下面，一起发给苏珊。\n\n8.\n\n![](8.png)\n\n苏珊收信后，取下数字签名，<font face=\"fantasy\" color=#0099ff>**用鲍勃的公钥解密，得到信件的摘要。**</font>由此证明，这封信确实是鲍勃发出的。\n\n9.\n\n![](9.png)\n\n苏珊再对信件本身<font face=\"fantasy\" color=#0099ff>**使用Hash函数，将得到的结果，与上一步得到的摘要进行对比。</font>**如果两者一致，就证明这封信未被修改过。\n\n10.\n\n![](10.png)\n\n复杂的情况出现了。道格（客户端2）想欺骗苏珊，他偷偷使用了苏珊的电脑，用自己的公钥换走了鲍勃的公钥。因此，他就可以冒充鲍勃，写信给苏珊。\n\n此时客户端2的想法是：自己生成了一对公私密钥对，然后给苏珊一个公钥，自己发送数据和签名给苏珊，苏珊在不知道公钥被替换的情况下不知道服务器已被冒充，这样便形成了数据信息的泄露。\n\n11.\n\n![](11.png)\n\n苏珊发现，自己无法确定公钥是否真的属于鲍勃。她想到了一个办法，要求鲍勃去找\"证书中心\"（certificate authority，简称CA），为公钥做认证。<font face=\"fantasy\" color=#0099ff>**证书中心用自己的私钥，对鲍勃的公钥和一些相关信息一起加密，生成\"数字证书\"(Digital Certificate).**</font>\n\n12.\n\n![](12.png)\n\n鲍勃拿到数字证书以后，就可以放心了。以后再给苏珊写信，只要在签名的同时，再附上数字证书就行了。\n\n13.\n\n![](13.png)\n\n苏珊收信后，用CA的公钥解开数字证书，就可以拿到鲍勃真实的公钥了，然后就能证明\"数字签名\"是否真的是鲍勃签的。\n\n14.\n\n![](14.jpg)\n\n下面，我们看一个应用\"数字证书\"的实例：https协议。这个协议主要用于网页加密。\n\n15.\n\n![](15.png)\n\n首先，客户端向服务器发出加密请求。\n\n16.\n\n![](16.png)\n\n服务器用自己的私钥加密网页以后，连同本身的数字证书，一起发送给客户端。\n\n17.\n\n![](17.png)\n\n客户端（浏览器）的\"证书管理器\"，有\"受信任的根证书颁发机构\"列表。客户端会根据这张列表，查看解开数字证书的公钥是否在列表之内。\n\n18.\n\n![](18.png)\n\n如果数字证书记载的网址，与你正在浏览的网址不一致，就说明这张证书可能被冒用，浏览器会发出警告。\n\n19.\n\n![](19.jpg)\n\n如果这张数字证书不是由受信任的机构颁发的，浏览器会发出另一种警告。\n\n20.\n\n![](20.png)\n\n数字证书如果是可靠的，客户端就可以使用证书中的服务器公钥，对信息进行加密，然后与服务器交换加密信息。\n\n\n\n\n","tags":["security"],"categories":["technologies","security"]},{"title":"Intel_tips_at_work","url":"/2021/03/13/technologies/Intel_tips_at_work/","content":"\n### 聊天邮件等添加表情\nwindows键+分号键\";\" 能够选择表情\n\n### 关于cube主机连接网络问题\ncube 台式机网线连接C口，宿主主机安装Virtualbox后再安装linux虚拟机，网络选择桥接模式，跟宿主主机网络处于同一网段，与宿主主机互不影响.\n台式机连接A口，宿主主机设置proxy后可以连接外网，但是virtualbox安装的虚拟机就连不到外网了\n安装好虚拟机OS，\n$ sudo passwd root\n输入user用户密码，再设置root密码，下次就可以$ su root 获取root权限了\n","categories":["technologies"]},{"title":"windows上不小心diskclean后如何恢复","url":"/2021/03/13/software/DiskGenius_windows上不小心diskclean后如何恢复/","content":"\n## windows操作某个磁盘不小心执行disk clean后如何恢复\n\n首先我们要明确，clean命令并没有删除数据，没有删除数据，没有删除数据，他只是让你的系统“忘记”这个地方曾经有一个磁盘，以及这些磁盘的信息。他会“忘记”磁盘中是如何分区的，以及每个分区的类型，功能等信息。就把他当做一个从来没见过的硬盘来处理.  \n\n格式化才是删除数据的罪魁祸首。所以如果你在之前已经执行过格式化操作，那么对不起，下面的内容可能对你已经没用了.  \n\n正是因为clean并没有执行格式化的操作，这也就为我们的数据恢复提供了基础。我们要做的工作就是让计算机“记起来”这里曾经有那么一块硬盘，上面有那么几个分区，分区里面有那么几个文件，就行了.  \n\n首先准备好需要的软件：DiskGenius，版本尽量新吧，下载的时候要注意自己的电脑是32位还是64位的\n\n## DiskGenius操作流程\n![](1.PNG)\n\n1. 在搜索分区时，软件会自动寻找你的硬盘数据，每当找到一个分区时会询问你是否要保留分区，并且你可以实时的在左边看到软件搜索到的目录。一般默认选择保留就好了（因为写这篇blog的时候我的电脑已经恢复了，所以没有截图）.  \n\n2. 最后点击保存更改，稍等一会, 就可以在我的电脑中找到原来的分区目录.\n\n直到最后保存更改之前的所有操作都是免费的，但是保存更改有可能是收费版才有的,自己下载的可以直接保存, 另外幸运的是，就算你没有收费版，你还是能在软件中进行文件复制粘贴和磁盘的复制粘贴，所以如果你找不到收费版又不嫌麻烦的话，那就一个个文件复制过去吧！\n","tags":["software"],"categories":["software"]},{"title":"Ceph 01 Storage Conception","url":"/2021/03/13/storage/ceph/ceph_01_conception/","content":"\n## Ceph\n官网: https://ceph.io/get/\ngithub: https://github.com/ceph/ceph\n中文文档: http://docs.ceph.org.cn/\n\n * 软件定义存储 -SDS\nSDS是减少存储基础设施的TCO(总体成本)所需要的。除了降低存储成本外， SDS还可以提供灵活性、可伸缩性和可靠\n性。 Ceph是一种真正的SDS;它运行在没有厂商锁定的普通硬件上。 与传统的存储系统(硬件与软件结合在一起)不同，\n在SDS中，您可以从任何制造商中自由选择硬件，也可以根据自己的需要自由设计异构硬件解决方案。 Ceph在此硬件\n之上的软件定义存储提供了您需要的所有， 并将负责所有事情，从软件层提供了所有企业存储特性。\n\n * 云存储\n目前已经和开源云架构 OpenStack 结合起来，成为 Openstack后端存储的标配，并且又同时支持用于 kubernetes 动态存储.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["storage"],"categories":["storage","ceph"]},{"title":"MinIO 01 conception","url":"/2021/03/13/storage/minio/minIO_01_conception/","content":"\n## **官网**\nofficial website:[https://min.io/](https://min.io/)\n\n\n## **存储形式**\n\n文件对象上传到 MinIO ，会在对应的数据存储磁盘中，以 Bucket 名称为目录，文件名称为下一级目录，文件名称下是 part.1 和 xl.json，前者是编码数据块及检验块，后者是元数据文件.  \n\n## **纠删码EC（Erasure Code）**\n\nMinIO 使用纠删码机制来保证高可靠性，使用 highwayhash 来处理数据损坏（ Bit Rot Protection ）。关于纠删码，简单来说就是可以通过数学计算，把丢失的数据进行还原，它可以将n份原始数据，增加m份数据，并能通过n+m份中的任意n份数据，还原为原始数据。即如果有任意小于等于m份的数据失效，仍然能通过剩下的数据还原出来。举个最简单例子就是有两个数据(d1, d2)，用一个校验和y（d1 + d2 = y）即可保证即使丢失其中一个，依然可以还原数据。如丢失 d1 ，则使用 y - d2 = d1 还原，同理，d2 丢失或者y丢失，均可通过计算得出.  \n\nEC 的具体应用实现中， RS（Reed-Solomen）是 EC 的一种更简单快捷的实现，可以通过矩阵运算，还原数据。MinIO 将对象拆分成N/2数据和N/2 校验块 。具体的数学矩阵运算及证明，可以参考文章《Erasure-Code-擦除码-1-原理篇》及《EC纠删码原理》.  \n\n\n\n","tags":["storage"],"categories":["storage","minio"]},{"title":"37 | 找到容器不容易：Service、DNS与服务发现","url":"/2021/03/13/reference/k8s/37.找到容器不容易：Service、DNS与服务发现/","content":"\n![](1.jpg)\n\n## 找到容器不容易：Service、DNS与服务发现\n\n在前面的文章中，我们已经多次使用到了 Service 这个 Kubernetes 里重要的服务对象。而 Kubernetes 之所以需要 Service，一方面是因为 Pod 的 IP 不是固定的，另一方面则是因为一组 Pod 实例之间总会有负载均衡的需求。\n\n一个最典型的 Service 定义，如下所示：\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: hostnames\n\tspec:\n\t  selector:\n\t    app: hostnames\n\t  ports:\n\t  - name: default\n\t    protocol: TCP\n\t    port: 80\n\t    targetPort: 9376\n这个 Service 的例子，相信你不会陌生。其中，我使用了 selector 字段来声明这个 Service 只代理携带了 app=hostnames 标签的 Pod。并且，这个 Service 的 80 端口，代理的是 Pod 的 9376 端口。\n\n然后，我们的应用的 Deployment，如下所示：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: hostnames\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: hostnames\n\t  replicas: 3\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: hostnames\n\t    spec:\n\t      containers:\n\t      - name: hostnames\n\t        image: k8s.gcr.io/serve_hostname\n\t        ports:\n\t        - containerPort: 9376\n\t          protocol: TCP\n这个应用的作用，就是每次访问 9376 端口时，返回它自己的 hostname。\n\n而被 selector 选中的 Pod，就称为 Service 的 Endpoints，你可以使用 kubectl get ep 命令看到它们，如下所示：\n\n\t$ kubectl get endpoints hostnames\n\tNAME        ENDPOINTS\n\thostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376\n\n需要注意的是，只有处于 Running 状态，且 readinessProbe 检查通过的 Pod，才会出现在 Service 的 Endpoints 列表里。并且，当某一个 Pod 出现问题时，Kubernetes 会自动把它从 Service 里摘除掉。\n\n而此时，通过该 Service 的 VIP 地址 10.0.1.175，你就可以访问到它所代理的 Pod 了：\n\n\t$ kubectl get svc hostnames\n\tNAME        TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n\thostnames   ClusterIP   10.0.1.175   <none>        80/TCP    5s\n\t$ curl 10.0.1.175:80\n\thostnames-0uton\n\t$ curl 10.0.1.175:80\n\thostnames-yp2kp\n\t$ curl 10.0.1.175:80\n\thostnames-bvc05\n这个 VIP 地址是 Kubernetes 自动为 Service 分配的。而像上面这样，通过三次连续不断地访问 Service 的 VIP 地址和代理端口 80，它就为我们依次返回了三个 Pod 的 hostname。这也正印证了 Service 提供的是 Round Robin 方式的负载均衡。对于这种方式，我们称为：ClusterIP 模式的 Service。\n\n你可能一直比较好奇，Kubernetes 里的 Service 究竟是如何工作的呢？\n\n实际上，**Service 是由 kube-proxy 组件，加上 iptables 来共同实现的**。\n\n举个例子，对于我们前面创建的名叫 hostnames 的 Service 来说，一旦它被提交给 Kubernetes，那么 kube-proxy 就可以通过 Service 的 Informer 感知到这样一个 Service 对象的添加。而作为对这个事件的响应，它就会在宿主机上创建这样一条 iptables 规则（你可以通过 iptables-save 看到它），如下所示：\n\n\t-A KUBE-SERVICES -d 10.0.1.175/32 -p tcp -m comment --comment \"default/hostnames: cluster IP\" -m tcp --dport 80 -j KUBE-SVC-NWV5X2332I4OT4T3\n\n可以看到，这条 iptables 规则的含义是：凡是目的地址是 10.0.1.175、目的端口是 80 的 IP 包，都应该跳转到另外一条名叫 KUBE-SVC-NWV5X2332I4OT4T3 的 iptables 链进行处理。\n\n而我们前面已经看到，**`10.0.1.175 正是这个 Service 的 VIP。所以这一条规则，就为这个 Service 设置了一个固定的入口地址。并且，由于 10.0.1.175 只是一条 iptables 规则上的配置，并没有真正的网络设备，所以你 ping 这个地址，是不会有任何响应的。`**\n\n那么，我们即将跳转到的 KUBE-SVC-NWV5X2332I4OT4T3 规则，又有什么作用呢？\n\n实际上，它是一组规则的集合，如下所示：\n\n\t-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.33332999982 -j KUBE-SEP-WNBA2IHDGP2BOBGZ\n\t-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-X3P2623AGDH6CDF3\n\t-A KUBE-SVC-NWV5X2332I4OT4T3 -m comment --comment \"default/hostnames:\" -j KUBE-SEP-57KPRZ3JQVENLNBR\n可以看到，这一组规则，实际上是一组随机模式（–mode random）的 iptables 链。\n\n而随机转发的目的地，分别是 KUBE-SEP-WNBA2IHDGP2BOBGZ、KUBE-SEP-X3P2623AGDH6CDF3 和 KUBE-SEP-57KPRZ3JQVENLNBR。\n\n而这三条链指向的最终目的地，其实就是这个 Service 代理的三个 Pod。所以这一组规则，就是 Service 实现负载均衡的位置。\n\n需要注意的是，iptables 规则的匹配是从上到下逐条进行的，所以为了保证上述三条规则每条被选中的概率都相同，我们应该将它们的 probability 字段的值分别设置为 1/3（0.333…）、1/2 和 1。\n\n这么设置的原理很简单：第一条规则被选中的概率就是 1/3；而如果第一条规则没有被选中，那么这时候就只剩下两条规则了，所以第二条规则的 probability 就必须设置为 1/2；类似地，最后一条就必须设置为 1。\n\n你可以想一下，如果把这三条规则的 probability 字段的值都设置成 1/3，最终每条规则被选中的概率会变成多少。\n\n通过查看上述三条链的明细，我们就很容易理解 Service 进行转发的具体原理了，如下所示：\n\n\t-A KUBE-SEP-57KPRZ3JQVENLNBR -s 10.244.3.6/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000\n\t-A KUBE-SEP-57KPRZ3JQVENLNBR -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.3.6:9376\n\t-A KUBE-SEP-WNBA2IHDGP2BOBGZ -s 10.244.1.7/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000\n\t-A KUBE-SEP-WNBA2IHDGP2BOBGZ -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.1.7:9376\n\t-A KUBE-SEP-X3P2623AGDH6CDF3 -s 10.244.2.3/32 -m comment --comment \"default/hostnames:\" -j MARK --set-xmark 0x00004000/0x00004000\n\t-A KUBE-SEP-X3P2623AGDH6CDF3 -p tcp -m comment --comment \"default/hostnames:\" -m tcp -j DNAT --to-destination 10.244.2.3:9376\n可以看到，这三条链，其实是三条 DNAT 规则。但在 DNAT 规则之前，iptables 对流入的 IP 包还设置了一个“标志”（–set-xmark）。这个“标志”的作用，我会在下一篇文章再为你讲解。\n\n而 DNAT 规则的作用，就是在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。\n\n这样，访问 Service VIP 的 IP 包经过上述 iptables 处理之后，就已经变成了访问具体某一个后端 Pod 的 IP 包了。不难理解，这些 Endpoints 对应的 iptables 规则，正是 kube-proxy 通过监听 Pod 的变化事件，在宿主机上生成并维护的。\n\n以上，就是 Service 最基本的工作原理。\n\n此外，你可能已经听说过，Kubernetes 的 kube-proxy 还支持一种叫作 IPVS 的模式。这又是怎么一回事儿呢？\n\n\n其实，通过上面的讲解，你可以看到，kube-proxy 通过 iptables 处理 Service 的过程，其实需要在宿主机上设置相当多的 iptables 规则。而且，kube-proxy 还需要在控制循环里不断地刷新这些规则来确保它们始终是正确的。\n\n不难想到，当你的宿主机上有大量 Pod 的时候，成百上千条 iptables 规则不断地被刷新，会大量占用该宿主机的 CPU 资源，甚至会让宿主机“卡”在这个过程中。所以说，**一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。**\n\n而 IPVS 模式的 Service，就是解决这个问题的一个行之有效的方法。\n\nIPVS 模式的工作原理，其实跟 iptables 模式类似。当我们创建了前面的 Service 之后，kube-proxy 首先会在宿主机上创建一个虚拟网卡（叫作：kube-ipvs0），并为它分配 Service VIP 作为 IP 地址，如下所示：\n\n\t# ip addr\n\t  ...\n\t  73：kube-ipvs0：<BROADCAST,NOARP>  mtu 1500 qdisc noop state DOWN qlen 1000\n\t  link/ether  1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff\n\t  inet 10.0.1.175/32  scope global kube-ipvs0\n\t  valid_lft forever  preferred_lft forever\n而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：\n\n\t# ipvsadm -ln\n\t IP Virtual Server version 1.2.1 (size=4096)\n\t  Prot LocalAddress:Port Scheduler Flags\n\t    ->  RemoteAddress:Port           Forward  Weight ActiveConn InActConn     \n\t  TCP  10.102.128.4:80 rr\n\t    ->  10.244.3.6:9376    Masq    1       0          0         \n\t    ->  10.244.1.7:9376    Masq    1       0          0\n\t    ->  10.244.2.3:9376    Masq    1       0          0\n可以看到，这三个 IPVS 虚拟主机的 IP 地址和端口，对应的正是三个被代理的 Pod。\n\n这时候，任何发往 10.102.128.4:80 的请求，就都会被 IPVS 模块转发到某一个后端 Pod 上了。\n\n而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。\n\n> 备注：这里你可以再回顾下第 33 篇文章《深入解析容器跨主机网络》中的相关内容。\n\n不过需要注意的是，IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。\n\n所以，在大规模集群里，我非常建议你为 kube-proxy 设置–proxy-mode=ipvs 来开启这个功能。它为 Kubernetes 集群规模带来的提升，还是非常巨大的。\n\n**此外，我在前面的文章中还介绍过 Service 与 DNS 的关系。**\n\n在 Kubernetes 中，Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。\n\n对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。\n\n而对于指定了 clusterIP=None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。\n\n此外，对于 ClusterIP 模式的 Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：..pod.cluster.local。这条记录指向 Pod 的 IP 地址。\n\n\n而对 Headless Service 来说，它代理的 Pod 被自动分配的 A 记录的格式是：...svc.cluster.local。这条记录也指向 Pod 的 IP 地址。\n\n但如果你为 Pod 指定了 Headless Service，并且 Pod 本身声明了 hostname 和 subdomain 字段，那么这时候 Pod 的 A 记录就会变成：<pod 的 hostname>...svc.cluster.local，比如：\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: default-subdomain\n\tspec:\n\t  selector:\n\t    name: busybox\n\t  clusterIP: None\n\t  ports:\n\t  - name: foo\n\t    port: 1234\n\t    targetPort: 1234\n\t---\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: busybox1\n\t  labels:\n\t    name: busybox\n\tspec:\n\t  hostname: busybox-1\n\t  subdomain: default-subdomain\n\t  containers:\n\t  - image: busybox\n\t    command:\n\t      - sleep\n\t      - \"3600\"\n\t    name: busybox\n在上面这个 Service 和 Pod 被创建之后，你就可以通过 busybox-1.default-subdomain.default.svc.cluster.local 解析到这个 Pod 的 IP 地址了。\n\n需要注意的是，在 Kubernetes 里，/etc/hosts 文件是单独挂载的，这也是为什么 kubelet 能够对 hostname 进行修改并且 Pod 重建后依然有效的原因。这跟 Docker 的 Init 层是一个原理。\n\n**总结**\n\n在这篇文章里，我为你详细讲解了 Service 的工作原理。实际上，Service 机制，以及 Kubernetes 里的 DNS 插件，都是在帮助你解决同样一个问题，即：如何找到我的某一个容器？\n\n这个问题在平台级项目中，往往就被称作服务发现，即：当我的一个服务（Pod）的 IP 地址是不固定的且没办法提前获知时，我该如何通过一个固定的方式访问到这个 Pod 呢？\n\n而我在这里讲解的、ClusterIP 模式的 Service 为你提供的，就是一个 Pod 的稳定的 IP 地址，即 VIP。并且，这里 Pod 和 Service 的关系是可以通过 Label 确定的。\n\n而 Headless Service 为你提供的，则是一个 Pod 的稳定的 DNS 名字，并且，这个名字是可以通过 Pod 名字和 Service 名字拼接出来的。\n\n在实际的场景里，你应该根据自己的具体需求进行合理选择。\n\n**思考题**\n\n请问，Kubernetes 的 Service 的负载均衡策略，在 iptables 和 ipvs 模式下，都有哪几种？具体工作模式是怎样的？\n","categories":["reference","k8s"]},{"title":"38 | 从外界连通Service与Service调试“三板斧”","url":"/2021/03/13/reference/k8s/38.从外界连通Service与Service调试“三板斧”/","content":"\n![](1.jpg)\n\n## 从外界连通Service与Service调试“三板斧”\n\n在上一篇文章中，我为你介绍了 Service 机制的工作原理。通过这些讲解，你应该能够明白这样一个事实：Service 的访问信息在 Kubernetes 集群之外，其实是无效的。\n\n这其实也容易理解：所谓 Service 的访问入口，其实就是每台宿主机上由 kube-proxy 生成的 iptables 规则，以及 kube-dns 生成的 DNS 记录。而一旦离开了这个集群，这些信息对用户来说，也就自然没有作用了。\n\n所以，在使用 Kubernetes 的 Service 时，一个必须要面对和解决的问题就是：**如何从外部（Kubernetes 集群之外），访问到 Kubernetes 里创建的 Service？**\n\n这里`最常用的一种方式就是：NodePort`。我来为你举个例子。\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: my-nginx\n\t  labels:\n\t    run: my-nginx\n\tspec:\n\t  type: NodePort\n\t  ports:\n\t  - nodePort: 8080\n\t    targetPort: 80\n\t    protocol: TCP\n\t    name: http\n\t  - nodePort: 443\n\t    protocol: TCP\n\t    name: https\n\t  selector:\n\t    run: my-nginx\n在这个 Service 的定义里，我们声明它的类型是，type=NodePort。然后，我在 ports 字段里声明了 Service 的 8080 端口代理 Pod 的 80 端口，Service 的 443 端口代理 Pod 的 443 端口。\n\n当然，如果你不显式地声明 nodePort 字段，Kubernetes 就会为你分配随机的可用端口来设置代理。这个端口的范围默认是 30000-32767，你可以通过 kube-apiserver 的–service-node-port-range 参数来修改它。\n\n那么这时候，要访问这个 Service，你只需要访问：\n\n\t<任何一台宿主机的IP地址>:8080\n就可以访问到某一个被代理的 Pod 的 80 端口了。\n\n而在理解了我在上一篇文章中讲解的 Service 的工作原理之后，NodePort 模式也就非常容易理解了。显然，kube-proxy 要做的，就是在每台宿主机上生成这样一条 iptables 规则：\n\n\t-A KUBE-NODEPORTS -p tcp -m comment --comment \"default/my-nginx: nodePort\" -m tcp --dport 8080 -j KUBE-SVC-67RL4FN6JRUPOJYM\n而我在上一篇文章中已经讲到，KUBE-SVC-67RL4FN6JRUPOJYM 其实就是一组随机模式的 iptables 规则。所以接下来的流程，就跟 ClusterIP 模式完全一样了。\n\n需要注意的是，在 NodePort 方式下，Kubernetes 会在 IP 包离开宿主机发往目的 Pod 时，对这个 IP 包做一次 SNAT 操作，如下所示：\n\n\t-A KUBE-POSTROUTING -m comment --comment \"kubernetes service traffic requiring SNAT\" -m mark --mark 0x4000/0x4000 -j MASQUERADE\n可以看到，这条规则设置在 POSTROUTING 检查点，也就是说，它给即将离开这台主机的 IP 包，进行了一次 SNAT 操作，将这个 IP 包的源地址替换成了这台宿主机上的 CNI 网桥地址，或者宿主机本身的 IP 地址（如果 CNI 网桥不存在的话）。\n\n当然，这个 SNAT 操作只需要对 Service 转发出来的 IP 包进行（否则普通的 IP 包就被影响了）。而 iptables 做这个判断的依据，就是查看该 IP 包是否有一个“0x4000”的“标志”。你应该还记得，这个标志正是在 IP 包被执行 DNAT 操作之前被打上去的。\n\n可是，`为什么一定要对流出的包做 SNAT操作呢？`\n\n这里的原理其实很简单，如下所示：\n\n\t           client\n\t             \\ ^\n\t              \\ \\\n\t               v \\\n\t   node 1 <--- node 2\n\t    | ^   SNAT\n\t    | |   --->\n\t    v |\n\t endpoint\n当一个外部的 client 通过 node 2 的地址访问一个 Service 的时候，node 2 上的负载均衡规则，就可能把这个 IP 包转发给一个在 node 1 上的 Pod。这里没有任何问题。\n\n而当 node 1 上的这个 Pod 处理完请求之后，它就会按照这个 IP 包的源地址发出回复。\n\n可是，如果没有做 SNAT 操作的话，这时候，被转发来的 IP 包的源地址就是 client 的 IP 地址。**所以此时，Pod 就会直接将回复发给client**。对于 client 来说，它的请求明明发给了 node 2，收到的回复却来自 node 1，这个 client 很可能会报错。\n\n所以，在上图中，当 IP 包离开 node 2 之后，它的源 IP 地址就会被 SNAT 改成 node 2 的 CNI 网桥地址或者 node 2 自己的地址。这样，Pod 在处理完成之后就会先回复给 node 2（而不是 client），然后再由 node 2 发送给 client。\n\n当然，这也就意味着这个 Pod 只知道该 IP 包来自于 node 2，而不是外部的 client。对于 Pod 需要明确知道所有请求来源的场景来说，这是不可以的。\n\n所以这时候，你就可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。\n\n而这个机制的实现原理也非常简单：**这时候，一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod**。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示：\n\n\t       client\n\t       ^ /   \\\n\t      / /     \\\n\t     / v       X\n\t   node 1     node 2\n\t    ^ |\n\t    | |\n\t    | v\n\t endpoint\n当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。\n\n`从外部访问 Service 的第二种方式，适用于公有云上的 Kubernetes 服务。这时候，你可以指定一个 LoadBalancer 类型的 Service，`如下所示：\n\n\t---\n\tkind: Service\n\tapiVersion: v1\n\tmetadata:\n\t  name: example-service\n\tspec:\n\t  ports:\n\t  - port: 8765\n\t    targetPort: 9376\n\t  selector:\n\t    app: example\n\t  type: LoadBalancer\n在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。\n\n`而第三种方式，是 Kubernetes 在 1.7 之后支持的一个新特性，叫作 ExternalName。`举个例子：\n\n\tkind: Service\n\tapiVersion: v1\n\tmetadata:\n\t  name: my-service\n\tspec:\n\t  type: ExternalName\n\t  externalName: my.database.example.com\n在上述 Service 的 YAML 文件中，我指定了一个 externalName=my.database.example.com 的字段。而且你应该会注意到，这个 YAML 文件里不需要指定 selector。\n\n这时候，当你通过 Service 的 DNS 名字访问它的时候，比如访问：my-service.default.svc.cluster.local。那么，Kubernetes 为你返回的就是my.database.example.com。所以说，ExternalName 类型的 Service，其实是在 kube-dns 里为你添加了一条 CNAME 记录。这时，访问 my-service.default.svc.cluster.local 就和访问 my.database.example.com 这个域名是一个效果了。\n\n此外，Kubernetes 的 Service 还允许你为 Service 分配公有 IP 地址，比如下面这个例子：\n\n\tkind: Service\n\tapiVersion: v1\n\tmetadata:\n\t  name: my-service\n\tspec:\n\t  selector:\n\t    app: MyApp\n\t  ports:\n\t  - name: http\n\t    protocol: TCP\n\t    port: 80\n\t    targetPort: 9376\n\t  externalIPs:\n\t  - 80.11.12.10\n在上述 Service 中，我为它指定的 externalIPs=80.11.12.10，那么此时，你就可以通过访问 80.11.12.10:80 访问到被代理的 Pod 了。不过，在这里 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点。你可以想一想这是为什么。\n\n实际上，`在理解了 Kubernetes Service 机制的工作原理之后，很多与 Service 相关的问题，其实都可以通过分析 Service 在宿主机上对应的 iptables 规则（或者 IPVS 配置）得到解决。`\n\n比如，当你的 Service 没办法通过 DNS 访问到的时候。你就需要区分到底是 Service 本身的配置问题，还是集群的 DNS 出了问题。一个行之有效的方法，就是检查 Kubernetes 自己的 Master 节点的 Service DNS 是否正常：\n\n\t# 在一个Pod里执行\n\t$ nslookup kubernetes.default\n\tServer:    10.0.0.10\n\tAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\tName:      kubernetes.default\n\tAddress 1: 10.0.0.1 kubernetes.default.svc.cluster.local\n\n如果上面访问 kubernetes.default 返回的值都有问题，那你就需要检查 kube-dns 的运行状态和日志了。否则的话，你应该去检查自己的 Service 定义是不是有问题。\n\n而如果你的 Service 没办法通过 ClusterIP 访问到的时候，你首先应该检查的是这个 Service 是否有 Endpoints：\n\n\t$ kubectl get endpoints hostnames\n\tNAME        ENDPOINTS\n\thostnames   10.244.0.5:9376,10.244.0.6:9376,10.244.0.7:9376\n需要注意的是，如果你的 Pod 的 readniessProbe 没通过，它也不会出现在 Endpoints 列表里。\n\n而如果 Endpoints 正常，那么你就需要确认 kube-proxy 是否在正确运行。在我们通过 kubeadm 部署的集群里，你应该看到 kube-proxy 输出的日志如下所示：\n\n\tI1027 22:14:53.995134    5063 server.go:200] Running in resource-only container \"/kube-proxy\"\n\tI1027 22:14:53.998163    5063 server.go:247] Using iptables Proxier.\n\tI1027 22:14:53.999055    5063 server.go:255] Tearing down userspace rules. Errors here are acceptable.\n\tI1027 22:14:54.038140    5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns-tcp\" to [10.244.1.3:53]\n\tI1027 22:14:54.038164    5063 proxier.go:352] Setting endpoints for \"kube-system/kube-dns:dns\" to [10.244.1.3:53]\n\tI1027 22:14:54.038209    5063 proxier.go:352] Setting endpoints for \"default/kubernetes:https\" to [10.240.0.2:443]\n\tI1027 22:14:54.038238    5063 proxier.go:429] Not syncing iptables until Services and Endpoints have been received from master\n\tI1027 22:14:54.040048    5063 proxier.go:294] Adding new service \"default/kubernetes:https\" at 10.0.0.1:443/TCP\n\tI1027 22:14:54.040154    5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns\" at 10.0.0.10:53/UDP\n\tI1027 22:14:54.040223    5063 proxier.go:294] Adding new service \"kube-system/kube-dns:dns-tcp\" at 10.0.0.10:53/TCP\n如果 kube-proxy 一切正常，你就应该仔细查看宿主机上的 iptables 了。而**一个 iptables 模式的 Service 对应的规则，我在上一篇以及这一篇文章里已经全部介绍到了，它们包括：**\n\nKUBE-SERVICES 或者 KUBE-NODEPORTS 规则对应的 Service 的入口链，这个规则应该与 VIP 和 Service 端口一一对应；\n\nKUBE-SEP-(hash) 规则对应的 DNAT 链，这些规则应该与 Endpoints 一一对应；\n\nKUBE-SVC-(hash) 规则对应的负载均衡链，这些规则的数目应该与 Endpoints 数目一致；\n\n如果是 NodePort 模式的话，还有 POSTROUTING 处的 SNAT 链。\n\n通过查看这些链的数量、转发目的地址、端口、过滤条件等信息，你就能很容易发现一些异常的蛛丝马迹。\n\n当然，**还有一种典型问题，就是 Pod 没办法通过 Service 访问到自己**。这往往就是因为 kubelet 的 hairpin-mode 没有被正确设置。关于 Hairpin 的原理我在前面已经介绍过，这里就不再赘述了。你只需要确保将 kubelet 的 hairpin-mode 设置为 hairpin-veth 或者 promiscuous-bridge 即可。\n\n> 这里，你可以再回顾下第 34 篇文章《Kubernetes 网络模型与 CNI 网络插件》中的相关内容。\n\n其中，在 hairpin-veth 模式下，你应该能看到 CNI 网桥对应的各个 VETH 设备，都将 Hairpin 模式设置为了 1，如下所示：\n\n\t$ for d in /sys/devices/virtual/net/cni0/brif/veth*/hairpin_mode; do echo \"$d = $(cat $d)\"; done\n\t/sys/devices/virtual/net/cni0/brif/veth4bfbfe74/hairpin_mode = 1\n\t/sys/devices/virtual/net/cni0/brif/vethfc2a18c5/hairpin_mode = 1\n而如果是 promiscuous-bridge 模式的话，你应该看到 CNI 网桥的混杂模式（PROMISC）被开启，如下所示：\n\n\t$ ifconfig cni0 |grep PROMISC\n\tUP BROADCAST RUNNING PROMISC MULTICAST  MTU:1460  Metric:1\n\n**总结**\n\n在本篇文章中，我为你详细讲解了从外部访问 Service 的三种方式（NodePort、LoadBalancer 和 External Name）和具体的工作原理。然后，我还为你讲述了当 Service 出现故障的时候，如何根据它的工作原理，按照一定的思路去定位问题的可行之道。\n\n通过上述讲解不难看出，所谓 Service，其实就是 Kubernetes 为 Pod 分配的、固定的、基于 iptables（或者 IPVS）的访问入口。而这些访问入口代理的 Pod 信息，则来自于 Etcd，由 kube-proxy 通过控制循环来维护。\n\n并且，你可以看到，Kubernetes 里面的 Service 和 DNS 机制，也都不具备强多租户能力。比如，在多租户情况下，每个租户应该拥有一套独立的 Service 规则（Service 只应该看到和代理同一个租户下的 Pod）。再比如 DNS，在多租户情况下，每个租户应该拥有自己的 kube-dns（kube-dns 只应该为同一个租户下的 Service 和 Pod 创建 DNS Entry）。\n\n当然，在 Kubernetes 中，kube-proxy 和 kube-dns 其实也是普通的插件而已。你完全可以根据自己的需求，实现符合自己预期的 Service。\n\n**思考题**\n\n为什么 Kubernetes 要求 externalIPs 必须是至少能够路由到一个 Kubernetes 的节点？\n","categories":["reference","k8s"]},{"title":"35 | 解读Kubernetes三层网络方案","url":"/2021/03/13/reference/k8s/35.解读Kubernetes三层网络方案/","content":"\n![](1.jpg)\n\n## 解读Kubernetes三层网络方案\n\n在上一篇文章中，我以网桥类型的 Flannel 插件为例，为你讲解了 Kubernetes 里容器网络和 CNI 插件的主要工作原理。不过，除了这种模式之外，还有一种纯三层（Pure Layer 3）网络方案非常值得你注意。其中的典型例子，莫过于 Flannel 的 host-gw 模式和 Calico 项目了。\n\n`我们先来看一下 Flannel 的 host-gw 模式。`\n\n它的工作原理非常简单，我用一张图就可以和你说清楚。为了方便叙述，接下来我会称这张图为“host-gw 示意图”。\n\n![](2.png)\n图1 Flannel host-gw示意图\n\n假设现在，Node 1 上的 Infra-container-1，要访问 Node 2 上的 Infra-container-2。\n\n当你设置 Flannel 使用 host-gw 模式之后，flanneld 会在宿主机上创建这样一条规则，以 Node 1 为例：\n\n\t$ ip route\n\t...\n\t10.244.1.0/24 via 10.168.0.3 dev eth0\n这条路由规则的含义是：目的 IP 地址属于 10.244.1.0/24 网段的 IP 包，应该经过本机的 eth0 设备发出去（即：dev eth0）；并且，它下一跳地址（next-hop）是 10.168.0.3（即：via 10.168.0.3）。\n\n所谓下一跳地址就是：如果 IP 包从主机 A 发到主机 B，需要经过路由设备 X 的中转。那么 X 的 IP 地址就应该配置为主机 A 的下一跳地址。\n\n而从 host-gw 示意图中我们可以看到，这个下一跳地址对应的，正是我们的目的宿主机 Node 2。\n\n一旦配置了下一跳地址，那么接下来，当 IP 包从网络层进入链路层封装成帧的时候，eth0 设备就会使用下一跳地址对应的 MAC 地址，作为该数据帧的目的 MAC 地址。显然，这个 MAC 地址，正是 Node 2 的 MAC 地址。\n\n这样，这个数据帧就会从 Node 1 通过宿主机的二层网络顺利到达 Node 2 上。\n\n而 Node 2 的内核网络栈从二层数据帧里拿到 IP 包后，会“看到”这个 IP 包的目的 IP 地址是 10.244.1.3，即 Infra-container-2 的 IP 地址。这时候，根据 Node 2 上的路由表，该目的地址会匹配到第二条路由规则（也就是 10.244.1.0 对应的路由规则），从而进入 cni0 网桥，进而进入到 Infra-container-2 当中。\n\n可以看到，**host-gw 模式的工作原理，其实就是将每个 Flannel 子网（Flannel Subnet，比如：10.244.1.0/24）的“下一跳”，设置成了该子网对应的宿主机的 IP 地址。**\n\n也就是说，这台“主机”（Host）会充当这条容器通信路径里的“网关”（Gateway）。这也正是“host-gw”的含义。\n\n当然，Flannel 子网和主机的信息，都是保存在 Etcd 当中的。flanneld 只需要 WACTH 这些数据的变化，然后实时更新路由表即可。\n\n> 注意：在 Kubernetes v1.7 之后，类似 Flannel、Calico 的 CNI 网络插件都是可以直接连接 Kubernetes 的 APIServer 来访问 Etcd 的，无需额外部署 Etcd 给它们使用。\n\n而在这种模式下，容器通信的过程就免除了额外的封包和解包带来的性能损耗。根据实际的测试，host-gw 的性能损失大约在 10% 左右，而其他所有基于 VXLAN“隧道”机制的网络方案，性能损失都在 20%~30% 左右。\n\n当然，通过上面的叙述，你也应该看到，host-gw 模式能够正常工作的核心，就在于 IP 包在封装成帧发送出去的时候，会使用路由表里的“下一跳”来设置目的 MAC 地址。这样，它就会经过二层网络到达目的宿主机。\n\n**所以说，Flannel host-gw 模式必须要求集群宿主机之间是二层连通的。**\n\n需要注意的是，宿主机之间二层不连通的情况也是广泛存在的。比如，宿主机分布在了不同的子网（VLAN）里。但是，在一个 Kubernetes 集群里，宿主机之间必须可以通过 IP 地址进行通信，也就是说至少是三层可达的。否则的话，你的集群将不满足上一篇文章中提到的宿主机之间 IP 互通的假设（Kubernetes 网络模型）。当然，“三层可达”也可以通过为几个子网设置三层转发来实现。\n\n`而在容器生态中，要说到像 Flannel host-gw 这样的三层网络方案，我们就不得不提到这个领域里的“龙头老大”Calico 项目了。`\n\n实际上，Calico 项目提供的网络解决方案，与 Flannel 的 host-gw 模式，几乎是完全一样的。也就是说，Calico 也会在每台宿主机上，添加一个格式如下所示的路由规则：\n\n\t<目的容器IP地址段> via <网关的IP地址> dev eth0\n\n其中，网关的 IP 地址，正是目的容器所在宿主机的 IP 地址。\n\n而正如前所述，这个三层网络方案得以正常工作的核心，是为每个容器的 IP 地址，找到它所对应的、“下一跳”的**网关。**\n\n\n不过，**不同于 Flannel 通过 Etcd 和宿主机上的 flanneld 来维护路由信息的做法，Calico 项目使用了一个“重型武器”来自动地在整个集群中分发路由信息。**\n\n这个“重型武器”，就是 BGP。\n\n**BGP 的全称是 Border Gateway Protocol，即：边界网关协议。**它是一个 Linux 内核原生就支持的、专门用在大规模数据中心里维护不同的“自治系统”之间路由信息的、无中心的路由协议。\n\n这个概念可能听起来有点儿“吓人”，但实际上，我可以用一个非常简单的例子来为你讲清楚。\n\n![](3.jpg)\n图2 自治系统\n\n在这个图中，我们有两个自治系统（Autonomous System，简称为 AS）：AS 1 和 AS 2。而所谓的一个自治系统，指的是一个组织管辖下的所有 IP 网络和路由器的全体。你可以把它想象成一个小公司里的所有主机和路由器。在正常情况下，自治系统之间不会有任何“来往”。\n\n但是，如果这样两个自治系统里的主机，要通过 IP 地址直接进行通信，我们就必须使用路由器把这两个自治系统连接起来。\n\n比如，AS 1 里面的主机 10.10.0.2，要访问 AS 2 里面的主机 172.17.0.3 的话。它发出的 IP 包，就会先到达自治系统 AS 1 上的路由器 Router 1。\n\n而在此时，Router 1 的路由表里，有这样一条规则，即：目的地址是 172.17.0.2 包，应该经过 Router 1 的 C 接口，发往网关 Router 2（即：自治系统 AS 2 上的路由器）。\n\n所以 IP 包就会到达 Router 2 上，然后经过 Router 2 的路由表，从 B 接口出来到达目的主机 172.17.0.3。\n\n但是反过来，如果主机 172.17.0.3 要访问 10.10.0.2，那么这个 IP 包，在到达 Router 2 之后，就不知道该去哪儿了。因为在 Router 2 的路由表里，并没有关于 AS 1 自治系统的任何路由规则。\n\n所以这时候，网络管理员就应该给 Router 2 也添加一条路由规则，比如：目标地址是 10.10.0.2 的 IP 包，应该经过 Router 2 的 C 接口，发往网关 Router 1。\n\n像上面这样负责把自治系统连接在一起的路由器，我们就把它形象地称为：**边界网关**。它跟普通路由器的不同之处在于，它的路由表里拥有其他自治系统里的主机路由信息。\n\n上面的这部分原理，相信你理解起来应该很容易。毕竟，路由器这个设备本身的主要作用，就是连通不同的网络。\n\n但是，你可以想象一下，假设我们现在的网络拓扑结构非常复杂，每个自治系统都有成千上万个主机、无数个路由器，甚至是由多个公司、多个网络提供商、多个自治系统组成的复合自治系统呢？\n\n这时候，如果还要依靠人工来对边界网关的路由表进行配置和维护，那是绝对不现实的。\n\n而这种情况下，BGP 大显身手的时刻就到了。\n\n在使用了 BGP 之后，你可以认为，在每个边界网关上都会运行着一个小程序，它们会将各自的路由表信息，通过 TCP 传输给其他的边界网关。而其他边界网关上的这个小程序，则会对收到的这些数据进行分析，然后将需要的信息添加到自己的路由表里。\n\n\n这样，图 2 中 Router 2 的路由表里，就会自动出现 10.10.0.2 和 10.10.0.3 对应的路由规则了。\n\n所以说，**所谓 BGP，就是在大规模网络中实现节点路由信息共享的一种协议。**\n\n而 BGP 的这个能力，正好可以取代 Flannel 维护主机上路由表的功能。而且，BGP 这种原生就是为大规模网络环境而实现的协议，其可靠性和可扩展性，远非 Flannel 自己的方案可比。\n\n> 需要注意的是，BGP 协议实际上是最复杂的一种路由协议。我在这里的讲述和所举的例子，仅是为了能够帮助你建立对 BGP 的感性认识，并不代表 BGP 真正的实现方式。\n\n接下来，我们还是回到 Calico 项目上来。\n\n在了解了 BGP 之后，Calico 项目的架构就非常容易理解了。它由三个部分组成：\n\nCalico 的 CNI 插件。这是 Calico 与 Kubernetes 对接的部分。我已经在上一篇文章中，和你详细分享了 CNI 插件的工作原理，这里就不再赘述了。\n\nFelix。它是一个 DaemonSet，负责在宿主机上插入路由规则（即：写入 Linux 内核的 FIB 转发信息库），以及维护 Calico 所需的网络设备等工作。\n\nBIRD。它就是 BGP 的客户端，专门负责在集群里分发路由规则信息。\n\n**除了对路由信息的维护方式之外，Calico 项目与 Flannel 的 host-gw 模式的另一个不同之处，就是它不会在宿主机上创建任何网桥设备。**这时候，Calico 的工作方式，可以用一幅示意图来描述，如下所示（在接下来的讲述中，我会统一用“BGP 示意图”来指代它）：\n\n![](4.jpg)\n图3 Calico工作原理\n\n其中的绿色实线标出的路径，就是一个 IP 包从 Node 1 上的 Container 1，到达 Node 2 上的 Container 4 的完整路径。\n\n可以看到，Calico 的 CNI 插件会为每个容器设置一个 Veth Pair 设备，然后把其中的一端放置在宿主机上（它的名字以 cali 前缀开头）。\n\n此外，由于 Calico 没有使用 CNI 的网桥模式，Calico 的 CNI 插件还需要在宿主机上为每个容器的 Veth Pair 设备配置一条路由规则，用于接收传入的 IP 包。比如，宿主机 Node 2 上的 Container 4 对应的路由规则，如下所示：\n\n\t10.233.2.3 dev cali5863f3 scope link\n即：发往 10.233.2.3 的 IP 包，应该进入 cali5863f3 设备。\n\n> 基于上述原因，Calico 项目在宿主机上设置的路由规则，肯定要比 Flannel 项目多得多。不过，Flannel host-gw 模式使用 CNI 网桥的主要原因，其实是为了跟 VXLAN 模式保持一致。否则的话，Flannel 就需要维护两套 CNI 插件了。\n\n有了这样的 Veth Pair 设备之后，容器发出的 IP 包就会经过 Veth Pair 设备出现在宿主机上。然后，宿主机网络栈就会根据路由规则的下一跳 IP 地址，把它们转发给正确的网关。接下来的流程就跟 Flannel host-gw 模式完全一致了。\n\n其中，这里最核心的“下一跳”路由规则，就是由 Calico 的 Felix 进程负责维护的。这些路由规则信息，则是通过 BGP Client 也就是 BIRD 组件，使用 BGP 协议传输而来的。\n\n而这些通过 BGP 协议传输的消息，你可以简单地理解为如下格式：\n\n\t[BGP消息]\n\t我是宿主机192.168.1.3\n\t10.233.2.0/24网段的容器都在我这里\n\t这些容器的下一跳地址是我\n不难发现，Calico 项目实际上将集群里的所有节点，都当作是边界路由器来处理，它们一起组成了一个全连通的网络，互相之间通过 BGP 协议交换路由规则。这些节点，我们称为 BGP Peer。\n\n需要注意的是，**Calico 维护的网络在默认配置下，是一个被称为“Node-to-Node Mesh”的模式。**这时候，每台宿主机上的 BGP Client 都需要跟其他所有节点的 BGP Client 进行通信以便交换路由信息。但是，随着节点数量 N 的增加，这些连接的数量就会以 N²的规模快速增长，从而给集群本身的网络带来巨大的压力。\n\n所以，Node-to-Node Mesh 模式一般推荐用在少于 100 个节点的集群里。而在更大规模的集群中，你需要用到的是一个叫作 Route Reflector 的模式。\n\n在这种模式下，Calico 会指定一个或者几个专门的节点，来负责跟所有节点建立 BGP 连接从而学习到全局的路由规则。而其他节点，只需要跟这几个专门的节点交换路由信息，就可以获得整个集群的路由规则信息了。\n\n这些专门的节点，就是所谓的 Route Reflector 节点，它们实际上扮演了“中间代理”的角色，从而把 BGP 连接的规模控制在 N 的数量级上。\n\n此外，我在前面提到过，Flannel host-gw 模式最主要的限制，就是要求集群宿主机之间是二层连通的。而这个限制对于 Calico 来说，也同样存在。\n\n举个例子，假如我们有两台处于不同子网的宿主机 Node 1 和 Node 2，对应的 IP 地址分别是 192.168.1.2 和 192.168.2.2。需要注意的是，这两台机器通过路由器实现了三层转发，所以这两个 IP 地址之间是可以相互通信的。\n\n而我们现在的需求，还是 Container 1 要访问 Container 4。\n\n按照我们前面的讲述，Calico 会尝试在 Node 1 上添加如下所示的一条路由规则：\n\n\t10.233.2.0/16 via 192.168.2.2 eth0\n但是，这时候问题就来了。\n\n上面这条规则里的下一跳地址是 192.168.2.2，可是它对应的 Node 2 跟 Node 1 却根本不在一个子网里，没办法通过二层网络把 IP 包发送到下一跳地址。\n\n**在这种情况下，你就需要为 Calico 打开 IPIP 模式。**\n\n我把这个模式下容器通信的原理，总结成了一张图片，如下所示（接下来我会称之为：IPIP 示意图）：\n\n![](5.jpg)\n图4 Calico IPIP模式工作原理\n\n在 Calico 的 IPIP 模式下，Felix 进程在 Node 1 上添加的路由规则，会稍微不同，如下所示：\n\n\t10.233.2.0/24 via 192.168.2.2 tunl0\n可以看到，尽管这条规则的下一跳地址仍然是 Node 2 的 IP 地址，但这一次，要负责将 IP 包发出去的设备，变成了 tunl0。注意，是 T-U-N-L-0，而不是 Flannel UDP 模式使用的 T-U-N-0（tun0），这两种设备的功能是完全不一样的。\n\nCalico 使用的这个 tunl0 设备，是一个 IP 隧道（IP tunnel）设备。\n\n在上面的例子中，IP 包进入 IP 隧道设备之后，就会被 Linux 内核的 IPIP 驱动接管。IPIP 驱动会将这个 IP 包直接封装在一个宿主机网络的 IP 包中，如下所示：\n\n![](6.jpg)\n图 5 IPIP 封包方式\n\n其中，经过封装后的新的 IP 包的目的地址（图 5 中的 Outer IP Header 部分），正是原 IP 包的下一跳地址，即 Node 2 的 IP 地址：192.168.2.2。\n\n而原 IP 包本身，则会被直接封装成新 IP 包的 Payload。\n\n这样，原先从容器到 Node 2 的 IP 包，就被伪装成了一个从 Node 1 到 Node 2 的 IP 包。\n\n由于宿主机之间已经使用路由器配置了三层转发，也就是设置了宿主机之间的“下一跳”。所以这个 IP 包在离开 Node 1 之后，就可以经过路由器，最终“跳”到 Node 2 上。\n\n这时，Node 2 的网络内核栈会使用 IPIP 驱动进行解包，从而拿到原始的 IP 包。然后，原始 IP 包就会经过路由规则和 Veth Pair 设备到达目的容器内部。\n\n以上，就是 Calico 项目主要的工作原理了。\n\n不难看到，当 Calico 使用 IPIP 模式的时候，集群的网络性能会因为额外的封包和解包工作而下降。在实际测试中，Calico IPIP 模式与 Flannel VXLAN 模式的性能大致相当。所以，在实际使用时，如非硬性需求，我建议你将所有宿主机节点放在一个子网里，避免使用 IPIP。\n\n不过，通过上面对 Calico 工作原理的讲述，你应该能发现这样一个事实：\n\n如果 Calico 项目能够让宿主机之间的路由设备（也就是网关），也通过 BGP 协议“学习”到 Calico 网络里的路由规则，那么从容器发出的 IP 包，不就可以通过这些设备路由到目的宿主机了么？\n\n比如，只要在上面“IPIP 示意图”中的 Node 1 上，添加如下所示的一条路由规则：\n\n\t10.233.2.0/24 via 192.168.1.1 eth0\n然后，在 Router 1 上（192.168.1.1），添加如下所示的一条路由规则：\n\n\t10.233.2.0/24 via 192.168.2.1 eth0\n那么 Container 1 发出的 IP 包，就可以通过两次“下一跳”，到达 Router 2（192.168.2.1）了。以此类推，我们可以继续在 Router 2 上添加“下一条”路由，最终把 IP 包转发到 Node 2 上。\n\n遗憾的是，上述流程虽然简单明了，但是在 Kubernetes 被广泛使用的公有云场景里，却完全不可行。\n\n这里的原因在于：公有云环境下，宿主机之间的网关，肯定不会允许用户进行干预和设置。\n\n> 当然，在大多数公有云环境下，宿主机（公有云提供的虚拟机）本身往往就是二层连通的，所以这个需求也不强烈。\n\n不过，在私有部署的环境下，宿主机属于不同子网（VLAN）反而是更加常见的部署状态。这时候，想办法将宿主机网关也加入到 BGP Mesh 里从而避免使用 IPIP，就成了一个非常迫切的需求。\n\n`而在 Calico 项目中，它已经为你提供了两种将宿主机网关设置成 BGP Peer 的解决方案。`\n\n**第一种方案**，就是所有宿主机都跟宿主机网关建立 BGP Peer 关系。\n\n这种方案下，Node 1 和 Node 2 就需要主动跟宿主机网关 Router 1 和 Router 2 建立 BGP 连接。从而将类似于 10.233.2.0/24 这样的路由信息同步到网关上去。\n\n需要注意的是，这种方式下，Calico 要求宿主机网关必须支持一种叫作 Dynamic Neighbors 的 BGP 配置方式。这是因为，在常规的路由器 BGP 配置里，运维人员必须明确给出所有 BGP Peer 的 IP 地址。考虑到 Kubernetes 集群可能会有成百上千个宿主机，而且还会动态地添加和删除节点，这时候再手动管理路由器的 BGP 配置就非常麻烦了。而 Dynamic Neighbors 则允许你给路由器配置一个网段，然后路由器就会自动跟该网段里的主机建立起 BGP Peer 关系。\n\n不过，相比之下，我更愿意推荐**第二种方案**。\n\n这种方案，是使用一个或多个独立组件负责搜集整个集群里的所有路由信息，然后通过 BGP 协议同步给网关。而我们前面提到，在大规模集群中，Calico 本身就推荐使用 Route Reflector 节点的方式进行组网。所以，这里负责跟宿主机网关进行沟通的独立组件，直接由 Route Reflector 兼任即可。\n\n更重要的是，这种情况下网关的 BGP Peer 个数是有限并且固定的。所以我们就可以直接把这些独立组件配置成路由器的 BGP Peer，而无需 Dynamic Neighbors 的支持。\n\n当然，这些独立组件的工作原理也很简单：它们只需要 WATCH Etcd 里的宿主机和对应网段的变化信息，然后把这些信息通过 BGP 协议分发给网关即可。\n\n**总结**\n\n在本篇文章中，我为你详细讲述了 Fannel host-gw 模式和 Calico 这两种纯三层网络方案的工作原理。\n\n需要注意的是，在大规模集群里，三层网络方案在宿主机上的路由规则可能会非常多，这会导致错误排查变得困难。此外，在系统故障的时候，路由规则出现重叠冲突的概率也会变大。\n\n基于上述原因，如果是在公有云上，由于宿主机网络本身比较“直白”，我一般会推荐更加简单的 Flannel host-gw 模式。\n\n但不难看到，在私有部署环境里，Calico 项目才能够覆盖更多的场景，并为你提供更加可靠的组网方案和架构思路。\n\n**思考题**\n\n你能否能总结一下三层网络方案和“隧道模式”的异同，以及各自的优缺点？\n\n","categories":["reference","k8s"]},{"title":"36 | 为什么说Kubernetes只有soft multi-tenancy？","url":"/2021/03/13/reference/k8s/36.为什么说Kubernetes只有soft multi-tenancy/","content":"\n![](1.jpg)\n\n## 为什么说Kubernetes只有soft multi-tenancy\n\n在前面的文章中，我为你详细讲解了 Kubernetes 生态里，主流容器网络方案的工作原理。\n\n不难发现，Kubernetes 的网络模型，以及前面这些网络方案的实现，都只关注容器之间网络的“连通”，却并不关心容器之间网络的“隔离”。这跟传统的 IaaS 层的网络方案，区别非常明显。\n\n你肯定会问了，Kubernetes 的网络方案对“隔离”到底是如何考虑的呢？难道 Kubernetes 就不管网络“多租户”的需求吗？\n\n接下来，在今天这篇文章中，我就来回答你的这些问题。\n\n在 Kubernetes 里，网络隔离能力的定义，是依靠一种专门的 API 对象来描述的，即：NetworkPolicy。\n\n`一个完整的 NetworkPolicy 对象的示例`，如下所示：\n\n\tapiVersion: networking.k8s.io/v1\n\tkind: NetworkPolicy\n\tmetadata:\n\t  name: test-network-policy\n\t  namespace: default\n\tspec:\n\t  podSelector:\n\t    matchLabels:\n\t      role: db\n\t  policyTypes:\n\t  - Ingress\n\t  - Egress\n\t  ingress:\n\t  - from:\n\t    - ipBlock:\n\t        cidr: 172.17.0.0/16\n\t        except:\n\t        - 172.17.1.0/24\n\t    - namespaceSelector:\n\t        matchLabels:\n\t          project: myproject\n\t    - podSelector:\n\t        matchLabels:\n\t          role: frontend\n\t    ports:\n\t    - protocol: TCP\n\t      port: 6379\n\t  egress:\n\t  - to:\n\t    - ipBlock:\n\t        cidr: 10.0.0.0/24\n\t    ports:\n\t    - protocol: TCP\n\t      port: 5978\n我在和你分享前面的内容时已经说过（这里你可以再回顾下第 34 篇文章《Kubernetes 网络模型与 CNI 网络插件》中的相关内容），**Kubernetes 里的 Pod 默认都是“允许所有”（Accept All）的**，即：Pod 可以接收来自任何发送方的请求；或者，向任何接收方发送请求。而如果你要对这个情况作出限制，就必须通过 NetworkPolicy 对象来指定。\n\n而在上面这个例子里，你首先会看到 podSelector 字段。它的作用，就是定义这个 NetworkPolicy 的限制范围，比如：当前 Namespace 里携带了 role=db 标签的 Pod。\n\n而如果你把 podSelector 字段留空：\n\n\tspec:\n\t podSelector: {}\n那么这个 NetworkPolicy 就会作用于当前 Namespace 下的所有 Pod。\n\n而一旦 Pod 被 NetworkPolicy 选中，那么这个 Pod 就会进入“拒绝所有”（Deny All）的状态，即：这个 Pod 既不允许被外界访问，也不允许对外界发起访问。\n\n**而 NetworkPolicy 定义的规则，其实就是“白名单”。**\n\n例如，在我们上面这个例子里，我在 policyTypes 字段，定义了这个 NetworkPolicy 的类型是 ingress 和 egress，即：它既会影响流入（ingress）请求，也会影响流出（egress）请求。\n\n然后，在 ingress 字段里，我定义了 from 和 ports，即：允许流入的“白名单”和端口。其中，这个允许流入的“白名单”里，我指定了**三种并列的情况，**分别是：ipBlock、namespaceSelector 和 podSelector。\n\n而在 egress 字段里，我则定义了 to 和 ports，即：允许流出的“白名单”和端口。这里允许流出的“白名单”的定义方法与 ingress 类似。只不过，这一次 ipblock 字段指定的，是目的地址的网段。\n\n综上所述，这个 NetworkPolicy 对象，指定的隔离规则如下所示：\n\n1. 该隔离规则只对 default Namespace 下的，携带了 role=db 标签的 Pod 有效。限制的请求类型包括 ingress（流入）和 egress（流出）。\n2. Kubernetes 会拒绝任何访问被隔离 Pod 的请求，除非这个请求来自于以下“白名单”里的对象，并且访问的是被隔离 Pod 的 6379 端口。这些“白名单”对象包括：\n  a. default Namespace 里的，携带了 role=fronted 标签的 Pod；\n  b. 携带了 project=myproject 标签的 Namespace 里的任何 Pod；\n  c. 任何源地址属于 172.17.0.0/16 网段，且不属于 172.17.1.0/24 网段的请求。\n3. Kubernetes 会拒绝被隔离 Pod 对外发起任何请求，除非请求的目的地址属于 10.0.0.0/24 网段，并且访问的是该网段地址的 5978 端口。\n\n需要注意的是，定义一个 NetworkPolicy 对象的过程，容易犯错的是“白名单”部分（from 和 to 字段）。\n\n举个例子：\n\n\t  ...\n\t  ingress:\n\t  - from:\n\t    - namespaceSelector:\n\t        matchLabels:\n\t          user: alice\n\t    - podSelector:\n\t        matchLabels:\n\t          role: client\n\t  ...\n像上面这样定义的 namespaceSelector 和 podSelector，是“或”（OR）的关系。所以说，这个 from 字段定义了两种情况，无论是 Namespace 满足条件，还是 Pod 满足条件，这个 NetworkPolicy 都会生效。\n\n而下面这个例子，虽然看起来类似，但是它定义的规则却完全不同：\n\n\t...\n\t  ingress:\n\t  - from:\n\t    - namespaceSelector:\n\t        matchLabels:\n\t          user: alice\n\t      podSelector:\n\t        matchLabels:\n\t          role: client\n\t  ...\n注意看，这样定义的 namespaceSelector 和 podSelector，其实是“与”（AND）的关系。所以说，这个 from 字段只定义了一种情况，只有 Namespace 和 Pod 同时满足条件，这个 NetworkPolicy 才会生效。\n\n**这两种定义方式的区别，请你一定要分清楚。**\n\n此外，如果要使上面定义的 NetworkPolicy 在 Kubernetes 集群里真正产生作用，你的 CNI 网络插件就必须是支持 Kubernetes 的 NetworkPolicy 的。\n\n在具体实现上，凡是支持 NetworkPolicy 的 CNI 网络插件，都维护着一个 NetworkPolicy Controller，通过控制循环的方式对 NetworkPolicy 对象的增删改查做出响应，然后在宿主机上完成 iptables 规则的配置工作。\n\n在 Kubernetes 生态里，目前已经实现了 NetworkPolicy 的网络插件包括 Calico、Weave 和 kube-router 等多个项目，但是并不包括 Flannel 项目。\n\n所以说，如果想要在使用 Flannel 的同时还使用 NetworkPolicy 的话，你就需要再额外安装一个网络插件，比如 Calico 项目，来负责执行 NetworkPolicy。\n\n> 安装 Flannel + Calico 的流程非常简单，你直接参考这个文档一键安装即可。\n\n`那么，这些网络插件，又是如何根据 NetworkPolicy 对 Pod 进行隔离的呢？`\n\n接下来，我就以三层网络插件为例（比如 Calico 和 kube-router），来为你分析一下这部分的原理。\n\n为了方便讲解，这一次我编写了一个比较简单的 NetworkPolicy 对象，如下所示：\n\n\tapiVersion: extensions/v1beta1\n\tkind: NetworkPolicy\n\tmetadata:\n\t  name: test-network-policy\n\t  namespace: default\n\tspec:\n\t  podSelector:\n\t    matchLabels:\n\t      role: db\n\t  ingress:\n\t   - from:\n\t     - namespaceSelector:\n\t         matchLabels:\n\t           project: myproject\n\t     - podSelector:\n\t         matchLabels:\n\t           role: frontend\n\t     ports:\n\t       - protocol: tcp\n\t         port: 6379\n可以看到，我们指定的 ingress“白名单”，是任何 Namespace 里，携带 project=myproject 标签的 Pod；以及 default Namespace 里，携带了 role=frontend 标签的 Pod。允许被访问的端口是：6379。\n\n而被隔离的对象，是所有携带了 role=db 标签的 Pod。\n\n那么这个时候，Kubernetes 的网络插件就会使用这个 NetworkPolicy 的定义，在宿主机上生成 iptables 规则。这个过程，我可以通过如下所示的一段 Go 语言风格的伪代码来为你描述：\n\n\tfor dstIP := range 所有被networkpolicy.spec.podSelector选中的Pod的IP地址\n\t  for srcIP := range 所有被ingress.from.podSelector选中的Pod的IP地址\n\t    for port, protocol := range ingress.ports {\n\t      iptables -A KUBE-NWPLCY-CHAIN -s $srcIP -d $dstIP -p $protocol -m $protocol --dport $port -j ACCEPT \n\t    }\n\t  }\n\t} \n\n可以看到，这是一条最基本的、通过匹配条件决定下一步动作的 iptables 规则。\n\n这条规则的名字是 KUBE-NWPLCY-CHAIN，含义是：当 IP 包的源地址是 srcIP、目的地址是 dstIP、协议是 protocol、目的端口是 port 的时候，就允许它通过（ACCEPT）。\n\n而正如这段伪代码所示，匹配这条规则所需的这四个参数，都是从 NetworkPolicy 对象里读取出来的。\n\n**可以看到，Kubernetes 网络插件对 Pod 进行隔离，其实是靠在宿主机上生成 NetworkPolicy 对应的 iptable 规则来实现的。**\n\n此外，在设置好上述“隔离”规则之后，网络插件还需要想办法，将所有对被隔离 Pod 的访问请求，都转发到上述 KUBE-NWPLCY-CHAIN 规则上去进行匹配。并且，如果匹配不通过，这个请求应该被“拒绝”。\n\n`在 CNI 网络插件中，上述需求可以通过设置两组 iptables 规则来实现。`\n\n**第一组规则，负责“拦截”对被隔离 Pod 的访问请求。**生成这一组规则的伪代码，如下所示：\n\n\n\tfor pod := range 该Node上的所有Pod {\n\t    if pod是networkpolicy.spec.podSelector选中的 {\n\t        iptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN\n\t        iptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN\n\t        ...\n\t    }\n\t}\n可以看到，这里的的 iptables 规则使用到了内置链：FORWARD。它是什么意思呢？\n\n说到这里，我就得为你稍微普及一下 iptables 的知识了。\n\n实际上，iptables 只是一个操作 Linux 内核 Netfilter 子系统的“界面”。顾名思义，Netfilter 子系统的作用，就是 Linux 内核里挡在“网卡”和“用户态进程”之间的一道“防火墙”。它们的关系，可以用如下的示意图来表示：\n\n![](2.png)\n可以看到，这幅示意图中，IP 包“一进一出”的两条路径上，有几个关键的“检查点”，它们正是 Netfilter 设置“防火墙”的地方。**在 iptables 中，这些“检查点”被称为：链（Chain）。**这是因为这些“检查点”对应的 iptables 规则，是按照定义顺序依次进行匹配的。这些“检查点”的具体工作原理，可以用如下所示的示意图进行描述：\n\n![](3.jpg)\n可以看到，当一个 IP 包通过网卡进入主机之后，它就进入了 Netfilter 定义的流入路径（Input Path）里。\n\n在这个路径中，IP 包要经过路由表路由来决定下一步的去向。而在这次路由之前，Netfilter 设置了一个名叫 PREROUTING 的“检查点”。在 Linux 内核的实现里，所谓“检查点”实际上就是内核网络协议栈代码里的 Hook（比如，在执行路由判断的代码之前，内核会先调用 PREROUTING 的 Hook）。\n\n而在经过路由之后，IP 包的去向就分为了两种：\n\n * 第一种，继续在本机处理；\n * 第二种，被转发到其他目的地。\n\n**我们先说一下 IP 包的第一种去向。**这时候，IP 包将继续向上层协议栈流动。在它进入传输层之前，Netfilter 会设置一个名叫 INPUT 的“检查点”。到这里，IP 包流入路径（Input Path）结束。\n\n\n接下来，这个 IP 包通过传输层进入用户空间，交给用户进程处理。而处理完成后，用户进程会通过本机发出返回的 IP 包。这时候，这个 IP 包就进入了流出路径（Output Path）。\n\n此时，IP 包首先还是会经过主机的路由表进行路由。路由结束后，Netfilter 就会设置一个名叫 OUTPUT 的“检查点”。然后，在 OUTPUT 之后，再设置一个名叫 POSTROUTING“检查点”。\n\n你可能会觉得奇怪，为什么在流出路径结束后，Netfilter 会连着设置两个“检查点”呢？\n\n这就要说到在流入路径里，**路由判断后的第二种去向**了。\n\n在这种情况下，这个 IP 包不会进入传输层，而是会继续在网络层流动，从而进入到转发路径（Forward Path）。在转发路径中，Netfilter 会设置一个名叫 FORWARD 的“检查点”。\n\n而在 FORWARD“检查点”完成后，IP 包就会来到流出路径。而转发的 IP 包由于目的地已经确定，它就不会再经过路由，也自然不会经过 OUTPUT，而是会直接来到 POSTROUTING“检查点”。\n\n\n所以说，POSTROUTING 的作用，其实就是上述两条路径，最终汇聚在一起的“最终检查点”。\n\n需要注意的是，在有网桥参与的情况下，上述 Netfilter 设置“检查点”的流程，实际上也会出现在链路层（二层），并且会跟我在上面讲述的网络层（三层）的流程有交互。\n\n这些链路层的“检查点”对应的操作界面叫作 ebtables。所以，准确地说，数据包在 Linux Netfilter 子系统里完整的流动过程，其实应该如下所示（这是一幅来自[Netfilter 官方的原理图](https://en.wikipedia.org/wiki/Iptables#/media/File:Netfilter-packet-flow.svg)，建议你点击图片以查看大图）：\n\n![](4.jpg)\n可以看到，我前面为你讲述的，正是上图中绿色部分，也就是网络层的 iptables 链的工作流程。\n\n另外，你应该还能看到，每一个白色的“检查点”上，还有一个绿色的“标签”，比如：raw、nat、filter 等等。\n\n在 iptables 里，这些标签叫作：表。比如，同样是 OUTPUT 这个“检查点”，filter Output 和 nat Output 在 iptables 里的语法和参数，就完全不一样，实现的功能也完全不同。\n\n所以说，iptables 表的作用，就是在某个具体的“检查点”（比如 Output）上，按顺序执行几个不同的检查动作（比如，先执行 nat，再执行 filter）。\n\n在理解了 iptables 的工作原理之后，我们再回到 NetworkPolicy 上来。这时候，前面由网络插件设置的、负责“拦截”进入 Pod 的请求的三条 iptables 规则，就很容易读懂了：\n\n\tiptables -A FORWARD -d $podIP -m physdev --physdev-is-bridged -j KUBE-POD-SPECIFIC-FW-CHAIN\n\tiptables -A FORWARD -d $podIP -j KUBE-POD-SPECIFIC-FW-CHAIN\n\t...\n其中，**第一条 FORWARD 链“拦截”的是一种特殊情况**：它对应的是同一台宿主机上容器之间经过 CNI 网桥进行通信的流入数据包。其中，–physdev-is-bridged 的意思就是，这个 FORWARD 链匹配的是，通过本机上的网桥设备，发往目的地址是 podIP 的 IP 包。\n\n当然，如果是像 Calico 这样的非网桥模式的 CNI 插件，就不存在这个情况了。\n\nkube-router 其实是一个简化版的 Calico，它也使用 BGP 来维护路由信息，但是使用 CNI bridge 插件负责跟 Kubernetes 进行交互。\n\n而**第二条 FORWARD 链“拦截”的则是最普遍的情况，即：容器跨主通信**。这时候，流入容器的数据包都是经过路由转发（FORWARD 检查点）来的。\n\n不难看到，这些规则最后都跳转（即：-j）到了名叫 KUBE-POD-SPECIFIC-FW-CHAIN 的规则上。它正是网络插件为 NetworkPolicy 设置的第二组规则。\n\n而这个 KUBE-POD-SPECIFIC-FW-CHAIN 的作用，就是做出“允许”或者“拒绝”的判断。这部分功能的实现，可以简单描述为下面这样的 iptables 规则：\n\n\tiptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j KUBE-NWPLCY-CHAIN\n\tiptables -A KUBE-POD-SPECIFIC-FW-CHAIN -j REJECT --reject-with icmp-port-unreachable\n可以看到，首先在第一条规则里，我们会把 IP 包转交给前面定义的 KUBE-NWPLCY-CHAIN 规则去进行匹配。按照我们之前的讲述，如果匹配成功，那么 IP 包就会被“允许通过”。\n\n而如果匹配失败，IP 包就会来到第二条规则上。可以看到，它是一条 REJECT 规则。通过这条规则，不满足 NetworkPolicy 定义的请求就会被拒绝掉，从而实现了对该容器的“隔离”。\n\n以上，就是 CNI 网络插件实现 NetworkPolicy 的基本方法了。当然，对于不同的插件来说，上述实现过程可能有不同的手段，但根本原理是不变的。\n\n**总结**\n\n在本篇文章中，我主要和你分享了 Kubernetes 对 Pod 进行“隔离”的手段，即：NetworkPolicy。\n\n可以看到，NetworkPolicy 实际上只是宿主机上的一系列 iptables 规则。这跟传统 IaaS 里面的安全组（Security Group）其实是非常类似的。\n\n而基于上述讲述，你就会发现这样一个事实：\n\nKubernetes 的网络模型以及大多数容器网络实现，其实既不会保证容器之间二层网络的互通，也不会实现容器之间的二层网络隔离。这跟 IaaS 项目管理虚拟机的方式，是完全不同的。\n\n所以说，Kubernetes 从底层的设计和实现上，更倾向于假设你已经有了一套完整的物理基础设施。然后，Kubernetes 负责在此基础上提供一种“弱多租户”（soft multi-tenancy）的能力。\n\n并且，基于上述思路，Kubernetes 将来也不大可能把 Namespace 变成一个具有实质意义的隔离机制，或者把它映射成为“子网”或者“租户”。毕竟你可以看到，NetworkPolicy 对象的描述能力，要比基于 Namespace 的划分丰富得多。\n\n这也是为什么，到目前为止，Kubernetes 项目在云计算生态里的定位，其实是基础设施与 PaaS 之间的中间层。这是非常符合“容器”这个本质上就是进程的抽象粒度的。\n\n当然，随着 Kubernetes 社区以及 CNCF 生态的不断发展，Kubernetes 项目也已经开始逐步下探，“吃”掉了基础设施领域的很多“蛋糕”。这也正是容器生态继续发展的一个必然方向。\n\n**思考题**\n\n请你编写这样一个 NetworkPolicy：它使得指定的 Namespace（比如 my-namespace）里的所有 Pod，都不能接收任何 Ingress 请求。然后，请你说说，这样的 NetworkPolicy 有什么实际的作用？\n\n","categories":["reference","k8s"]},{"title":"31 | 容器存储实践：CSI插件编写指南","url":"/2021/03/13/reference/k8s/31.容器存储实践：CSI插件编写指南/","content":"\n![](1.jpg)\n\n## 容器存储实践：CSI插件编写指南\n\n在上一篇文章中，我已经为你详细讲解了 CSI 插件机制的设计原理。今天我将继续和你一起实践一个 CSI 插件的编写过程。\n\n为了能够覆盖到 CSI 插件的所有功能，我这一次选择了 DigitalOcean 的块存储（Block Storage）服务，来作为实践对象。\n\nDigitalOcean 是业界知名的“最简”公有云服务，即：它只提供虚拟机、存储、网络等为数不多的几个基础功能，其他功能一概不管。而这，恰恰就使得 DigitalOcean 成了我们在公有云上实践 Kubernetes 的最佳选择。\n\n**`我们这次编写的 CSI 插件的功能，就是：让我们运行在 DigitalOcean 上的 Kubernetes 集群能够使用它的块存储服务，作为容器的持久化存储。`**\n\n> 备注：在 DigitalOcean 上部署一个 Kubernetes 集群的过程，也很简单。你只需要先在 DigitalOcean 上创建几个虚拟机，然后按照我们在第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》中从 0 到 1 的步骤直接部署即可。\n\n而有了 CSI 插件之后，持久化存储的用法就非常简单了，你只需要创建一个如下所示的 StorageClass 对象即可：\n\n\tkind: StorageClass\n\tapiVersion: storage.k8s.io/v1\n\tmetadata:\n\t  name: do-block-storage\n\t  namespace: kube-system\n\t  annotations:\n\t    storageclass.kubernetes.io/is-default-class: \"true\"\n\tprovisioner: com.digitalocean.csi.dobs\n有了这个 StorageClass，External Provisoner 就会为集群中新出现的 PVC 自动创建出 PV，然后调用 CSI 插件创建出这个 PV 对应的 Volume，这正是 CSI 体系中 Dynamic Provisioning 的实现方式。\n\n> 备注：storageclass.kubernetes.io/is-default-class: \"true\"的意思，是使用这个 StorageClass 作为默认的持久化存储提供者。\n\n不难看到，这个 StorageClass 里唯一引人注意的，是 provisioner=com.digitalocean.csi.dobs 这个字段。显然，这个字段告诉了 Kubernetes，请使用名叫 com.digitalocean.csi.dobs 的 CSI 插件来为我处理这个 StorageClass 相关的所有操作。\n\n那么，`Kubernetes 又是如何知道一个 CSI 插件的名字的呢？`\n\n**这就需要从 CSI 插件的第一个服务 CSI Identity 说起了。**\n\n其实，一个 CSI 插件的代码结构非常简单，如下所示：\n\n\ttree $GOPATH/src/github.com/digitalocean/csi-digitalocean/driver  \n\t$GOPATH/src/github.com/digitalocean/csi-digitalocean/driver \n\t├── controller.go\n\t├── driver.go\n\t├── identity.go\n\t├── mounter.go\n\t└── node.go\n其中，CSI Identity 服务的实现，就定义在了 driver 目录下的 identity.go 文件里。\n\n当然，为了能够让 Kubernetes 访问到 CSI Identity 服务，我们需要先在 driver.go 文件里，定义一个标准的 gRPC Server，如下所示：\n\n\t// Run starts the CSI plugin by communication over the given endpoint\n\tfunc (d *Driver) Run() error {\n\t ...\n\t \n\t listener, err := net.Listen(u.Scheme, addr)\n\t ...\n\t \n\t d.srv = grpc.NewServer(grpc.UnaryInterceptor(errHandler))\n\t csi.RegisterIdentityServer(d.srv, d)\n\t csi.RegisterControllerServer(d.srv, d)\n\t csi.RegisterNodeServer(d.srv, d)\n\t \n\t d.ready = true // we're now ready to go!\n\t ...\n\t return d.srv.Serve(listener)\n\t}\n可以看到，只要把编写好的 gRPC Server 注册给 CSI，它就可以响应来自 External Components 的 CSI 请求了。\n\n**CSI Identity 服务中，最重要的接口是 GetPluginInfo，**它返回的就是这个插件的名字和版本号，如下所示：\n\n> 备注：CSI 各个服务的接口我在上一篇文章中已经介绍过，你也可以在这里找到[它的 protoc 文件](https://github.com/container-storage-interface/spec/blob/master/csi.proto)。\n\n\tfunc (d *Driver) GetPluginInfo(ctx context.Context, req *csi.GetPluginInfoRequest) (*csi.GetPluginInfoResponse, error) {\n\t resp := &csi.GetPluginInfoResponse{\n\t  Name:          driverName,\n\t  VendorVersion: version,\n\t }\n\t ...\n\t}\n其中，driverName 的值，正是\"com.digitalocean.csi.dobs\"。所以说，Kubernetes 正是通过 GetPluginInfo 的返回值，来找到你在 StorageClass 里声明要使用的 CSI 插件的。\n\n> 备注：CSI 要求插件的名字遵守[“反向 DNS”格式](https://en.wikipedia.org/wiki/Reverse_domain_name_notation)。\n\n另外一个 **GetPluginCapabilities 接口也很重要**。这个接口返回的是这个 CSI 插件的“能力”。\n\n比如，当你编写的 CSI 插件不准备实现“Provision 阶段”和“Attach 阶段”（比如，一个最简单的 NFS 存储插件就不需要这两个阶段）时，你就可以通过这个接口返回：本插件不提供 CSI Controller 服务，即：没有 csi.PluginCapability_Service_CONTROLLER_SERVICE 这个“能力”。这样，Kubernetes 就知道这个信息了。\n\n最后，**CSI Identity 服务还提供了一个 Probe 接口**。Kubernetes 会调用它来检查这个 CSI 插件是否正常工作。\n\n一般情况下，我建议你在编写插件时给它设置一个 Ready 标志，当插件的 gRPC Server 停止的时候，把这个 Ready 标志设置为 false。或者，你可以在这里访问一下插件的端口，类似于健康检查的做法。\n\n> 备注：关于健康检查的问题，你可以再回顾一下第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中的相关内容。\n\n然后，**`我们要开始编写 CSI 插件的第二个服务，即 CSI Controller 服务了`**。它的代码实现，在 controller.go 文件里。\n\n在上一篇文章中我已经为你讲解过，这个服务主要实现的就是 Volume 管理流程中的“Provision 阶段”和“Attach 阶段”。\n\n**“Provision 阶段”对应的接口，是 CreateVolume 和 DeleteVolume，**它们的调用者是 External Provisoner。以 CreateVolume 为例，它的主要逻辑如下所示：\n\n\tfunc (d *Driver) CreateVolume(ctx context.Context, req *csi.CreateVolumeRequest) (*csi.CreateVolumeResponse, error) {\n\t ...\n\t \n\t volumeReq := &godo.VolumeCreateRequest{\n\t  Region:        d.region,\n\t  Name:          volumeName,\n\t  Description:   createdByDO,\n\t  SizeGigaBytes: size / GB,\n\t }\n\t \n\t ...\n\t \n\t vol, _, err := d.doClient.Storage.CreateVolume(ctx, volumeReq)\n\t \n\t ...\n\t \n\t resp := &csi.CreateVolumeResponse{\n\t  Volume: &csi.Volume{\n\t   Id:            vol.ID,\n\t   CapacityBytes: size,\n\t   AccessibleTopology: []*csi.Topology{\n\t    {\n\t     Segments: map[string]string{\n\t      \"region\": d.region,\n\t     },\n\t    },\n\t   },\n\t  },\n\t }\n\t \n\t return resp, nil\n\t}\n可以看到，对于 DigitalOcean 这样的公有云来说，CreateVolume 需要做的操作，就是调用 DigitalOcean 块存储服务的 API，创建出一个存储卷（d.doClient.Storage.CreateVolume）。如果你使用的是其他类型的块存储（比如 Cinder、Ceph RBD 等），对应的操作也是类似地调用创建存储卷的 API。\n\n而**“Attach 阶段”对应的接口是 ControllerPublishVolume 和 ControllerUnpublishVolume，**它们的调用者是 External Attacher。以 ControllerPublishVolume 为例，它的逻辑如下所示：\n\n\tfunc (d *Driver) ControllerPublishVolume(ctx context.Context, req *csi.ControllerPublishVolumeRequest) (*csi.ControllerPublishVolumeResponse, error) {\n\t ...\n\t \n\t  dropletID, err := strconv.Atoi(req.NodeId)\n\t  \n\t  // check if volume exist before trying to attach it\n\t  _, resp, err := d.doClient.Storage.GetVolume(ctx, req.VolumeId)\n\t \n\t ...\n\t \n\t  // check if droplet exist before trying to attach the volume to the droplet\n\t  _, resp, err = d.doClient.Droplets.Get(ctx, dropletID)\n\t \n\t ...\n\t \n\t  action, resp, err := d.doClient.StorageActions.Attach(ctx, req.VolumeId, dropletID)\n\t ...\n\t \n\t if action != nil {\n\t  ll.Info(\"waiting until volume is attached\")\n\t if err := d.waitAction(ctx, req.VolumeId, action.ID); err != nil {\n\t  return nil, err\n\t  }\n\t  }\n\t  \n\t  ll.Info(\"volume is attached\")\n\t return &csi.ControllerPublishVolumeResponse{}, nil\n\t}\n可以看到，对于 DigitalOcean 来说，ControllerPublishVolume 在“Attach 阶段”需要做的工作，是调用 DigitalOcean 的 API，将我们前面创建的存储卷，挂载到指定的虚拟机上（d.doClient.StorageActions.Attach）。\n\n其中，存储卷由请求中的 VolumeId 来指定。而虚拟机，也就是将要运行 Pod 的宿主机，则由请求中的 NodeId 来指定。这些参数，都是 External Attacher 在发起请求时需要设置的。\n\n我在上一篇文章中已经为你介绍过，External Attacher 的工作原理，是监听（Watch）了一种名叫 VolumeAttachment 的 API 对象。这种 API 对象的主要字段如下所示：\n\n\t// VolumeAttachmentSpec is the specification of a VolumeAttachment request.\n\ttype VolumeAttachmentSpec struct {\n\t // Attacher indicates the name of the volume driver that MUST handle this\n\t // request. This is the name returned by GetPluginName().\n\t Attacher string\n\t \n\t // Source represents the volume that should be attached.\n\t Source VolumeAttachmentSource\n\t \n\t // The node that the volume should be attached to.\n\t NodeName string\n\t}\n而这个对象的生命周期，正是由 AttachDetachController 负责管理的（这里，你可以再回顾一下第 28 篇文章《PV、PVC、StorageClass，这些到底在说啥？》中的相关内容）。\n\n这个控制循环的职责，是不断检查 Pod 所对应的 PV，在它所绑定的宿主机上的挂载情况，从而决定是否需要对这个 PV 进行 Attach（或者 Dettach）操作。\n\n而这个 Attach 操作，在 CSI 体系里，就是创建出上面这样一个 VolumeAttachment 对象。可以看到，Attach 操作所需的 PV 的名字（Source）、宿主机的名字（NodeName）、存储插件的名字（Attacher），都是这个 VolumeAttachment 对象的一部分。\n\n而当 External Attacher 监听到这样的一个对象出现之后，就可以立即使用 VolumeAttachment 里的这些字段，封装成一个 gRPC 请求调用 CSI Controller 的 ControllerPublishVolume 方法。\n\n最后，**`我们就可以编写 CSI Node 服务了。`**\n\nCSI Node 服务对应的，是 Volume 管理流程里的“Mount 阶段”。它的代码实现，在 node.go 文件里。\n\n我在上一篇文章里曾经提到过，kubelet 的 VolumeManagerReconciler 控制循环会直接调用 CSI Node 服务来完成 Volume 的“Mount 阶段”。\n\n不过，在具体的实现中，这个“Mount 阶段”的处理其实被细分成了 NodeStageVolume 和 NodePublishVolume 这两个接口。\n\n这里的原因其实也很容易理解：我在第 28 篇文章《PV、PVC、StorageClass，这些到底在说啥？》中曾经介绍过，对于磁盘以及块设备来说，它们被 Attach 到宿主机上之后，就成为了宿主机上的一个待用存储设备。而到了“Mount 阶段”，我们首先需要格式化这个设备，然后才能把它挂载到 Volume 对应的宿主机目录上。\n\n在 kubelet 的 VolumeManagerReconciler 控制循环中，这两步操作分别叫作** MountDevice 和 SetUp。**\n\n其中，MountDevice 操作，就是直接调用了 CSI Node 服务里的 NodeStageVolume 接口。顾名思义，这个接口的作用，就是格式化 Volume 在宿主机上对应的存储设备，然后挂载到一个临时目录（Staging 目录）上。\n\n对于 DigitalOcean 来说，它对 NodeStageVolume 接口的实现如下所示：\n\n\tfunc (d *Driver) NodeStageVolume(ctx context.Context, req *csi.NodeStageVolumeRequest) (*csi.NodeStageVolumeResponse, error) {\n\t ...\n\t \n\t vol, resp, err := d.doClient.Storage.GetVolume(ctx, req.VolumeId)\n\t \n\t ...\n\t \n\t source := getDiskSource(vol.Name)\n\t target := req.StagingTargetPath\n\t \n\t ...\n\t \n\t if !formatted {\n\t  ll.Info(\"formatting the volume for staging\")\n\t  if err := d.mounter.Format(source, fsType); err != nil {\n\t   return nil, status.Error(codes.Internal, err.Error())\n\t  }\n\t } else {\n\t  ll.Info(\"source device is already formatted\")\n\t }\n\t \n\t...\n\t if !mounted {\n\t  if err := d.mounter.Mount(source, target, fsType, options...); err != nil {\n\t   return nil, status.Error(codes.Internal, err.Error())\n\t  }\n\t } else {\n\t  ll.Info(\"source device is already mounted to the target path\")\n\t }\n\t \n\t ...\n\t return &csi.NodeStageVolumeResponse{}, nil\n\t}\n可以看到，在 NodeStageVolume 的实现里，我们首先通过 DigitalOcean 的 API 获取到了这个 Volume 对应的设备路径（getDiskSource）；然后，我们把这个设备格式化成指定的格式（ d.mounter.Format）；最后，我们把格式化后的设备挂载到了一个临时的 Staging 目录（StagingTargetPath）下。\n\n而 SetUp 操作则会调用 CSI Node 服务的 NodePublishVolume 接口。有了上述对设备的预处理工作后，它的实现就非常简单了，如下所示：\n\n\tfunc (d *Driver) NodePublishVolume(ctx context.Context, req *csi.NodePublishVolumeRequest) (*csi.NodePublishVolumeResponse, error) {\n\t ...\n\t source := req.StagingTargetPath\n\t target := req.TargetPath\n\t \n\t mnt := req.VolumeCapability.GetMount()\n\t options := mnt.MountFlag\n\t    ...\n\t    \n\t if !mounted {\n\t  ll.Info(\"mounting the volume\")\n\t  if err := d.mounter.Mount(source, target, fsType, options...); err != nil {\n\t   return nil, status.Error(codes.Internal, err.Error())\n\t  }\n\t } else {\n\t  ll.Info(\"volume is already mounted\")\n\t }\n\t \n\t return &csi.NodePublishVolumeResponse{}, nil\n\t}\n可以看到，在这一步实现中，我们只需要做一步操作，即：将 Staging 目录，绑定挂载到 Volume 对应的宿主机目录上。\n\n由于 Staging 目录，正是 Volume 对应的设备被格式化后挂载在宿主机上的位置，所以当它和 Volume 的宿主机目录绑定挂载之后，这个 Volume 宿主机目录的“持久化”处理也就完成了。\n\n当然，我在前面也曾经提到过，对于文件系统类型的存储服务来说，比如 NFS 和 GlusterFS 等，它们并没有一个对应的磁盘“设备”存在于宿主机上，所以 kubelet 在 VolumeManagerReconciler 控制循环中，会跳过 MountDevice 操作而直接执行 SetUp 操作。所以对于它们来说，也就不需要实现 NodeStageVolume 接口了。\n\n**`在编写完了 CSI 插件之后，我们就可以把这个插件和 External Components 一起部署起来。`**\n\n首先，我们需要创建一个 DigitalOcean client 授权需要使用的 Secret 对象，如下所示：\n\n\tapiVersion: v1\n\tkind: Secret\n\tmetadata:\n\t  name: digitalocean\n\t  namespace: kube-system\n\tstringData:\n\t  access-token: \"a05dd2f26b9b9ac2asdas__REPLACE_ME____123cb5d1ec17513e06da\"\n接下来，我们通过一句指令就可以将 CSI 插件部署起来：\n\n\t$ kubectl apply -f https://raw.githubusercontent.com/digitalocean/csi-digitalocean/master/deploy/kubernetes/releases/csi-digitalocean-v0.2.0.yaml\n\t这个 CSI 插件的 YAML 文件的主要内容如下所示（其中，非重要的内容已经被略去）：\n\tkind: DaemonSet\n\tapiVersion: apps/v1beta2\n\tmetadata:\n\t  name: csi-do-node\n\t  namespace: kube-system\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: csi-do-node\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: csi-do-node\n\t        role: csi-do\n\t    spec:\n\t      serviceAccount: csi-do-node-sa\n\t      hostNetwork: true\n\t      containers:\n\t        - name: driver-registrar\n\t          image: quay.io/k8scsi/driver-registrar:v0.3.0\n\t          ...\n\t        - name: csi-do-plugin\n\t          image: digitalocean/do-csi-plugin:v0.2.0\n\t          args :\n\t            - \"--endpoint=$(CSI_ENDPOINT)\"\n\t            - \"--token=$(DIGITALOCEAN_ACCESS_TOKEN)\"\n\t            - \"--url=$(DIGITALOCEAN_API_URL)\"\n\t          env:\n\t            - name: CSI_ENDPOINT\n\t              value: unix:///csi/csi.sock\n\t            - name: DIGITALOCEAN_API_URL\n\t              value: https://api.digitalocean.com/\n\t            - name: DIGITALOCEAN_ACCESS_TOKEN\n\t              valueFrom:\n\t                secretKeyRef:\n\t                  name: digitalocean\n\t                  key: access-token\n\t          imagePullPolicy: \"Always\"\n\t          securityContext:\n\t            privileged: true\n\t            capabilities:\n\t              add: [\"SYS_ADMIN\"]\n\t            allowPrivilegeEscalation: true\n\t          volumeMounts:\n\t            - name: plugin-dir\n\t              mountPath: /csi\n\t            - name: pods-mount-dir\n\t              mountPath: /var/lib/kubelet\n\t              mountPropagation: \"Bidirectional\"\n\t            - name: device-dir\n\t              mountPath: /dev\n\t      volumes:\n\t        - name: plugin-dir\n\t          hostPath:\n\t            path: /var/lib/kubelet/plugins/com.digitalocean.csi.dobs\n\t            type: DirectoryOrCreate\n\t        - name: pods-mount-dir\n\t          hostPath:\n\t            path: /var/lib/kubelet\n\t            type: Directory\n\t        - name: device-dir\n\t          hostPath:\n\t            path: /dev\n\t---\n\tkind: StatefulSet\n\tapiVersion: apps/v1beta1\n\tmetadata:\n\t  name: csi-do-controller\n\t  namespace: kube-system\n\tspec:\n\t  serviceName: \"csi-do\"\n\t  replicas: 1\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: csi-do-controller\n\t        role: csi-do\n\t    spec:\n\t      serviceAccount: csi-do-controller-sa\n\t      containers:\n\t        - name: csi-provisioner\n\t          image: quay.io/k8scsi/csi-provisioner:v0.3.0\n\t          ...\n\t        - name: csi-attacher\n\t          image: quay.io/k8scsi/csi-attacher:v0.3.0\n\t          ...\n\t        - name: csi-do-plugin\n\t          image: digitalocean/do-csi-plugin:v0.2.0\n\t          args :\n\t            - \"--endpoint=$(CSI_ENDPOINT)\"\n\t            - \"--token=$(DIGITALOCEAN_ACCESS_TOKEN)\"\n\t            - \"--url=$(DIGITALOCEAN_API_URL)\"\n\t          env:\n\t            - name: CSI_ENDPOINT\n\t              value: unix:///var/lib/csi/sockets/pluginproxy/csi.sock\n\t            - name: DIGITALOCEAN_API_URL\n\t              value: https://api.digitalocean.com/\n\t            - name: DIGITALOCEAN_ACCESS_TOKEN\n\t              valueFrom:\n\t                secretKeyRef:\n\t                  name: digitalocean\n\t                  key: access-token\n\t          imagePullPolicy: \"Always\"\n\t          volumeMounts:\n\t            - name: socket-dir\n\t              mountPath: /var/lib/csi/sockets/pluginproxy/\n\t      volumes:\n\t        - name: socket-dir\n\t          emptyDir: {}\n可以看到，我们编写的 CSI 插件只有一个二进制文件，它的镜像是 digitalocean/do-csi-plugin:v0.2.0。\n\n而我们**部署 CSI 插件的常用原则是：**\n\n**第一，通过 DaemonSet 在每个节点上都启动一个 CSI 插件，来为 kubelet 提供 CSI Node 服务。**这是因为，CSI Node 服务需要被 kubelet 直接调用，所以它要和 kubelet“一对一”地部署起来。\n\n此外，在上述 DaemonSet 的定义里面，除了 CSI 插件，我们还以 sidecar 的方式运行着 driver-registrar 这个外部组件。它的作用，是向 kubelet 注册这个 CSI 插件。这个注册过程使用的插件信息，则通过访问同一个 Pod 里的 CSI 插件容器的 Identity 服务获取到。\n\n需要注意的是，由于 CSI 插件运行在一个容器里，那么 CSI Node 服务在“Mount 阶段”执行的挂载操作，实际上是发生在这个容器的 Mount Namespace 里的。可是，我们真正希望执行挂载操作的对象，都是宿主机 /var/lib/kubelet 目录下的文件和目录。\n\n所以，在定义 DaemonSet Pod 的时候，我们需要把宿主机的 /var/lib/kubelet 以 Volume 的方式挂载进 CSI 插件容器的同名目录下，然后设置这个 Volume 的 mountPropagation=Bidirectional，即开启双向挂载传播，从而将容器在这个目录下进行的挂载操作“传播”给宿主机，反之亦然。\n\n**第二，通过 StatefulSet 在任意一个节点上再启动一个 CSI 插件，为 External Components 提供 CSI Controller 服务。**所以，作为 CSI Controller 服务的调用者，External Provisioner 和 External Attacher 这两个外部组件，就需要以 sidecar 的方式和这次部署的 CSI 插件定义在同一个 Pod 里。\n\n你可能会好奇，为什么我们会用 StatefulSet 而不是 Deployment 来运行这个 CSI 插件呢。\n\n这是因为，由于 StatefulSet 需要确保应用拓扑状态的稳定性，所以它对 Pod 的更新，是严格保证顺序的，即：只有在前一个 Pod 停止并删除之后，它才会创建并启动下一个 Pod。\n\n而像我们上面这样将 StatefulSet 的 replicas 设置为 1 的话，StatefulSet 就会确保 Pod 被删除重建的时候，永远有且只有一个 CSI 插件的 Pod 运行在集群中。这对 CSI 插件的正确性来说，至关重要。\n\n而在今天这篇文章一开始，我们就已经定义了这个 CSI 插件对应的 StorageClass（即：do-block-storage），所以你接下来只需要定义一个声明使用这个 StorageClass 的 PVC 即可，如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolumeClaim\n\tmetadata:\n\t  name: csi-pvc\n\tspec:\n\t  accessModes:\n\t  - ReadWriteOnce\n\t  resources:\n\t    requests:\n\t      storage: 5Gi\n\t  storageClassName: do-block-storage\n当你把上述 PVC 提交给 Kubernetes 之后，你就可以在 Pod 里声明使用这个 csi-pvc 来作为持久化存储了。这一部分使用 PV 和 PVC 的内容，我就不再赘述了。\n\n**总结**\n\n在今天这篇文章中，我以一个 DigitalOcean 的 CSI 插件为例，和你分享了编写 CSI 插件的具体流程。\n\n基于这些讲述，你现在应该已经对 Kubernetes 持久化存储体系有了一个更加全面和深入的认识。\n\n举个例子，对于一个部署了 CSI 存储插件的 Kubernetes 集群来说：\n\n当用户创建了一个 PVC 之后，你前面部署的 StatefulSet 里的 External Provisioner 容器，就会监听到这个 PVC 的诞生，然后调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 CreateVolume 方法，为你创建出对应的 PV。\n\n这时候，运行在 Kubernetes Master 节点上的 Volume Controller，就会通过 PersistentVolumeController 控制循环，发现这对新创建出来的 PV 和 PVC，并且看到它们声明的是同一个 StorageClass。所以，它会把这一对 PV 和 PVC 绑定起来，使 PVC 进入 Bound 状态。\n\n然后，用户创建了一个声明使用上述 PVC 的 Pod，并且这个 Pod 被调度器调度到了宿主机 A 上。这时候，Volume Controller 的 AttachDetachController 控制循环就会发现，上述 PVC 对应的 Volume，需要被 Attach 到宿主机 A 上。所以，AttachDetachController 会创建一个 VolumeAttachment 对象，这个对象携带了宿主机 A 和待处理的 Volume 的名字。\n\n这样，StatefulSet 里的 External Attacher 容器，就会监听到这个 VolumeAttachment 对象的诞生。于是，它就会使用这个对象里的宿主机和 Volume 名字，调用同一个 Pod 里的 CSI 插件的 CSI Controller 服务的 ControllerPublishVolume 方法，完成“Attach 阶段”。\n\n上述过程完成后，运行在宿主机 A 上的 kubelet，就会通过 VolumeManagerReconciler 控制循环，发现当前宿主机上有一个 Volume 对应的存储设备（比如磁盘）已经被 Attach 到了某个设备目录下。于是 kubelet 就会调用同一台宿主机上的 CSI 插件的 CSI Node 服务的 NodeStageVolume 和 NodePublishVolume 方法，完成这个 Volume 的“Mount 阶段”。\n\n至此，一个完整的持久化 Volume 的创建和挂载流程就结束了。\n\n**思考题**\n\n请你根据编写 FlexVolume 和 CSI 插件的流程，分析一下什么时候该使用 FlexVolume，什么时候应该使用 CSI？\n\n\n","categories":["reference","k8s"]},{"title":"32 | 浅谈容器网络","url":"/2021/03/13/reference/k8s/32.浅谈容器网络/","content":"\n![](1.jpg)\n\n## 浅谈容器网络\n\n在前面讲解容器基础时，我曾经提到过一个 Linux 容器能看见的“网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。\n\n而所谓“网络栈”，就包括了：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。\n\n需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net=host），即：不开启 Network Namespace，比如：\n\n\t$ docker run –d –net=host --name nginx-host nginx\n在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。\n\n像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。所以，**在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口。**\n\n这时候，一个显而易见的问题就是：**`这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？`**\n\n为了理解这个问题，你其实可以把每一个容器看做一台主机，它们都有一套独立的“网络栈”。\n\n如果你想要实现两台主机之间的通信，最直接的办法，就是把它们用一根网线连接起来；而如果你想要实现多台主机之间的通信，那就需要用网线，把它们连接在一台交换机上。\n\n在 Linux 中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址学习来将数据包转发到网桥的不同端口（Port）上。\n\n当然，至于为什么这些主机之间需要 MAC 地址才能进行通信，这就是网络分层模型的基础知识了。不熟悉这块内容的读者，可以通过[这篇文章](https://www.lifewire.com/layers-of-the-osi-model-illustrated-818017)来学习一下。\n\n而为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。\n\n可是，我们又该如何把这些容器“连接”到 docker0 网桥上呢？\n\n这时候，我们就需要使用一种名叫 **Veth Pair** 的虚拟设备了。\n\nVeth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。\n\n这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。\n\n比如，现在我们启动了一个叫作 nginx-1 的容器：\n\n\t$ docker run –d --name nginx-1 nginx\n然后进入到这个容器中查看一下它的网络设备：\n\n\t# 在宿主机上\n\t$ docker exec -it nginx-1 /bin/bash\n\t# 在容器里\n\troot@2b3c181aecf1:/# ifconfig\n\teth0: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500\n\t        inet 172.17.0.2  netmask 255.255.0.0  broadcast 0.0.0.0\n\t        inet6 fe80::42:acff:fe11:2  prefixlen 64  scopeid 0x20<link>\n\t        ether 02:42:ac:11:00:02  txqueuelen 0  (Ethernet)\n\t        RX packets 364  bytes 8137175 (7.7 MiB)\n\t        RX errors 0  dropped 0  overruns 0  frame 0\n\t        TX packets 281  bytes 21161 (20.6 KiB)\n\t        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\t        \n\tlo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536\n\t        inet 127.0.0.1  netmask 255.0.0.0\n\t        inet6 ::1  prefixlen 128  scopeid 0x10<host>\n\t        loop  txqueuelen 1000  (Local Loopback)\n\t        RX packets 0  bytes 0 (0.0 B)\n\t        RX errors 0  dropped 0  overruns 0  frame 0\n\t        TX packets 0  bytes 0 (0.0 B)\n\t        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\t        \n\t$ route\n\tKernel IP routing table\n\tDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n\tdefault         172.17.0.1      0.0.0.0         UG    0      0        0 eth0\n\t172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 eth0\n可以看到，这个容器里有一张叫作 eth0 的网卡，它正是一个 Veth Pair 设备在容器里的这一端。\n\n通过 route 命令查看 nginx-1 容器的路由表，我们可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则）。\n\n而这个 Veth Pair 设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它，如下所示：\n\n\t# 在宿主机上\n\t$ ifconfig\n\t...\n\tdocker0   Link encap:Ethernet  HWaddr 02:42:d8:e4:df:c1  \n\t          inet addr:172.17.0.1  Bcast:0.0.0.0  Mask:255.255.0.0\n\t          inet6 addr: fe80::42:d8ff:fee4:dfc1/64 Scope:Link\n\t          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n\t          RX packets:309 errors:0 dropped:0 overruns:0 frame:0\n\t          TX packets:372 errors:0 dropped:0 overruns:0 carrier:0\n\t collisions:0 txqueuelen:0 \n\t          RX bytes:18944 (18.9 KB)  TX bytes:8137789 (8.1 MB)\n\tveth9c02e56 Link encap:Ethernet  HWaddr 52:81:0b:24:3d:da  \n\t          inet6 addr: fe80::5081:bff:fe24:3dda/64 Scope:Link\n\t          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n\t          RX packets:288 errors:0 dropped:0 overruns:0 frame:0\n\t          TX packets:371 errors:0 dropped:0 overruns:0 carrier:0\n\t collisions:0 txqueuelen:0 \n\t          RX bytes:21608 (21.6 KB)  TX bytes:8137719 (8.1 MB)\n\t          \n\t$ brctl show\n\tbridge name bridge id  STP enabled interfaces\n\tdocker0  8000.0242d8e4dfc1 no  veth9c02e56\n通过 ifconfig 命令的输出，你可以看到，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth9c02e56。并且，通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上。\n\n这时候，如果我们再在这台宿主机上启动另一个 Docker 容器，比如 nginx-2：\n\n\t$ docker run –d --name nginx-2 nginx\n\t$ brctl show\n\tbridge name bridge id  STP enabled interfaces\n\tdocker0  8000.0242d8e4dfc1 no  veth9c02e56\n\t       vethb4963f3\n你就会发现一个新的、名叫 vethb4963f3 的虚拟网卡，也被“插”在了 docker0 网桥上。\n\n这时候，如果你在 nginx-1 容器里 ping 一下 nginx-2 容器的 IP 地址（172.17.0.3），就会发现同一宿主机上的两个容器默认就是相互连通的。\n\n这其中的原理其实非常简单，我来解释一下。\n\n当你在 nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。可以看到，这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机。\n\n而要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址。\n\n> 备注：ARP（Address Resolution Protocol），是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。\n\n我们前面提到过，这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。\n\n一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥。\n\n所以，在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。\n\n有了这个目的 MAC 地址，nginx-1 容器的 eth0 网卡就可以将数据包发出去。\n\n而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth9c02e56 虚拟网卡上。不过，此时这个 veth9c02e56 网卡的网络协议栈的资格已经被“剥夺”，所以这个数据包就直接流入到了 docker0 网桥里。\n\ndocker0 处理转发的过程，则继续扮演二层交换机的角色。此时，docker0 网桥根据数据包的目的 MAC 地址（也就是 nginx-2 容器的 MAC 地址），在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：vethb4963f3，然后把数据包发往这个端口。\n\n而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。\n\n所以，nginx-2 容器看到的情况是，它自己的 eth0 网卡上出现了流入的数据包。这样，nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。\n\n以上，就是同一个宿主机上的不同容器通过 docker0 网桥进行通信的流程了。我把这个流程总结成了一幅示意图，如下所示：\n\n![](2.png)\n\n需要注意的是，在实际的数据传递时，上述数据的传递过程在网络协议栈的不同层次，都有 Linux 内核 Netfilter 参与其中。所以，如果感兴趣的话，你可以通过打开 iptables 的 TRACE 功能查看到数据包的传输过程，具体方法如下所示：\n\n\t# 在宿主机上执行\n\t$ iptables -t raw -A OUTPUT -p icmp -j TRACE\n\t$ iptables -t raw -A PREROUTING -p icmp -j TRACE\n通过上述设置，你就可以在 /var/log/syslog 里看到数据包传输的日志了。这一部分内容，你可以在课后结合[iptables 的相关知识](https://en.wikipedia.org/wiki/Iptables)进行实践，从而验证我和你分享的数据包传递流程。\n\n熟悉了 docker0 网桥的工作方式，你就可以理解，在默认情况下，**被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。**\n\n与之类似地，当你在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里。这个过程的示意图，如下所示：\n\n![](3.png)\n\n同样地，当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3，它发出的请求数据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。\n\n所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。当然，这个过程的实现要求这两台宿主机本身是连通的。这个过程的示意图，如下所示：\n\n![](4.png)\n\n所以说，**当你遇到容器连不通“外网”的时候，你都应该先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常，往往就能够找到问题的答案了。**\n\n不过，在最后一个“Docker 容器连接其他宿主机”的例子里，你可能已经联想到了这样一个问题：如果在另外一台宿主机（比如：10.168.0.3）上，也有一个 Docker 容器。那么，我们的 nginx-1 容器又该如何访问它呢？\n\n这个问题，其实就是容器的“跨主通信”问题。\n\n在 Docker 的默认配置下，一台宿主机上的 docker0 网桥，和其他宿主机上的 docker0 网桥，没有任何关联，它们互相之间也没办法连通。所以，连接在这些网桥上的容器，自然也没办法进行通信了。\n\n不过，万变不离其宗。\n\n如果我们通过软件的方式，创建一个整个集群“公用”的网桥，然后把集群里的所有容器都连接到这个网桥上，不就可以相互通信了吗？\n\n说得没错。\n\n这样一来，我们整个集群里的容器网络就会类似于下图所示的样子：\n\n![](5.png)\n\n可以看到，构建这种容器网络的核心在于：我们需要在已有的宿主机网络上，再通过软件构建一个覆盖在已有宿主机网络之上的、可以把所有容器连通在一起的虚拟网络。所以，这种技术就被称为：Overlay Network（覆盖网络）。\n\n而这个 Overlay Network 本身，可以由每台宿主机上的一个“特殊网桥”共同组成。比如，当 Node 1 上的 Container 1 要访问 Node 2 上的 Container 3 的时候，Node 1 上的“特殊网桥”在收到数据包之后，能够通过某种方式，把数据包发送到正确的宿主机，比如 Node 2 上。而 Node 2 上的“特殊网桥”在收到数据包后，也能够通过某种方式，把数据包转发给正确的容器，比如 Container 3。\n\n甚至，每台宿主机上，都不需要有一个这种特殊的网桥，而仅仅通过某种方式配置宿主机的路由表，就能够把数据包转发到正确的宿主机上。这些内容，我在后面的文章中会为你一一讲述。\n\n**总结**\n\n在今天这篇文章中，我主要为你介绍了在本地环境下，单机容器网络的实现原理和 docker0 网桥的作用。\n\n这里的关键在于，容器要想跟外界进行通信，它发出的 IP 包就必须从它的 Network Namespace 里出来，来到宿主机上。\n\n而解决这个问题的方法就是：为容器创建一个一端在容器里充当默认网卡、另一端在宿主机上的 Veth Pair 设备。\n\n上述单机容器网络的知识，是后面我们讲解多机容器网络的重要基础，请务必认真消化理解。\n\n**思考题**\n\n尽管容器的 Host Network 模式有一些缺点，但是它性能好、配置简单，并且易于调试，所以很多团队会直接使用 Host Network。那么，如果要在生产环境中使用容器的 Host Network 模式，你觉得需要做哪些额外的准备工作呢？\n","categories":["reference","k8s"]},{"title":"33 | 深入解析容器跨主机网络","url":"/2021/03/13/reference/k8s/33.深入解析容器跨主机网络/","content":"\n![](1.jpg)\n\n## 深入解析容器跨主机网络\n\n在上一篇文章中，我为你详细讲解了在单机环境下，Linux 容器网络的实现原理（网桥模式）。并且提到了，在 Docker 的默认配置下，不同宿主机上的容器通过 IP 地址进行互相访问是根本做不到的。\n\n而正是为了解决这个容器“跨主通信”的问题，社区里才出现了那么多的容器网络方案。而且，相信你一直以来都有这样的疑问：这些网络方案的工作原理到底是什么？\n\n要理解容器“跨主通信”的原理，就一定要先从 Flannel 这个项目说起。\n\nFlannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别是：\n\n1. VXLAN；\n2. host-gw；\n3. UDP。\n\n这三种不同的后端实现，正代表了三种容器跨主网络的主流实现方法。其中，host-gw 模式，我会在下一篇文章中再做详细介绍。\n\n而 UDP 模式，是 Flannel 项目最早支持的一种方式，却也是性能最差的一种方式。所以，这个模式目前已经被弃用。不过，Flannel 之所以最先选择 UDP 模式，就是因为这种模式是最直接、也是最容易理解的容器跨主网络实现。\n\n所以，在今天这篇文章中，**`我会先从 UDP 模式开始，来为你讲解容器“跨主网络”的实现原理。`**\n\n在这个例子中，我有两台宿主机。\n\n * 宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。\n\n * 宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。\n\n我们现在的任务，就是让 container-1 访问 container-2。\n\n这种情况下，container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。\n\n这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示：\n\n\n\t# 在Node 1上\n\t$ ip route\n\tdefault via 10.168.0.1 dev eth0\n\t100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0\n\t100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1\n\t10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2\n可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。\n\n而这个 flannel0 设备的类型就比较有意思了：它是一个 TUN 设备（Tunnel 设备）。\n\n在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：**在操作系统内核和用户应用程序之间传递 IP 包。**\n\n以 flannel0 设备为例：像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。\n\n反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。\n\n所以，当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。\n\n等一下，**flanneld 又是如何知道这个 IP 地址对应的容器，是运行在 Node 2 上的呢？**\n\n这里，就用到了 Flannel 项目里一个非常重要的概念：子网（Subnet）。\n\n事实上，在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。\n\n而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示：\n\n\t$ etcdctl ls /coreos.com/network/subnets\n\t/coreos.com/network/subnets/100.96.1.0-24\n\t/coreos.com/network/subnets/100.96.2.0-24\n\t/coreos.com/network/subnets/100.96.3.0-24\n所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3，如下所示：\n\n\t$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24\n\t{\"PublicIP\":\"10.168.0.3\"}\n而对于 flanneld 来说，只要 Node 1 和 Node 2 是互通的，那么 flanneld 作为 Node 1 上的一个普通进程，就一定可以通过上述 IP 地址（10.168.0.3）访问到 Node 2，这没有任何问题。\n\n所以说，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。\n\n当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。\n\n通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node 2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。\n\n而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。\n\n根据我前面讲解的 TUN 设备的原理，这正是一个从用户态向内核态的流动方向（Flannel 进程向 TUN 设备发送数据包），所以 Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。\n\n而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示：\n\n\t# 在Node 2上\n\t$ ip route\n\tdefault via 10.168.0.1 dev eth0\n\t100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.2.0\n\t100.96.2.0/24 dev docker0  proto kernel  scope link  src 100.96.2.1\n\t10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.3\n由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。\n\n接下来的流程，就如同我在上一篇文章《浅谈容器网络》中和你分享的那样，docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。\n\n而 container-2 返回给 container-1 的数据包，则会经过与上述过程完全相反的路径回到 container-1 中。\n\n需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可：\n\n\t$ FLANNEL_SUBNET=100.96.1.1/24\n\t$ dockerd --bip=$FLANNEL_SUBNET ...\n以上，就是基于 Flannel UDP 模式的跨主通信的基本原理了。我把它总结成了一幅原理图，如下所示。\n\n![](2.jpg)\n图1 基于Flannel UDP模式的跨主通信的基本原理\n\n可以看到，Flannel UDP 模式提供的其实是一个三层的 Overlay 网络，即：它首先对发出端的 IP 包进行 UDP 封装，然后在接收端进行解封装拿到原始的 IP 包，进而把这个 IP 包转发给目标容器。这就好比，Flannel 在不同宿主机上的两个容器之间打通了一条“隧道”，使得这两个容器可以直接使用 IP 地址进行通信，而无需关心容器和宿主机的分布情况。\n\n我前面曾经提到，上述 UDP 模式有严重的性能问题，所以已经被废弃了。通过我上面的讲述，你有没有发现性能问题出现在了哪里呢？\n\n实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：\n\n![](3.png)\n图2 TUN设备示意图\n\n我们可以看到：\n\n第一次，用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；\n\n第二次，IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；\n\n第三次，flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。\n\n此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因。\n\n所以说，**我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行。**这也是为什么，Flannel 后来支持的**`VXLAN 模式，逐渐成为了主流的容器网络方案的原因。`**\n\nVXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。\n\nVXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。\n\n而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。\n\n而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。\n\n上述基于 VTEP 设备进行“隧道”通信的流程，我也为你总结成了一幅图，如下所示：\n\n![](4.jpg)\n图3 基于Flannel VXLAN模式的跨主通信的基本原理\n\n可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。\n\n现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。\n\n那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。也就是说，来到了“隧道”的入口。为了方便叙述，我接下来会把这个 IP 包称为“原始 IP 包”。\n\n为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。\n\n而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。\n\n比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则：\n\n\t$ route -n\n\tKernel IP routing table\n\tDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n\t...\n\t10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1\n这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。\n\n从图 3 的 Flannel VXLAN 模式的流程图中我们可以看到，10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。\n\n为了方便叙述，接下来我会把 Node 1 和 Node 2 上的 flannel.1 设备分别称为“源 VTEP 设备”和“目的 VTEP 设备”。\n\n而这些 VTEP 设备之间，就需要想办法组成一个虚拟的二层网络，即：通过二层数据帧进行通信。\n\n所以在我们的例子中，“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。\n\n这里需要解决的问题就是：**“目的 VTEP 设备”的 MAC 地址是什么？**\n\n此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。\n\n而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：\n\n\t# 在Node 1上\n\t$ ip neigh show dev flannel.1\n\t10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT\n这条记录的意思非常明确，即：IP 地址 10.1.16.0，对应的 MAC 地址是 5e:f8:4f:00:e3:37。\n\n可以看到，最新版本的 Flannel 并不依赖 L3 MISS 事件和 ARP 学习，而会在每台节点启动时把它的 VTEP 设备对应的 ARP 记录，直接下放到其他每台宿主机上。\n\n有了这个“目的 VTEP 设备”的 MAC 地址，**Linux 内核就可以开始二层封包工作了。**这个二层帧的格式，如下所示：\n\n![](5.jpg)\n图4 Flannel VXLAN模式的内部帧\n\n可以看到，Linux 内核会把“目的 VTEP 设备”的 MAC 地址，填写在图中的 Inner Ethernet Header 字段，得到一个二层数据帧。\n\n需要注意的是，上述封包过程只是加一个二层头，不会改变“原始 IP 包”的内容。所以图中的 Inner IP Header 字段，依然是 container-2 的 IP 地址，即 10.1.16.3。\n\n但是，上面提到的这些 VTEP 设备的 MAC 地址，对于宿主机网络来说并没有什么实际意义。所以上面封装出来的这个数据帧，并不能在我们的宿主机二层网络里传输。为了方便叙述，我们把它称为“内部数据帧”（Inner Ethernet Frame）。\n\n所以接下来，Linux 内核还需要再把“内部数据帧”进一步封装成为宿主机网络里的一个普通的数据帧，好让它“载着”“内部数据帧”，通过宿主机的 eth0 网卡进行传输。\n\n我们把这次要封装出来的、宿主机对应的数据帧称为“外部数据帧”（Outer Ethernet Frame）。\n\n为了实现这个“搭便车”的机制，Linux 内核会在“内部数据帧”前面，加上一个特殊的 VXLAN 头，用来表示这个“乘客”实际上是一个 VXLAN 要使用的数据帧。\n\n而这个 VXLAN 头里有一个重要的标志叫作 VNI，它是 VTEP 设备识别某个数据帧是不是应该归自己处理的重要标识。而在 Flannel 中，VNI 的默认值是 1，这也是为何，宿主机上的 VTEP 设备都叫作 flannel.1 的原因，这里的“1”，其实就是 VNI 的值。\n\n**然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。**\n\n所以，跟 UDP 模式类似，在宿主机看来，它会以为自己的 flannel.1 设备只是在向另外一台宿主机的 flannel.1 设备，发起了一次普通的 UDP 链接。它哪里会知道，这个 UDP 包里面，其实是一个完整的二层数据帧。这是不是跟特洛伊木马的故事非常像呢？\n\n不过，不要忘了，一个 flannel.1 设备只知道另一端的 flannel.1 设备的 MAC 地址，却不知道对应的宿主机地址是什么。\n\n也就是说，这个 UDP 包该发给哪台宿主机呢？\n\n在这种场景下，flannel.1 设备实际上要扮演一个“网桥”的角色，在二层网络进行 UDP 包的转发。而在 Linux 内核里面，“网桥”设备进行转发的依据，来自于一个叫作 FDB（Forwarding Database）的转发数据库。\n\n不难想到，这个 flannel.1“网桥”对应的 FDB 信息，也是 flanneld 进程负责维护的。它的内容可以通过 bridge fdb 命令查看到，如下所示：\n\n\t# 在Node 1上，使用“目的VTEP设备”的MAC地址进行查询\n\t$ bridge fdb show flannel.1 | grep 5e:f8:4f:00:e3:37\n\t5e:f8:4f:00:e3:37 dev flannel.1 dst 10.168.0.3 self permanent\n可以看到，在上面这条 FDB 记录里，指定了这样一条规则，即：\n\n发往我们前面提到的“目的 VTEP 设备”（MAC 地址是 5e:f8:4f:00:e3:37）的二层数据帧，应该通过 flannel.1 设备，发往 IP 地址为 10.168.0.3 的主机。显然，这台主机正是 Node 2，UDP 包要发往的目的地就找到了。\n\n所以**接下来的流程，就是一个正常的、宿主机网络上的封包工作。**\n\n我们知道，UDP 包是一个四层数据包，所以 Linux 内核会在它前面加上一个 IP 头，即原理图中的 Outer IP Header，组成一个 IP 包。并且，在这个 IP 头里，会填上前面通过 FDB 查询出来的目的主机的 IP 地址，即 Node 2 的 IP 地址 10.168.0.3。\n\n然后，Linux 内核再在这个 IP 包前面加上二层数据帧头，即原理图中的 Outer Ethernet Header，并把 Node 2 的 MAC 地址填进去。这个 MAC 地址本身，是 Node 1 的 ARP 表要学习的内容，无需 Flannel 维护。这时候，我们封装出来的“外部数据帧”的格式，如下所示：\n\n![](6.jpg)\n图5 Flannel VXLAN模式的外部帧\n\n这样，封包工作就宣告完成了。\n\n接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。\n\n这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。\n\n而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。\n\n以上，就是 Flannel VXLAN 模式的具体工作原理了。\n\n**总结**\n\n在本篇文章中，我为你详细讲解了 Flannel UDP 和 VXLAN 模式的工作原理。这两种模式其实都可以称作“隧道”机制，也是很多其他容器网络插件的基础。比如 Weave 的两种模式，以及 Docker 的 Overlay 模式。\n\n此外，从上面的讲解中我们可以看到，VXLAN 模式组建的覆盖网络，其实就是一个由不同宿主机上的 VTEP 设备，也就是 flannel.1 设备组成的虚拟二层网络。对于 VTEP 设备来说，它发出的“内部数据帧”就仿佛是一直在这个虚拟的二层网络上流动。这，也正是覆盖网络的含义。\n\n> 备注：如果你想要在我们前面部署的集群中实践 Flannel 的话，可以在 Master 节点上执行如下命令来替换网络插件。\n> 第一步，执行$ rm -rf /etc/cni/net.d/*；\n> 第二步，执行$ kubectl delete -f \"https://cloud.weave.works/k8s/net?k8s-version=1.11\"；\n> 第三步，在/etc/kubernetes/manifests/kube-controller-manager.yaml里，为容器启动命令添加如下两个参数：\n> --allocate-node-cidrs=true\n> --cluster-cidr=10.244.0.0/16\n> 第四步， 重启所有 kubelet；\n> 第五步， 执行$ kubectl create -f https://raw.githubusercontent.com/coreos/flannel/bc79dd1505b0c8681ece4de4c0d86c5cd2643275/Documentation/kube-flannel.yml。\n\n**思考题**\n\n可以看到，Flannel 通过上述的“隧道”机制，实现了容器之间三层网络（IP 地址）的连通性。但是，根据这个机制的工作原理，你认为 Flannel 能负责保证二层网络（MAC 地址）的连通性吗？为什么呢？","categories":["reference","k8s"]},{"title":"34 | Kubernetes网络模型与CNI网络插件","url":"/2021/03/13/reference/k8s/34.Kubernetes网络模型与CNI网络插件/","content":"\n![](1.jpg)\n\n## Kubernetes网络模型与CNI网络插件\n\n在上一篇文章中，我以 Flannel 项目为例，为你详细讲解了容器跨主机网络的两种实现方法：UDP 和 VXLAN。\n\n不难看到，这些例子有一个共性，那就是用户的容器都连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作。\n\n然后，网络插件真正要做的事情，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。\n\n实际上，上面这个流程，也正是 Kubernetes 对容器网络的主要处理方法。只不过，Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。\n\n以 Flannel 的 VXLAN 模式为例，在 Kubernetes 环境里，它的工作方式跟我们在上一篇文章中讲解的没有任何不同。只不过，docker0 网桥被替换成了 CNI 网桥而已，如下所示：\n\n![](2.jpg)\n\n在这里，Kubernetes 为 Flannel 分配的子网范围是 10.244.0.0/16。这个参数可以在部署的时候指定，比如：\n\n\t$ kubeadm init --pod-network-cidr=10.244.0.0/16\n也可以在部署完成后，通过修改 kube-controller-manager 的配置文件来指定。\n\n这时候，假设 Infra-container-1 要访问 Infra-container-2（也就是 Pod-1 要访问 Pod-2），这个 IP 包的源地址就是 10.244.0.2，目的 IP 地址是 10.244.1.3。而此时，Infra-container-1 里的 eth0 设备，同样是以 Veth Pair 的方式连接在 Node 1 的 cni0 网桥上。所以这个 IP 包就会经过 cni0 网桥出现在宿主机上。\n\n此时，Node 1 上的路由表，如下所示：\n\n\t# 在Node 1上\n\t$ route -n\n\tKernel IP routing table\n\tDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n\t...\n\t10.244.0.0      0.0.0.0         255.255.255.0   U     0      0        0 cni0\n\t10.244.1.0      10.244.1.0      255.255.255.0   UG    0      0        0 flannel.1\n\t172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n因为我们的 IP 包的目的 IP 地址是 10.244.1.3，所以它只能匹配到第二条规则，也就是 10.244.1.0 对应的这条路由规则。\n\n可以看到，这条规则指定了本机的 flannel.1 设备进行处理。并且，flannel.1 在处理完后，要将 IP 包转发到的网关（Gateway），正是“隧道”另一端的 VTEP 设备，也就是 Node 2 的 flannel.1 设备。所以，接下来的流程，就跟上一篇文章中介绍过的 Flannel VXLAN 模式完全一样了。\n\n需要注意的是，CNI 网桥只是接管所有 CNI 插件负责的、即 Kubernetes 创建的容器（Pod）。而此时，如果你用 docker run 单独启动一个容器，那么 Docker 项目还是会把这个容器连接到 docker0 网桥上。所以这个容器的 IP 地址，一定是属于 docker0 网桥的 172.17.0.0/16 网段。\n\nKubernetes 之所以要设置这样一个与 docker0 网桥功能几乎一样的 CNI 网桥，主要原因包括两个方面：\n\n一方面，Kubernetes 项目并没有使用 Docker 的网络模型（CNM），所以它并不希望、也不具备配置 docker0 网桥的能力；\n\n另一方面，这还与 Kubernetes 如何配置 Pod，也就是 Infra 容器的 Network Namespace 密切相关。\n\n我们知道，Kubernetes 创建一个 Pod 的第一步，就是创建并启动一个 Infra 容器，用来“hold”住这个 Pod 的 Network Namespace（这里，你可以再回顾一下专栏第 13 篇文章《为什么我们需要 Pod？》中的相关内容）。\n\n所以，CNI 的设计思想，就是：**Kubernetes 在启动 Infra 容器之后，就可以直接调用 CNI 网络插件，为这个 Infra 容器的 Network Namespace，配置符合预期的网络栈。**\n\n> 备注：在前面第 32 篇文章《浅谈容器网络》中，我讲解单机容器网络时，已经和你分享过，一个 Network Namespace 的网络栈包括：网卡（Network Interface）、回环设备（Loopback Device）、路由表（Routing Table）和 iptables 规则。\n\n那么，这个网络栈的配置工作又是如何完成的呢？\n\n为了回答这个问题，我们就需要从 **CNI 插件的部署和实现方式谈起了。**\n\n我们在部署 Kubernetes 的时候，有一个步骤是安装 kubernetes-cni 包，它的目的就是在宿主机上安装 CNI 插件所需的基础可执行文件。\n\n在安装完成后，你可以在宿主机的 /opt/cni/bin 目录下看到它们，如下所示：\n\n\t$ ls -al /opt/cni/bin/\n\ttotal 73088\n\t-rwxr-xr-x 1 root root  3890407 Aug 17  2017 bridge\n\t-rwxr-xr-x 1 root root  9921982 Aug 17  2017 dhcp\n\t-rwxr-xr-x 1 root root  2814104 Aug 17  2017 flannel\n\t-rwxr-xr-x 1 root root  2991965 Aug 17  2017 host-local\n\t-rwxr-xr-x 1 root root  3475802 Aug 17  2017 ipvlan\n\t-rwxr-xr-x 1 root root  3026388 Aug 17  2017 loopback\n\t-rwxr-xr-x 1 root root  3520724 Aug 17  2017 macvlan\n\t-rwxr-xr-x 1 root root  3470464 Aug 17  2017 portmap\n\t-rwxr-xr-x 1 root root  3877986 Aug 17  2017 ptp\n\t-rwxr-xr-x 1 root root  2605279 Aug 17  2017 sample\n\t-rwxr-xr-x 1 root root  2808402 Aug 17  2017 tuning\n\t-rwxr-xr-x 1 root root  3475750 Aug 17  2017 vlan\n这些 CNI 的基础可执行文件，按照功能可以分为三类：\n\n**第一类，叫作 Main 插件，它是用来创建具体网络设备的二进制文件。**比如，bridge（网桥设备）、ipvlan、loopback（lo 设备）、macvlan、ptp（Veth Pair 设备），以及 vlan。\n\n我在前面提到过的 Flannel、Weave 等项目，都属于“网桥”类型的 CNI 插件。所以在具体的实现中，它们往往会调用 bridge 这个二进制文件。这个流程，我马上就会详细介绍到。\n\n**第二类，叫作 IPAM（IP Address Management）插件，它是负责分配 IP 地址的二进制文件**。比如，dhcp，这个文件会向 DHCP 服务器发起请求；host-local，则会使用预先配置的 IP 地址段来进行分配。\n\n**第三类，是由 CNI 社区维护的内置 CNI 插件**。比如：flannel，就是专门为 Flannel 项目提供的 CNI 插件；tuning，是一个通过 sysctl 调整网络设备参数的二进制文件；portmap，是一个通过 iptables 配置端口映射的二进制文件；bandwidth，是一个使用 Token Bucket Filter (TBF) 来进行限流的二进制文件。\n\n从这些二进制文件中，我们可以看到，如果要实现一个给 Kubernetes 用的容器网络方案，其实需要做两部分工作，以 Flannel 项目为例：\n\n**首先，实现这个网络方案本身**。这一部分需要编写的，其实就是 flanneld 进程里的主要逻辑。比如，创建和配置 flannel.1 设备、配置宿主机路由、配置 ARP 和 FDB 表里的信息等等。\n\n**然后，实现该网络方案对应的 CNI 插件**。这一部分主要需要做的，就是配置 Infra 容器里面的网络栈，并把它连接在 CNI 网桥上。\n\n由于 Flannel 项目对应的 CNI 插件已经被内置了，所以它无需再单独安装。而对于 Weave、Calico 等其他项目来说，我们就必须在安装插件的时候，把对应的 CNI 插件的可执行文件放在 /opt/cni/bin/ 目录下。\n\n> 实际上，对于 Weave、Calico 这样的网络方案来说，它们的 DaemonSet 只需要挂载宿主机的 /opt/cni/bin/，就可以实现插件可执行文件的安装了。你可以想一下具体应该怎么做，就当作一个课后小问题留给你去实践了。\n\n接下来，你就需要在宿主机上安装 flanneld（网络方案本身）。而在这个过程中，flanneld 启动后会在每台宿主机上生成它对应的 **CNI 配置文件**（它其实是一个 ConfigMap），从而告诉 Kubernetes，这个集群要使用 Flannel 作为容器网络方案。\n\n这个 CNI 配置文件的内容如下所示：\n\n\t$ cat /etc/cni/net.d/10-flannel.conflist \n\t{\n\t  \"name\": \"cbr0\",\n\t  \"plugins\": [\n\t    {\n\t      \"type\": \"flannel\",\n\t      \"delegate\": {\n\t        \"hairpinMode\": true,\n\t        \"isDefaultGateway\": true\n\t      }\n\t    },\n\t    {\n\t      \"type\": \"portmap\",\n\t      \"capabilities\": {\n\t        \"portMappings\": true\n\t      }\n\t    }\n\t  ]\n\t}\n需要注意的是，在 Kubernetes 中，处理容器网络相关的逻辑并不会在 kubelet 主干代码里执行，而是会在具体的 CRI（Container Runtime Interface，容器运行时接口）实现里完成。对于 Docker 项目来说，它的 CRI 实现叫作 dockershim，你可以在 kubelet 的代码里找到它。\n\n所以，接下来 dockershim 会加载上述的 CNI 配置文件。\n\n需要注意，Kubernetes 目前不支持多个 CNI 插件混用。如果你在 CNI 配置目录（/etc/cni/net.d）里放置了多个 CNI 配置文件的话，dockershim 只会加载按字母顺序排序的第一个插件。\n\n但另一方面，CNI 允许你在一个 CNI 配置文件里，通过 plugins 字段，定义多个插件进行协作。\n\n比如，在我们上面这个例子里，Flannel 项目就指定了 flannel 和 portmap 这两个插件。\n\n**这时候，dockershim 会把这个 CNI 配置文件加载起来，并且把列表里的第一个插件、也就是 flannel 插件，设置为默认插件**。而在后面的执行过程中，flannel 和 portmap 插件会按照定义顺序被调用，从而依次完成“配置容器网络”和“配置端口映射”这两步操作。\n\n接下来，我就来为你讲解一下这样一个 CNI 插件的工作原理。\n\n当 kubelet 组件需要创建 Pod 的时候，它第一个创建的一定是 Infra 容器。所以在这一步，dockershim 就会先调用 Docker API 创建并启动 Infra 容器，紧接着执行一个叫作 SetUpPod 的方法。这个方法的作用就是：为 CNI 插件准备参数，然后调用 CNI 插件为 Infra 容器配置网络。\n\n这里要调用的 CNI 插件，就是 /opt/cni/bin/flannel；而调用它所需要的参数，分为两部分。\n\n**第一部分，是由 dockershim 设置的一组 CNI 环境变量。**\n\n其中，最重要的环境变量参数叫作：CNI_COMMAND。它的取值只有两种：ADD 和 DEL。\n\n**这个 ADD 和 DEL 操作，就是 CNI 插件唯一需要实现的两个方法。**\n\n其中 ADD 操作的含义是：把容器添加到 CNI 网络里；DEL 操作的含义则是：把容器从 CNI 网络里移除掉。\n\n而对于网桥类型的 CNI 插件来说，这两个操作意味着把容器以 Veth Pair 的方式“插”到 CNI 网桥上，或者从网桥上“拔”掉。\n\n接下来，我以 ADD 操作为重点进行讲解。\n\nCNI 的 ADD 操作需要的参数包括：容器里网卡的名字 eth0（CNI_IFNAME）、Pod 的 Network Namespace 文件的路径（CNI_NETNS）、容器的 ID（CNI_CONTAINERID）等。这些参数都属于上述环境变量里的内容。其中，Pod（Infra 容器）的 Network Namespace 文件的路径，我在前面讲解容器基础的时候提到过，即：/proc/< 容器进程的 PID>/ns/net。\n\n> 备注：这里你也可以再回顾下专栏第 8 篇文章《白话容器基础（四）：重新认识 Docker 容器》中的相关内容。\n\n除此之外，在 CNI 环境变量里，还有一个叫作 CNI_ARGS 的参数。通过这个参数，CRI 实现（比如 dockershim）就可以以 Key-Value 的格式，传递自定义信息给网络插件。这是用户将来自定义 CNI 协议的一个重要方法。\n\n**第二部分，则是 dockershim 从 CNI 配置文件里加载到的、默认插件的配置信息。**\n\n这个配置信息在 CNI 中被叫作 Network Configuration，它的完整定义你可以参考[这个文档](https://github.com/containernetworking/cni/blob/master/SPEC.md#network-configuration)。dockershim 会把 Network Configuration 以 JSON 数据的格式，通过标准输入（stdin）的方式传递给 Flannel CNI 插件。\n\n而有了这两部分参数，Flannel CNI 插件实现 ADD 操作的过程就非常简单了。\n\n不过，需要注意的是，Flannel 的 CNI 配置文件（ /etc/cni/net.d/10-flannel.conflist）里有这么一个字段，叫作 delegate：\n\n\t...\n\t     \"delegate\": {\n\t        \"hairpinMode\": true,\n\t        \"isDefaultGateway\": true\n\t      }\n\tDelegate 字段的意思是，这个 CNI 插件并不会自己做事儿，而是会调用 Delegate 指定的某种 CNI 内置插件来完成。对于 Flannel 来说，它调用的 Delegate 插件，就是前面介绍到的 CNI bridge 插件。\n所以说，dockershim 对 Flannel CNI 插件的调用，其实就是走了个过场。Flannel CNI 插件唯一需要做的，就是对 dockershim 传来的 Network Configuration 进行补充。比如，将 Delegate 的 Type 字段设置为 bridge，将 Delegate 的 IPAM 字段设置为 host-local 等。\n\n经过 Flannel CNI 插件补充后的、完整的 Delegate 字段如下所示：\n\n\t{\n\t    \"hairpinMode\":true,\n\t    \"ipMasq\":false,\n\t    \"ipam\":{\n\t        \"routes\":[\n\t            {\n\t                \"dst\":\"10.244.0.0/16\"\n\t            }\n\t        ],\n\t        \"subnet\":\"10.244.1.0/24\",\n\t        \"type\":\"host-local\"\n\t    },\n\t    \"isDefaultGateway\":true,\n\t    \"isGateway\":true,\n\t    \"mtu\":1410,\n\t    \"name\":\"cbr0\",\n\t    \"type\":\"bridge\"\n\t}\n其中，ipam 字段里的信息，比如 10.244.1.0/24，读取自 Flannel 在宿主机上生成的 Flannel 配置文件，即：宿主机上的 /run/flannel/subnet.env 文件。\n\n接下来，Flannel CNI 插件就会调用 CNI bridge 插件，也就是执行：/opt/cni/bin/bridge 二进制文件。\n\n这一次，调用 CNI bridge 插件需要的两部分参数的第一部分、也就是 CNI 环境变量，并没有变化。所以，它里面的 CNI_COMMAND 参数的值还是“ADD”。\n\n而第二部分 Network Configration，正是上面补充好的 Delegate 字段。Flannel CNI 插件会把 Delegate 字段的内容以标准输入（stdin）的方式传递给 CNI bridge 插件。\n\n> 此外，Flannel CNI 插件还会把 Delegate 字段以 JSON 文件的方式，保存在 /var/lib/cni/flannel 目录下。这是为了给后面删除容器调用 DEL 操作时使用的。\n\n有了这两部分参数，接下来 CNI bridge 插件就可以“代表”Flannel，进行“将容器加入到 CNI 网络里”这一步操作了。而这一部分内容，与容器 Network Namespace 密切相关，所以我要为你详细讲解一下。\n\n首先，CNI bridge 插件会在宿主机上检查 CNI 网桥是否存在。如果没有的话，那就创建它。这相当于在宿主机上执行：\n\n\t# 在宿主机上\n\t$ ip link add cni0 type bridge\n\t$ ip link set cni0 up\n接下来，CNI bridge 插件会通过 Infra 容器的 Network Namespace 文件，进入到这个 Network Namespace 里面，然后创建一对 Veth Pair 设备。\n\n紧接着，它会把这个 Veth Pair 的其中一端，“移动”到宿主机上。这相当于在容器里执行如下所示的命令：\n\n\t#在容器里\n\t# 创建一对Veth Pair设备。其中一个叫作eth0，另一个叫作vethb4963f3\n\t$ ip link add eth0 type veth peer name vethb4963f3\n\t# 启动eth0设备\n\t$ ip link set eth0 up \n\t# 将Veth Pair设备的另一端（也就是vethb4963f3设备）放到宿主机（也就是Host Namespace）里\n\t$ ip link set vethb4963f3 netns $HOST_NS\n\t# 通过Host Namespace，启动宿主机上的vethb4963f3设备\n\t$ ip netns exec $HOST_NS ip link set vethb4963f3 up \n这样，vethb4963f3 就出现在了宿主机上，而且这个 Veth Pair 设备的另一端，就是容器里面的 eth0。\n\n当然，你可能已经想到，上述创建 Veth Pair 设备的操作，其实也可以先在宿主机上执行，然后再把该设备的一端放到容器的 Network Namespace 里，这个原理是一样的。\n\n不过，CNI 插件之所以要“反着”来，是因为 CNI 里对 Namespace 操作函数的设计就是如此，如下所示：\n\n\terr := containerNS.Do(func(hostNS ns.NetNS) error {\n\t  ...\n\t  return nil\n\t})\n这个设计其实很容易理解。在编程时，容器的 Namespace 是可以直接通过 Namespace 文件拿到的；而 Host Namespace，则是一个隐含在上下文的参数。所以，像上面这样，先通过容器 Namespace 进入容器里面，然后再反向操作 Host Namespace，对于编程来说要更加方便。\n\n接下来，CNI bridge 插件就可以把 vethb4963f3 设备连接在 CNI 网桥上。这相当于在宿主机上执行：\n\n\t# 在宿主机上\n\t$ ip link set vethb4963f3 master cni0\n在将 vethb4963f3 设备连接在 CNI 网桥之后，CNI bridge 插件还会为它设置 **Hairpin Mode（发夹模式）**。这是因为，在默认情况下，网桥设备是不允许一个数据包从一个端口进来后，再从这个端口发出去的。但是，它允许你为这个端口开启 Hairpin Mode，从而取消这个限制。\n\n这个特性，主要用在容器需要通过[NAT（即：端口映射）](https://en.wikipedia.org/wiki/Network_address_translation)的方式，“自己访问自己”的场景下。\n\n举个例子，比如我们执行 docker run -p 8080:80，就是在宿主机上通过 iptables 设置了一条[DNAT（目的地址转换）](http://linux-ip.net/html/nat-dnat.html)转发规则。这条规则的作用是，当宿主机上的进程访问“< 宿主机的 IP 地址 >:8080”时，iptables 会把该请求直接转发到“< 容器的 IP 地址 >:80”上。也就是说，这个请求最终会经过 docker0 网桥进入容器里面。\n\n但如果你是在容器里面访问宿主机的 8080 端口，那么这个容器里发出的 IP 包会经过 vethb4963f3 设备（端口）和 docker0 网桥，来到宿主机上。此时，根据上述 DNAT 规则，这个 IP 包又需要回到 docker0 网桥，并且还是通过 vethb4963f3 端口进入到容器里。所以，这种情况下，我们就需要开启 vethb4963f3 端口的 Hairpin Mode 了。\n\n所以说，Flannel 插件要在 CNI 配置文件里声明 hairpinMode=true。这样，将来这个集群里的 Pod 才可以通过它自己的 Service 访问到自己。\n\n接下来，CNI bridge 插件会调用 CNI ipam 插件，从 ipam.subnet 字段规定的网段里为容器分配一个可用的 IP 地址。然后，CNI bridge 插件就会把这个 IP 地址添加在容器的 eth0 网卡上，同时为容器设置默认路由。这相当于在容器里执行：\n\n\t# 在容器里\n\t$ ip addr add 10.244.0.2/24 dev eth0\n\t$ ip route add default via 10.244.0.1 dev eth0\n\n最后，CNI bridge 插件会为 CNI 网桥添加 IP 地址。这相当于在宿主机上执行：\n\n\t# 在宿主机上\n\t$ ip addr add 10.244.0.1/24 dev cni0\n在执行完上述操作之后，CNI 插件会把容器的 IP 地址等信息返回给 dockershim，然后被 kubelet 添加到 Pod 的 Status 字段。\n\n至此，CNI 插件的 ADD 方法就宣告结束了。接下来的流程，就跟我们上一篇文章中容器跨主机通信的过程完全一致了。\n\n需要注意的是，对于非网桥类型的 CNI 插件，上述“将容器添加到 CNI 网络”的操作流程，以及网络方案本身的工作原理，就都不太一样了。我将会在后续文章中，继续为你分析这部分内容。\n\n**总结**\n\n在本篇文章中，我为你详细讲解了 Kubernetes 中 CNI 网络的实现原理。根据这个原理，你其实就很容易理解所谓的“Kubernetes 网络模型”了：\n\n所有容器都可以直接使用 IP 地址与其他容器通信，而无需使用 NAT。\n\n所有宿主机都可以直接使用 IP 地址与所有容器通信，而无需使用 NAT。反之亦然。\n\n容器自己“看到”的自己的 IP 地址，和别人（宿主机或者容器）看到的地址是完全一样的。\n\n可以看到，这个网络模型，其实可以用一个字总结，那就是“通”。\n\n容器与容器之间要“通”，容器与宿主机之间也要“通”。并且，Kubernetes 要求这个“通”，还必须是直接基于容器和宿主机的 IP 地址来进行的。\n\n当然，考虑到不同用户之间的隔离性，在很多场合下，我们还要求容器之间的网络“不通”。这个问题，我会在后面的文章中会为你解决。\n\n**思考题**\n\n请你思考一下，为什么 Kubernetes 项目不自己实现容器网络，而是要通过 CNI 做一个如此简单的假设呢？\n","categories":["reference","k8s"]},{"title":"26 | 基于角色的权限控制：RBAC","url":"/2021/03/13/reference/k8s/26.基于角色的权限控制：RBAC/","content":"\n![](01.jpg)\n\n## 基于角色的权限控制之 RBAC。\n\n在前面的文章中，我已经为你讲解了很多种 Kubernetes 内置的编排对象，以及对应的控制器模式的实现原理。此外，我还剖析了自定义 API 资源类型和控制器的编写方式。\n\n这时候，你可能已经冒出了这样一个想法：控制器模式看起来好像也不难嘛，我能不能自己写一个编排对象呢？\n\n答案当然是可以的。而且，这才是 Kubernetes 项目最具吸引力的地方。\n\n毕竟，在互联网级别的大规模集群里，Kubernetes 内置的编排对象，很难做到完全满足所有需求。所以，很多实际的容器化工作，都会要求你设计一个自己的编排对象，实现自己的控制器模式。\n\n而在 Kubernetes 项目里，我们可以基于插件机制来完成这些工作，而完全不需要修改任何一行代码。\n\n不过，你要通过一个外部插件，在 Kubernetes 里新增和操作 API 对象，那么就必须先了解一个非常重要的知识：RBAC。\n\n我们知道，Kubernetes 中所有的 API 对象，都保存在 Etcd 里。可是，对这些 API 对象的操作，却一定都是通过访问 kube-apiserver 实现的。其中一个非常重要的原因，就是你需要 APIServer 来帮助你做授权工作。\n\n而在 **Kubernetes 项目中，负责完成授权（Authorization）工作的机制，就是 RBAC：基于角色的访问控制（Role-Based Access Control）。**\n\n如果你直接查看 Kubernetes 项目中关于 RBAC 的文档的话，可能会感觉非常复杂。但实际上，等到你用到这些 RBAC 的细节时，再去查阅也不迟。\n\n而在这里，我只希望你能明确三个最基本的概念。\n\n1. Role：角色，它其实是一组规则，定义了一组对 Kubernetes API 对象的操作权限。\n2. Subject：被作用者，既可以是“人”，也可以是“机器”，也可以是你在 Kubernetes 里定义的“用户”。\n3. RoleBinding：定义了“被作用者”和“角色”的绑定关系。\n\n\n而这三个概念，其实就是整个 RBAC 体系的核心所在。\n\n**`我先来讲解一下 Role。`**\n\n实际上，Role 本身就是一个 Kubernetes 的 API 对象，定义如下所示：\n\n\tkind: Role\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  namespace: mynamespace\n\t  name: example-role\n\trules:\n\t- apiGroups: [\"\"]\n\t  resources: [\"pods\"]\n\t  verbs: [\"get\", \"watch\", \"list\"]\n首先，这个 Role 对象指定了它能产生作用的 Namepace 是：mynamespace。\n\nNamespace 是 Kubernetes 项目里的一个逻辑管理单位。不同 Namespace 的 API 对象，在通过 kubectl 命令进行操作的时候，是互相隔离开的。\n\n比如，kubectl get pods -n mynamespace。\n\n当然，这仅限于逻辑上的“隔离”，Namespace 并不会提供任何实际的隔离或者多租户能力。而在前面文章中用到的大多数例子里，我都没有指定 Namespace，那就是使用的是默认 Namespace：default。\n\n然后，这个 Role 对象的 rules 字段，就是它所定义的权限规则。在上面的例子里，这条规则的含义就是：允许“被作用者”，对 mynamespace 下面的 Pod 对象，进行 GET、WATCH 和 LIST 操作。\n\n那么，**`这个具体的“被作用者”又是如何指定的呢？这就需要通过 RoleBinding 来实现了。`**\n\n当然，RoleBinding 本身也是一个 Kubernetes 的 API 对象。它的定义如下所示：\n\n\tkind: RoleBinding\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  name: example-rolebinding\n\t  namespace: mynamespace\n\tsubjects:\n\t- kind: User\n\t  name: example-user\n\t  apiGroup: rbac.authorization.k8s.io\n\troleRef:\n\t  kind: Role\n\t  name: example-role\n\t  apiGroup: rbac.authorization.k8s.io\n可以看到，这个 RoleBinding 对象里定义了一个 subjects 字段，即“被作用者”。它的类型是 User，即 Kubernetes 里的用户。这个用户的名字是 example-user。\n\n可是，在 Kubernetes 中，其实并没有一个叫作“User”的 API 对象。而且，我们在前面和部署使用 Kubernetes 的流程里，既不需要 User，也没有创建过 User。\n\n**这个 User 到底是从哪里来的呢？**\n\n实际上，Kubernetes 里的“User”，也就是“用户”，只是一个授权系统里的逻辑概念。它需要通过外部认证服务，比如 Keystone，来提供。或者，你也可以直接给 APIServer 指定一个用户名、密码文件。那么 Kubernetes 的授权系统，就能够从这个文件里找到对应的“用户”了。当然，在大多数私有的使用环境中，我们只要使用 Kubernetes 提供的内置“用户”，就足够了。这部分知识，我后面马上会讲到。\n\n接下来，我们会看到一个 roleRef 字段。正是通过这个字段，**`RoleBinding 对象就可以直接通过名字，来引用我们前面定义的 Role 对象（example-role），从而定义了“被作用者（Subject）”和“角色（Role）”之间的绑定关系。`**\n\n需要再次提醒的是，Role 和 RoleBinding 对象都是 Namespaced 对象（Namespaced Object），它们对权限的限制规则仅在它们自己的 Namespace 内有效，roleRef 也只能引用当前 Namespace 里的 Role 对象。\n\n那么，对于非 Namespaced（Non-namespaced）对象（比如：Node），或者，某一个 Role 想要作用于所有的 Namespace 的时候，我们又该如何去做授权呢？\n\n这时候，我们就必须要使用 ClusterRole 和 ClusterRoleBinding 这两个组合了。这两个 API 对象的用法跟 Role 和 RoleBinding 完全一样。只不过，它们的定义里，没有了 Namespace 字段，如下所示：\n\n\tkind: ClusterRole\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  name: example-clusterrole\n\trules:\n\t- apiGroups: [\"\"]\n\t  resources: [\"pods\"]\n\t  verbs: [\"get\", \"watch\", \"list\"]\n\n\n\tkind: ClusterRoleBinding\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  name: example-clusterrolebinding\n\tsubjects:\n\t- kind: User\n\t  name: example-user\n\t  apiGroup: rbac.authorization.k8s.io\n\troleRef:\n\t  kind: ClusterRole\n\t  name: example-clusterrole\n\t  apiGroup: rbac.authorization.k8s.io\n上面的例子里的 ClusterRole 和 ClusterRoleBinding 的组合，意味着名叫 example-user 的用户，拥有对所有 Namespace 里的 Pod 进行 GET、WATCH 和 LIST 操作的权限。\n\n更进一步地，在 Role 或者 ClusterRole 里面，如果要赋予用户 example-user 所有权限，那你就可以给它指定一个 verbs 字段的全集，如下所示：\n\n\tverbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\"]\n这些就是当前 Kubernetes（v1.11）里能够对 API 对象进行的所有操作了。\n\n类似地，Role 对象的 rules 字段也可以进一步细化。比如，你可以只针对某一个具体的对象进行权限设置，如下所示：\n\n\trules:\n\t- apiGroups: [\"\"]\n\t  resources: [\"configmaps\"]\n\t  resourceNames: [\"my-config\"]\n\t  verbs: [\"get\"]\n这个例子就表示，这条规则的“被作用者”，只对名叫“my-config”的 ConfigMap 对象，有进行 GET 操作的权限。\n\n而正如我前面介绍过的，在大多数时候，我们其实都不太使用“用户”这个功能，而是直接使用 Kubernetes 里的“内置用户”。\n\n**`这个由 Kubernetes 负责管理的“内置用户”，正是我们前面曾经提到过的：ServiceAccount。`**\n\n接下来，我通过一个具体的实例来为你讲解一下为 ServiceAccount 分配权限的过程。\n\n**首先，我们要定义一个 ServiceAccount。**它的 API 对象非常简单，如下所示：\n\n\tapiVersion: v1\n\tkind: ServiceAccount\n\tmetadata:\n\t  namespace: mynamespace\n\t  name: example-sa\n可以看到，一个最简单的 ServiceAccount 对象只需要 Name 和 Namespace 这两个最基本的字段。\n\n**然后，我们通过编写 RoleBinding 的 YAML 文件，来为这个 ServiceAccount 分配权限：**\n\n\tkind: RoleBinding\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  name: example-rolebinding\n\t  namespace: mynamespace\n\tsubjects:\n\t- kind: ServiceAccount\n\t  name: example-sa\n\t  namespace: mynamespace\n\troleRef:\n\t  kind: Role\n\t  name: example-role\n\t  apiGroup: rbac.authorization.k8s.io\n可以看到，在这个 RoleBinding 对象里，subjects 字段的类型（kind），不再是一个 User，而是一个名叫 example-sa 的 ServiceAccount。而 roleRef 引用的 Role 对象，依然名叫 example-role，也就是我在这篇文章一开始定义的 Role 对象。\n\n**接着，我们用 kubectl 命令创建这三个对象：**\n\n\t$ kubectl create -f svc-account.yaml\n\t$ kubectl create -f role-binding.yaml\n\t$ kubectl create -f role.yaml\n然后，我们来查看一下这个 ServiceAccount 的详细信息：\n\n\t$ kubectl get sa -n mynamespace -o yaml\n\t- apiVersion: v1\n\t  kind: ServiceAccount\n\t  metadata:\n\t    creationTimestamp: 2018-09-08T12:59:17Z\n\t    name: example-sa\n\t    namespace: mynamespace\n\t    resourceVersion: \"409327\"\n\t    ...\n\t  secrets:\n\t  - name: example-sa-token-vmfg6\n可以看到，**`Kubernetes 会为一个 ServiceAccount 自动创建并分配一个 Secret 对象，即：上述 ServiceAcount 定义里最下面的 secrets 字段。`**\n\n**`这个 Secret，就是这个 ServiceAccount 对应的、用来跟 APIServer 进行交互的授权文件，我们一般称它为：Token。Token 文件的内容一般是证书或者密码，它以一个 Secret 对象的方式保存在 Etcd 当中。`**\n\n这时候，用户的 Pod，就可以声明使用这个 ServiceAccount 了，比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  namespace: mynamespace\n\t  name: sa-token-test\n\tspec:\n\t  containers:\n\t  - name: nginx\n\t    image: nginx:1.7.9\n\t  serviceAccountName: example-sa\n在这个例子里，我定义了 Pod 要使用的要使用的 ServiceAccount 的名字是：example-sa。\n\n**`等这个 Pod 运行起来之后，我们就可以看到，该 ServiceAccount 的 token，也就是一个 Secret 对象，被 Kubernetes 自动挂载到了容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录下，`**如下所示：\n\n\t$ kubectl describe pod sa-token-test -n mynamespace\n\tName:               sa-token-test\n\tNamespace:          mynamespace\n\t...\n\tContainers:\n\t  nginx:\n\t    ...\n\t    Mounts:\n\t      /var/run/secrets/kubernetes.io/serviceaccount from example-sa-token-vmfg6 (ro)\n这时候，我们可以通过 kubectl exec 查看到这个目录里的文件：\n\n\t$ kubectl exec -it sa-token-test -n mynamespace -- /bin/bash\n\troot@sa-token-test:/# ls /var/run/secrets/kubernetes.io/serviceaccount\n\tca.crt namespace  token\n如上所示，容器里的应用，就可以使用这个 ca.crt 来访问 APIServer 了。更重要的是，此时它只能够做 GET、WATCH 和 LIST 操作。因为 example-sa 这个 ServiceAccount 的权限，已经被我们绑定了 Role 做了限制。\n此外，我在第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中曾经提到过，如果一个 Pod 没有声明 serviceAccountName，Kubernetes 会自动在它的 Namespace 下创建一个名叫 default 的默认 ServiceAccount，然后分配给这个 Pod。\n但在这种情况下，这个默认 ServiceAccount 并没有关联任何 Role。也就是说，此时它有访问 APIServer 的绝大多数权限。当然，这个访问所需要的 Token，还是默认 ServiceAccount 对应的 Secret 对象为它提供的，如下所示。\n\n\t$kubectl describe sa default\n\tName:                default\n\tNamespace:           default\n\tLabels:              <none>\n\tAnnotations:         <none>\n\tImage pull secrets:  <none>\n\tMountable secrets:   default-token-s8rbq\n\tTokens:              default-token-s8rbq\n\tEvents:              <none>\n\n\n\t$ kubectl get secret\n\tNAME                  TYPE                                  DATA      AGE\n\tkubernetes.io/service-account-token   3         82d\n\n\n\t$ kubectl describe secret default-token-s8rbq\n\tName:         default-token-s8rbq\n\tNamespace:    default\n\tLabels:       <none>\n\tAnnotations:  kubernetes.io/service-account.name=default\n\t\t\t\tkubernetes.io/service-account.uid=ffcb12b2-917f-11e8-abde-42010aa80002\n\tType:  kubernetes.io/service-account-token\n\tData\n\t====\n\tca.crt:     1025 bytes\n\tnamespace:  7 bytes\n\ttoken:      <TOKEN数据>\n可以看到，Kubernetes 会自动为默认 ServiceAccount 创建并绑定一个特殊的 **`Secret：`** 它的类型是kubernetes.io/service-account-token；**`它的 Annotation 字段，声明了kubernetes.io/service-account.name=default，即这个 Secret 会跟同一 Namespace 下名叫 default 的 ServiceAccount 进行绑定。`**\n\n**`所以，在生产环境中，我强烈建议你为所有 Namespace 下的默认 ServiceAccount，绑定一个只读权限的 Role。`**这个具体怎么做，就当作思考题留给你了。\n\n除了前面使用的“用户”（User），**`Kubernetes 还拥有“用户组”（Group）的概念，也就是一组“用户”的意思。`**如果你为 Kubernetes 配置了外部认证服务的话，这个“用户组”的概念就会由外部认证服务提供。\n\n而对于 Kubernetes 的内置“用户”ServiceAccount 来说，上述“用户组”的概念也同样适用。\n\n实际上，一个 ServiceAccount，在 Kubernetes 里对应的“用户”的名字是：\n\n\tsystem:serviceaccount:<Namespace名字>:<ServiceAccount名字>\n而它对应的内置“用户组”的名字，就是：\n\n\tsystem:serviceaccounts:<Namespace名字>\n这两个对应关系，请你一定要牢记。\n\n比如，现在我们可以在 RoleBinding 里定义如下的 subjects：\n\n\tsubjects:\n\t- kind: Group\n\t  name: system:serviceaccounts:mynamespace\n\t  apiGroup: rbac.authorization.k8s.io\n这就意味着这个 Role 的权限规则，作用于 mynamespace 里的所有 ServiceAccount。这就用到了“用户组”的概念。\n\n而下面这个例子：\n\n\tsubjects:\n\t- kind: Group\n\t  name: system:serviceaccounts\n\t  apiGroup: rbac.authorization.k8s.io\n就意味着这个 Role 的权限规则，作用于整个系统里的所有 ServiceAccount。\n\n最后，值得一提的是，在 **`Kubernetes 中已经内置了很多个为系统保留的 ClusterRole，它们的名字都以 system: 开头。你可以通过 kubectl get clusterroles 查看到它们。`**\n\n一般来说，这些系统 ClusterRole，是绑定给 Kubernetes 系统组件对应的 ServiceAccount 使用的。\n\n比如，其中一个名叫 system:kube-scheduler 的 ClusterRole，定义的权限规则是 kube-scheduler（Kubernetes 的调度器组件）运行所需要的必要权限。你可以通过如下指令查看这些权限的列表：\n\n\t$ kubectl describe clusterrole system:kube-scheduler\n\tName:         system:kube-scheduler\n\t...\n\tPolicyRule:\n\t  Resources                    Non-Resource URLs Resource Names    Verbs\n\t  ---------                    -----------------  --------------    -----\n\t...\n\t  services                     []                 []                [get list watch]\n\t  replicasets.apps             []                 []                [get list watch]\n\t  statefulsets.apps            []                 []                [get list watch]\n\t  replicasets.extensions       []                 []                [get list watch]\n\t  poddisruptionbudgets.policy  []                 []                [get list watch]\n\t  pods/status                  []                 []                [patch update]\n这个 system:kube-scheduler 的 ClusterRole，就会被绑定给 kube-system Namesapce 下名叫 kube-scheduler 的 ServiceAccount，它正是 Kubernetes 调度器的 Pod 声明使用的 ServiceAccount。\n\n除此之外，Kubernetes 还提供了四个预先定义好的 ClusterRole 来供用户直接使用：\n\n1. cluster-admin；\n2. admin；\n3. edit；\n4. view。\n\n通过它们的名字，你应该能大致猜出它们都定义了哪些权限。比如，这个名叫 view 的 ClusterRole，就规定了被作用者只有 Kubernetes API 的只读权限。\n\n而我还要提醒你的是，上面这个 cluster-admin 角色，对应的是整个 Kubernetes 项目中的最高权限（verbs=*），如下所示：\n\n\t$ kubectl describe clusterrole cluster-admin -n kube-system\n\tName:         cluster-admin\n\tLabels:       kubernetes.io/bootstrapping=rbac-defaults\n\tAnnotations:  rbac.authorization.kubernetes.io/autoupdate=true\n\tPolicyRule:\n\t  Resources  Non-Resource URLs Resource Names  Verbs\n\t  ---------  -----------------  --------------  -----\n\t  *.*        []                 []              [*]\n\t             [*]                []              [*]\n所以，请你务必要谨慎而小心地使用 cluster-admin。\n\n**总结**\n\n在今天这篇文章中，我主要为你讲解了基于角色的访问控制（RBAC）。\n\n其实，你现在已经能够理解，所谓角色（Role），其实就是一组权限规则列表。而我们分配这些权限的方式，就是通过创建 RoleBinding 对象，将被作用者（subject）和权限列表进行绑定。\n\n另外，与之对应的 ClusterRole 和 ClusterRoleBinding，则是 Kubernetes 集群级别的 Role 和 RoleBinding，它们的作用范围不受 Namespace 限制。\n\n而尽管权限的被作用者可以有很多种（比如，User、Group 等），但在我们平常的使用中，最普遍的用法还是 ServiceAccount。所以，Role + RoleBinding + ServiceAccount 的权限分配方式是你要重点掌握的内容。我们在后面编写和安装各种插件的时候，会经常用到这个组合。\n\n**思考题**\n\n请问，如何为所有 Namespace 下的默认 ServiceAccount（default ServiceAccount），绑定一个只读权限的 Role 呢？请你提供 ClusterRoleBinding（或者 RoleBinding）的 YAML 文件\n\n\n\tkind: ClusterRoleBinding\n\tapiVersion: rbac.authorization.k8s.io/v1\n\tmetadata:\n\t  name: readonly-all-default\n\tsubjects:\n\t- kind: ServiceAccount\n\t  name: system.serviceaccount.default  // 或者name直接写default，不指定namespace\n\troleRef:\n\t  kind: ClusterRole\n\t  name: view\n\t  apiGroup: rbac.authorization.k8s.io\n\n","categories":["reference","k8s"]},{"title":"27 | 聪明的微创新：Operator工作原理解读","url":"/2021/03/13/reference/k8s/27.聪明的微创新：Operator工作原理解读/","content":"\n![](1.jpg)\n\n\n## 聪明的微创新之 Operator 工作原理解读。\n\n在前面的几篇文章中，我已经和你分享了 Kubernetes 项目中的大部分编排对象（比如 Deployment、StatefulSet、DaemonSet，以及 Job），也介绍了“有状态应用”的管理方法，还阐述了为 Kubernetes 添加自定义 API 对象和编写自定义控制器的原理和流程。\n\n可能你已经感觉到，在 Kubernetes 中，管理“有状态应用”是一个比较复杂的过程，尤其是编写 Pod 模板的时候，总有一种“在 YAML 文件里编程序”的感觉，让人很不舒服。\n\n而在 Kubernetes 生态中，还有一个相对更加灵活和编程友好的管理“有状态应用”的解决方案，它就是：Operator。\n\n接下来，我就以 Etcd Operator 为例，来为你讲解一下 Operator 的工作原理和编写方法。\n\nEtcd Operator 的使用方法非常简单，只需要两步即可完成：\n\n**第一步，将这个 Operator 的代码 Clone 到本地：**\n\n\t$ git clone https://github.com/coreos/etcd-operator\n**第二步，将这个 Etcd Operator 部署在 Kubernetes 集群里。**\n\n不过，在部署 Etcd Operator 的 Pod 之前，你需要先执行这样一个脚本：\n\n\t$ example/rbac/create_role.sh\n不用我多说你也能够明白：这个脚本的作用，就是为 Etcd Operator 创建 RBAC 规则。这是因为，Etcd Operator 需要访问 Kubernetes 的 APIServer 来创建对象。\n\n更具体地说，上述脚本为 Etcd Operator 定义了如下所示的权限：\n\n1. 对 Pod、Service、PVC、Deployment、Secret 等 API 对象，有所有权限；\n2. 对 CRD 对象，有所有权限；\n3. 对属于 etcd.database.coreos.com 这个 API Group 的 CR（Custom Resource）对象，有所有权限。\n\n而 Etcd Operator 本身，其实就是一个 Deployment，它的 YAML 文件如下所示：\n\n\n\tapiVersion: extensions/v1beta1\n\tkind: Deployment\n\tmetadata:\n\t  name: etcd-operator\n\tspec:\n\t  replicas: 1\n\t  template:\n\t    metadata:\n\t      labels:\n\t        name: etcd-operator\n\t    spec:\n\t      containers:\n\t      - name: etcd-operator\n\t        image: quay.io/coreos/etcd-operator:v0.9.2\n\t        command:\n\t        - etcd-operator\n\t        env:\n\t        - name: MY_POD_NAMESPACE\n\t          valueFrom:\n\t            fieldRef:\n\t              fieldPath: metadata.namespace\n\t        - name: MY_POD_NAME\n\t          valueFrom:\n\t            fieldRef:\n\t              fieldPath: metadata.name\n\t...\n所以，我们就可以使用上述的 YAML 文件来创建 Etcd Operator，如下所示：\n\n\t$ kubectl create -f example/deployment.yaml\n而一旦 Etcd Operator 的 Pod 进入了 Running 状态，你就会发现，有一个 CRD 被自动创建了出来，如下所示：\n\n\t$ kubectl get pods\n\tNAME                              READY     STATUS      RESTARTS   AGE\n\tetcd-operator-649dbdb5cb-bzfzp    1/1       Running     0          20s\n\n\t$ kubectl get crd\n\tNAME                                    CREATED AT\n\tetcdclusters.etcd.database.coreos.com   2018-09-18T11:42:55Z\n这个 CRD 名叫etcdclusters.etcd.database.coreos.com 。你可以通过 kubectl describe 命令看到它的细节，如下所示：\n\n\t$ kubectl describe crd  etcdclusters.etcd.database.coreos.com\n\t...\n\tGroup:   etcd.database.coreos.com\n\t  Names:\n\t    Kind:       EtcdCluster\n\t    List Kind:  EtcdClusterList\n\t    Plural:     etcdclusters\n\t    Short Names:\n\t      etcd\n\t    Singular:  etcdcluster\n\t  Scope:       Namespaced\n\t  Version:     v1beta2\n\t  \n\t...\n可以看到，这个 CRD 相当于告诉了 Kubernetes：接下来，如果有 API 组（Group）是etcd.database.coreos.com、API 资源类型（Kind）是“EtcdCluster”的 YAML 文件被提交上来，你可一定要认识啊。\n\n所以说，通过上述两步操作，你实际上是在 Kubernetes 里添加了一个名叫 EtcdCluster 的自定义资源类型。而 Etcd Operator 本身，就是这个自定义资源类型对应的自定义控制器。\n\n而当 Etcd Operator 部署好之后，接下来在这个 Kubernetes 里创建一个 Etcd 集群的工作就非常简单了。你只需要编写一个 EtcdCluster 的 YAML 文件，然后把它提交给 Kubernetes 即可，如下所示：\n\n\t$ kubectl apply -f example/example-etcd-cluster.yaml\n这个 example-etcd-cluster.yaml 文件里描述的，是一个 3 个节点的 Etcd 集群。我们可以看到它被提交给 Kubernetes 之后，就会有三个 Etcd 的 Pod 运行起来，如下所示：\n\n\t$ kubectl get pods\n\tNAME                            READY     STATUS    RESTARTS   AGE\n\texample-etcd-cluster-dp8nqtjznc   1/1       Running     0          1m\n\texample-etcd-cluster-mbzlg6sd56   1/1       Running     0          2m\n\texample-etcd-cluster-v6v6s6stxd   1/1       Running     0          2m\n那么，`究竟发生了什么，让创建一个 Etcd 集群的工作如此简单呢？`\n\n我们当然还是得从这个 example-etcd-cluster.yaml 文件开始说起。\n\n不难想到，这个文件里定义的，正是 EtcdCluster 这个 CRD 的一个具体实例，也就是一个 Custom Resource（CR）。而它的内容非常简单，如下所示：\n\n\tapiVersion: \"etcd.database.coreos.com/v1beta2\"\n\tkind: \"EtcdCluster\"\n\tmetadata:\n\t  name: \"example-etcd-cluster\"\n\tspec:\n\t  size: 3\n\t  version: \"3.2.13\"\n可以看到，EtcdCluster 的 spec 字段非常简单。其中，size=3 指定了它所描述的 Etcd 集群的节点个数。而 version=“3.2.13”，则指定了 Etcd 的版本，仅此而已。\n\n而真正把这样一个 Etcd 集群创建出来的逻辑，就是 Etcd Operator 要实现的主要工作了。\n\n看到这里，相信你应该已经对 Operator 有了一个初步的认知：\n\n**Operator 的工作原理，实际上是利用了 Kubernetes 的自定义 API 资源（CRD），来描述我们想要部署的“有状态应用”；然后在自定义控制器里，根据自定义 API 对象的变化，来完成具体的部署和运维工作。**\n\n所以，编写一个 Etcd Operator，与我们前面编写一个自定义控制器的过程，没什么不同。\n\n不过，考虑到你可能还不太清楚`Etcd 集群的组建方式`，我在这里先简单介绍一下这部分知识。\n\n**Etcd Operator 部署 Etcd 集群，采用的是静态集群（Static）的方式。**\n\n静态集群的好处是，它不必依赖于一个额外的服务发现机制来组建集群，非常适合本地容器化部署。而它的难点，则在于你必须在部署的时候，就规划好这个集群的拓扑结构，并且能够知道这些节点固定的 IP 地址。比如下面这个例子：\n\n\t$ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\\n\t  --listen-peer-urls http://10.0.1.10:2380 \\\n\t...\n\t  --initial-cluster-token etcd-cluster-1 \\\n\t  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n\t  --initial-cluster-state new\n\t  \n\t$ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\\n\t  --listen-peer-urls http://10.0.1.11:2380 \\\n\t...\n\t  --initial-cluster-token etcd-cluster-1 \\\n\t  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n\t  --initial-cluster-state new\n\t  \n\t$ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\\n\t  --listen-peer-urls http://10.0.1.12:2380 \\\n\t...\n\t  --initial-cluster-token etcd-cluster-1 \\\n\t  --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n\t  --initial-cluster-state new\n在这个例子中，我启动了三个 Etcd 进程，组成了一个三节点的 Etcd 集群。\n\n其中，这些节点启动参数里的–initial-cluster 参数，非常值得你关注。它的含义，正是**当前节点启动时集群的拓扑结构。说得更详细一点，就是当前这个节点启动时，需要跟哪些节点通信来组成集群。**\n\n举个例子，我们可以看一下上述 infra2 节点的–initial-cluster 的值，如下所示：\n\n\t...\n\t--initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\\n可以看到，–initial-cluster 参数是由“< 节点名字 >=< 节点地址 >”格式组成的一个数组。而上面这个配置的意思就是，当 infra2 节点启动之后，这个 Etcd 集群里就会有 infra0、infra1 和 infra2 三个节点。\n\n同时，这些 Etcd 节点，需要通过 2380 端口进行通信以便组成集群，这也正是上述配置中–listen-peer-urls 字段的含义。\n\n此外，一个 Etcd 集群还需要用–initial-cluster-token 字段，来声明一个该集群独一无二的 Token 名字。\n\n像上述这样为每一个 Ectd 节点配置好它对应的启动参数之后把它们启动起来，一个 Etcd 集群就可以自动组建起来了。\n\n而我们要编写的 Etcd Operator，就是要把上述过程自动化。这其实等同于：用代码来生成每个 Etcd 节点 Pod 的启动命令，然后把它们启动起来。\n\n接下来，我们一起来实践一下这个流程。\n\n当然，在编写自定义控制器之前，我们首先需要完成 EtcdCluster 这个 CRD 的定义，它对应的 types.go 文件的主要内容，如下所示：\n\n\t// +genclient\n\t// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\ttype EtcdCluster struct {\n\t  metav1.TypeMeta   `json:\",inline\"`\n\t  metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\t  Spec              ClusterSpec   `json:\"spec\"`\n\t  Status            ClusterStatus `json:\"status\"`\n\t}\n\ttype ClusterSpec struct {\n\t // Size is the expected size of the etcd cluster.\n\t // The etcd-operator will eventually make the size of the running\n\t // cluster equal to the expected size.\n\t // The vaild range of the size is from 1 to 7.\n\t Size int `json:\"size\"`\n\t ... \n\t}\n可以看到，EtcdCluster 是一个有 Status 字段的 CRD。在这里，我们可以不必关心 ClusterSpec 里的其他字段，只关注 Size（即：Etcd 集群的大小）字段即可。\n\nSize 字段的存在，就意味着将来如果我们想要调整集群大小的话，应该直接修改 YAML 文件里 size 的值，并执行 kubectl apply -f。\n\n这样，Operator 就会帮我们完成 Etcd 节点的增删操作。这种“scale”能力，也是 Etcd Operator 自动化运维 Etcd 集群需要实现的主要功能。\n\n而为了能够支持这个功能，我们就不再像前面那样在–initial-cluster 参数里把拓扑结构固定死。\n\n所以，Etcd Operator 的实现，虽然选择的也是静态集群，但这个集群具体的组建过程，是逐个节点动态添加的方式，即：\n\n**首先，Etcd Operator 会创建一个“种子节点”；**\n\n**然后，Etcd Operator 会不断创建新的 Etcd 节点，然后将它们逐一加入到这个集群当中，直到集群的节点数等于 size。**\n\n这就意味着，在生成不同角色的 Etcd Pod 时，Operator 需要能够区分种子节点与普通节点。\n\n而这两种节点的不同之处，就在于一个名叫–initial-cluster-state 的启动参数：\n\n * 当这个参数值设为 new 时，就代表了该节点是种子节点。而我们前面提到过，种子节点还必须通过–initial-cluster-token 声明一个独一无二的 Token。\n * 而如果这个参数值设为 existing，那就是说明这个节点是一个普通节点，Etcd Operator 需要把它加入到已有集群里。\n\n那么接下来的问题就是，每个 Etcd 节点的–initial-cluster 字段的值又是怎么生成的呢？\n\n由于这个方案要求种子节点先启动，所以对于种子节点 infra0 来说，它启动后的集群只有它自己，即：`–initial-cluster=infra0=http://10.0.1.10:2380`.\n\n而对于接下来要加入的节点，比如 infra1 来说，它启动后的集群就有两个节点了，所以它的–initial-cluster 参数的值应该是：`infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380`.\n\n其他节点，都以此类推。\n\n现在，你就应该能在脑海中构思出上述三节点 Etcd 集群的部署过程了。\n\n首先，只要用户提交 YAML 文件时声明创建一个 EtcdCluster 对象（一个 Etcd 集群），那么 Etcd Operator 都应该先创建一个单节点的种子集群（Seed Member），并启动这个种子节点。\n\n以 infra0 节点为例，它的 IP 地址是 10.0.1.10，那么 Etcd Operator 生成的种子节点的启动命令，如下所示：\n\n\t$ etcd\n\t  --data-dir=/var/etcd/data\n\t  --name=infra0\n\t  --initial-advertise-peer-urls=http://10.0.1.10:2380\n\t  --listen-peer-urls=http://0.0.0.0:2380\n\t  --listen-client-urls=http://0.0.0.0:2379\n\t  --advertise-client-urls=http://10.0.1.10:2379\n\t  --initial-cluster=infra0=http://10.0.1.10:2380\n\t  --initial-cluster-state=new\n\t  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8\n可以看到，这个种子节点的 initial-cluster-state 是 new，并且指定了唯一的 initial-cluster-token 参数。\n\n我们可以把这个创建种子节点（集群）的阶段称为：**Bootstrap**。\n\n接下来，**对于其他每一个节点，Operator 只需要执行如下两个操作即可**，以 infra1 为例。\n\n第一步：通过 Etcd 命令行添加一个新成员：\n\n\t$ etcdctl member add infra1 http://10.0.1.11:2380\n第二步：为这个成员节点生成对应的启动参数，并启动它：\n\n\t$ etcd\n\t    --data-dir=/var/etcd/data\n\t    --name=infra1\n\t    --initial-advertise-peer-urls=http://10.0.1.11:2380\n\t    --listen-peer-urls=http://0.0.0.0:2380\n\t    --listen-client-urls=http://0.0.0.0:2379\n\t    --advertise-client-urls=http://10.0.1.11:2379\n\t    --initial-cluster=infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380\n\t    --initial-cluster-state=existing\n\n\n可以看到，对于这个 infra1 成员节点来说，它的 initial-cluster-state 是 existing，也就是要加入已有集群。而它的 initial-cluster 的值，则变成了 infra0 和 infra1 两个节点的 IP 地址。\n\n所以，以此类推，不断地将 infra2 等后续成员添加到集群中，直到整个集群的节点数目等于用户指定的 size 之后，部署就完成了。\n\n在熟悉了这个部署思路之后，我再为你讲解`Etcd Operator 的工作原理`，就非常简单了。\n\n跟所有的自定义控制器一样，Etcd Operator 的启动流程也是围绕着 Informer 展开的，如下所示：\n\n\tfunc (c *Controller) Start() error {\n\t for {\n\t  err := c.initResource()\n\t  ...\n\t  time.Sleep(initRetryWaitTime)\n\t }\n\t c.run()\n\t}\n\tfunc (c *Controller) run() {\n\t ...\n\t \n\t _, informer := cache.NewIndexerInformer(source, &api.EtcdCluster{}, 0, cache.ResourceEventHandlerFuncs{\n\t  AddFunc:    c.onAddEtcdClus,\n\t  UpdateFunc: c.onUpdateEtcdClus,\n\t  DeleteFunc: c.onDeleteEtcdClus,\n\t }, cache.Indexers{})\n\t \n\t ctx := context.TODO()\n\t // TODO: use workqueue to avoid blocking\n\t informer.Run(ctx.Done())\n\t}\n可以看到，**Etcd Operator 启动要做的第一件事**（ c.initResource），是创建 EtcdCluster 对象所需要的 CRD，即：前面提到的etcdclusters.etcd.database.coreos.com。这样 Kubernetes 就能够“认识”EtcdCluster 这个自定义 API 资源了。\n\n而**接下来，Etcd Operator 会定义一个 EtcdCluster 对象的 Informer。**\n\n不过，需要注意的是，由于 Etcd Operator 的完成时间相对较早，所以它里面有些代码的编写方式会跟我们之前讲解的最新的编写方式不太一样。在具体实践的时候，你还是应该以我讲解的模板为主。\n\n比如，在上面的代码最后，你会看到有这样一句注释：\n\n\t// TODO: use workqueue to avoid blocking\n\t...\n也就是说，Etcd Operator 并没有用工作队列来协调 Informer 和控制循环。这其实正是我在第 25 篇文章《深入解析声明式 API（二）：编写自定义控制器》中，给你留的关于工作队列的思考题的答案。\n\n具体来讲，我们在控制循环里执行的业务逻辑，往往是比较耗时间的。比如，创建一个真实的 Etcd 集群。而 Informer 的 WATCH 机制对 API 对象变化的响应，则非常迅速。所以，控制器里的业务逻辑就很可能会拖慢 Informer 的执行周期，甚至可能 Block 它。而要协调这样两个快、慢任务的一个典型解决方法，就是引入一个工作队列。\n\n> 备注：如果你感兴趣的话，可以给 Etcd Operator 提一个 patch 来修复这个问题。提 PR 修 TODO，是给一个开源项目做有意义的贡献的一个重要方式。\n\n由于 Etcd Operator 里没有工作队列，那么在它的 EventHandler 部分，就不会有什么入队操作，而直接就是每种事件对应的具体的业务逻辑了。\n\n不过，Etcd Operator 在业务逻辑的实现方式上，与常规的自定义控制器略有不同。我把在这一部分的工作原理，提炼成了一个详细的流程图，如下所示：\n\n![](2.jpg)\n\n可以看到，Etcd Operator 的特殊之处在于，它为每一个 EtcdCluster 对象，都启动了一个控制循环，“并发”地响应这些对象的变化。显然，这种做法不仅可以简化 Etcd Operator 的代码实现，还有助于提高它的响应速度。\n\n以文章一开始的 example-etcd-cluster 的 YAML 文件为例。\n\n当这个 YAML 文件第一次被提交到 Kubernetes 之后，Etcd Operator 的 Informer，就会立刻“感知”到一个新的 EtcdCluster 对象被创建了出来。所以，EventHandler 里的“添加”事件会被触发。\n\n而这个 Handler 要做的操作也很简单，即：在 Etcd Operator 内部创建一个对应的 Cluster 对象（cluster.New），比如流程图里的 Cluster1。\n\n这个 Cluster 对象，就是一个 Etcd 集群在 Operator 内部的描述，所以它与真实的 Etcd 集群的生命周期是一致的。\n\n而一个 Cluster 对象需要具体负责的，其实有两个工作。\n\n**其中，第一个工作只在该 Cluster 对象第一次被创建的时候才会执行。这个工作，就是我们前面提到过的 Bootstrap，即：创建一个单节点的种子集群。**\n\n由于种子集群只有一个节点，所以这一步直接就会生成一个 Etcd 的 Pod 对象。这个 Pod 里有一个 InitContainer，负责检查 Pod 的 DNS 记录是否正常。如果检查通过，用户容器也就是 Etcd 容器就会启动起来。\n\n而这个 Etcd 容器最重要的部分，当然就是它的启动命令了。\n\n以我们在文章一开始部署的集群为例，它的种子节点的容器启动命令如下所示：\n\n\t/usr/local/bin/etcd\n\t  --data-dir=/var/etcd/data\n\t  --name=example-etcd-cluster-mbzlg6sd56\n\t  --initial-advertise-peer-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380\n\t  --listen-peer-urls=http://0.0.0.0:2380\n\t  --listen-client-urls=http://0.0.0.0:2379\n\t  --advertise-client-urls=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2379\n\t  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380\n\t  --initial-cluster-state=new\n\t  --initial-cluster-token=4b5215fa-5401-4a95-a8c6-892317c9bef8\n上述启动命令里的各个参数的含义，我已经在前面介绍过。\n\n可以看到，在这些启动参数（比如：initial-cluster）里，Etcd Operator 只会使用 Pod 的 DNS 记录，而不是它的 IP 地址。\n\n这当然是因为，在 Operator 生成上述启动命令的时候，Etcd 的 Pod 还没有被创建出来，它的 IP 地址自然也无从谈起。\n\n这也就意味着，每个 Cluster 对象，都会事先创建一个与该 EtcdCluster 同名的 Headless Service。这样，Etcd Operator 在接下来的所有创建 Pod 的步骤里，就都可以使用 Pod 的 DNS 记录来代替它的 IP 地址了。\n\n> 备注：Headless Service 的 DNS 记录格式是：...svc.cluster.local。如果你记不太清楚了，可以借此再回顾一下第 18 篇文章《深入理解 StatefulSet（一）：拓扑状态》中的相关内容。\n\n**Cluster 对象的第二个工作，则是启动该集群所对应的控制循环。**\n\n这个控制循环每隔一定时间，就会执行一次下面的 Diff 流程。\n\n首先，控制循环要获取到所有正在运行的、属于这个 Cluster 的 Pod 数量，也就是该 Etcd 集群的“实际状态”。\n\n而这个 Etcd 集群的“期望状态”，正是用户在 EtcdCluster 对象里定义的 size。\n\n所以接下来，控制循环会对比这两个状态的差异。\n\n如果实际的 Pod 数量不够，那么控制循环就会执行一个添加成员节点的操作（即：上述流程图中的 addOneMember 方法）；反之，就执行删除成员节点的操作（即：上述流程图中的 removeOneMember 方法）。\n\n以 addOneMember 方法为例，它执行的流程如下所示：\n\n1. 生成一个新节点的 Pod 的名字，比如：example-etcd-cluster-v6v6s6stxd；\n\n2. 调用 Etcd Client，执行前面提到过的 etcdctl member add example-etcd-cluster-v6v6s6stxd 命令；\n\n3. 使用这个 Pod 名字，和已经存在的所有节点列表，组合成一个新的 initial-cluster 字段的值；\n\n4. 使用这个 initial-cluster 的值，生成这个 Pod 里 Etcd 容器的启动命令。如下所示：\n\n\n\t/usr/local/bin/etcd\n\t  --data-dir=/var/etcd/data\n\t  --name=example-etcd-cluster-v6v6s6stxd\n\t  --initial-advertise-peer-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380\n\t  --listen-peer-urls=http://0.0.0.0:2380\n\t  --listen-client-urls=http://0.0.0.0:2379\n\t  --advertise-client-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379\n\t  --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380\n\t  --initial-cluster-state=existing\n\n这样，当这个容器启动之后，一个新的 Etcd 成员节点就被加入到了集群当中。控制循环会重复这个过程，直到正在运行的 Pod 数量与 EtcdCluster 指定的 size 一致。\n\n在有了这样一个与 EtcdCluster 对象一一对应的控制循环之后，你后续对这个 EtcdCluster 的任何修改，比如：修改 size 或者 Etcd 的 version，它们对应的更新事件都会由这个 Cluster 对象的控制循环进行处理。\n\n以上，就是一个 Etcd Operator 的工作原理了。\n\n如果对比一下 Etcd Operator 与我在第 20 篇文章《深入理解 StatefulSet（三）：有状态应用实践》中讲解过的 MySQL StatefulSet 的话，你可能会有两个问题。\n\n**第一个问题是**，在 StatefulSet 里，它为 Pod 创建的名字是带编号的，这样就把整个集群的拓扑状态固定了下来（比如：一个三节点的集群一定是由名叫 web-0、web-1 和 web-2 的三个 Pod 组成）。可是，**在 Etcd Operator 里，为什么我们使用随机名字就可以了呢？**\n\n这是因为，Etcd Operator 在每次添加 Etcd 节点的时候，都会先执行 etcdctl member add <Pod 名字 >；每次删除节点的时候，则会执行 etcdctl member remove <Pod 名字 >。这些操作，其实就会更新 Etcd 内部维护的拓扑信息，所以 Etcd Operator 无需在集群外部通过编号来固定这个拓扑关系。\n\n**第二个问题是，为什么我没有在 EtcdCluster 对象里声明 Persistent Volume？**\n\n难道，我们不担心节点宕机之后 Etcd 的数据会丢失吗？\n\n我们知道，Etcd 是一个基于 Raft 协议实现的高可用 Key-Value 存储。根据 Raft 协议的设计原则，当 Etcd 集群里只有半数以下（在我们的例子里，小于等于一个）的节点失效时，当前集群依然可以正常工作。此时，Etcd Operator 只需要通过控制循环创建出新的 Pod，然后将它们加入到现有集群里，就完成了“期望状态”与“实际状态”的调谐工作。这个集群，是一直可用的 。\n\n> 备注：关于 [Etcd 的工作原理和 Raft 协议](http://www.infoq.com/cn/articles/etcd-interpretation-application-scenario-implement-principle)的设计思想，你可以阅读这篇文章来进行学习。\n\n但是，当这个 Etcd 集群里有半数以上（在我们的例子里，大于等于两个）的节点失效的时候，这个集群就会丧失数据写入的能力，从而进入“不可用”状态。此时，即使 Etcd Operator 创建出新的 Pod 出来，Etcd 集群本身也无法自动恢复起来。\n\n这个时候，我们就必须使用 Etcd 本身的备份数据来对集群进行恢复操作。\n\n在有了 Operator 机制之后，上述 Etcd 的备份操作，是由一个单独的 Etcd Backup Operator 负责完成的。\n\n创建和使用这个 Operator 的流程，如下所示：\n\n\t# 首先，创建etcd-backup-operator\n\t$ kubectl create -f example/etcd-backup-operator/deployment.yaml\n\n\t# 确认etcd-backup-operator已经在正常运行\n\t$ kubectl get pod\n\tNAME                                    READY     STATUS    RESTARTS   AGE\n\tetcd-backup-operator-1102130733-hhgt7   1/1       Running   0          3s\n\n\t# 可以看到，Backup Operator会创建一个叫etcdbackups的CRD\n\t$ kubectl get crd\n\tNAME                                    KIND\n\tetcdbackups.etcd.database.coreos.com    CustomResourceDefinition.v1beta1.apiextensions.k8s.io\n\n\t# 我们这里要使用AWS S3来存储备份，需要将S3的授权信息配置在文件里\n\t$ cat $AWS_DIR/credentials\n\t[default]\n\taws_access_key_id = XXX\n\taws_secret_access_key = XXX\n\n\t$ cat $AWS_DIR/config\n\t[default]\n\tregion = <region>\n\n\t# 然后，将上述授权信息制作成一个Secret\n\t$ kubectl create secret generic aws --from-file=$AWS_DIR/credentials --from-file=$AWS_DIR/config\n\n\t# 使用上述S3的访问信息，创建一个EtcdBackup对象\n\t$ sed -e 's|<full-s3-path>|mybucket/etcd.backup|g' \\\n\t    -e 's|<aws-secret>|aws|g' \\\n\t    -e 's|<etcd-cluster-endpoints>|\"http://example-etcd-cluster-client:2379\"|g' \\\n\t    example/etcd-backup-operator/backup_cr.yaml \\\n\t    | kubectl create -f -\n需要注意的是，每当你创建一个 EtcdBackup 对象（[backup_cr.yaml](https://github.com/coreos/etcd-operator/blob/master/example/etcd-backup-operator/backup_cr.yaml)），就相当于为它所指定的 Etcd 集群做了一次备份。EtcdBackup 对象的 etcdEndpoints 字段，会指定它要备份的 Etcd 集群的访问地址。\n\n所以，在实际的环境里，我建议你把最后这个备份操作，编写成一个 Kubernetes 的 CronJob 以便定时运行。\n\n而当 Etcd 集群发生了故障之后，你就可以通过创建一个 EtcdRestore 对象来完成恢复操作。当然，这就意味着你也需要事先启动 Etcd Restore Operator。\n\n这个流程的完整过程，如下所示：\n\n\t# 创建etcd-restore-operator\n\t$ kubectl create -f example/etcd-restore-operator/deployment.yaml\n\t# 确认它已经正常运行\n\t$ kubectl get pods\n\tNAME                                     READY     STATUS    RESTARTS   AGE\n\tetcd-restore-operator-4203122180-npn3g   1/1       Running   0          7s\n\t# 创建一个EtcdRestore对象，来帮助Etcd Operator恢复数据，记得替换模板里的S3的访问信息\n\t$ sed -e 's|<full-s3-path>|mybucket/etcd.backup|g' \\\n\t    -e 's|<aws-secret>|aws|g' \\\n\t    example/etcd-restore-operator/restore_cr.yaml \\\n\t    | kubectl create -f -\n上面例子里的 EtcdRestore 对象（restore_cr.yaml），会指定它要恢复的 Etcd 集群的名字和备份数据所在的 S3 存储的访问信息。\n\n而当一个 EtcdRestore 对象成功创建后，Etcd Restore Operator 就会通过上述信息，恢复出一个全新的 Etcd 集群。然后，Etcd Operator 会把这个新集群直接接管过来，从而重新进入可用的状态。\n\nEtcdBackup 和 EtcdRestore 这两个 Operator 的工作原理，与 Etcd Operator 的实现方式非常类似。所以，这一部分就交给你课后去探索了。\n\n**总结**\n\n在今天这篇文章中，我以 Etcd Operator 为例，详细介绍了一个 Operator 的工作原理和编写过程。\n\n可以看到，Etcd 集群本身就拥有良好的分布式设计和一定的高可用能力。在这种情况下，StatefulSet“为 Pod 编号”和“将 Pod 同 PV 绑定”这两个主要的特性，就不太有用武之地了。\n\n而相比之下，Etcd Operator 把一个 Etcd 集群，抽象成了一个具有一定“自治能力”的整体。而当这个“自治能力”本身不足以解决问题的时候，我们可以通过两个专门负责备份和恢复的 Operator 进行修正。这种实现方式，不仅更加贴近 Etcd 的设计思想，也更加编程友好。\n\n不过，如果我现在要部署的应用，既需要用 StatefulSet 的方式维持拓扑状态和存储状态，又有大量的编程工作要做，那我到底该如何选择呢？\n\n其实，Operator 和 StatefulSet 并不是竞争关系。你完全可以编写一个 Operator，然后在 Operator 的控制循环里创建和控制 StatefulSet 而不是 Pod。比如，业界知名的[Prometheus 项目的 Operator](https://github.com/prometheus-operator/prometheus-operator)，正是这么实现的。\n\n此外，CoreOS 公司在被 RedHat 公司收购之后，已经把 Operator 的编写过程封装成了一个叫作[Operator SDK](https://github.com/operator-framework/operator-sdk)的工具（整个项目叫作 Operator Framework），它可以帮助你生成 Operator 的框架代码。感兴趣的话，你可以试用一下。\n\n**思考题**\n\n在 Operator 的实现过程中，我们再一次用到了 CRD。可是，你一定要明白，CRD 并不是万能的，它有很多场景不适用，还有性能瓶颈。你能列举出一些不适用 CRD 的场景么？你知道造成 CRD 性能瓶颈的原因主要在哪里么？\n\n\n","categories":["reference","k8s"]},{"title":"28 | PV、PVC、StorageClass，这些到底在说啥？","url":"/2021/03/13/reference/k8s/28.PV、PVC、StorageClass，这些到底在说啥/","content":"\n![](1.jpg)\n\n## PV、PVC、StorageClass，这些到底在说啥？\n\n在前面的文章中，我重点为你分析了 Kubernetes 的各种编排能力。\n\n在这些讲解中，你应该已经发现，容器化一个应用比较麻烦的地方，莫过于对其“状态”的管理。而最常见的“状态”，又莫过于存储状态了。\n\n所以，从今天这篇文章开始，我会通过 **4 篇文章为你剖析 Kubernetes 项目处理容器持久化存储的核心原理**，从而帮助你更好地理解和使用这部分内容。\n\n首先，我们来回忆一下我在第 19 篇文章《深入理解 StatefulSet（二）：存储状态》中，和你分享 StatefulSet 如何管理存储状态的时候，介绍过的`Persistent Volume（PV）和 Persistent Volume Claim（PVC）`这套持久化存储体系。\n\n其中，**PV 描述的，是持久化存储数据卷。**这个 API 对象主要定义的是一个持久化存储在宿主机上的目录，比如一个 NFS 的挂载目录。\n\n通常情况下，PV 对象是由运维人员事先创建在 Kubernetes 集群里待用的。比如，运维人员可以定义这样一个 NFS 类型的 PV，如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolume\n\tmetadata:\n\t  name: nfs\n\tspec:\n\t  storageClassName: manual\n\t  capacity:\n\t    storage: 1Gi\n\t  accessModes:\n\t    - ReadWriteMany\n\t  nfs:\n\t    server: 10.244.1.4\n\t    path: \"/\"\n而 **PVC 描述的，则是 Pod 所希望使用的持久化存储的属性。** 比如，Volume 存储的大小、可读写权限等等。\n\nPVC 对象通常由开发人员创建；或者以 PVC 模板的方式成为 StatefulSet 的一部分，然后由 StatefulSet 控制器负责创建带编号的 PVC。\n\n比如，开发人员可以声明一个 1 GiB 大小的 PVC，如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolumeClaim\n\tmetadata:\n\t  name: nfs\n\tspec:\n\t  accessModes:\n\t    - ReadWriteMany\n\t  storageClassName: manual\n\t  resources:\n\t    requests:\n\t      storage: 1Gi\n而用户创建的 PVC 要真正被容器使用起来，就必须先和某个符合条件的 PV 进行绑定。这里要检查的条件，包括两部分：\n\n * 第一个条件，当然是 PV 和 PVC 的 spec 字段。比如，PV 的存储（storage）大小，就必须满足 PVC 的要求。\n * 而第二个条件，则是 PV 和 PVC 的 storageClassName 字段必须一样。这个机制我会在本篇文章的最后一部分专门介绍。\n\n在成功地将 PVC 和 PV 进行绑定之后，Pod 就能够像使用 hostPath 等常规类型的 Volume 一样，在自己的 YAML 文件里声明使用这个 PVC 了，如下所示：\n\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  labels:\n\t    role: web-frontend\n\tspec:\n\t  containers:\n\t  - name: web\n\t    image: nginx\n\t    ports:\n\t      - name: web\n\t        containerPort: 80\n\t    volumeMounts:\n\t        - name: nfs\n\t          mountPath: \"/usr/share/nginx/html\"\n\t  volumes:\n\t  - name: nfs\n\t    persistentVolumeClaim:\n\t      claimName: nfs\n可以看到，Pod 需要做的，就是在 volumes 字段里声明自己要使用的 PVC 名字。接下来，等这个 Pod 创建之后，kubelet 就会把这个 PVC 所对应的 PV，也就是一个 NFS 类型的 Volume，挂载在这个 Pod 容器内的目录上。\n\n不难看出，**PVC 和 PV 的设计，其实跟“面向对象”的思想完全一致。**\n\nPVC 可以理解为持久化存储的“接口”，它提供了对某种持久化存储的描述，但不提供具体的实现；而这个持久化存储的实现部分则由 PV 负责完成。\n\n这样做的好处是，作为应用开发者，我们只需要跟 PVC 这个“接口”打交道，而不必关心具体的实现是 NFS 还是 Ceph。毕竟这些存储相关的知识太专业了，应该交给专业的人去做。\n\n而在上面的讲述中，`其实还有一个比较棘手的情况。`\n\n比如，你在创建 Pod 的时候，系统里并没有合适的 PV 跟它定义的 PVC 绑定，也就是说此时容器想要使用的 Volume 不存在。这时候，Pod 的启动就会报错。\n\n但是，过了一会儿，运维人员也发现了这个情况，所以他赶紧创建了一个对应的 PV。这时候，我们当然希望 Kubernetes 能够再次完成 PVC 和 PV 的绑定操作，从而启动 Pod。\n\n所以在 Kubernetes 中，实际上存在着一个专门处理持久化存储的控制器，叫作 Volume Controller。这个 Volume Controller 维护着多个控制循环，其中有一个循环，扮演的就是撮合 PV 和 PVC 的“红娘”的角色。它的名字叫作 PersistentVolumeController。\n\nPersistentVolumeController 会不断地查看当前每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与这个“单身”的 PVC 进行绑定。这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态，从而结束“单身”之旅。\n\n**`而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。所以，接下来 Kubernetes 只要获取到这个 PVC 对象，就一定能够找到它所绑定的 PV。`**\n\n那么，`这个 PV 对象，又是如何变成容器里的一个持久化存储的呢？`\n\n我在前面讲解容器基础的时候，已经为你详细剖析了容器 Volume 的挂载机制。用一句话总结，**所谓容器的 Volume，其实就是将一个宿主机上的目录，跟一个容器里的目录绑定挂载在了一起**。（你可以借此机会，再回顾一下专栏的第 8 篇文章《白话容器基础（四）：重新认识 Docker 容器》中的相关内容）\n\n**而所谓的“持久化 Volume”，指的就是这个宿主机上的目录，具备“持久性”**。即：**`这个目录里面的内容，既不会因为容器的删除而被清理掉，也不会跟当前的宿主机绑定。这样，当容器被重启或者在其他节点上重建出来之后，它仍然能够通过挂载这个 Volume，访问到这些内容。`**\n\n显然，我们前面使用的 hostPath 和 emptyDir 类型的 Volume 并不具备这个特征：它们既有可能被 kubelet 清理掉，也不能被“迁移”到其他节点上。\n\n所以，大多数情况下，持久化 Volume 的实现，往往依赖于一个远程存储服务，比如：远程文件存储（比如，NFS、GlusterFS）、远程块存储（比如，公有云提供的远程磁盘）等等。\n\n而 Kubernetes 需要做的工作，就是使用这些存储服务，来为容器准备一个持久化的宿主机目录，以供将来进行绑定挂载时使用。**`而所谓“持久化”，指的是容器在这个目录里写入的文件，都会保存在远程存储中，从而使得这个目录具备了“持久性”。`**\n\n**这个准备“持久化”宿主机目录的过程，我们可以形象地称为“两阶段处理”**。\n\n接下来，我通过一个具体的例子为你说明。\n\n当一个 Pod 调度到一个节点上之后，kubelet 就要负责为这个 Pod 创建它的 Volume 目录。默认情况下，kubelet 为 Volume 创建的目录是如下所示的一个宿主机上的路径：\n\n\t/var/lib/kubelet/pods/<Pod的ID>/volumes/kubernetes.io~<Volume类型>/<Volume名字>\n接下来，kubelet 要做的操作就取决于你的 Volume 类型了。\n\n如果你的 Volume 类型是远程块存储，比如 Google Cloud 的 Persistent Disk（GCE 提供的远程磁盘服务），那么 kubelet 就需要先调用 Goolge Cloud 的 API，将它所提供的 Persistent Disk 挂载到 Pod 所在的宿主机上。\n\n> 备注：你如果不太了解块存储的话，可以直接把它理解为：**一块磁盘。**\n\n这相当于执行：\n\n\t$ gcloud compute instances attach-disk <虚拟机名字> --disk <远程磁盘名字>\n**这一步为虚拟机挂载远程磁盘的操作，对应的正是“两阶段处理”的第一阶段。在 Kubernetes 中，我们把这个阶段称为 Attach。**\n\nAttach 阶段完成后，为了能够使用这个远程磁盘，kubelet 还要进行第二个操作，即：格式化这个磁盘设备，然后将它挂载到宿主机指定的挂载点上。不难理解，这个挂载点，正是我在前面反复提到的 Volume 的宿主机目录。所以，这一步相当于执行：\n\n\t# 通过lsblk命令获取磁盘设备ID\n\t$ sudo lsblk\n\t# 格式化成ext4格式\n\t$ sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/<磁盘设备ID>\n\t# 挂载到挂载点\n\t$ sudo mkdir -p /var/lib/kubelet/pods/<Pod的ID>/volumes/kubernetes.io~<Volume类型>/<Volume名字>\n\n**这个将磁盘设备格式化并挂载到 Volume 宿主机目录的操作，对应的正是“两阶段处理”的第二个阶段，我们一般称为：Mount。**\n\nMount 阶段完成后，这个 Volume 的宿主机目录就是一个“持久化”的目录了，容器在它里面写入的内容，会保存在 Google Cloud 的远程磁盘中。\n\n而如果你的 Volume 类型是远程文件存储（比如 NFS）的话，kubelet 的处理过程就会更简单一些。\n\n因为在这种情况下，kubelet 可以跳过“第一阶段”（Attach）的操作，这是因为一般来说，远程文件存储并没有一个“存储设备”需要挂载在宿主机上。\n\n所以，kubelet 会直接从“第二阶段”（Mount）开始准备宿主机上的 Volume 目录。\n\n在这一步，kubelet 需要作为 client，将远端 NFS 服务器的目录（比如：“/”目录），挂载到 Volume 的宿主机目录上，即相当于执行如下所示的命令：\n\n\t$ mount -t nfs <NFS服务器地址>:/ /var/lib/kubelet/pods/<Pod的ID>/volumes/kubernetes.io~<Volume类型>/<Volume名字> \n通过这个挂载操作，Volume 的宿主机目录就成为了一个远程 NFS 目录的挂载点，后面你在这个目录里写入的所有文件，都会被保存在远程 NFS 服务器上。所以，我们也就完成了对这个 Volume 宿主机目录的“持久化”。\n\n**到这里，你可能会有疑问，Kubernetes 又是如何定义和区分这两个阶段的呢？**\n\n其实很简单，在具体的 Volume 插件的实现接口上，Kubernetes 分别给这两个阶段提供了两种不同的参数列表：\n\n * 对于“第一阶段”（Attach），Kubernetes 提供的可用参数是 nodeName，即宿主机的名字。\n * 而对于“第二阶段”（Mount），Kubernetes 提供的可用参数是 dir，即 Volume 的宿主机目录。\n\n所以，作为一个存储插件，你只需要根据自己的需求进行选择和实现即可。在后面关于编写存储插件的文章中，我会对这个过程做深入讲解。\n\n而经过了“两阶段处理”，我们就得到了一个“持久化”的 Volume 宿主机目录。所以，接下来，kubelet 只要把这个 Volume 目录通过 CRI 里的 Mounts 参数，传递给 Docker，然后就可以为 Pod 里的容器挂载这个“持久化”的 Volume 了。其实，这一步相当于执行了如下所示的命令：\n\n\n\t$ docker run -v /var/lib/kubelet/pods/<Pod的ID>/volumes/kubernetes.io~<Volume类型>/<Volume名字>:/<容器内的目标目录> 我的镜像 ...\n以上，就是 Kubernetes 处理 PV 的具体原理了。\n\n> 备注：对应地，在删除一个 PV 的时候，Kubernetes 也需要 Unmount 和 Dettach 两个阶段来处理。这个过程我就不再详细介绍了，执行“反向操作”即可。\n\n实际上，你可能已经发现，这个 PV 的处理流程似乎跟 Pod 以及容器的启动流程没有太多的耦合，只要 kubelet 在向 Docker 发起 CRI 请求之前，确保“持久化”的宿主机目录已经处理完毕即可。\n\n所以，在 Kubernetes 中，上述**关于 PV 的“两阶段处理”流程，是靠独立于 kubelet 主控制循环（Kubelet Sync Loop）之外的两个控制循环来实现的。**\n\n其中，“第一阶段”的 Attach（以及 Dettach）操作，是由 Volume Controller 负责维护的，这个控制循环的名字叫作： **AttachDetachController**。而它的作用，就是不断地检查每一个 Pod 对应的 PV，和这个 Pod 所在宿主机之间挂载情况。从而决定，是否需要对这个 PV 进行 Attach（或者 Dettach）操作。\n\n需要注意，作为一个 Kubernetes 内置的控制器，Volume Controller 自然是 kube-controller-manager 的一部分。所以，AttachDetachController 也一定是运行在 Master 节点上的。当然，Attach 操作只需要调用公有云或者具体存储项目的 API，并不需要在具体的宿主机上执行操作，所以这个设计没有任何问题。\n\n而“第二阶段”的 Mount（以及 Unmount）操作，必须发生在 Pod 对应的宿主机上，所以它必须是 kubelet 组件的一部分。这个控制循环的名字，叫作： **VolumeManagerReconciler**，它运行起来之后，是一个独立于 kubelet 主循环的 Goroutine。\n\n通过这样将 Volume 的处理同 kubelet 的主循环解耦，Kubernetes 就避免了这些耗时的远程挂载操作拖慢 kubelet 的主控制循环，进而导致 Pod 的创建效率大幅下降的问题。实际上，**kubelet 的一个主要设计原则，就是它的主控制循环绝对不可以被 block**。这个思想，我在后续的讲述容器运行时的时候还会提到。\n\n`在了解了 Kubernetes 的 Volume 处理机制之后，我再来为你介绍这个体系里最后一个重要概念：StorageClass。`\n\n我在前面介绍 PV 和 PVC 的时候，曾经提到过，PV 这个对象的创建，是由运维人员完成的。但是，在大规模的生产环境里，这其实是一个非常麻烦的工作。\n\n这是因为，一个大规模的 Kubernetes 集群里很可能有成千上万个 PVC，这就意味着运维人员必须得事先创建出成千上万个 PV。更麻烦的是，随着新的 PVC 不断被提交，运维人员就不得不继续添加新的、能满足条件的 PV，否则新的 Pod 就会因为 PVC 绑定不到 PV 而失败。在实际操作中，这几乎没办法靠人工做到。\n\n所以，Kubernetes 为我们提供了一套可以自动创建 PV 的机制，即：Dynamic Provisioning。\n\n相比之下，前面人工管理 PV 的方式就叫作 Static Provisioning。\n\nDynamic Provisioning 机制工作的核心，在于一个名叫 StorageClass 的 API 对象。\n\n**而 StorageClass 对象的作用，其实就是创建 PV 的模板。**\n\n具体地说，StorageClass 对象会定义如下两个部分内容：\n\n * 第一，PV 的属性。比如，存储类型、Volume 的大小等等。\n * 第二，创建这种 PV 需要用到的存储插件。比如，Ceph 等等。\n\n有了这样两个信息之后，Kubernetes 就能够根据用户提交的 PVC，找到一个对应的 StorageClass 了。然后，Kubernetes 就会调用该 StorageClass 声明的存储插件，创建出需要的 PV。\n\n举个例子，假如我们的 Volume 的类型是 GCE 的 Persistent Disk 的话，运维人员就需要定义一个如下所示的 StorageClass：\n\n\tapiVersion: storage.k8s.io/v1\n\tkind: StorageClass\n\tmetadata:\n\t  name: block-service\n\tprovisioner: kubernetes.io/gce-pd\n\tparameters:\n\t  type: pd-ssd\n在这个 YAML 文件里，我们定义了一个名叫 block-service 的 StorageClass。\n\n这个 StorageClass 的 provisioner 字段的值是：kubernetes.io/gce-pd，这正是 Kubernetes 内置的 GCE PD 存储插件的名字。\n\n而这个 StorageClass 的 parameters 字段，就是 PV 的参数。比如：上面例子里的 type=pd-ssd，指的是这个 PV 的类型是“SSD 格式的 GCE 远程磁盘”。\n\n需要注意的是，由于需要使用 GCE Persistent Disk，上面这个例子只有在 GCE 提供的 Kubernetes 服务里才能实践。如果你想使用我们之前部署在本地的 Kubernetes 集群以及 Rook 存储服务的话，你的 StorageClass 需要使用如下所示的 YAML 文件来定义：\n\n\tapiVersion: ceph.rook.io/v1beta1\n\tkind: Pool\n\tmetadata:\n\t  name: replicapool\n\t  namespace: rook-ceph\n\tspec:\n\t  replicated:\n\t    size: 3\n\t---\n\tapiVersion: storage.k8s.io/v1\n\tkind: StorageClass\n\tmetadata:\n\t  name: block-service\n\tprovisioner: ceph.rook.io/block\n\tparameters:\n\t  pool: replicapool\n\t  #The value of \"clusterNamespace\" MUST be the same as the one in which your rook cluster exist\n\t  clusterNamespace: rook-ceph\n在这个 YAML 文件中，我们定义的还是一个名叫 block-service 的 StorageClass，只不过它声明使的存储插件是由 Rook 项目。\n\n有了 StorageClass 的 YAML 文件之后，运维人员就可以在 Kubernetes 里创建这个 StorageClass 了：\n\n\t$ kubectl create -f sc.yaml\n这时候，作为应用开发者，我们只需要在 PVC 里指定要使用的 StorageClass 名字即可，如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolumeClaim\n\tmetadata:\n\t  name: claim1\n\tspec:\n\t  accessModes:\n\t    - ReadWriteOnce\n\t  storageClassName: block-service\n\t  resources:\n\t    requests:\n\t      storage: 30Gi\n可以看到，我们在这个 PVC 里添加了一个叫作 storageClassName 的字段，用于指定该 PVC 所要使用的 StorageClass 的名字是：block-service。\n\n以 Google Cloud 为例。\n\n当我们通过 kubectl create 创建上述 PVC 对象之后，Kubernetes 就会调用 Google Cloud 的 API，创建出一块 SSD 格式的 Persistent Disk。然后，再使用这个 Persistent Disk 的信息，自动创建出一个对应的 PV 对象。\n\n我们可以一起来实践一下这个过程（如果使用 Rook 的话下面的流程也是一样的，只不过 Rook 创建出的是 Ceph 类型的 PV）：\n\n\t$ kubectl create -f pvc.yaml\n可以看到，我们创建的 PVC 会绑定一个 Kubernetes 自动创建的 PV，如下所示：\n\n\t$ kubectl describe pvc claim1\n\tName:           claim1\n\tNamespace:      default\n\tStorageClass:   block-service\n\tStatus:         Bound\n\tVolume:         pvc-e5578707-c626-11e6-baf6-08002729a32b\n\tLabels:         <none>\n\tCapacity:       30Gi\n\tAccess Modes:   RWO\n\tNo Events.\n而且，通过查看这个自动创建的 PV 的属性，你就可以看到它跟我们在 PVC 里声明的存储的属性是一致的，如下所示：\n\n\t$ kubectl describe pv pvc-e5578707-c626-11e6-baf6-08002729a32b\n\tName:            pvc-e5578707-c626-11e6-baf6-08002729a32b\n\tLabels:          <none>\n\tStorageClass:    block-service\n\tStatus:          Bound\n\tClaim:           default/claim1\n\tReclaim Policy:  Delete\n\tAccess Modes:    RWO\n\tCapacity:        30Gi\n\t...\n\tNo events.\n此外，你还可以看到，这个自动创建出来的 PV 的 StorageClass 字段的值，也是 block-service。**`这是因为，Kubernetes 只会将 StorageClass 相同的 PVC 和 PV 绑定起来。`**\n\n有了 Dynamic Provisioning 机制，运维人员只需要在 Kubernetes 集群里创建出数量有限的 StorageClass 对象就可以了。这就好比，运维人员在 Kubernetes 集群里创建出了各种各样的 PV 模板。这时候，当开发人员提交了包含 StorageClass 字段的 PVC 之后，Kubernetes 就会根据这个 StorageClass 创建出对应的 PV。\n\n> [Kubernetes 的官方文档](https://kubernetes.io/docs/concepts/storage/storage-classes/#provisioner)里已经列出了默认支持 Dynamic Provisioning 的内置存储插件。而对于不在文档里的插件，比如 NFS，或者其他非内置存储插件，你其实可以通过[kubernetes-incubator/external-storage](https://github.com/kubernetes-incubator/external-storage)这个库来自己编写一个外部插件完成这个工作。像我们之前部署的 Rook，已经内置了 external-storage 的实现，所以 Rook 是完全支持 Dynamic Provisioning 特性的。\n\n**例子:**\n[Local StorageClass](https://kubernetes.io/docs/concepts/storage/storage-classes/#local): \n\n\tapiVersion: storage.k8s.io/v1\n\tkind: StorageClass\n\tmetadata:\n\t  name: local-storage\n\tprovisioner: kubernetes.io/no-provisioner\n\tvolumeBindingMode: WaitForFirstConsumer\nLocal volumes do not currently support dynamic provisioning, however a StorageClass should still be created to delay volume binding until Pod scheduling. This is specified by the WaitForFirstConsumer volume binding mode.\n\nDelaying volume binding allows the scheduler to consider all of a Pod's scheduling constraints when choosing an appropriate PersistentVolume for a PersistentVolumeClaim.\n\n需要注意的是，**StorageClass 并不是专门为了 Dynamic Provisioning 而设计的。**\n\n比如，在本篇一开始的例子里，我在 PV 和 PVC 里都声明了 storageClassName=manual。而我的集群里，实际上并没有一个名叫 manual 的 StorageClass 对象。这完全没有问题，这个时候 Kubernetes 进行的是 Static Provisioning，但在做绑定决策的时候，它依然会考虑 PV 和 PVC 的 StorageClass 定义。\n\n而这么做的好处也很明显：这个 PVC 和 PV 的绑定关系，就完全在我自己的掌控之中。\n\n这里，你可能会有疑问，我在之前讲解 StatefulSet 存储状态的例子时，好像并没有声明 StorageClass 啊？\n\n实际上，**如果你的集群已经开启了名叫 DefaultStorageClass 的 Admission Plugin，它就会为 PVC 和 PV 自动添加一个默认的 StorageClass；否则，PVC 的 storageClassName 的值就是“”，这也意味着它只能够跟 storageClassName 也是“”的 PV 进行绑定。**\n\n**总结**\n\n在今天的分享中，我为你详细解释了 PVC 和 PV 的设计与实现原理，并为你阐述了 StorageClass 到底是干什么用的。这些概念之间的关系，可以用如下所示的一幅示意图描述：\n\n![](2.png)\n\n从图中我们可以看到，在这个体系中：\n\nPVC 描述的，是 Pod 想要使用的持久化存储的属性，比如存储的大小、读写权限等。\n\nPV 描述的，则是一个具体的 Volume 的属性，比如 Volume 的类型、挂载目录、远程存储服务器地址等。\n\n而 StorageClass 的作用，则是充当 PV 的模板。并且，只有同属于一个 StorageClass 的 PV 和 PVC，才可以绑定在一起。\n\n当然，StorageClass 的另一个重要作用，是指定 PV 的 Provisioner（存储插件）。这时候，如果你的存储插件支持 Dynamic Provisioning 的话，Kubernetes 就可以自动为你创建 PV 了。\n\n基于上述讲述，为了统一概念和方便叙述，在本专栏中，我以后凡是提到“Volume”，指的就是一个远程存储服务挂载在宿主机上的持久化目录；而“PV”，指的是这个 Volume 在 Kubernetes 里的 API 对象。\n\n需要注意的是，这套容器持久化存储体系，完全是 Kubernetes 项目自己负责管理的，并不依赖于 docker volume 命令和 Docker 的存储插件。当然，这套体系本身就比 docker volume 命令的诞生时间还要早得多。\n\n**容器持久化存储涉及的概念比较多，试着总结一下整体流程:**\n\n1. 用户提交请求创建pod，Kubernetes发现这个pod声明使用了PVC，那就靠PersistentVolumeController帮它找一个PV配对。\n2. 没有现成的PV，就去找对应的StorageClass，帮它新创建一个PV，然后和PVC完成绑定。\n3. 新创建的PV，还只是一个API 对象，需要经过“两阶段处理”变成宿主机上的“持久化 Volume”才真正有用：\n * 第一阶段由运行在master上的AttachDetachController负责，为这个PV完成 Attach 操作，为宿主机挂载远程磁盘；\n * 第二阶段是运行在每个节点上kubelet组件的内部，把第一步attach的远程磁盘 mount 到宿主机目录。这个控制循环叫VolumeManagerReconciler，运行在独立的Goroutine，不会阻塞kubelet主循环。\n   完成这两步，PV对应的“持久化 Volume”就准备好了，POD可以正常启动，将“持久化 Volume”挂载在容器内指定的路径。\n\n**思考题**\n\n在了解了 PV、PVC 的设计和实现原理之后，你是否依然觉得它有“过度设计”的嫌疑？或者，你是否有更加简单、足以解决你 90% 需求的 Volume 的用法？\n\n\n","categories":["reference","k8s"]},{"title":"29 | PV、PVC体系是不是多此一举？从本地持久化卷谈起","url":"/2021/03/13/reference/k8s/29.PV、PVC体系是不是多此一举？从本地持久化卷谈起/","content":"\n![](1.jpg)\n\n## PV、PVC体系是不是多此一举？从本地持久化卷谈起\n\n在上一篇文章中，我为你详细讲解了 PV、PVC 持久化存储体系在 Kubernetes 项目中的设计和实现原理。而在文章最后的思考题中，我为你留下了这样一个讨论话题：像 PV、PVC 这样的用法，是不是有“过度设计”的嫌疑？\n\n比如，我们公司的运维人员可以像往常一样维护一套 NFS 或者 Ceph 服务器，根本不必学习 Kubernetes。而开发人员，则完全可以靠“复制粘贴”的方式，在 Pod 的 YAML 文件里填上 Volumes 字段，而不需要去使用 PV 和 PVC。\n\n实际上，如果只是为了职责划分，PV、PVC 体系确实不见得比直接在 Pod 里声明 Volumes 字段有什么优势。\n\n不过，你有没有想过这样一个问题，如果[Kubernetes 内置的 20 种持久化数据卷实现](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#types-of-persistent-volumes)，都没办法满足你的容器存储需求时，该怎么办？\n\n这个情况乍一听起来有点不可思议。但实际上，凡是鼓捣过开源项目的读者应该都有所体会，“不能用”“不好用”“需要定制开发”，这才是落地开源基础设施项目的三大常态。\n\n而在持久化存储领域，用户呼声最高的定制化需求，莫过于支持“本地”持久化存储了。\n\n也就是说，用户希望 Kubernetes 能够直接使用宿主机上的本地磁盘目录，而不依赖于远程存储服务，来提供“持久化”的容器 Volume。\n\n这样做的好处很明显，由于这个 Volume 直接使用的是本地磁盘，尤其是 SSD 盘，它的读写性能相比于大多数远程存储来说，要好得多。这个需求对本地物理服务器部署的私有 Kubernetes 集群来说，非常常见。\n\n所以，Kubernetes 在 v1.10 之后，就逐渐依靠 PV、PVC 体系实现了这个特性。这个特性的名字叫作：Local Persistent Volume。\n\n不过，首先需要明确的是，**Local Persistent Volume 并不适用于所有应用**。事实上，它的适用范围非常固定，比如：高优先级的系统应用，需要在多个不同节点上存储数据，并且对 I/O 较为敏感。典型的应用包括：分布式数据存储比如 MongoDB、Cassandra 等，分布式文件系统比如 GlusterFS、Ceph 等，以及需要在本地磁盘上进行大量数据缓存的分布式应用。\n\n其次，相比于正常的 PV，一旦这些节点宕机且不能恢复时，Local Persistent Volume 的数据就可能丢失。这就要求**使用 Local Persistent Volume 的应用必须具备数据备份和恢复的能力**，允许你把这些数据定时备份在其他位置。\n\n接下来，我就为你深入讲解一下这个特性。\n\n不难想象，`Local Persistent Volume 的设计，主要面临两个难点。`\n\n**第一个难点在于**：如何把本地磁盘抽象成 PV。\n\n可能你会说，Local Persistent Volume，不就等同于 hostPath 加 NodeAffinity 吗？\n\n比如，一个 Pod 可以声明使用类型为 Local 的 PV，而这个 PV 其实就是一个 hostPath 类型的 Volume。如果这个 hostPath 对应的目录，已经在节点 A 上被事先创建好了。那么，我只需要再给这个 Pod 加上一个 nodeAffinity=nodeA，不就可以使用这个 Volume 了吗？\n\n事实上，**`你绝不应该把一个宿主机上的目录当作 PV 使用`**。这是因为，这种本地目录的存储行为完全不可控，它所在的磁盘随时都可能被应用写满，甚至造成整个宿主机宕机。而且，不同的本地目录之间也缺乏哪怕最基础的 I/O 隔离机制。\n\n所以，一个 Local Persistent Volume 对应的存储介质，一定是一块额外挂载在宿主机的磁盘或者块设备（“额外”的意思是，它不应该是宿主机根目录所使用的主硬盘）。这个原则，我们可以称为**`“一个 PV 一块盘”`**。\n\n**第二个难点在于**：调度器如何保证 Pod 始终能被正确地调度到它所请求的 Local Persistent Volume 所在的节点上呢？\n\n造成这个问题的原因在于，对于常规的 PV 来说，Kubernetes 都是先调度 Pod 到某个节点上，然后，再通过“两阶段处理”来“持久化”这台机器上的 Volume 目录，进而完成 Volume 目录与容器的绑定挂载。\n\n可是，对于 Local PV 来说，节点上可供使用的磁盘（或者块设备），必须是运维人员提前准备好的。它们在不同节点上的挂载情况可以完全不同，甚至有的节点可以没这种磁盘。\n\n所以，这时候，调度器就必须能够知道所有节点与 Local Persistent Volume 对应的磁盘的关联关系，然后根据这个信息来调度 Pod。\n\n这个原则，我们可以称为**“在调度的时候考虑 Volume 分布”**。在 Kubernetes 的调度器里，有一个叫作 VolumeBindingChecker 的过滤条件专门负责这个事情。在 Kubernetes v1.11 中，这个过滤条件已经默认开启了。\n\n基于上述讲述，`在开始使用 Local Persistent Volume 之前，你首先需要在集群里配置好磁盘或者块设备`。在公有云上，这个操作等同于给虚拟机额外挂载一个磁盘，比如 GCE 的 Local SSD 类型的磁盘就是一个典型例子。\n\n而在我们部署的私有环境中，你有两种办法来完成这个步骤。\n\n * 第一种，当然就是给你的宿主机挂载并格式化一个可用的本地磁盘，这也是最常规的操作；\n\n * 第二种，对于实验环境，你其实可以在宿主机上挂载几个 RAM Disk（内存盘）来模拟本地磁盘。\n\n接下来，我会使用第二种方法，在我们之前部署的 Kubernetes 集群上进行实践。\n\n**首先**，在名叫 node-1 的宿主机上创建一个挂载点，比如 /mnt/disks；然后，用几个 RAM Disk 来模拟本地磁盘，如下所示：\n\n\t# 在node-1上执行\n\t$ mkdir /mnt/disks\n\t$ for vol in vol1 vol2 vol3; do\n\t    mkdir /mnt/disks/$vol\n\t    mount -t tmpfs $vol /mnt/disks/$vol\n\tdone\n需要注意的是，如果你希望其他节点也能支持 Local Persistent Volume 的话，那就需要为它们也执行上述操作，并且确保这些磁盘的名字（vol1、vol2 等）都不重复。\n\n`接下来，我们就可以为这些本地磁盘定义对应的 PV 了`，如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolume\n\tmetadata:\n\t  name: example-pv\n\tspec:\n\t  capacity:\n\t    storage: 5Gi\n\t  volumeMode: Filesystem\n\t  accessModes:\n\t  - ReadWriteOnce\n\t  persistentVolumeReclaimPolicy: Delete\n\t  storageClassName: local-storage\n\t  local:\n\t    path: /mnt/disks/vol1\n\t  nodeAffinity:\n\t    required:\n\t      nodeSelectorTerms:\n\t      - matchExpressions:\n\t        - key: kubernetes.io/hostname\n\t          operator: In\n\t          values:\n\t          - node-1\n可以看到，这个 PV 的定义里：local 字段，指定了它是一个 Local Persistent Volume；而 path 字段，指定的正是这个 PV 对应的本地磁盘的路径，即：/mnt/disks/vol1。\n\n当然了，这也就意味着如果 Pod 要想使用这个 PV，那它就必须运行在 node-1 上。所以，在这个 PV 的定义里，需要有一个 nodeAffinity 字段指定 node-1 这个节点的名字。这样，调度器在调度 Pod 的时候，就能够知道一个 PV 与节点的对应关系，从而做出正确的选择。**这正是 Kubernetes 实现“在调度的时候就考虑 Volume 分布”的主要方法。**\n\n**接下来**，我们就可以使用 kubect create 来创建这个 PV，如下所示：\n\n\t$ kubectl create -f local-pv.yaml \n\tpersistentvolume/example-pv created\n\t$ kubectl get pv\n\tNAME         CAPACITY   ACCESS MODES   RECLAIM POLICY  STATUS      CLAIM             STORAGECLASS    REASON    AGE\n\texample-pv   5Gi        RWO            Delete           Available                     local-storage             16s\n可以看到，这个 PV 创建后，进入了 Available（可用）状态。\n\n而正如我在上一篇文章里所建议的那样，使用 PV 和 PVC 的最佳实践，是你要创建一个 StorageClass 来描述这个 PV，如下所示：\n\n\tkind: StorageClass\n\tapiVersion: storage.k8s.io/v1\n\tmetadata:\n\t  name: local-storage\n\tprovisioner: kubernetes.io/no-provisioner\n\tvolumeBindingMode: WaitForFirstConsumer\n这个 StorageClass 的名字，叫作 local-storage。需要注意的是，在它的 provisioner 字段，我们指定的是 no-provisioner。这是因为 Local Persistent Volume 目前尚不支持 Dynamic Provisioning，所以它没办法在用户创建 PVC 的时候，就自动创建出对应的 PV。也就是说，我们前面创建 PV 的操作，是不可以省略的。\n\n与此同时，这个 StorageClass 还定义了一个 volumeBindingMode=WaitForFirstConsumer 的属性。它是 Local Persistent Volume 里一个非常重要的特性，即：**`延迟绑定`**。\n\n我们知道，当你提交了 PV 和 PVC 的 YAML 文件之后，Kubernetes 就会根据它们俩的属性，以及它们指定的 StorageClass 来进行绑定。只有绑定成功后，Pod 才能通过声明这个 PVC 来使用对应的 PV。\n\n可是，如果你使用的是 Local Persistent Volume 的话，就会发现，这个流程根本行不通。\n\n比如，现在你有一个 Pod，它声明使用的 PVC 叫作 pvc-1。并且，我们规定，这个 Pod 只能运行在 node-2 上。\n\n而在 Kubernetes 集群中，有两个属性（比如：大小、读写权限）相同的 Local 类型的 PV。\n\n其中，第一个 PV 的名字叫作 pv-1，它对应的磁盘所在的节点是 node-1。而第二个 PV 的名字叫作 pv-2，它对应的磁盘所在的节点是 node-2。\n\n假设现在，Kubernetes 的 Volume 控制循环里，首先检查到了 pvc-1 和 pv-1 的属性是匹配的，于是就将它们俩绑定在一起。\n\n然后，你用 kubectl create 创建了这个 Pod。\n\n这时候，问题就出现了。\n\n调度器看到，这个 Pod 所声明的 pvc-1 已经绑定了 pv-1，而 pv-1 所在的节点是 node-1，根据“调度器必须在调度的时候考虑 Volume 分布”的原则，这个 Pod 自然会被调度到 node-1 上。\n\n可是，我们前面已经规定过，这个 Pod 根本不允许运行在 node-1 上。所以。最后的结果就是，这个 Pod 的调度必然会失败。\n\n**这就是为什么，在使用 Local Persistent Volume 的时候，我们必须想办法推迟这个“绑定”操作。**\n\n那么，具体推迟到什么时候呢？\n\n**答案是：推迟到调度的时候。**\n\n所以说，StorageClass 里的 volumeBindingMode=WaitForFirstConsumer 的含义，就是告诉 Kubernetes 里的 Volume 控制循环（“红娘”）：虽然你已经发现这个 StorageClass 关联的 PVC 与 PV 可以绑定在一起，但请不要现在就执行绑定操作（即：设置 PVC 的 VolumeName 字段）。\n\n而要等到第一个声明使用该 PVC 的 Pod 出现在调度器之后，调度器再综合考虑所有的调度规则，当然也包括每个 PV 所在的节点位置，来统一决定，这个 Pod 声明的 PVC，到底应该跟哪个 PV 进行绑定。\n\n这样，在上面的例子里，由于这个 Pod 不允许运行在 pv-1 所在的节点 node-1，所以它的 PVC 最后会跟 pv-2 绑定，并且 Pod 也会被调度到 node-2 上。\n\n所以，通过这个延迟绑定机制，原本实时发生的 PVC 和 PV 的绑定过程，就被延迟到了 Pod 第一次调度的时候在调度器中进行，从而保证了这个**绑定结果不会影响 Pod 的正常调度。**\n\n当然，在具体实现中，调度器实际上维护了一个与 Volume Controller 类似的控制循环，专门负责为那些声明了“延迟绑定”的 PV 和 PVC 进行绑定工作。\n\n通过这样的设计，这个额外的绑定操作，并不会拖慢调度器的性能。而当一个 Pod 的 PVC 尚未完成绑定时，调度器也不会等待，而是会直接把这个 Pod 重新放回到待调度队列，等到下一个调度周期再做处理。\n\n在明白了这个机制之后，我们就可以创建 StorageClass 了，如下所示：\n\n\t$ kubectl create -f local-sc.yaml \n\tstorageclass.storage.k8s.io/local-storage created\n`接下来，我们只需要定义一个非常普通的 PVC，就可以让 Pod 使用到上面定义好的 Local Persistent Volume 了`，如下所示：\n\n\tkind: PersistentVolumeClaim\n\tapiVersion: v1\n\tmetadata:\n\t  name: example-local-claim\n\tspec:\n\t  accessModes:\n\t  - ReadWriteOnce\n\t  resources:\n\t    requests:\n\t      storage: 5Gi\n\t  storageClassName: local-storage\n可以看到，这个 PVC 没有任何特别的地方。唯一需要注意的是，它声明的 storageClassName 是 local-storage。所以，将来 Kubernetes 的 Volume Controller 看到这个 PVC 的时候，不会为它进行绑定操作。\n\n现在，我们来创建这个 PVC：\n\n\t$ kubectl create -f local-pvc.yaml \n\tpersistentvolumeclaim/example-local-claim created\n\t$ kubectl get pvc\n\tNAME                  STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS    AGE\n\texample-local-claim   Pending                                       local-storage   7s\n可以看到，尽管这个时候，Kubernetes 里已经存在了一个可以与 PVC 匹配的 PV，但这个 PVC 依然处于 Pending 状态，也就是等待绑定的状态。\n\n`然后，我们编写一个 Pod 来声明使用这个 PVC`，如下所示：\n\n\tkind: Pod\n\tapiVersion: v1\n\tmetadata:\n\t  name: example-pv-pod\n\tspec:\n\t  volumes:\n\t    - name: example-pv-storage\n\t      persistentVolumeClaim:\n\t       claimName: example-local-claim\n\t  containers:\n\t    - name: example-pv-container\n\t      image: nginx\n\t      ports:\n\t        - containerPort: 80\n\t          name: \"http-server\"\n\t      volumeMounts:\n\t        - mountPath: \"/usr/share/nginx/html\"\n\t          name: example-pv-storage\n这个 Pod 没有任何特别的地方，你只需要注意，它的 volumes 字段声明要使用前面定义的、名叫 example-local-claim 的 PVC 即可。\n\n而我们一旦使用 kubectl create 创建这个 Pod，就会发现，我们前面定义的 PVC，会立刻变成 Bound 状态，与前面定义的 PV 绑定在了一起，如下所示：\n\n\t$ kubectl create -f local-pod.yaml \n\tpod/example-pv-pod created\n\t$ kubectl get pvc\n\tNAME                  STATUS    VOLUME       CAPACITY   ACCESS MODES   STORAGECLASS    AGE\n\texample-local-claim   Bound     example-pv   5Gi        RWO            local-storage   6h\n也就是说，在我们创建的 Pod 进入调度器之后，“绑定”操作才开始进行。\n\n这时候，我们可以尝试在这个 Pod 的 Volume 目录里，创建一个测试文件，比如：\n\n\t$ kubectl exec -it example-pv-pod -- /bin/sh\n\t# cd /usr/share/nginx/html\n\t# touch test.txt\n然后，登录到 node-1 这台机器上，查看一下它的 /mnt/disks/vol1 目录下的内容，你就可以看到刚刚创建的这个文件：\n\n\t# 在node-1上\n\t$ ls /mnt/disks/vol1\n\ttest.txt\n而如果你重新创建这个 Pod 的话，就会发现，我们之前创建的测试文件，依然被保存在这个持久化 Volume 当中：\n\n\t$ kubectl delete -f local-pod.yaml \n\t$ kubectl create -f local-pod.yaml \n\t$ kubectl exec -it example-pv-pod -- /bin/sh\n\t# ls /usr/share/nginx/html\n\t# touch test.txt\n这就说明，像 Kubernetes 这样构建出来的、基于本地存储的 Volume，完全可以提供容器持久化存储的功能。所以，像 StatefulSet 这样的有状态编排工具，也完全可以通过声明 Local 类型的 PV 和 PVC，来管理应用的存储状态。\n\n**要注意的是，我们上面手动创建 PV 的方式，即 Static 的 PV 管理方式，在删除 PV 时需要按如下流程执行操作：**\n\n1. 删除使用这个 PV 的 Pod；\n2. 从宿主机移除本地磁盘（比如，umount 它）；\n3. 删除 PVC；\n4. 删除 PV。\n\n\n如果不按照这个流程的话，这个 PV 的删除就会失败。\n\n当然，由于上面这些创建 PV 和删除 PV 的操作比较繁琐，Kubernetes 其实提供了一个 Static Provisioner 来帮助你管理这些 PV。\n\n比如，我们现在的所有磁盘，都挂载在宿主机的 /mnt/disks 目录下。\n\n那么，当 Static Provisioner 启动后，它就会通过 DaemonSet，自动检查每个宿主机的 /mnt/disks 目录。然后，调用 Kubernetes API，为这些目录下面的每一个挂载，创建一个对应的 PV 对象出来。这些自动创建的 PV，如下所示：\n\n\t$ kubectl get pv\n\tNAME                CAPACITY    ACCESSMODES   RECLAIMPOLICY   STATUS      CLAIM     STORAGECLASS    REASON    AGE\n\tlocal-pv-ce05be60   1024220Ki   RWO           Delete          Available             local-storage             26s\n\t$ kubectl describe pv local-pv-ce05be60 \n\tName:  local-pv-ce05be60\n\t...\n\tStorageClass: local-storage\n\tStatus:  Available\n\tClaim:  \n\tReclaim Policy: Delete\n\tAccess Modes: RWO\n\tCapacity: 1024220Ki\n\tNodeAffinity:\n\t  Required Terms:\n\t      Term 0:  kubernetes.io/hostname in [node-1]\n\tMessage: \n\tSource:\n\t    Type: LocalVolume (a persistent volume backed by local storage on a node)\n\t    Path: /mnt/disks/vol1\n这个 PV 里的各种定义，比如 StorageClass 的名字、本地磁盘挂载点的位置，都可以通过 provisioner 的[配置文件指定](https://github.com/kubernetes-retired/external-storage/tree/master/local-volume/helm)。当然，provisioner 也会负责前面提到的 PV 的删除工作。\n\n而这个 provisioner 本身，其实也是一个我们前面提到过的[External Provisioner](https://github.com/kubernetes-incubator/external-storage/tree/master/local-volume)，它的部署方法，在[对应的文档里](https://github.com/kubernetes-retired/external-storage/tree/master/local-volume#option-1-using-the-local-volume-static-provisioner)有详细描述。这部分内容，就留给你课后自行探索了。\n\n**总结**\n\n在今天这篇文章中，我为你详细介绍了 Kubernetes 里 Local Persistent Volume 的实现方式。\n\n可以看到，正是通过 PV 和 PVC，以及 StorageClass 这套存储体系，这个后来新添加的持久化存储方案，对 Kubernetes 已有用户的影响，几乎可以忽略不计。作为用户，你的 Pod 的 YAML 和 PVC 的 YAML，并没有任何特殊的改变，这个特性所有的实现只会影响到 PV 的处理，也就是由运维人员负责的那部分工作。\n\n而这，正是这套存储体系带来的“解耦”的好处。\n\n其实，Kubernetes 很多看起来比较“繁琐”的设计（比如“声明式 API”，以及我今天讲解的“PV、PVC 体系”）的主要目的，都是希望为开发者提供更多的“可扩展性”，给使用者带来更多的“稳定性”和“安全感”。这两个能力的高低，是衡量开源基础设施项目水平的重要标准。\n\n**思考题**\n\n正是由于需要使用“延迟绑定”这个特性，Local Persistent Volume 目前还不能支持 Dynamic Provisioning。你是否能说出，为什么“延迟绑定”会跟 Dynamic Provisioning 有冲突呢？\n\n\n","categories":["reference","k8s"]},{"title":"30 | 编写自己的存储插件：FlexVolume与CSI","url":"/2021/03/13/reference/k8s/30.编写自己的存储插件：FlexVolume与CSI/","content":"\n![](1.jpg)\n\n## 编写自己的存储插件：FlexVolume与CSI\n\n在上一篇文章中，我为你详细介绍了 Kubernetes 里的持久化存储体系，讲解了 PV 和 PVC 的具体实现原理，并提到了这样的设计实际上是出于对整个存储体系的可扩展性的考虑。\n\n而在今天这篇文章中，我就和你分享一下如何借助这些机制，来开发自己的存储插件。\n\n在 Kubernetes 中，存储插件的开发有两种方式：FlexVolume 和 CSI。\n\n接下来，我就先为你剖析一下 `Flexvolume 的原理和使用方法`。\n\n举个例子，现在我们要编写的是一个使用 NFS 实现的 FlexVolume 插件。\n\n对于一个 FlexVolume 类型的 PV 来说，它的 YAML 文件如下所示：\n\n\tapiVersion: v1\n\tkind: PersistentVolume\n\tmetadata:\n\t  name: pv-flex-nfs\n\tspec:\n\t  capacity:\n\t    storage: 10Gi\n\t  accessModes:\n\t    - ReadWriteMany\n\t  flexVolume:\n\t    driver: \"k8s/nfs\"\n\t    fsType: \"nfs\"\n\t    options:\n\t      server: \"10.10.0.25\" # 改成你自己的NFS服务器地址\n\t      share: \"export\"\n可以看到，这个 PV 定义的 Volume 类型是 flexVolume。并且，我们指定了这个 Volume 的 driver 叫作 k8s/nfs。这个名字很重要，我后面马上会为你解释它的含义。\n\n而 Volume 的 options 字段，则是一个自定义字段。也就是说，它的类型，其实是 map[string]string。所以，你可以在这一部分自由地加上你想要定义的参数。\n\n在我们这个例子里，options 字段指定了 NFS 服务器的地址（server: “10.10.0.25”），以及 NFS 共享目录的名字（share: “export”）。当然，你这里定义的所有参数，后面都会被 FlexVolume 拿到。\n\n> 备注：你可以使用这个 [Docker 镜像](https://github.com/ehough/docker-nfs-server)轻松地部署一个试验用的 NFS 服务器。\n\n像这样的一个 PV 被创建后，一旦和某个 PVC 绑定起来，这个 FlexVolume 类型的 Volume 就会进入到我们前面讲解过的 Volume 处理流程。\n\n你应该还记得，这个流程的名字叫作“两阶段处理”，即“Attach 阶段”和“Mount 阶段”。它们的主要作用，是在 Pod 所绑定的宿主机上，完成这个 Volume 目录的持久化过程，比如为虚拟机挂载磁盘（Attach），或者挂载一个 NFS 的共享目录（Mount）。\n\n> 备注：你可以再回顾一下第 28 篇文章《PV、PVC、StorageClass，这些到底在说啥？》中的相关内容。\n\n而在具体的控制循环中，这两个操作实际上调用的，正是 Kubernetes 的 pkg/volume 目录下的存储插件（Volume Plugin）。在我们这个例子里，就是 pkg/volume/flexvolume 这个目录里的代码。\n\n当然了，这个目录其实只是 FlexVolume 插件的入口。以“Mount 阶段”为例，在 FlexVolume 目录里，它的处理过程非常简单，如下所示：\n\n\t// SetUpAt creates new directory.\n\tfunc (f *flexVolumeMounter) SetUpAt(dir string, fsGroup *int64) error {\n\t  ...\n\t  call := f.plugin.NewDriverCall(mountCmd)\n\t  \n\t  // Interface parameters\n\t  call.Append(dir)\n\t  \n\t  extraOptions := make(map[string]string)\n\t  \n\t  // pod metadata\n\t  extraOptions[optionKeyPodName] = f.podName\n\t  extraOptions[optionKeyPodNamespace] = f.podNamespace\n\t  \n\t  ...\n\t  \n\t  call.AppendSpec(f.spec, f.plugin.host, extraOptions)\n\t  \n\t  _, err = call.Run()\n\t  \n\t  ...\n\t  \n\t  return nil\n\t}\n上面这个名叫 SetUpAt() 的方法，正是 FlexVolume 插件对“Mount 阶段”的实现位置。而 SetUpAt() 实际上只做了一件事，那就是封装出了一行命令（即：NewDriverCall），由 kubelet 在“Mount 阶段”去执行。\n\n在我们这个例子中，**kubelet 要通过插件在宿主机上执行的命令，如下所示：**\n\n\t/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount <mount dir> <json param>\n其中，/usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs 就是插件的可执行文件的路径。这个名叫 nfs 的文件，正是你要编写的插件的实现。它可以是一个二进制文件，也可以是一个脚本。总之，只要能在宿主机上被执行起来即可。\n\n而且这个路径里的 k8s~nfs 部分，正是这个插件在 Kubernetes 里的名字。它是从 driver=\"k8s/nfs\"字段解析出来的。\n\n这个 driver 字段的格式是：vendor/driver。比如，一家存储插件的提供商（vendor）的名字叫作 k8s，提供的存储驱动（driver）是 nfs，那么 Kubernetes 就会使用 k8s~nfs 来作为插件名。\n\n所以说，**当你编写完了 FlexVolume 的实现之后，一定要把它的可执行文件放在每个节点的插件目录下。**\n\n而紧跟在可执行文件后面的“mount”参数，定义的就是当前的操作。在 FlexVolume 里，这些操作参数的名字是固定的，比如 init、mount、unmount、attach，以及 dettach 等等，分别对应不同的 Volume 处理操作。\n\n而跟在 mount 参数后面的两个字段：<mount dir>和<json params>，则是 FlexVolume 必须提供给这条命令的两个执行参数。\n\n其中第一个执行参数<mount dir>，正是 kubelet 调用 SetUpAt() 方法传递来的 dir 的值。它代表的是当前正在处理的 Volume 在宿主机上的目录。在我们的例子里，这个路径如下所示：\n\n\t/var/lib/kubelet/pods/<Pod ID>/volumes/k8s~nfs/test\n其中，test 正是我们前面定义的 PV 的名字；而 k8s~nfs，则是插件的名字。可以看到，插件的名字正是从你声明的 driver=\"k8s/nfs\"字段里解析出来的。\n\n而第二个执行参数<json params>，则是一个 JSON Map 格式的参数列表。我们在前面 PV 里定义的 options 字段的值，都会被追加在这个参数里。此外，在 SetUpAt() 方法里可以看到，这个参数列表里还包括了 Pod 的名字、Namespace 等元数据（Metadata）。\n\n在明白了存储插件的调用方式和参数列表之后，`这个插件的可执行文件的实现部分`就非常容易理解了。\n\n在这个例子中，我直接编写了一个简单的 shell 脚本来作为插件的实现，它对“Mount 阶段”的处理过程，如下所示：\n\n\tdomount() {\n\t MNTPATH=$1\n\t \n\t NFS_SERVER=$(echo $2 | jq -r '.server')\n\t SHARE=$(echo $2 | jq -r '.share')\n\t \n\t ...\n\t \n\t mkdir -p ${MNTPATH} &> /dev/null\n\t \n\t mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} &> /dev/null\n\t if [ $? -ne 0 ]; then\n\t  err \"{ \\\"status\\\": \\\"Failure\\\", \\\"message\\\": \\\"Failed to mount ${NFS_SERVER}:${SHARE} at ${MNTPATH}\\\"}\"\n\t  exit 1\n\t fi\n\t log '{\"status\": \"Success\"}'\n\t exit 0\n\t}\n可以看到，当 kubelet 在宿主机上执行“nfs mount <mount dir> <json params>”的时候，这个名叫 nfs 的脚本，就可以直接从<mount dir>参数里拿到 Volume 在宿主机上的目录，即：MNTPATH=$1。而你在 PV 的 options 字段里定义的 NFS 的服务器地址（options.server）和共享目录名字（options.share），则可以从第二个<json params>参数里解析出来。这里，我们使用了 jq 命令，来进行解析工作。\n\n有了这三个参数之后，这个脚本最关键的一步，当然就是执行：mount -t nfs ${NFS_SERVER}:/${SHARE} ${MNTPATH} 。这样，一个 NFS 的数据卷就被挂载到了 MNTPATH，也就是 Volume 所在的宿主机目录上，一个持久化的 Volume 目录就处理完了。\n\n需要注意的是，当这个 mount -t nfs 操作完成后，你必须把一个 JOSN 格式的字符串，比如：{“status”: “Success”}，返回给调用者，也就是 kubelet。这是 kubelet 判断这次调用是否成功的唯一依据。\n\n综上所述，在“Mount 阶段”，kubelet 的 VolumeManagerReconcile 控制循环里的一次“调谐”操作的执行流程，如下所示：\n\n\tkubelet --> pkg/volume/flexvolume.SetUpAt() --> /usr/libexec/kubernetes/kubelet-plugins/volume/exec/k8s~nfs/nfs mount <mount dir> <json param>\n\n> 备注：这个 NFS 的 FlexVolume 的完整实现，在[这个 GitHub 库](https://github.com/kubernetes/examples/blob/master/staging/volumes/flexvolume/nfs)里。而你如果想用 Go 语言编写 FlexVolume 的话，我也有一个[很好的例子](https://github.com/kubernetes/frakti/tree/master/pkg/flexvolume)供你参考。\n\n当然，在前面文章中我也提到过，像 NFS 这样的文件系统存储，并不需要在宿主机上挂载磁盘或者块设备。所以，我们也就不需要实现 attach 和 dettach 操作了。\n\n不过，**像这样的 FlexVolume 实现方式，虽然简单，但局限性却很大。**\n\n比如，跟 Kubernetes 内置的 NFS 插件类似，这个 NFS FlexVolume 插件，也不能支持 Dynamic Provisioning（即：为每个 PVC 自动创建 PV 和对应的 Volume）。除非你再为它编写一个专门的 External Provisioner。\n\n再比如，我的插件在执行 mount 操作的时候，可能会生成一些挂载信息。这些信息，在后面执行 unmount 操作的时候会被用到。可是，在上述 FlexVolume 的实现里，你没办法把这些信息保存在一个变量里，等到 unmount 的时候直接使用。\n\n这个原因也很容易理解：**FlexVolume 每一次对插件可执行文件的调用，都是一次完全独立的操作**。所以，我们只能把这些信息写在一个宿主机上的临时文件里，等到 unmount 的时候再去读取。\n\n这也是为什么，我们需要有 Container Storage Interface（CSI）这样更完善、更编程友好的插件方式。\n\n接下来，我就来为你讲解一下开发存储插件的第二种方式 CSI。我们先来看一下**`CSI 插件体系的设计原理。`**\n\n其实，通过前面对 FlexVolume 的讲述，你应该可以明白，默认情况下，Kubernetes 里通过存储插件管理容器持久化存储的原理，可以用如下所示的示意图来描述：\n\n![](2.png)\n\n可以看到，在上述体系下，无论是 FlexVolume，还是 Kubernetes 内置的其他存储插件，它们实际上担任的角色，仅仅是 Volume 管理中的“Attach 阶段”和“Mount 阶段”的具体执行者。而像 Dynamic Provisioning 这样的功能，就不是存储插件的责任，而是 Kubernetes 本身存储管理功能的一部分。\n\n相比之下，**`CSI 插件体系的设计思想，就是把这个 Provision 阶段，以及 Kubernetes 里的一部分存储管理功能，从主干代码里剥离出来，做成了几个单独的组件。`**这些组件会通过 Watch API 监听 Kubernetes 里与存储相关的事件变化，比如 PVC 的创建，来执行具体的存储管理动作。\n\n而这些管理动作，比如“Attach 阶段”和“Mount 阶段”的具体操作，实际上就是通过调用 CSI 插件来完成的。\n\n这种设计思路，我可以用如下所示的一幅示意图来表示：\n\n![](3.png)\n\n可以看到，这套存储插件体系多了三个独立的外部组件（External Components），即：Driver Registrar、External Provisioner 和 External Attacher，对应的正是从 Kubernetes 项目里面剥离出来的那部分存储管理功能。\n\n需要注意的是，External Components 虽然是外部组件，但依然由 Kubernetes 社区来开发和维护。\n\n而图中最右侧的部分，就是需要我们编写代码来实现的 CSI 插件。一个 CSI 插件只有一个二进制文件，但它会以 gRPC 的方式对外提供三个服务（gRPC Service），分别叫作：CSI Identity、CSI Controller 和 CSI Node。\n\n**`我先来为你讲解一下这三个 External Components。`**\n\n其中，**Driver Registrar 组件，负责将插件注册到 kubelet 里面**（这可以类比为，将可执行文件放在插件目录下）。而在具体实现上，Driver Registrar 需要请求 CSI 插件的 Identity 服务来获取插件信息。\n\n而 **External Provisioner 组件，负责的正是 Provision 阶段。**在具体实现上，External Provisioner 监听（Watch）了 APIServer 里的 PVC 对象。当一个 PVC 被创建时，它就会调用 CSI Controller 的 CreateVolume 方法，为你创建对应 PV。\n\n此外，如果你使用的存储是公有云提供的磁盘（或者块设备）的话，这一步就需要调用公有云（或者块设备服务）的 API 来创建这个 PV 所描述的磁盘（或者块设备）了。\n\n不过，由于 CSI 插件是独立于 Kubernetes 之外的，所以在 CSI 的 API 里不会直接使用 Kubernetes 定义的 PV 类型，而是会自己定义一个单独的 Volume 类型。\n\n**为了方便叙述，在本专栏里，我会把 Kubernetes 里的持久化卷类型叫作 PV，把 CSI 里的持久化卷类型叫作 CSI Volume，请你务必区分清楚。**\n\n最后一个 **External Attacher 组件，负责的正是“Attach 阶段”。**在具体实现上，它监听了 APIServer 里 VolumeAttachment 对象的变化。VolumeAttachment 对象是 Kubernetes 确认一个 Volume 可以进入“Attach 阶段”的重要标志，我会在下一篇文章里为你详细讲解。\n\n一旦出现了 VolumeAttachment 对象，External Attacher 就会调用 CSI Controller 服务的 ControllerPublish 方法，完成它所对应的 Volume 的 Attach 阶段。\n\n而 Volume 的“Mount 阶段”，并不属于 External Components 的职责。当 kubelet 的 VolumeManagerReconciler 控制循环检查到它需要执行 Mount 操作的时候，会通过 pkg/volume/csi 包，直接调用 CSI Node 服务完成 Volume 的“Mount 阶段”。\n\n在实际使用 CSI 插件的时候，我们会将这三个 External Components 作为 sidecar 容器和 CSI 插件放置在同一个 Pod 中。由于 External Components 对 CSI 插件的调用非常频繁，所以这种 sidecar 的部署方式非常高效。\n\n接下来，**`我再为你讲解一下 CSI 插件的里三个服务：CSI Identity、CSI Controller 和 CSI Node。`**\n\n其中，**CSI 插件的 CSI Identity 服务，负责对外暴露这个插件本身的信息**，如下所示：\n\n\tservice Identity {\n\t  // return the version and name of the plugin\n\t  rpc GetPluginInfo(GetPluginInfoRequest)\n\t    returns (GetPluginInfoResponse) {}\n\t  // reports whether the plugin has the ability of serving the Controller interface\n\t  rpc GetPluginCapabilities(GetPluginCapabilitiesRequest)\n\t    returns (GetPluginCapabilitiesResponse) {}\n\t  // called by the CO just to check whether the plugin is running or not\n\t  rpc Probe (ProbeRequest)\n\t    returns (ProbeResponse) {}\n\t}\n而 **CSI Controller 服务，定义的则是对 CSI Volume（对应 Kubernetes 里的 PV）的管理接口**，比如：创建和删除 CSI Volume、对 CSI Volume 进行 Attach/Dettach（在 CSI 里，这个操作被叫作 Publish/Unpublish），以及对 CSI Volume 进行 Snapshot 等，它们的接口定义如下所示：\n\n\tservice Controller {\n\t  // provisions a volume\n\t  rpc CreateVolume (CreateVolumeRequest)\n\t    returns (CreateVolumeResponse) {}\n\t    \n\t  // deletes a previously provisioned volume\n\t  rpc DeleteVolume (DeleteVolumeRequest)\n\t    returns (DeleteVolumeResponse) {}\n\t    \n\t  // make a volume available on some required node\n\t  rpc ControllerPublishVolume (ControllerPublishVolumeRequest)\n\t    returns (ControllerPublishVolumeResponse) {}\n\t    \n\t  // make a volume un-available on some required node\n\t  rpc ControllerUnpublishVolume (ControllerUnpublishVolumeRequest)\n\t    returns (ControllerUnpublishVolumeResponse) {}\n\t    \n\t  ...\n\t  \n\t  // make a snapshot\n\t  rpc CreateSnapshot (CreateSnapshotRequest)\n\t    returns (CreateSnapshotResponse) {}\n\t    \n\t  // Delete a given snapshot\n\t  rpc DeleteSnapshot (DeleteSnapshotRequest)\n\t    returns (DeleteSnapshotResponse) {}\n\t    \n\t  ...\n\t}\n不难发现，CSI Controller 服务里定义的这些操作有个共同特点，那就是它们都无需在宿主机上进行，而是属于 Kubernetes 里 Volume Controller 的逻辑，也就是属于 Master 节点的一部分。\n\n需要注意的是，正如我在前面提到的那样，CSI Controller 服务的实际调用者，并不是 Kubernetes（即：通过 pkg/volume/csi 发起 CSI 请求），而是 External Provisioner 和 External Attacher。这两个 External Components，分别通过监听 PVC 和 VolumeAttachement 对象，来跟 Kubernetes 进行协作。\n\n而 CSI Volume 需要在宿主机上执行的操作，都定义在了 CSI Node 服务里面，如下所示：\n\n\tservice Node {\n\t  // temporarily mount the volume to a staging path\n\t  rpc NodeStageVolume (NodeStageVolumeRequest)\n\t    returns (NodeStageVolumeResponse) {}\n\t    \n\t  // unmount the volume from staging path\n\t  rpc NodeUnstageVolume (NodeUnstageVolumeRequest)\n\t    returns (NodeUnstageVolumeResponse) {}\n\t    \n\t  // mount the volume from staging to target path\n\t  rpc NodePublishVolume (NodePublishVolumeRequest)\n\t    returns (NodePublishVolumeResponse) {}\n\t    \n\t  // unmount the volume from staging path\n\t  rpc NodeUnpublishVolume (NodeUnpublishVolumeRequest)\n\t    returns (NodeUnpublishVolumeResponse) {}\n\t    \n\t  // stats for the volume\n\t  rpc NodeGetVolumeStats (NodeGetVolumeStatsRequest)\n\t    returns (NodeGetVolumeStatsResponse) {}\n\t    \n\t  ...\n\t  \n\t  // Similar to NodeGetId\n\t  rpc NodeGetInfo (NodeGetInfoRequest)\n\t    returns (NodeGetInfoResponse) {}\n\t}\n需要注意的是，“Mount 阶段”在 CSI Node 里的接口，是由 NodeStageVolume 和 NodePublishVolume 两个接口共同实现的。我会在下一篇文章中，为你详细介绍这个设计的目的和具体的实现方式。\n\n**总结**\n\n在本篇文章里，我为你详细讲解了 FlexVolume 和 CSI 这两种自定义存储插件的工作原理。\n\n可以看到，相比于 FlexVolume，CSI 的设计思想，把插件的职责从“两阶段处理”，扩展成了 Provision、Attach 和 Mount 三个阶段。其中，Provision 等价于“创建磁盘”，Attach 等价于“挂载磁盘到虚拟机”，Mount 等价于“将该磁盘格式化后，挂载在 Volume 的宿主机目录上”。\n\n在有了 CSI 插件之后，Kubernetes 本身依然按照我在第 28 篇文章《PV、PVC、StorageClass，这些到底在说啥？》中所讲述的方式工作，唯一区别在于：\n\n * 当 AttachDetachController 需要进行“Attach”操作时（“Attach 阶段”），它实际上会执行到 pkg/volume/csi 目录中，创建一个 VolumeAttachment 对象，从而触发 External Attacher 调用 CSI Controller 服务的 ControllerPublishVolume 方法。\n\n * 当 VolumeManagerReconciler 需要进行“Mount”操作时（“Mount 阶段”），它实际上也会执行到 pkg/volume/csi 目录中，直接向 CSI Node 服务发起调用 NodePublishVolume 方法的请求。\n\n以上，就是 CSI 插件最基本的工作原理了。\n\n在下一篇文章里，我会和你一起实践一个 CSI 存储插件的完整实现过程。\n\n**思考题**\n\n假设现在，你的宿主机是阿里云的一台虚拟机，你要实现的容器持久化存储，是基于阿里云提供的云盘。你能准确地描述出，在 Provision、Attach 和 Mount 阶段，CSI 插件都需要做哪些操作吗？","categories":["reference","k8s"]},{"title":"25 | 深入解析声明式API（二）：编写自定义控制器","url":"/2021/03/13/reference/k8s/25.深入解析声明式API(二)：编写自定义控制器/","content":"\n![](1.jpg)\n\n在上一篇文章中，我和你详细分享了 Kubernetes 中声明式 API 的实现原理，并且通过一个添加 Network 对象的实例，为你讲述了在 Kubernetes 里添加 API 资源的过程。\n\n在今天的这篇文章中，我就继续和你一起完成剩下一半的工作，即：为 Network 这个自定义 API 对象编写一个自定义控制器（Custom Controller）。\n\n正如我在上一篇文章结尾处提到的，“声明式 API”并不像“命令式 API”那样有着明显的执行逻辑。这就使得基于声明式 API 的业务功能实现，往往需要通过控制器模式来“监视”API 对象的变化（比如，创建或者删除 Network），然后以此来决定实际要执行的具体工作。\n\n接下来，我就和你一起通过编写代码来实现这个过程。这个项目和上一篇文章里的代码是同一个项目，你可以从[这个 GitHub 库](https://github.com/resouer/k8s-controller-custom-resource)里找到它们。我在代码里还加上了丰富的注释，你可以随时参考。\n\n总得来说，编写自定义控制器代码的过程包括：编写 main 函数、编写自定义控制器的定义，以及编写控制器里的业务逻辑三个部分。\n\n首先，我们来编写这 * 个自定义控制器的 main 函数。\n\nmain 函数的主要工作就是，定义并初始化一个自定义控制器（Custom Controller），然后启动它。这部分代码的主要内容如下所示：\n\n\tfunc main() {\n\t  ...\n\t  \n\t  cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig)\n\t  ...\n\t  kubeClient, err := kubernetes.NewForConfig(cfg)\n\t  ...\n\t  networkClient, err := clientset.NewForConfig(cfg)\n\t  ...\n\t  \n\t  networkInformerFactory := informers.NewSharedInformerFactory(networkClient, ...)\n\t  \n\t  controller := NewController(kubeClient, networkClient,\n\t  networkInformerFactory.Samplecrd().V1().Networks())\n\t  \n\t  go networkInformerFactory.Start(stopCh)\n\t \n\t  if err = controller.Run(2, stopCh); err != nil {\n\t    glog.Fatalf(\"Error running controller: %s\", err.Error())\n\t  }\n\t}\n可以看到，这个 main 函数主要通过三步完成了初始化并启动一个自定义控制器的工作。\n\n第一步：main 函数根据我提供的 Master 配置（APIServer 的地址端口和 kubeconfig 的路径），创建一个 Kubernetes 的 client（kubeClient）和 Network 对象的 client（networkClient）。\n\n但是，如果我没有提供 Master 配置呢？\n\n这时，main 函数会直接使用一种名叫InClusterConfig的方式来创建这个 client。这个方式，会假设你的自定义控制器是以 Pod 的方式运行在 Kubernetes 集群里的。\n\n而我在第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中曾经提到过，Kubernetes 里所有的 Pod 都会以 Volume 的方式自动挂载 Kubernetes 的默认 ServiceAccount。所以，这个控制器就会直接使用默认 ServiceAccount 数据卷里的授权信息，来访问 APIServer。\n\n第二步：main 函数为 Network 对象创建一个叫作 InformerFactory（即：networkInformerFactory）的工厂，并使用它生成一个 Network 对象的 Informer，传递给控制器。\n\n第三步：main 函数启动上述的 Informer，然后执行 controller.Run，启动自定义控制器。\n\n至此，main 函数就结束了。\n\n看到这，你可能会感到非常困惑：编写自定义控制器的过程难道就这么简单吗？这个 Informer 又是个什么东西呢？\n\n别着急。\n\n接下来，我就为你详细解释一下这个自定义控制器的工作原理。\n\n在 Kubernetes 项目中，一个自定义控制器的工作原理，可以用下面这样一幅流程图来表示（在后面的叙述中，我会用“示意图”来指代它）：\n\n![](2.png)\n\n图 1 自定义控制器的工作流程示意图\n\n我们先从这幅示意图的最左边看起。\n\n这个控制器要做的第一件事，是从 Kubernetes 的 APIServer 里获取它所关心的对象，也就是我定义的 Network 对象。\n\n这个操作，依靠的是一个叫作 Informer（可以翻译为：通知器）的代码库完成的。Informer 与 API 对象是一一对应的，所以我传递给自定义控制器的，正是一个 Network 对象的 Informer（Network Informer）。\n\n不知你是否已经注意到，我在创建这个 Informer 工厂的时候，需要给它传递一个 networkClient。\n\n事实上，Network Informer 正是使用这个 networkClient，跟 APIServer 建立了连接。不过，真正负责维护这个连接的，则是 Informer 所使用的 Reflector 包。\n\n更具体地说，Reflector 使用的是一种叫作ListAndWatch的方法，来“获取”并“监听”这些 Network 对象实例的变化。\n\n在 ListAndWatch 机制下，一旦 APIServer 端有新的 Network 实例被创建、删除或者更新，Reflector 都会收到“事件通知”。这时，该事件及它对应的 API 对象这个组合，就被称为增量（Delta），它会被放进一个 Delta FIFO Queue（即：增量先进先出队列）中。\n\n而另一方面，Informe 会不断地从这个 Delta FIFO Queue 里读取（Pop）增量。每拿到一个增量，Informer 就会判断这个增量里的事件类型，然后创建或者更新本地对象的缓存。这个缓存，在 Kubernetes 里一般被叫作 Store。\n\n比如，如果事件类型是 Added（添加对象），那么 Informer 就会通过一个叫作 Indexer 的库把这个增量里的 API 对象保存在本地缓存中，并为它创建索引。相反地，如果增量的事件类型是 Deleted（删除对象），那么 Informer 就会从本地缓存中删除这个对象。\n\n这个同步本地缓存的工作，是 Informer 的第一个职责，也是它最重要的职责。\n\n而Informer 的第二个职责，则是根据这些事件的类型，触发事先注册好的 ResourceEventHandler。这些 Handler，需要在创建控制器的时候注册给它对应的 Informer。\n\n接下来，我们就来编写这个控制器的定义，它的主要内容如下所示：\n\n\tfunc NewController(\n\t  kubeclientset kubernetes.Interface,\n\t  networkclientset clientset.Interface,\n\t  networkInformer informers.NetworkInformer) *Controller {\n\t  ...\n\t  controller := &Controller{\n\t    kubeclientset:    kubeclientset,\n\t    networkclientset: networkclientset,\n\t    networksLister:   networkInformer.Lister(),\n\t    networksSynced:   networkInformer.Informer().HasSynced,\n\t    workqueue:        workqueue.NewNamedRateLimitingQueue(...,  \"Networks\"),\n\t    ...\n\t  }\n\t    networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{\n\t    AddFunc: controller.enqueueNetwork,\n\t    UpdateFunc: func(old, new interface{}) {\n\t      oldNetwork := old.(*samplecrdv1.Network)\n\t      newNetwork := new.(*samplecrdv1.Network)\n\t      if oldNetwork.ResourceVersion == newNetwork.ResourceVersion {\n\t        return\n\t      }\n\t      controller.enqueueNetwork(new)\n\t    },\n\t    DeleteFunc: controller.enqueueNetworkForDelete,\n\t return controller\n\t}\n我前面在 main 函数里创建了两个 client（kubeclientset 和 networkclientset），然后在这段代码里，使用这两个 client 和前面创建的 Informer，初始化了自定义控制器。\n\n值得注意的是，在这个自定义控制器里，我还设置了一个工作队列（work queue），它正是处于示意图中间位置的 WorkQueue。这个工作队列的作用是，负责同步 Informer 和控制循环之间的数据。\n\n> 实际上，Kubernetes 项目为我们提供了很多个工作队列的实现，你可以根据需要选择合适的库直接使用。\n\n然后，我为 networkInformer 注册了三个 Handler（AddFunc、UpdateFunc 和 DeleteFunc），分别对应 API 对象的“添加”“更新”和“删除”事件。而具体的处理操作，都是将该事件对应的 API 对象加入到工作队列中。\n\n需要注意的是，实际入队的并不是 API 对象本身，而是它们的 Key，即：该 API 对象的/。\n\n而我们后面即将编写的控制循环，则会不断地从这个工作队列里拿到这些 Key，然后开始执行真正的控制逻辑。\n\n综合上面的讲述，你现在应该就能明白，所谓 Informer，其实就是一个带有本地缓存和索引机制的、可以注册 EventHandler 的 client。它是自定义控制器跟 APIServer 进行数据同步的重要组件。\n\n更具体地说，Informer 通过一种叫作 ListAndWatch 的方法，把 APIServer 中的 API 对象缓存在了本地，并负责更新和维护这个缓存。\n\n其中，ListAndWatch 方法的含义是：首先，通过 APIServer 的 LIST API“获取”所有最新版本的 API 对象；然后，再通过 WATCH API 来“监听”所有这些 API 对象的变化。\n\n而通过监听到的事件变化，Informer 就可以实时地更新本地缓存，并且调用这些事件对应的 EventHandler 了。\n\n此外，在这个过程中，每经过 resyncPeriod 指定的时间，Informer 维护的本地缓存，都会使用最近一次 LIST 返回的结果强制更新一次，从而保证缓存的有效性。在 Kubernetes 中，这个缓存强制更新的操作就叫作：resync。\n\n需要注意的是，这个定时 resync 操作，也会触发 Informer 注册的“更新”事件。但此时，这个“更新”事件对应的 Network 对象实际上并没有发生变化，即：新、旧两个 Network 对象的 ResourceVersion 是一样的。在这种情况下，Informer 就不需要对这个更新事件再做进一步的处理了。\n\n这也是为什么我在上面的 UpdateFunc 方法里，先判断了一下新、旧两个 Network 对象的版本（ResourceVersion）是否发生了变化，然后才开始进行的入队操作。\n\n以上，就是 Kubernetes 中的 Informer 库的工作原理了。\n\n接下来，我们就来到了示意图中最后面的控制循环（Control Loop）部分，也正是我在 main 函数最后调用 controller.Run() 启动的“控制循环”。它的主要内容如下所示：\n\n\tfunc (c *Controller) Run(threadiness int, stopCh <-chan struct{}) error {\n\t ...\n\t  if ok := cache.WaitForCacheSync(stopCh, c.networksSynced); !ok {\n\t    return fmt.Errorf(\"failed to wait for caches to sync\")\n\t  }\n\t  \n\t  ...\n\t  for i := 0; i < threadiness; i++ {\n\t    go wait.Until(c.runWorker, time.Second, stopCh)\n\t  }\n\t  \n\t  ...\n\t  return nil\n\t}\n可以看到，启动控制循环的逻辑非常简单：\n\n * 首先，等待 Informer 完成一次本地缓存的数据同步操作；\n * 然后，直接通过 goroutine 启动一个（或者并发启动多个）“无限循环”的任务。\n而这个“无限循环”任务的每一个循环周期，执行的正是我们真正关心的业务逻辑。\n\n所以接下来，我们就来编写这个自定义控制器的业务逻辑，它的主要内容如下所示：\n\n\tfunc (c *Controller) runWorker() {\n\t  for c.processNextWorkItem() {\n\t  }\n\t}\n\t \n\tfunc (c *Controller) processNextWorkItem() bool {\n\t  obj, shutdown := c.workqueue.Get()\n\t  \n\t  ...\n\t  \n\t  err := func(obj interface{}) error {\n\t    ...\n\t    if err := c.syncHandler(key); err != nil {\n\t     return fmt.Errorf(\"error syncing '%s': %s\", key, err.Error())\n\t    }\n\t    \n\t    c.workqueue.Forget(obj)\n\t    ...\n\t    return nil\n\t  }(obj)\n\t  \n\t  ...\n\t  \n\t  return true\n\t}\n\t \n\tfunc (c *Controller) syncHandler(key string) error {\n\t \n\t  namespace, name, err := cache.SplitMetaNamespaceKey(key)\n\t  ...\n\t  \n\t  network, err := c.networksLister.Networks(namespace).Get(name)\n\t  if err != nil {\n\t    if errors.IsNotFound(err) {\n\t      glog.Warningf(\"Network does not exist in local cache: %s/%s, will delete it from Neutron ...\",\n\t      namespace, name)\n\t      \n\t      glog.Warningf(\"Network: %s/%s does not exist in local cache, will delete it from Neutron ...\",\n\t    namespace, name)\n\t    \n\t     // FIX ME: call Neutron API to delete this network by name.\n\t     //\n\t     // neutron.Delete(namespace, name)\n\t     \n\t     return nil\n\t  }\n\t    ...\n\t    \n\t    return err\n\t  }\n\t  \n\t  glog.Infof(\"[Neutron] Try to process network: %#v ...\", network)\n\t  \n\t  // FIX ME: Do diff().\n\t  //\n\t  // actualNetwork, exists := neutron.Get(namespace, name)\n\t  //\n\t  // if !exists {\n\t  //   neutron.Create(namespace, name)\n\t  // } else if !reflect.DeepEqual(actualNetwork, network) {\n\t  //   neutron.Update(namespace, name)\n\t  // }\n\t  \n\t  return nil\n\t}\n可以看到，在这个执行周期里（processNextWorkItem），我们首先从工作队列里出队（workqueue.Get）了一个成员，也就是一个 Key（Network 对象的：namespace/name）。\n\n然后，在 syncHandler 方法中，我使用这个 Key，尝试从 Informer 维护的缓存中拿到了它所对应的 Network 对象。\n\n可以看到，在这里，我使用了 networksLister 来尝试获取这个 Key 对应的 Network 对象。这个操作，其实就是在访问本地缓存的索引。实际上，在 Kubernetes 的源码中，你会经常看到控制器从各种 Lister 里获取对象，比如：podLister、nodeLister 等等，它们使用的都是 Informer 和缓存机制。\n\n而如果控制循环从缓存中拿不到这个对象（即：networkLister 返回了 IsNotFound 错误），那就意味着这个 Network 对象的 Key 是通过前面的“删除”事件添加进工作队列的。所以，尽管队列里有这个 Key，但是对应的 Network 对象已经被删除了。\n\n这时候，我就需要调用 Neutron 的 API，把这个 Key 对应的 Neutron 网络从真实的集群里删除掉。\n\n而如果能够获取到对应的 Network 对象，我就可以执行控制器模式里的对比“期望状态”和“实际状态”的逻辑了。\n\n其中，自定义控制器“千辛万苦”拿到的这个 Network 对象，正是 APIServer 里保存的“期望状态”，即：用户通过 YAML 文件提交到 APIServer 里的信息。当然，在我们的例子里，它已经被 Informer 缓存在了本地。\n\n那么，“实际状态”又从哪里来呢？\n\n当然是来自于实际的集群了。\n\n所以，我们的控制循环需要通过 Neutron API 来查询实际的网络情况。\n\n比如，我可以先通过 Neutron 来查询这个 Network 对象对应的真实网络是否存在。\n\n如果不存在，这就是一个典型的“期望状态”与“实际状态”不一致的情形。这时，我就需要使用这个 Network 对象里的信息（比如：CIDR 和 Gateway），调用 Neutron API 来创建真实的网络。\n如果存在，那么，我就要读取这个真实网络的信息，判断它是否跟 Network 对象里的信息一致，从而决定我是否要通过 Neutron 来更新这个已经存在的真实网络。\n这样，我就通过对比“期望状态”和“实际状态”的差异，完成了一次调协（Reconcile）的过程。\n\n至此，一个完整的自定义 API 对象和它所对应的自定义控制器，就编写完毕了。\n\n> 备注：与 Neutron 相关的业务代码并不是本篇文章的重点，所以我仅仅通过注释里的伪代码为你表述了这部分内容。如果你对这些代码感兴趣的话，可以自行完成。最简单的情况，你可以自己编写一个 Neutron Mock，然后输出对应的操作日志。\n\n接下来，我们就一起来把这个项目运行起来，查看一下它的工作情况。\n\n你可以自己编译这个项目，也可以直接使用我编译好的二进制文件（samplecrd-controller）。编译并启动这个项目的具体流程如下所示：\n\n\t# Clone repo\n\t$ git clone https://github.com/resouer/k8s-controller-custom-resource$ cd k8s-controller-custom-resource\n\t\n\t### Skip this part if you don't want to build\n\t# Install dependency\n\t$ go get github.com/tools/godep\n\t$ godep restore\n\t# Build\n\t$ go build -o samplecrd-controller .\n\t\n\t$ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true\n\tI0915 12:50:29.051349   27159 controller.go:84] Setting up event handlers\n\tI0915 12:50:29.051615   27159 controller.go:113] Starting Network control loop\n\tI0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync\n\tE0915 12:50:29.066745   27159 reflector.go:134] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)\n\t...\n你可以看到，自定义控制器被启动后，一开始会报错。\n\n这是因为，此时 Network 对象的 CRD 还没有被创建出来，所以 Informer 去 APIServer 里“获取”（List）Network 对象时，并不能找到 Network 这个 API 资源类型的定义，即：\n\n\tFailed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io)\n所以，接下来我就需要创建 Network 对象的 CRD，这个操作在上一篇文章里已经介绍过了。\n\n在另一个 shell 窗口里执行：\n\n\t$ kubectl apply -f crd/network.yaml\n这时候，你就会看到控制器的日志恢复了正常，控制循环启动成功：\n\n\t...\n\tI0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync\n\t...\n\tI0915 12:52:54.346854   25245 controller.go:121] Starting workers\n\tI0915 12:52:54.346914   25245 controller.go:127] Started workers\n接下来，我就可以进行 Network 对象的增删改查操作了。\n\n首先，创建一个 Network 对象：\n\n\t$ cat example/example-network.yaml \n\tapiVersion: samplecrd.k8s.io/v1\n\tkind: Network\n\tmetadata:\n\t  name: example-network\n\tspec:\n\t  cidr: \"192.168.0.0/16\"\n\t  gateway: \"192.168.0.1\"\n\t  \n\t$ kubectl apply -f example/example-network.yaml \n\tnetwork.samplecrd.k8s.io/example-network created\n这时候，查看一下控制器的输出：\n\n\t...\n\tI0915 12:50:29.051349   27159 controller.go:84] Setting up event handlers\n\tI0915 12:50:29.051615   27159 controller.go:113] Starting Network control loop\n\tI0915 12:50:29.051630   27159 controller.go:116] Waiting for informer caches to sync\n\t...\n\tI0915 12:52:54.346854   25245 controller.go:121] Starting workers\n\tI0915 12:52:54.346914   25245 controller.go:127] Started workers\n\tI0915 12:53:18.064409   25245 controller.go:229] [Neutron] Try to process network: &v1.Network{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"example-network\", GenerateName:\"\", Namespace:\"default\", ... ResourceVersion:\"479015\", ... Spec:v1.NetworkSpec{Cidr:\"192.168.0.0/16\", Gateway:\"192.168.0.1\"}} ...\n\tI0915 12:53:18.064650   25245 controller.go:183] Successfully synced 'default/example-network'\n\t...\n可以看到，我们上面创建 example-network 的操作，触发了 EventHandler 的“添加”事件，从而被放进了工作队列。\n\n紧接着，控制循环就从队列里拿到了这个对象，并且打印出了正在“处理”这个 Network 对象的日志。\n\n可以看到，这个 Network 的 ResourceVersion，也就是 API 对象的版本号，是 479015，而它的 Spec 字段的内容，跟我提交的 YAML 文件一摸一样，比如，它的 CIDR 网段是：192.168.0.0/16。\n\n这时候，我来修改一下这个 YAML 文件的内容，如下所示：\n\n\t$ cat example/example-network.yaml \n\tapiVersion: samplecrd.k8s.io/v1\n\tkind: Network\n\tmetadata:\n\t  name: example-network\n\tspec:\n\t  cidr: \"192.168.1.0/16\"\n\t  gateway: \"192.168.1.1\"\n可以看到，我把这个 YAML 文件里的 CIDR 和 Gateway 字段的修改成了 192.168.1.0/16 网段。\n\n然后，我们执行了 kubectl apply 命令来提交这次更新，如下所示：\n\n\t$ kubectl apply -f example/example-network.yaml \n\tnetwork.samplecrd.k8s.io/example-network configured\n这时候，我们就可以观察一下控制器的输出：\n\n\t...\n\tI0915 12:53:51.126029   25245 controller.go:229] [Neutron] Try to process network: &v1.Network{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"example-network\", GenerateName:\"\", Namespace:\"default\", ...  ResourceVersion:\"479062\", ... Spec:v1.NetworkSpec{Cidr:\"192.168.1.0/16\", Gateway:\"192.168.1.1\"}} ...\n\tI0915 12:53:51.126348   25245 controller.go:183] Successfully synced 'default/example-network'\n可以看到，这一次，Informer 注册的“更新”事件被触发，更新后的 Network 对象的 Key 被添加到了工作队列之中。\n\n所以，接下来控制循环从工作队列里拿到的 Network 对象，与前一个对象是不同的：它的 ResourceVersion 的值变成了 479062；而 Spec 里的字段，则变成了 192.168.1.0/16 网段。\n\n最后，我再把这个对象删除掉：\n\n\t$ kubectl delete -f example/example-network.yaml\n这一次，在控制器的输出里，我们就可以看到，Informer 注册的“删除”事件被触发，并且控制循环“调用”Neutron API“删除”了真实环境里的网络。这个输出如下所示：\n\n\tW0915 12:54:09.738464   25245 controller.go:212] Network: default/example-network does not exist in local cache, will delete it from Neutron ...\n\tI0915 12:54:09.738832   25245 controller.go:215] [Neutron] Deleting network: default/example-network ...\n\tI0915 12:54:09.738854   25245 controller.go:183] Successfully synced 'default/example-network'\n以上，就是编写和使用自定义控制器的全部流程了。\n\n实际上，这套流程不仅可以用在自定义 API 资源上，也完全可以用在 Kubernetes 原生的默认 API 对象上。\n\n比如，我们在 main 函数里，除了创建一个 Network Informer 外，还可以初始化一个 Kubernetes 默认 API 对象的 Informer 工厂，比如 Deployment 对象的 Informer。这个具体做法如下所示：\n\n\tfunc main() {\n\t  ...\n\t  \n\t  kubeInformerFactory := kubeinformers.NewSharedInformerFactory(kubeClient, time.Second*30)\n\t  \n\t  controller := NewController(kubeClient, exampleClient,\n\t  kubeInformerFactory.Apps().V1().Deployments(),\n\t  networkInformerFactory.Samplecrd().V1().Networks())\n\t  \n\t  go kubeInformerFactory.Start(stopCh)\n\t  ...\n\t}\n在这段代码中，我们首先使用 Kubernetes 的 client（kubeClient）创建了一个工厂；\n\n然后，我用跟 Network 类似的处理方法，生成了一个 Deployment Informer；\n\n接着，我把 Deployment Informer 传递给了自定义控制器；当然，我也要调用 Start 方法来启动这个 Deployment Informer。\n\n而有了这个 Deployment Informer 后，这个控制器也就持有了所有 Deployment 对象的信息。接下来，它既可以通过 deploymentInformer.Lister() 来获取 Etcd 里的所有 Deployment 对象，也可以为这个 Deployment Informer 注册具体的 Handler 来。\n\n更重要的是，这就使得在这个自定义控制器里面，我可以通过对自定义 API 对象和默认 API 对象进行协同，从而实现更加复杂的编排功能。\n\n比如：用户每创建一个新的 Deployment，这个自定义控制器，就可以为它创建一个对应的 Network 供它使用。\n\n这些对 Kubernetes API 编程范式的更高级应用，我就留给你在实际的场景中去探索和实践了。\n\n**总结**\n在今天这篇文章中，我为你剖析了 Kubernetes API 编程范式的具体原理，并编写了一个自定义控制器。\n\n这其中，有如下几个概念和机制，是你一定要理解清楚的：\n\n所谓的 Informer，就是一个自带缓存和索引机制，可以触发 Handler 的客户端库。这个本地缓存在 Kubernetes 中一般被称为 Store，索引一般被称为 Index。\n\nInformer 使用了 Reflector 包，它是一个可以通过 ListAndWatch 机制获取并监视 API 对象变化的客户端封装。\n\nReflector 和 Informer 之间，用到了一个“增量先进先出队列”进行协同。而 Informer 与你要编写的控制循环之间，则使用了一个工作队列来进行协同。\n\n在实际应用中，除了控制循环之外的所有代码，实际上都是 Kubernetes 为你自动生成的，即：pkg/client/{informers, listers, clientset}里的内容。\n\n而这些自动生成的代码，就为我们提供了一个可靠而高效地获取 API 对象“期望状态”的编程库。\n\n所以，接下来，作为开发者，你就只需要关注如何拿到“实际状态”，然后如何拿它去跟“期望状态”做对比，从而决定接下来要做的业务逻辑即可。\n\n以上内容，就是 Kubernetes API 编程范式的核心思想。\n\n**思考题**\n请思考一下，为什么 Informer 和你编写的控制循环之间，一定要使用一个工作队列来进行协作呢？\n\n","categories":["reference","k8s"]},{"title":"24 | 深入解析声明式API（一）：API对象的奥秘","url":"/2021/03/13/reference/k8s/24.深入解析声明式API(一)：API对象的奥秘/","content":"\n![](1.jpg)\n\n在上一篇文章中，我为你详细讲解了 Kubernetes 声明式 API 的设计、特点，以及使用方式。\n\n而在今天这篇文章中，我就来为你讲解一下 Kubernetes 声明式 API 的工作原理，以及如何利用这套 API 机制，在 Kubernetes 里添加自定义的 API 对象。\n\n你可能一直就很好奇：当我把一个 YAML 文件提交给 Kubernetes 之后，它究竟是如何创建出一个 API 对象的呢？\n\n这得从声明式 API 的设计谈起了。\n\n在 Kubernetes 项目中，一个 API 对象在 Etcd 里的完整资源路径，是由：Group（API 组）、Version（API 版本）和 Resource（API 资源类型）三个部分组成的。\n\n通过这样的结构，整个 Kubernetes 里的所有 API 对象，实际上就可以用如下的树形结构表示出来：\n\n![](2.png)\n\n在这幅图中，你可以很清楚地看到Kubernetes 里 API 对象的组织方式，其实是层层递进的。\n\n比如，现在我要声明要创建一个 CronJob 对象，那么我的 YAML 文件的开始部分会这么写：\n\n\tapiVersion: batch/v2alpha1\n\tkind: CronJob\n\t...\n在这个 YAML 文件中，“CronJob”就是这个 API 对象的资源类型（Resource），“batch”就是它的组（Group），v2alpha1 就是它的版本（Version）。\n\n当我们提交了这个 YAML 文件之后，Kubernetes 就会把这个 YAML 文件里描述的内容，转换成 Kubernetes 里的一个 CronJob 对象。\n\n那么，`Kubernetes 是如何对 Resource、Group 和 Version 进行解析，从而在 Kubernetes 项目里找到 CronJob 对象的定义呢？`\n\n**首先，Kubernetes 会匹配 API 对象的组。**\n\n需要明确的是，对于 Kubernetes 里的核心 API 对象，比如：Pod、Node 等，是不需要 Group 的（即：它们 Group 是“”）。所以，对于这些 API 对象来说，Kubernetes 会直接在 /api 这个层级进行下一步的匹配过程。\n\n而对于 CronJob 等非核心 API 对象来说，Kubernetes 就必须在 /apis 这个层级里查找它对应的 Group，进而根据“batch”这个 Group 的名字，找到 /apis/batch。\n\n不难发现，这些 API Group 的分类是以对象功能为依据的，比如 Job 和 CronJob 就都属于“batch” （离线业务）这个 Group。\n\n**然后，Kubernetes 会进一步匹配到 API 对象的版本号。**\n\n对于 CronJob 这个 API 对象来说，Kubernetes 在 batch 这个 Group 下，匹配到的版本号就是 v2alpha1。\n\n在 Kubernetes 中，同一种 API 对象可以有多个版本，这正是 Kubernetes 进行 API 版本化管理的重要手段。这样，比如在 CronJob 的开发过程中，对于会影响到用户的变更就可以通过升级新版本来处理，从而保证了向后兼容。\n\n**最后，Kubernetes 会匹配 API 对象的资源类型。**\n\n在前面匹配到正确的版本之后，Kubernetes 就知道，我要创建的原来是一个 /apis/batch/v2alpha1 下的 CronJob 对象。\n\n这时候，`APIServer 就可以继续创建这个 CronJob 对象了。`为了方便理解，我为你总结了一个如下所示流程图来阐述这个创建过程：\n\n![](3.png)\n\n**首先，**当我们发起了创建 CronJob 的 POST 请求之后，我们编写的 YAML 的信息就被提交给了 APIServer。\n\n而 APIServer 的第一个功能，就是过滤这个请求，并完成一些前置性的工作，比如授权、超时处理、审计等。\n\n**然后，**请求会进入 MUX 和 Routes 流程。如果你编写过 Web Server 的话就会知道，MUX 和 Routes 是 APIServer 完成 URL 和 Handler 绑定的场所。而 APIServer 的 Handler 要做的事情，就是按照我刚刚介绍的匹配过程，找到对应的 CronJob 类型定义。\n\n**接着，**APIServer 最重要的职责就来了：根据这个 CronJob 类型定义，使用用户提交的 YAML 文件里的字段，创建一个 CronJob 对象。\n\n而在这个过程中，APIServer 会进行一个 Convert 工作，即：把用户提交的 YAML 文件，转换成一个叫作 Super Version 的对象，它正是该 API 资源类型所有版本的字段全集。这样用户提交的不同版本的 YAML 文件，就都可以用这个 Super Version 对象来进行处理了。\n\n**接下来，**APIServer 会先后进行 Admission() 和 Validation() 操作。比如，我在上一篇文章中提到的 Admission Controller 和 Initializer，就都属于 Admission 的内容。\n\n而 Validation，则负责验证这个对象里的各个字段是否合法。这个被验证过的 API 对象，都保存在了 APIServer 里一个叫作 Registry 的数据结构中。也就是说，只要一个 API 对象的定义能在 Registry 里查到，它就是一个有效的 Kubernetes API 对象。\n\n**最后，**APIServer 会把验证过的 API 对象转换成用户最初提交的版本，进行序列化操作，并调用 Etcd 的 API 把它保存起来。\n\n由此可见，声明式 API 对于 Kubernetes 来说非常重要。所以，**APIServer 这样一个在其他项目里“平淡无奇”的组件，却成了 Kubernetes 项目的重中之重。**它不仅是 Google Borg 设计思想的集中体现，也是 Kubernetes 项目里唯一一个被 Google 公司和 RedHat 公司双重控制、其他势力根本无法参与其中的组件。\n\n此外，由于同时要兼顾性能、API 完备性、版本化、向后兼容等很多工程化指标，所以 Kubernetes 团队在 APIServer 项目里大量使用了 Go 语言的代码生成功能，来自动化诸如 Convert、DeepCopy 等与 API 资源相关的操作。这部分自动生成的代码，曾一度占到 Kubernetes 项目总代码的 20%~30%。\n\n这也是为何，在过去很长一段时间里，在这样一个极其“复杂”的 APIServer 中，添加一个 Kubernetes 风格的 API 资源类型，是一个非常困难的工作。\n\n不过，在 Kubernetes v1.7 之后，这个工作就变得轻松得多了。这，当然得益于一个全新的 API 插件机制：CRD。\n\nCRD 的全称是 Custom Resource Definition。顾名思义，它指的就是，允许用户在 Kubernetes 中添加一个跟 Pod、Node 类似的、新的 API 资源类型，即：自定义 API 资源。\n\n举个例子，`我现在要为 Kubernetes 添加一个名叫 Network 的 API 资源类型`。\n\n它的作用是，一旦用户创建一个 Network 对象，那么 Kubernetes 就应该使用这个对象定义的网络参数，调用真实的网络插件，比如 Neutron 项目，为用户创建一个真正的“网络”。这样，将来用户创建的 Pod，就可以声明使用这个“网络”了。\n\n这个 Network 对象的 YAML 文件，名叫 example-network.yaml，它的内容如下所示：\n\n\tapiVersion: samplecrd.k8s.io/v1\n\tkind: Network\n\tmetadata:\n\t  name: example-network\n\tspec:\n\t  cidr: \"192.168.0.0/16\"\n\t  gateway: \"192.168.0.1\"\n可以看到，我想要描述“网络”的 API 资源类型是 Network；API 组是samplecrd.k8s.io；API 版本是 v1。\n\n那么，`Kubernetes 又该如何知道这个 API（samplecrd.k8s.io/v1/network）的存在呢？`\n\n其实，上面的这个 YAML 文件，就是一个具体的“自定义 API 资源”实例，也叫 CR（Custom Resource）。而为了能够让 Kubernetes 认识这个 CR，你就需要让 Kubernetes 明白这个 CR 的宏观定义是什么，也就是 CRD（Custom Resource Definition）。\n\n这就好比，你想让计算机认识各种兔子的照片，就得先让计算机明白，兔子的普遍定义是什么。比如，兔子“是哺乳动物”“有长耳朵，三瓣嘴”。\n\n所以，接下来，我就先需编写一个 CRD 的 YAML 文件，它的名字叫作 network.yaml，内容如下所示：\n\n\tapiVersion: apiextensions.k8s.io/v1beta1\n\tkind: CustomResourceDefinition\n\tmetadata:\n\t  name: networks.samplecrd.k8s.io\n\tspec:\n\t  group: samplecrd.k8s.io\n\t  version: v1\n\t  names:\n\t    kind: Network\n\t    plural: networks\n\t  scope: Namespaced\n可以看到，在这个 CRD 中，我指定了“group: samplecrd.k8s.io”“version: v1”这样的 API 信息，也指定了这个 CR 的资源类型叫作 Network，复数（plural）是 networks。\n\n然后，我还声明了它的 scope 是 Namespaced，即：我们定义的这个 Network 是一个属于 Namespace 的对象，类似于 Pod。\n\n这就是一个 Network API 资源类型的 API 部分的宏观定义了。这就等同于告诉了计算机：“兔子是哺乳动物”。所以这时候，Kubernetes 就能够认识和处理所有声明了 API 类型是“samplecrd.k8s.io/v1/network”的 YAML 文件了。\n\n接下来，我还需要让 Kubernetes“认识”这种 YAML 文件里描述的“网络”部分，比如“cidr”（网段），“gateway”（网关）这些字段的含义。这就相当于我要告诉计算机：“兔子有长耳朵和三瓣嘴”。\n\n这时候呢，我就需要稍微做些代码工作了。\n\n**首先，我要在 GOPATH 下，创建一个结构如下的项目：**\n\n> 备注：在这里，我并不要求你具有完备的 Go 语言知识体系，但我会假设你已经了解了 Golang 的一些基本知识（比如，知道什么是 GOPATH）。而如果你还不了解的话，可以在涉及到相关内容时，再去查阅一些相关资料。\n\n\t$ tree $GOPATH/src/github.com/<your-name>/k8s-controller-custom-resource\n\t.\n\t├── controller.go\n\t├── crd\n\t│   └── network.yaml\n\t├── example\n\t│   └── example-network.yaml\n\t├── main.go\n\t└── pkg\n\t    └── apis\n\t        └── samplecrd\n\t            ├── register.go\n\t            └── v1\n\t                ├── doc.go\n\t                ├── register.go\n\t                └── types.go\n其中，pkg/apis/samplecrd 就是 API 组的名字，v1 是版本，而 v1 下面的 types.go 文件里，则定义了 Network 对象的完整描述。我已经把这个项目[上传到了 GitHub 上](https://github.com/resouer/k8s-controller-custom-resource)，你可以随时参考。\n\n**然后，我在 pkg/apis/samplecrd 目录下创建了一个 register.go 文件，用来放置后面要用到的全局变量。**这个文件的内容如下所示：\n\n\tpackage samplecrd\n\t \n\tconst (\n\t GroupName = \"samplecrd.k8s.io\"\n\t Version   = \"v1\"\n\t)\n**接着，我需要在 pkg/apis/samplecrd 目录下添加一个 doc.go 文件（Golang 的文档源文件）**。这个文件里的内容如下所示：\n\n\t// +k8s:deepcopy-gen=package\n\t\n\t// +groupName=samplecrd.k8s.io\n\tpackage v1\n在这个文件中，你会看到 +<tag_name>[=value] 格式的注释，这就是 Kubernetes 进行代码生成要用的 Annotation 风格的注释。\n\n其中，+k8s:deepcopy-gen=package 意思是，请为整个 v1 包里的所有类型定义自动生成 DeepCopy 方法；而+groupName=samplecrd.k8s.io，则定义了这个包对应的 API 组的名字。\n\n可以看到，这些定义在 doc.go 文件的注释，起到的是全局的代码生成控制的作用，所以也被称为 Global Tags。\n\n**接下来，我需要添加 types.go 文件。**顾名思义，它的作用就是定义一个 Network 类型到底有哪些字段（比如，spec 字段里的内容）。这个文件的主要内容如下所示：\n\n\tpackage v1\n\t...\n\t// +genclient\n\t// +genclient:noStatus\n\t// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\t \n\t// Network describes a Network resource\n\ttype Network struct {\n\t // TypeMeta is the metadata for the resource, like kind and apiversion\n\t metav1.TypeMeta `json:\",inline\"`\n\t // ObjectMeta contains the metadata for the particular object, including\n\t // things like...\n\t //  - name\n\t //  - namespace\n\t //  - self link\n\t //  - labels\n\t //  - ... etc ...\n\t metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\t \n\t Spec networkspec `json:\"spec\"`\n\t}\n\t// networkspec is the spec for a Network resource\n\ttype networkspec struct {\n\t Cidr    string `json:\"cidr\"`\n\t Gateway string `json:\"gateway\"`\n\t}\n\t \n\t// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\t \n\t// NetworkList is a list of Network resources\n\ttype NetworkList struct {\n\t metav1.TypeMeta `json:\",inline\"`\n\t metav1.ListMeta `json:\"metadata\"`\n\t \n\t Items []Network `json:\"items\"`\n\t}\n在上面这部分代码里，你可以看到 Network 类型定义方法跟标准的 Kubernetes 对象一样，都包括了 TypeMeta（API 元数据）和 ObjectMeta（对象元数据）字段。\n\n而其中的 Spec 字段，就是需要我们自己定义的部分。所以，在 networkspec 里，我定义了 Cidr 和 Gateway 两个字段。其中，每个字段最后面的部分比如json:\"cidr\"，指的就是这个字段被转换成 JSON 格式之后的名字，也就是 YAML 文件里的字段名字。\n\n> 如果你不熟悉这个用法的话，可以查阅一下 Golang 的文档。\n\n此外，除了定义 Network 类型，你还需要定义一个 NetworkList 类型，用来描述**一组 Network 对象**应该包括哪些字段。之所以需要这样一个类型，是因为在 Kubernetes 中，获取所有 X 对象的 List() 方法，返回值都是List 类型，而不是 X 类型的数组。这是不一样的。\n\n同样地，在 Network 和 NetworkList 类型上，也有代码生成注释。\n\n其中，+genclient 的意思是：请为下面这个 API 资源类型生成对应的 Client 代码（这个 Client，我马上会讲到）。而 +genclient:noStatus 的意思是：这个 API 资源类型定义里，没有 Status 字段。否则，生成的 Client 就会自动带上 UpdateStatus 方法。\n\n如果你的类型定义包括了 Status 字段的话，就不需要这句 +genclient:noStatus 注释了。比如下面这个例子：\n\n\t// +genclient\n\t \n\t// Network is a specification for a Network resource\n\ttype Network struct {\n\t metav1.TypeMeta   `json:\",inline\"`\n\t metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\t \n\t Spec   NetworkSpec   `json:\"spec\"`\n\t Status NetworkStatus `json:\"status\"`\n\t}\n需要注意的是，+genclient 只需要写在 Network 类型上，而不用写在 NetworkList 上。因为 NetworkList 只是一个返回值类型，Network 才是“主类型”。\n\n而由于我在 Global Tags 里已经定义了为所有类型生成 DeepCopy 方法，所以这里就不需要再显式地加上 +k8s:deepcopy-gen=true 了。当然，这也就意味着你可以用 +k8s:deepcopy-gen=false 来阻止为某些类型生成 DeepCopy。\n\n你可能已经注意到，在这两个类型上面还有一句+k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object的注释。它的意思是，请在生成 DeepCopy 的时候，实现 Kubernetes 提供的 runtime.Object 接口。否则，在某些版本的 Kubernetes 里，你的这个类型定义会出现编译错误。这是一个固定的操作，记住即可。\n\n不过，你或许会有这样的顾虑：这些代码生成注释这么灵活，我该怎么掌握呢？\n\n其实，上面我所讲述的内容，已经足以应对 99% 的场景了。当然，如果你对代码生成感兴趣的话，我推荐你阅读[这篇博客](https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/)，它详细地介绍了 Kubernetes 的代码生成语法。\n\n**最后，我需要再编写的一个 pkg/apis/samplecrd/v1/register.go 文件。**\n\n在前面对 APIServer 工作原理的讲解中，我已经提到，“registry”的作用就是注册一个类型（Type）给 APIServer。其中，Network 资源类型在服务器端的注册的工作，APIServer 会自动帮我们完成。但与之对应的，我们还需要让客户端也能“知道”Network 资源类型的定义。这就需要我们在项目里添加一个 register.go 文件。它最主要的功能，就是定义了如下所示的 addKnownTypes() 方法：\n\n\tpackage v1\n\t...\n\t// addKnownTypes adds our types to the API scheme by registering\n\t// Network and NetworkList\n\tfunc addKnownTypes(scheme *runtime.Scheme) error {\n\t scheme.AddKnownTypes(\n\t  SchemeGroupVersion,\n\t  &Network{},\n\t  &NetworkList{},\n\t )\n\t \n\t // register the type in the scheme\n\t metav1.AddToGroupVersion(scheme, SchemeGroupVersion)\n\t return nil\n\t}\n有了这个方法，Kubernetes 就能够在后面生成客户端的时候，“知道”Network 以及 NetworkList 类型的定义了。\n\n像上面这种register.go 文件里的内容其实是非常固定的，你以后可以直接使用我提供的这部分代码做模板，然后把其中的资源类型、GroupName 和 Version 替换成你自己的定义即可。\n\n这样，Network 对象的定义工作就全部完成了。可以看到，它其实定义了两部分内容：\n\n * 第一部分是，自定义资源类型的 API 描述，包括：组（Group）、版本（Version）、资源类型（Resource）等。这相当于告诉了计算机：兔子是哺乳动物。\n * 第二部分是，自定义资源类型的对象描述，包括：Spec、Status 等。这相当于告诉了计算机：兔子有长耳朵和三瓣嘴。\n接下来，`我就要使用 Kubernetes 提供的代码生成工具，为上面定义的 Network 资源类型自动生成 clientset、informer 和 lister。`其中，clientset 就是操作 Network 对象所需要使用的客户端，而 informer 和 lister 这两个包的主要功能，我会在下一篇文章中重点讲解。\n\n这个代码生成工具名叫k8s.io/code-generator，使用方法如下所示：\n\n\t# 代码生成的工作目录，也就是我们的项目路径\n\t$ ROOT_PACKAGE=\"github.com/resouer/k8s-controller-custom-resource\"\n\t# API Group\n\t$ CUSTOM_RESOURCE_NAME=\"samplecrd\"\n\t# API Version\n\t$ CUSTOM_RESOURCE_VERSION=\"v1\"\n\t\n\t# 安装 k8s.io/code-generator\n\t$ go get -u k8s.io/code-generator/...\n\t$ cd $GOPATH/src/k8s.io/code-generator\n\t\n\t# 执行代码自动生成，其中 pkg/client 是生成目标目录，pkg/apis 是类型定义目录\n\t$ ./generate-groups.sh all \"$ROOT_PACKAGE/pkg/client\" \"$ROOT_PACKAGE/pkg/apis\" \"$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION\"\n代码生成工作完成之后，我们再查看一下这个项目的目录结构：\n\n\t$ tree\n\t.\n\t├── controller.go\n\t├── crd\n\t│   └── network.yaml\n\t├── example\n\t│   └── example-network.yaml\n\t├── main.go\n\t└── pkg\n\t    ├── apis\n\t    │   └── samplecrd\n\t    │       ├── constants.go\n\t    │       └── v1\n\t    │           ├── doc.go\n\t    │           ├── register.go\n\t    │           ├── types.go\n\t    │           └── zz_generated.deepcopy.go\n\t    └── client\n\t        ├── clientset\n\t        ├── informers\n\t        └── listers\n其中，pkg/apis/samplecrd/v1 下面的 zz_generated.deepcopy.go 文件，就是自动生成的 DeepCopy 代码文件。\n\n而整个 client 目录，以及下面的三个包（clientset、informers、 listers），都是 Kubernetes 为 Network 类型生成的客户端库，这些库会在后面编写自定义控制器的时候用到。\n\n可以看到，到目前为止的这些工作，其实并不要求你写多少代码，主要考验的是“复制、粘贴、替换”这样的“基本功”。\n\n而有了这些内容，现在你就可以`在 Kubernetes 集群里创建一个 Network 类型的 API 对象了`。我们不妨一起来实验一下。\n\n**首先，**使用 network.yaml 文件，在 Kubernetes 中创建 Network 对象的 CRD（Custom Resource Definition）：\n\n\t$ kubectl apply -f crd/network.yaml\n\tcustomresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created\n这个操作，就告诉了 Kubernetes，我现在要添加一个自定义的 API 对象。而这个对象的 API 信息，正是 network.yaml 里定义的内容。我们可以通过 kubectl get 命令，查看这个 CRD：\n\n\t$ kubectl get crd\n\tNAME                        CREATED AT\n\tnetworks.samplecrd.k8s.io   2018-09-15T10:57:12Z\n**然后，**我们就可以创建一个 Network 对象了，这里用到的是 example-network.yaml：\n\n\t$ kubectl apply -f example/example-network.yaml \n\tnetwork.samplecrd.k8s.io/example-network created\n通过这个操作，你就在 Kubernetes 集群里创建了一个 Network 对象。它的 API 资源路径是samplecrd.k8s.io/v1/networks。\n\n这时候，你就可以通过 kubectl get 命令，查看到新创建的 Network 对象：\n\n\t$ kubectl get network\n\tNAME              AGE\n\texample-network   8s\n你还可以通过 kubectl describe 命令，看到这个 Network 对象的细节：\n\n\t$ kubectl describe network example-network\n\tName:         example-network\n\tNamespace:    default\n\tLabels:       <none>\n\t...API Version:  samplecrd.k8s.io/v1\n\tKind:         Network\n\tMetadata:\n\t  ...\n\t  Generation:          1\n\t  Resource Version:    468239\n\t  ...\n\tSpec:\n\t  Cidr:     192.168.0.0/16\n\t  Gateway:  192.168.0.1\n当然 ，你也可以编写更多的 YAML 文件来创建更多的 Network 对象，这和创建 Pod、Deployment 的操作，没有任何区别。\n\n**总结**\n在今天这篇文章中，我为你详细解析了 Kubernetes 声明式 API 的工作原理，讲解了如何遵循声明式 API 的设计，为 Kubernetes 添加一个名叫 Network 的 API 资源类型。从而达到了通过标准的 kubectl create 和 get 操作，来管理自定义 API 对象的目的。\n\n不过，创建出这样一个自定义 API 对象，我们只是完成了 Kubernetes 声明式 API 的一半工作。\n\n接下来的另一半工作是：为这个 API 对象编写一个自定义控制器（Custom Controller）。这样， Kubernetes 才能根据 Network API 对象的“增、删、改”操作，在真实环境中做出相应的响应。比如，“创建、删除、修改”真正的 Neutron 网络。\n\n而这，正是 Network 这个 API 对象所关注的“业务逻辑”。\n\n这个业务逻辑的实现过程，以及它所使用的 Kubernetes API 编程库的工作原理，就是我要在下一篇文章中讲解的主要内容。\n\n**思考题**\n在了解了 CRD 的定义方法之后，你是否已经在考虑使用 CRD（或者已经使用了 CRD）来描述现实中的某种实体了呢？能否分享一下你的思路？（举个例子：某技术团队使用 CRD 描述了“宿主机”，然后用 Kubernetes 部署了 Kubernetes）\n\n","categories":["reference","k8s"]},{"title":"18 | 深入理解StatefulSet（一）：拓扑状态","url":"/2021/03/13/reference/k8s/18.深入理解StatefulSet(一)：拓扑状态/","content":"\n![](1.jpg)\n\n在上一篇文章中，我在结尾处讨论到了 Deployment 实际上并不足以覆盖所有的应用编排问题。\n\n造成这个问题的根本原因，在于 Deployment 对应用做了一个简单化假设。\n\n它认为，一个应用的所有 Pod，是完全一样的。所以，它们互相之间没有顺序，也无所谓运行在哪台宿主机上。需要的时候，Deployment 就可以通过 Pod 模板创建新的 Pod；不需要的时候，Deployment 就可以“杀掉”任意一个 Pod。\n\n但是，在实际的场景中，并不是所有的应用都可以满足这样的要求。\n\n尤其是分布式应用，它的多个实例之间，往往有依赖关系，比如：主从关系、主备关系。\n\n还有就是数据存储类应用，它的多个实例，往往都会在本地磁盘上保存一份数据。而这些实例一旦被杀掉，即便重建出来，实例与数据之间的对应关系也已经丢失，从而导致应用失败。\n\n所以，这种实例之间有不对等关系，以及实例对外部数据有依赖关系的应用，就被称为**`“有状态应用”（Stateful Application）。`**\n\n容器技术诞生后，大家很快发现，它用来封装“无状态应用”（Stateless Application），尤其是 Web 服务，非常好用。但是，一旦你想要用容器运行“有状态应用”，其困难程度就会直线上升。而且，这个问题解决起来，单纯依靠容器技术本身已经无能为力，这也就导致了很长一段时间内，“有状态应用”几乎成了容器技术圈子的“忌讳”，大家一听到这个词，就纷纷摇头。\n\n不过，Kubernetes 项目还是成为了“第一个吃螃蟹的人”。\n\n得益于“控制器模式”的设计思想，Kubernetes 项目很早就在 Deployment 的基础上，扩展出了对“有状态应用”的初步支持。这个编排功能，就是：StatefulSet。\n\nStatefulSet 的设计其实非常容易理解。它把真实世界里的应用状态，抽象为了两种情况：\n\n1. 拓扑状态。这种情况意味着，应用的多个实例之间不是完全对等的关系。这些应用实例，必须按照某些顺序启动，比如应用的主节点 A 要先于从节点 B 启动。而如果你把 A 和 B 两个 Pod 删除掉，它们再次被创建出来时也必须严格按照这个顺序才行。并且，新创建出来的 Pod，必须和原来 Pod 的网络标识一样，这样原先的访问者才能使用同样的方法，访问到这个新 Pod。\n2. 存储状态。这种情况意味着，应用的多个实例分别绑定了不同的存储数据。对于这些应用实例来说，Pod A 第一次读取到的数据，和隔了十分钟之后再次读取到的数据，应该是同一份，哪怕在此期间 Pod A 被重新创建过。这种情况最典型的例子，就是一个数据库应用的多个存储实例。\n\n所以，**StatefulSet 的核心功能，就是通过某种方式记录这些状态，然后在 Pod 被重新创建时，能够为新 Pod 恢复这些状态。**\n**(StatefulSet的核心功能，能通过某种方式记录应用的拓扑状态和存储状态，在Pod重新创建时，为新Pod恢复这些状态。)**\n\n在开始讲述 StatefulSet 的工作原理之前，我就必须先为你讲解一个 `Kubernetes 项目中非常实用的概念：Headless Service。`\n\n我在和你一起讨论 Kubernetes 架构的时候就曾介绍过，Service 是 Kubernetes 项目中用来将一组 Pod 暴露给外界访问的一种机制。比如，一个 Deployment 有 3 个 Pod，那么我就可以定义一个 Service。然后，用户只要能访问到这个 Service，它就能访问到某个具体的 Pod。\n\n那么，这个 Service 又是如何被访问的呢？\n\n** * 第一种方式，是以 Service 的 VIP（Virtual IP，即：虚拟 IP）方式。**比如：当我访问 10.0.23.1 这个 Service 的 IP 地址时，10.0.23.1 其实就是一个 VIP，它会把请求转发到该 Service 所代理的某一个 Pod 上。这里的具体原理，我会在后续的 Service 章节中进行详细介绍。\n** * 第二种方式，就是以 Service 的 DNS 方式(只能在容器的网络里面，在宿主机里面是不行的)。**比如：这时候，只要我访问“my-svc.my-namespace.svc.cluster.local”这条 DNS 记录，就可以访问到名叫 my-svc 的 Service 所代理的某一个 Pod。\n\n而在第二种 Service DNS 的方式下，具体还可以分为两种处理方法：\n\n第一种处理方法，是 Normal Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，正是 my-svc 这个 Service 的 VIP，后面的流程就跟 VIP 方式一致了。\n\n而第二种处理方法，正是 Headless Service。这种情况下，你访问“my-svc.my-namespace.svc.cluster.local”解析到的，直接就是 my-svc 代理的某一个 Pod 的 IP 地址。可以看到，**这里的区别在于，Headless Service 不需要分配一个 VIP，而是可以直接以 DNS 记录的方式解析出被代理 Pod 的 IP 地址。**\n\n那么，这样的设计又有什么作用呢？\n\n想要回答这个问题，我们需要从 Headless Service 的定义方式看起。\n\n下面是一个标准的 Headless Service 对应的 YAML 文件：\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: nginx\n\t  labels:\n\t    app: nginx\n\tspec:\n\t  ports:\n\t  - port: 80\n\t    name: web\n\t  clusterIP: None\n\t  selector:\n\t    app: nginx\n可以看到，所谓的 Headless Service，其实仍是一个标准 Service 的 YAML 文件。只不过，它的 clusterIP 字段的值是：None，即：这个 Service，没有一个 VIP 作为“头”。这也就是 Headless 的含义。所以，这个 Service 被创建后并不会被分配一个 VIP，而是会以 DNS 记录的方式暴露出它所代理的 Pod。\n\n而它所代理的 Pod，依然是采用我在前面第 12 篇文章《牛刀小试：我的第一个容器化应用》中提到的 Label Selector 机制选择出来的，即：所有携带了 app=nginx 标签的 Pod，都会被这个 Service 代理起来。\n\n然后关键来了。\n\n当你按照这样的方式创建了一个 Headless Service 之后，它所代理的所有 Pod 的 IP 地址，都会被绑定一个这样格式的 DNS 记录，如下所示：\n\n\t<pod-name>.<svc-name>.<namespace>.svc.cluster.local\n这个 DNS 记录，正是 Kubernetes 项目为 Pod 分配的唯一的“可解析身份”（Resolvable Identity）。\n\n有了这个“可解析身份”，只要你知道了一个 Pod 的名字，以及它对应的 Service 的名字，你就可以非常确定地通过这条 DNS 记录访问到 Pod 的 IP 地址。\n\n那么，`StatefulSet 又是如何使用这个 DNS 记录来维持 Pod 的拓扑状态的呢？`\n\n为了回答这个问题，现在我们就来编写一个 StatefulSet 的 YAML 文件，如下所示：\n\n\tapiVersion: apps/v1\n\tkind: StatefulSet\n\tmetadata:\n\t  name: web\n\tspec:\n\t  serviceName: \"nginx\"\n\t  replicas: 2\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.9.1\n\t        ports:\n\t        - containerPort: 80\n\t          name: web\n这个 YAML 文件，和我们在前面文章中用到的 nginx-deployment 的唯一区别，就是多了一个 serviceName=nginx 字段。\n\n这个字段的作用，就是告诉 StatefulSet 控制器，在执行控制循环（Control Loop）的时候，请使用 nginx 这个 Headless Service 来保证 Pod 的“可解析身份”。\n\n所以，当你通过 kubectl create 创建了上面这个 Service 和 StatefulSet 之后，就会看到如下两个对象：\n\n\t$ kubectl create -f svc.yaml\n\t$ kubectl get service nginx\n\tNAME      TYPE         CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\n\tnginx     ClusterIP    None         <none>        80/TCP    10s\n\t\n\t$ kubectl create -f statefulset.yaml\n\t$ kubectl get statefulset web\n\tNAME      DESIRED   CURRENT   AGE\n\tweb       2         1         19s\n这时候，如果你手比较快的话，还可以通过 kubectl 的 -w 参数，即：Watch 功能，实时查看 StatefulSet 创建两个有状态实例的过程：\n\n> 备注：如果手不够快的话，Pod 很快就创建完了。不过，你依然可以通过这个 StatefulSet 的 Events 看到这些信息。\n\n\t$ kubectl get pods -w -l app=nginx\n\tNAME      READY     STATUS    RESTARTS   AGE\n\tweb-0     0/1       Pending   0          0s\n\tweb-0     0/1       Pending   0         0s\n\tweb-0     0/1       ContainerCreating   0         0s\n\tweb-0     1/1       Running   0         19s\n\tweb-1     0/1       Pending   0         0s\n\tweb-1     0/1       Pending   0         0s\n\tweb-1     0/1       ContainerCreating   0         0s\n\tweb-1     1/1       Running   0         20s\n通过上面这个 Pod 的创建过程，我们不难看到，StatefulSet 给它所管理的所有 Pod 的名字，进行了编号，编号规则是：-。\n\n而且这些编号都是从 0 开始累加，与 StatefulSet 的每个 Pod 实例一一对应，绝不重复。\n\n更重要的是，这些 Pod 的创建，也是严格按照编号顺序进行的。比如，在 web-0 进入到 Running 状态、并且细分状态（Conditions）成为 Ready 之前，web-1 会一直处于 Pending 状态。\n\n> 备注：Ready 状态再一次提醒了我们，为 Pod 设置 livenessProbe 和 readinessProbe 的重要性。\n\n当这两个 Pod 都进入了 Running 状态之后，你就可以查看到它们各自唯一的“网络身份”了。\n\n我们使用 kubectl exec 命令进入到容器中查看它们的 hostname：\n\n\t$ kubectl exec web-0 -- sh -c 'hostname'\n\tweb-0\n\t$ kubectl exec web-1 -- sh -c 'hostname'\n\tweb-1\n可以看到，这两个 Pod 的 hostname 与 Pod 名字是一致的，都被分配了对应的编号。接下来，我们再试着以 DNS 的方式，访问一下这个 Headless Service：\n\n\t$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh \n通过这条命令，我们**启动了一个一次性的 Pod，因为`--rm` 意味着 Pod 退出后就会被删除掉**。然后，在这个 Pod 的容器里面，我们尝试用 nslookup 命令，解析一下 Pod 对应的 Headless Service：\n\n\t$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh\n\t$ nslookup web-0.nginx\n\tServer:    10.0.0.10\n\tAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\t\n\tName:      web-0.nginx\n\tAddress 1: 10.244.1.7\n\t\n\t$ nslookup web-1.nginx\n\tServer:    10.0.0.10\n\tAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\t\n\tName:      web-1.nginx\n\tAddress 1: 10.244.2.7\n从 nslookup 命令的输出结果中，我们可以看到，在访问 web-0.nginx 的时候，最后解析到的，正是 web-0 这个 Pod 的 IP 地址；而当访问 web-1.nginx 的时候，解析到的则是 web-1 的 IP 地址。\n\n这时候，如果你在另外一个 Terminal 里把这两个“有状态应用”的 Pod 删掉：\n\n\t$ kubectl delete pod -l app=nginx\n\tpod \"web-0\" deleted\n\tpod \"web-1\" deleted\n然后，再在当前 Terminal 里 Watch 一下这两个 Pod 的状态变化，就会发现一个有趣的现象：\n\n\t$ kubectl get pod -w -l app=nginx\n\tNAME      READY     STATUS              RESTARTS   AGE\n\tweb-0     0/1       ContainerCreating   0          0s\n\tNAME      READY     STATUS    RESTARTS   AGE\n\tweb-0     1/1       Running   0          2s\n\tweb-1     0/1       Pending   0         0s\n\tweb-1     0/1       ContainerCreating   0         0s\n\tweb-1     1/1       Running   0         32s\n可以看到，当我们把这两个 Pod 删除之后，Kubernetes 会按照原先编号的顺序，创建出了两个新的 Pod。并且，Kubernetes 依然为它们分配了与原来相同的“网络身份”：web-0.nginx 和 web-1.nginx。\n\n通过这种严格的对应规则，StatefulSet 就保证了 Pod 网络标识的稳定性。\n\n比如，如果 web-0 是一个需要先启动的主节点，web-1 是一个后启动的从节点，那么只要这个 StatefulSet 不被删除，你访问 web-0.nginx 时始终都会落在主节点上，访问 web-1.nginx 时，则始终都会落在从节点上，这个关系绝对不会发生任何变化。\n\n所以，如果我们再用 nslookup 命令，查看一下这个新 Pod 对应的 Headless Service 的话：\n\n\t$ kubectl run -i --tty --image busybox dns-test --restart=Never --rm /bin/sh \n\t$ nslookup web-0.nginx\n\tServer:    10.0.0.10\n\tAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\t\n\tName:      web-0.nginx\n\tAddress 1: 10.244.1.8\n\t\n\t$ nslookup web-1.nginx\n\tServer:    10.0.0.10\n\tAddress 1: 10.0.0.10 kube-dns.kube-system.svc.cluster.local\n\t\n\tName:      web-1.nginx\n\tAddress 1: 10.244.2.8\n我们可以看到，在这个 StatefulSet 中，这两个新 Pod 的“网络标识”（比如：web-0.nginx 和 web-1.nginx），再次解析到了正确的 IP 地址（比如：web-0 Pod 的 IP 地址 10.244.1.8）。\n\n通过这种方法，**Kubernetes 就成功地将 Pod 的拓扑状态（比如：哪个节点先启动，哪个节点后启动），按照 Pod 的“名字 + 编号”的方式固定了下来。** 此外，Kubernetes 还为每一个 Pod 提供了一个固定并且唯一的访问入口，即：这个 Pod 对应的 DNS 记录。\n\n这些状态，在 StatefulSet 的整个生命周期里都会保持不变，绝不会因为对应 Pod 的删除或者重新创建而失效。\n\n不过，相信你也已经注意到了，尽管 web-0.nginx 这条记录本身不会变，但它解析到的 Pod 的 IP 地址，并不是固定的。这就意味着，对于“有状态应用”实例的访问，你必须使用 DNS 记录或者 hostname 的方式，而绝不应该直接访问这些 Pod 的 IP 地址。\n\n**总结**\n在今天这篇文章中，我首先和你分享了 StatefulSet 的基本概念，解释了什么是应用的“状态”。\n\n紧接着 ，我为你分析了 StatefulSet 如何保证应用实例之间“拓扑状态”的稳定性。\n\n如果用一句话来总结的话，你可以这么理解这个过程：\n\n> StatefulSet 这个控制器的主要作用之一，就是使用 Pod 模板创建 Pod 的时候，对它们进行编号，并且按照编号顺序逐一完成创建工作。而当 StatefulSet 的“控制循环”发现 Pod 的“实际状态”与“期望状态”不一致，需要新建或者删除 Pod 进行“调谐”的时候，它会严格按照这些 Pod 编号的顺序，逐一完成这些操作。\n\n所以，StatefulSet 其实可以认为是对 Deployment 的改良。\n\n与此同时，通过 Headless Service 的方式，StatefulSet 为每个 Pod 创建了一个固定并且稳定的 DNS 记录，来作为它的访问入口。\n\n实际上，在部署“有状态应用”的时候，应用的每个实例拥有唯一并且稳定的“网络标识”，是一个非常重要的假设。\n\n在下一篇文章中，我将会继续为你剖析 StatefulSet 如何处理存储状态。\n\n**思考题**\n你曾经运维过哪些有拓扑状态的应用呢（比如：主从、主主、主备、一主多从等结构）？你觉得这些应用实例之间的拓扑关系，能否借助这种为 Pod 实例编号的方式表达出来呢？如果不能，你觉得 Kubernetes 还应该为你提供哪些支持来管理这个拓扑状态呢？","categories":["reference","k8s"]},{"title":"19 | 深入理解StatefulSet（二）：存储状态","url":"/2021/03/13/reference/k8s/19.深入理解StatefulSet(二)：存储状态/","content":"\n![](1.jpg)\n\n在上一篇文章中，我和你分享了 StatefulSet 如何保证应用实例的拓扑状态，在 Pod 删除和再创建的过程中保持稳定。\n\n而在今天这篇文章中，我将继续为你解读 StatefulSet 对存储状态的管理机制。这个机制，主要使用的是一个叫作 Persistent Volume Claim 的功能。\n\n在前面介绍 Pod 的时候，我曾提到过，要在一个 Pod 里声明 Volume，只要在 Pod 里加上 spec.volumes 字段即可。然后，你就可以在这个字段里定义一个具体类型的 Volume 了，比如：hostPath。\n\n可是，你有没有想过这样一个场景：如果你并不知道有哪些 Volume 类型可以用，要怎么办呢？\n\n更具体地说，作为一个应用开发者，我可能对持久化存储项目（比如 Ceph、GlusterFS 等）一窍不通，也不知道公司的 Kubernetes 集群里到底是怎么搭建出来的，我也自然不会编写它们对应的 Volume 定义文件。\n\n所谓“术业有专攻”，这些关于 Volume 的管理和远程持久化存储的知识，不仅超越了开发者的知识储备，还会有暴露公司基础设施秘密的风险。\n\n比如，下面这个例子，就是一个声明了 Ceph RBD 类型 Volume 的 Pod：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: rbd\n\tspec:\n\t  containers:\n\t    - image: kubernetes/pause\n\t      name: rbd-rw\n\t      volumeMounts:\n\t      - name: rbdpd\n\t        mountPath: /mnt/rbd\n\t  volumes:\n\t    - name: rbdpd\n\t      rbd:\n\t        monitors:\n\t        - '10.16.154.78:6789'\n\t        - '10.16.154.82:6789'\n\t        - '10.16.154.83:6789'\n\t        pool: kube\n\t        image: foo\n\t        fsType: ext4\n\t        readOnly: true\n\t        user: admin\n\t        keyring: /etc/ceph/keyring\n\t        imageformat: \"2\"\n\t        imagefeatures: \"layering\"\n其一，如果不懂得 Ceph RBD 的使用方法，那么这个 Pod 里 Volumes 字段，你十有八九也完全看不懂。其二，这个 Ceph RBD 对应的存储服务器的地址、用户名、授权文件的位置，也都被轻易地暴露给了全公司的所有开发人员，这是一个典型的信息被“过度暴露”的例子。\n\n这也是为什么，在后来的演化中，**Kubernetes 项目引入了一组叫作 Persistent Volume Claim（PVC）和 Persistent Volume（PV）的 API 对象，大大降低了用户声明和使用持久化 Volume 的门槛。**\n\n举个例子，有了 PVC 之后，一个开发人员想要使用一个 Volume，只需要简单的两步即可。\n\n第一步：定义一个 PVC，声明想要的 Volume 的属性：\n\n\tkind: PersistentVolumeClaim\n\tapiVersion: v1\n\tmetadata:\n\t  name: pv-claim\n\tspec:\n\t  accessModes:\n\t  - ReadWriteOnce\n\t  resources:\n\t    requests:\n\t      storage: 1Gi\n可以看到，在这个 PVC 对象里，不需要任何关于 Volume 细节的字段，只有描述性的属性和定义。比如，storage: 1Gi，表示我想要的 Volume 大小至少是 1 GiB；accessModes: ReadWriteOnce，表示这个 Volume 的挂载方式是可读写，并且只能被挂载在一个节点上而非被多个节点共享。\n\n> 备注：关于哪种类型的 Volume 支持哪种类型的 AccessMode，你可以查看 Kubernetes 项目官方文档中的详细列表。\n\n第二步：在应用的 Pod 中，声明使用这个 PVC：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: pv-pod\n\tspec:\n\t  containers:\n\t    - name: pv-container\n\t      image: nginx\n\t      ports:\n\t        - containerPort: 80\n\t          name: \"http-server\"\n\t      volumeMounts:\n\t        - mountPath: \"/usr/share/nginx/html\"\n\t          name: pv-storage\n\t  volumes:\n\t    - name: pv-storage\n\t      persistentVolumeClaim:\n\t        claimName: pv-claim\n可以看到，在这个 Pod 的 Volumes 定义中，我们只需要声明它的类型是 persistentVolumeClaim，然后指定 PVC 的名字，而完全不必关心 Volume 本身的定义。\n\n这时候，只要我们创建这个 PVC 对象，Kubernetes 就会自动为它绑定一个符合条件的 Volume。可是，这些符合条件的 Volume 又是从哪里来的呢？\n\n答案是，它们来自于由运维人员维护的 PV（Persistent Volume）对象。接下来，我们一起看一个常见的 PV 对象的 YAML 文件：\n\n\tkind: PersistentVolume\n\tapiVersion: v1\n\tmetadata:\n\t  name: pv-volume\n\t  labels:\n\t    type: local\n\tspec:\n\t  capacity:\n\t    storage: 10Gi\n\t  rbd:\n\t    monitors:\n\t    - '10.16.154.78:6789'\n\t    - '10.16.154.82:6789'\n\t    - '10.16.154.83:6789'\n\t    pool: kube\n\t    image: foo\n\t    fsType: ext4\n\t    readOnly: true\n\t    user: admin\n\t    keyring: /etc/ceph/keyring\n\t    imageformat: \"2\"\n\t    imagefeatures: \"layering\"\n可以看到，这个 PV 对象的 spec.rbd 字段，正是我们前面介绍过的 Ceph RBD Volume 的详细定义。而且，它还声明了这个 PV 的容量是 10 GiB。这样，Kubernetes 就会为我们刚刚创建的 PVC 对象绑定这个 PV。\n\n所以，**`Kubernetes 中 PVC 和 PV 的设计，实际上类似于“接口”和“实现”的思想。开发者只要知道并会使用“接口”，即：PVC；而运维人员则负责给“接口”绑定具体的实现，即：PV。`**\n\n这种解耦，就避免了因为向开发者暴露过多的存储系统细节而带来的隐患。此外，这种职责的分离，往往也意味着出现事故时可以更容易定位问题和明确责任，从而避免“扯皮”现象的出现。\n\n而 PVC、PV 的设计，也使得 StatefulSet 对存储状态的管理成为了可能。我们还是以上一篇文章中用到的 StatefulSet 为例（你也可以借此再回顾一下《深入理解 StatefulSet（一）：拓扑状态》中的相关内容）：\n\n\tapiVersion: apps/v1\n\tkind: StatefulSet\n\tmetadata:\n\t  name: web\n\tspec:\n\t  serviceName: \"nginx\"\n\t  replicas: 2\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.9.1\n\t        ports:\n\t        - containerPort: 80\n\t          name: web\n\t        volumeMounts:\n\t        - name: www\n\t          mountPath: /usr/share/nginx/html\n\t  volumeClaimTemplates:\n\t  - metadata:\n\t      name: www\n\t    spec:\n\t      accessModes:\n\t      - ReadWriteOnce\n\t      resources:\n\t        requests:\n\t          storage: 1Gi\n这次，我们为这个 StatefulSet 额外添加了一个 volumeClaimTemplates 字段。从名字就可以看出来，它跟 Deployment 里 Pod 模板（PodTemplate）的作用类似。也就是说，凡是被这个 StatefulSet 管理的 Pod，都会声明一个对应的 PVC；而这个 PVC 的定义，就来自于 volumeClaimTemplates 这个模板字段。更重要的是，这个 PVC 的名字，会被分配一个与这个 Pod 完全一致的编号。\n\n这个自动创建的 PVC，与 PV 绑定成功后，就会进入 Bound 状态，这就意味着这个 Pod 可以挂载并使用这个 PV 了。\n\n如果你还是不太理解 PVC 的话，可以先记住这样一个结论：PVC 其实就是一种特殊的 Volume。只不过一个 PVC 具体是什么类型的 Volume，要在跟某个 PV 绑定之后才知道。关于 PV、PVC 更详细的知识，我会在容器存储部分做进一步解读。\n\n当然，PVC 与 PV 的绑定得以实现的前提是，运维人员已经在系统里创建好了符合条件的 PV（比如，我们在前面用到的 pv-volume）；或者，你的 Kubernetes 集群运行在公有云上，这样 Kubernetes 就会通过 Dynamic Provisioning 的方式，自动为你创建与 PVC 匹配的 PV。\n\n所以，我们在使用 kubectl create 创建了 StatefulSet 之后，就会看到 Kubernetes 集群里出现了两个 PVC：\n\n\t$ kubectl create -f statefulset.yaml\n\t$ kubectl get pvc -l app=nginx\n\tNAME        STATUS    VOLUME                                     CAPACITY   ACCESSMODES   AGE\n\twww-web-0   Bound     pvc-15c268c7-b507-11e6-932f-42010a800002   1Gi        RWO           48s\n\twww-web-1   Bound     pvc-15c79307-b507-11e6-932f-42010a800002   1Gi        RWO           48s\n可以看到，这些 PVC，都以“<PVC 名字 >-<StatefulSet 名字 >-< 编号 >”的方式命名，并且处于 Bound 状态。\n\n我们前面已经讲到过，这个 StatefulSet 创建出来的所有 Pod，都会声明使用编号的 PVC。比如，在名叫 web-0 的 Pod 的 volumes 字段，它会声明使用名叫 www-web-0 的 PVC，从而挂载到这个 PVC 所绑定的 PV。\n\n所以，我们就可以使用如下所示的指令，在 Pod 的 Volume 目录里写入一个文件，来验证一下上述 Volume 的分配情况：\n\n\t$ for i in 0 1; do kubectl exec web-$i -- sh -c 'echo hello $(hostname) > /usr/share/nginx/html/index.html'; done\n如上所示，通过 kubectl exec 指令，我们在每个 Pod 的 Volume 目录里，写入了一个 index.html 文件。这个文件的内容，正是 Pod 的 hostname。比如，我们在 web-0 的 index.html 里写入的内容就是 \"hello web-0\"。\n\n此时，如果你在这个 Pod 容器里访问“http://localhost”，你实际访问到的就是 Pod 里 Nginx 服务器进程，而它会为你返回 /usr/share/nginx/html/index.html 里的内容。这个操作的执行方法如下所示：\n\n\t$ for i in 0 1; do kubectl exec -it web-$i -- curl localhost; done\n\thello web-0\n\thello web-1\n现在，关键来了。\n\n如果你使用 kubectl delete 命令删除这两个 Pod，这些 Volume 里的文件会不会丢失呢？\n\n\t$ kubectl delete pod -l app=nginx\n\tpod \"web-0\" deleted\n\tpod \"web-1\" deleted\n可以看到，正如我们前面介绍过的，在被删除之后，这两个 Pod 会被按照编号的顺序被重新创建出来。而这时候，如果你在新创建的容器里通过访问“http://localhost”的方式去访问 web-0 里的 Nginx 服务：\n\n\t# 在被重新创建出来的 Pod 容器里访问 http://localhost\n\t$ kubectl exec -it web-0 -- curl localhost\n\thello web-0\n就会发现，这个请求依然会返回：hello web-0。也就是说，原先与名叫 web-0 的 Pod 绑定的 PV，在这个 Pod 被重新创建之后，依然同新的名叫 web-0 的 Pod 绑定在了一起。对于 Pod web-1 来说，也是完全一样的情况。\n\n这是怎么做到的呢？\n\n其实，我和你分析一下 StatefulSet 控制器恢复这个 Pod 的过程，你就可以很容易理解了。\n\n首先，当你把一个 Pod，比如 web-0，删除之后，这个 Pod 对应的 PVC 和 PV，并不会被删除，而这个 Volume 里已经写入的数据，也依然会保存在远程存储服务里（比如，我们在这个例子里用到的 Ceph 服务器）。\n\n此时，StatefulSet 控制器发现，一个名叫 web-0 的 Pod 消失了。所以，控制器就会重新创建一个新的、名字还是叫作 web-0 的 Pod 来，“纠正”这个不一致的情况。\n\n需要注意的是，在这个新的 Pod 对象的定义里，它声明使用的 PVC 的名字，还是叫作：www-web-0。这个 PVC 的定义，还是来自于 PVC 模板（volumeClaimTemplates），这是 StatefulSet 创建 Pod 的标准流程。\n\n所以，在这个新的 web-0 Pod 被创建出来之后，Kubernetes 为它查找名叫 www-web-0 的 PVC 时，就会直接找到旧 Pod 遗留下来的同名的 PVC，进而找到跟这个 PVC 绑定在一起的 PV。\n\n这样，新的 Pod 就可以挂载到旧 Pod 对应的那个 Volume，并且获取到保存在 Volume 里的数据。\n\n通过这种方式，Kubernetes 的 StatefulSet 就实现了对应用存储状态的管理。\n\n看到这里，你是不是已经大致理解了 StatefulSet 的工作原理呢？现在，我再为你详细梳理一下吧。\n\n**首先，StatefulSet 的控制器直接管理的是 Pod。** 这是因为，StatefulSet 里的不同 Pod 实例，不再像 ReplicaSet 中那样都是完全一样的，而是有了细微区别的。比如，每个 Pod 的 hostname、名字等都是不同的、携带了编号的。而 StatefulSet 区分这些实例的方式，就是通过在 Pod 的名字里加上事先约定好的编号。\n\n**其次，Kubernetes 通过 Headless Service，为这些有编号的 Pod，在 DNS 服务器中生成带有同样编号的 DNS 记录。**只要 StatefulSet 能够保证这些 Pod 名字里的编号不变，那么 Service 里类似于 web-0.nginx.default.svc.cluster.local 这样的 DNS 记录也就不会变，而这条记录解析出来的 Pod 的 IP 地址，则会随着后端 Pod 的删除和再创建而自动更新。这当然是 Service 机制本身的能力，不需要 StatefulSet 操心。\n\n**最后，StatefulSet 还为每一个 Pod 分配并创建一个同样编号的 PVC。**这样，Kubernetes 就可以通过 Persistent Volume 机制为这个 PVC 绑定上对应的 PV，从而保证了每一个 Pod 都拥有一个独立的 Volume。\n\n在这种情况下，即使 Pod 被删除，它所对应的 PVC 和 PV 依然会保留下来。所以当这个 Pod 被重新创建出来之后，Kubernetes 会为它找到同样编号的 PVC，挂载这个 PVC 对应的 Volume，从而获取到以前保存在 Volume 里的数据。\n\n这么一看，原本非常复杂的 StatefulSet，是不是也很容易理解了呢？\n\n**总结**\n在今天这篇文章中，我为你详细介绍了 StatefulSet 处理存储状态的方法。然后，以此为基础，我为你梳理了 StatefulSet 控制器的工作原理。\n\n从这些讲述中，我们不难看出 StatefulSet 的设计思想：StatefulSet 其实就是一种特殊的 Deployment，而其独特之处在于，它的每个 Pod 都被编号了。而且，这个编号会体现在 Pod 的名字和 hostname 等标识信息上，这不仅代表了 Pod 的创建顺序，也是 Pod 的重要网络标识（即：在整个集群里唯一的、可被的访问身份）。\n**`(1、StatefulSet是一种特殊的Deployment 2、他的每个Pod都被编号 3、编号体现在Pod的名字和hostname等标识信息上，代表了Pod的创建顺序，也是Pod的重要网络标识)`**\n\n有了这个编号后，StatefulSet 就使用 Kubernetes 里的两个标准功能：Headless Service 和 PV/PVC，实现了对 Pod 的拓扑状态和存储状态的维护。\n\n实际上，在下一篇文章的“有状态应用”实践环节，以及后续的讲解中，你就会逐渐意识到，StatefulSet 可以说是 Kubernetes 中作业编排的“集大成者”。\n\n因为，几乎每一种 Kubernetes 的编排功能，都可以在编写 StatefulSet 的 YAML 文件时被用到。\n\n**思考题**\n在实际场景中，有一些分布式应用的集群是这么工作的：当一个新节点加入到集群时，或者老节点被迁移后重建时，这个节点可以从主节点或者其他从节点那里同步到自己所需要的数据。\n\n在这种情况下，你认为是否还有必要将这个节点 Pod 与它的 PV 进行一对一绑定呢？（提示：这个问题的答案根据不同的项目是不同的。关键在于，重建后的节点进行数据恢复和同步的时候，是不是一定需要原先它写在本地磁盘里的数据）\n\n\n\n","categories":["reference","k8s"]},{"title":"21 | 容器化守护进程的意义：DaemonSet","url":"/2021/03/13/reference/k8s/21.容器化守护进程的意义：DaemonSet/","content":"\n![](1.jpg)\n\n在上一篇文章中，我和你详细分享了使用 StatefulSet 编排“有状态应用”的过程。从中不难看出，StatefulSet 其实就是对现有典型运维业务的容器化抽象。也就是说，你一定有方法在不使用 Kubernetes、甚至不使用容器的情况下，自己 DIY 一个类似的方案出来。但是，一旦涉及到升级、版本管理等更工程化的能力，Kubernetes 的好处，才会更加凸现。\n\n比如，如何对 StatefulSet 进行“滚动更新”（rolling update）？\n\n很简单。你只要修改 StatefulSet 的 Pod 模板，就会自动触发“滚动更新”:\n\n\t$ kubectl patch statefulset mysql --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"mysql:5.7.23\"}]'\n\tstatefulset.apps/mysql patched\n在这里，我使用了 **`kubectl patch` 命令。它的意思是，以“补丁”的方式（JSON 格式的）修改一个 API 对象的指定字段**，也就是我在后面指定的“spec/template/spec/containers/0/image”。\n\n这样，StatefulSet Controller 就会按照与 Pod 编号相反的顺序，从最后一个 Pod 开始，逐一更新这个 StatefulSet 管理的每个 Pod。而如果更新发生了错误，这次“滚动更新”就会停止。此外，StatefulSet 的“滚动更新”还允许我们进行更精细的控制，比如金丝雀发布（Canary Deploy）或者灰度发布，这意味着应用的多个实例中被指定的一部分不会被更新到最新的版本。\n\n这个字段，正是 StatefulSet 的 spec.updateStrategy.rollingUpdate 的 partition 字段。\n\n比如，现在我将前面这个 StatefulSet 的 partition 字段设置为 2：\n\n\t$ kubectl patch statefulset mysql -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":2}}}}'\n\tstatefulset.apps/mysql patched\n其中，kubectl patch 命令后面的参数（JSON 格式的），就是 partition 字段在 API 对象里的路径。所以，上述操作等同于直接使用 kubectl edit 命令，打开这个对象，把 partition 字段修改为 2。\n\n这样，我就指定了当 Pod 模板发生变化的时候，比如 MySQL 镜像更新到 5.7.23，那么只有序号大于或者等于 2 的 Pod 会被更新到这个版本。并且，如果你删除或者重启了序号小于 2 的 Pod，等它再次启动后，也会保持原先的 5.7.2 版本，绝不会被升级到 5.7.23 版本。\n\nStatefulSet 可以说是 Kubernetes 项目中最为复杂的编排对象，希望你课后能认真消化，动手实践一下这个例子。\n\n而在今天这篇文章中，我会为你重点讲解一个相对轻松的知识点：DaemonSet。\n\n顾名思义，DaemonSet 的主要作用，是让你在 Kubernetes 集群里，运行一个 Daemon Pod。 所以，这个 Pod 有如下三个特征：\n\n1. 这个 Pod 运行在 Kubernetes 集群里的每一个节点（Node）上；\n2. 每个节点上只有一个这样的 Pod 实例；\n3. 当有新的节点加入 Kubernetes 集群后，该 Pod 会自动地在新节点上被创建出来；而当旧节点被删除后，它上面的 Pod 也相应地会被回收掉。\n\n这个机制听起来很简单，但 Daemon Pod 的意义确实是非常重要的。我随便给你列举几个例子：\n\n1. 各种网络插件的 Agent 组件，都必须运行在每一个节点上，用来处理这个节点上的容器网络；\n2. 各种存储插件的 Agent 组件，也必须运行在每一个节点上，用来在这个节点上挂载远程存储目录，操作容器的 Volume 目录；\n3. 各种监控组件和日志组件，也必须运行在每一个节点上，负责这个节点上的监控信息和日志搜集。\n\n更重要的是，跟其他编排对象不一样，DaemonSet 开始运行的时机，很多时候比整个 Kubernetes 集群出现的时机都要早。\n\n这个乍一听起来可能有点儿奇怪。但其实你来想一下：如果这个 DaemonSet 正是一个网络插件的 Agent 组件呢？\n\n这个时候，整个 Kubernetes 集群里还没有可用的容器网络，所有 Worker 节点的状态都是 NotReady（NetworkReady=false）。这种情况下，普通的 Pod 肯定不能运行在这个集群上。所以，这也就意味着 DaemonSet 的设计，必须要有某种“过人之处”才行。\n\n为了弄清楚 DaemonSet 的工作原理，我们还是按照老规矩，先从它的 API 对象的定义说起。\n\n\tapiVersion: apps/v1\n\tkind: DaemonSet\n\tmetadata:\n\t  name: fluentd-elasticsearch\n\t  namespace: kube-system\n\t  labels:\n\t    k8s-app: fluentd-logging\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      name: fluentd-elasticsearch\n\t  template:\n\t    metadata:\n\t      labels:\n\t        name: fluentd-elasticsearch\n\t    spec:\n\t      tolerations:\n\t      - key: node-role.kubernetes.io/master\n\t        effect: NoSchedule\n\t      containers:\n\t      - name: fluentd-elasticsearch\n\t        image: k8s.gcr.io/fluentd-elasticsearch:1.20\n\t        resources:\n\t          limits:\n\t            memory: 200Mi\n\t          requests:\n\t            cpu: 100m\n\t            memory: 200Mi\n\t        volumeMounts:\n\t        - name: varlog\n\t          mountPath: /var/log\n\t        - name: varlibdockercontainers\n\t          mountPath: /var/lib/docker/containers\n\t          readOnly: true\n\t      terminationGracePeriodSeconds: 30\n\t      volumes:\n\t      - name: varlog\n\t        hostPath:\n\t          path: /var/log\n\t      - name: varlibdockercontainers\n\t        hostPath:\n\t          path: /var/lib/docker/containers\n这个 DaemonSet，管理的是一个 fluentd-elasticsearch 镜像的 Pod。这个镜像的功能非常实用：通过 fluentd 将 Docker 容器里的日志转发到 ElasticSearch 中。\n\n可以看到，DaemonSet 跟 Deployment 其实非常相似，只不过是没有 replicas 字段；它也使用 selector 选择管理所有携带了 name=fluentd-elasticsearch 标签的 Pod。\n\n而这些 Pod 的模板，也是用 template 字段定义的。在这个字段中，我们定义了一个使用 fluentd-elasticsearch:1.20 镜像的容器，而且这个容器挂载了两个 hostPath 类型的 Volume，分别对应宿主机的 /var/log 目录和 /var/lib/docker/containers 目录。\n\n显然，fluentd 启动之后，它会从这两个目录里搜集日志信息，并转发给 ElasticSearch 保存。这样，我们通过 ElasticSearch 就可以很方便地检索这些日志了。\n\n需要注意的是，Docker 容器里应用的日志，默认会保存在宿主机的 `/var/lib/docker/containers/<容器 ID>/<容器 ID>-json.log` 文件里，所以这个目录正是 fluentd 的搜集目标。\n\n那么，**DaemonSet 又是如何保证每个 Node 上有且只有一个被管理的 Pod 呢？**\n\n显然，这是一个典型的“控制器模型”能够处理的问题。\n\nDaemonSet Controller，首先从 Etcd 里获取所有的 Node 列表，然后遍历所有的 Node。这时，它就可以很容易地去检查，当前这个 Node 上是不是有一个携带了 name=fluentd-elasticsearch 标签的 Pod 在运行。\n\n而检查的结果，可能有这么三种情况：\n\n1. 没有这种 Pod，那么就意味着要在这个 Node 上创建这样一个 Pod；\n2. 有这种 Pod，但是数量大于 1，那就说明要把多余的 Pod 从这个 Node 上删除掉；\n3. 正好只有一个这种 Pod，那说明这个节点是正常的。\n\n其中，删除节点（Node）上多余的 Pod 非常简单，直接调用 Kubernetes API 就可以了。\n\n但是，如何在指定的 Node 上创建新 Pod 呢？\n\n如果你已经熟悉了 Pod API 对象的话，那一定可以立刻说出答案：用 nodeSelector，选择 Node 的名字即可。\n\n\tnodeSelector:\n\t    name: <Node 名字 >\n没错。\n\n不过，在 Kubernetes 项目里，**nodeSelector 其实已经是一个将要被废弃的字段了**。因为，现在有了一个新的、功能更完善的字段可以代替它，即：nodeAffinity。我来举个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: with-node-affinity\n\tspec:\n\t  affinity:\n\t    nodeAffinity:\n\t      requiredDuringSchedulingIgnoredDuringExecution:\n\t        nodeSelectorTerms:\n\t        - matchExpressions:\n\t          - key: metadata.name\n\t            operator: In\n\t            values:\n\t            - node-geektime\n在这个 Pod 里，我声明了一个 spec.affinity 字段，然后定义了一个 nodeAffinity。其中，spec.affinity 字段，是 Pod 里跟调度相关的一个字段。关于它的完整内容，我会在讲解调度策略的时候再详细阐述。\n\n而在这里，我定义的 nodeAffinity 的含义是：\n\n1. requiredDuringSchedulingIgnoredDuringExecution：它的意思是说，这个 nodeAffinity 必须在每次调度的时候予以考虑。同时，这也意味着你可以设置在某些情况下不考虑这个 nodeAffinity；\n2. 这个 Pod，将来只允许运行在“metadata.name”是“node-geektime”的节点上。\n\n在这里，你应该注意到 nodeAffinity 的定义，可以支持更加丰富的语法，比如 operator: In（即：部分匹配；如果你定义 operator: Equal，就是完全匹配），这也正是 nodeAffinity 会取代 nodeSelector 的原因之一。\n\n> 备注：其实在大多数时候，这些 Operator 语义没啥用处。所以说，在学习开源项目的时候，一定要学会抓住“主线”。不要顾此失彼。\n\n所以，**我们的 DaemonSet Controller 会在创建 Pod 的时候，自动在这个 Pod 的 API 对象里，加上这样一个 nodeAffinity 定义**。其中，需要绑定的节点名字，正是当前正在遍历的这个 Node。\n\n当然，DaemonSet 并不需要修改用户提交的 YAML 文件里的 Pod 模板，而是在向 Kubernetes 发起请求之前，直接修改根据模板生成的 Pod 对象。这个思路，也正是我在前面讲解 Pod 对象时介绍过的。\n\n此外，DaemonSet 还会给这个 Pod 自动加上另外一个与调度相关的字段，叫作 tolerations。这个字段意味着这个 Pod，会“容忍”（Toleration）某些 Node 的“污点”（Taint）。\n\n而 DaemonSet 自动加上的 tolerations 字段，格式如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: with-toleration\n\tspec:\n\t  tolerations:\n\t  - key: node.kubernetes.io/unschedulable\n\t    operator: Exists\n\t    effect: NoSchedule\n这个 Toleration 的含义是：“容忍”所有被标记为 unschedulable“污点”的 Node；“容忍”的效果是允许调度。\n\n> 备注：关于如何给一个 Node 标记上“污点”，以及这里具体的语法定义，我会在后面介绍调度器的时候做详细介绍。这里，你可以简单地把“污点”理解为一种特殊的 Label。\n\n而在正常情况下，被标记了 unschedulable“污点”的 Node，是不会有任何 Pod 被调度上去的（effect: NoSchedule）。可是，DaemonSet 自动地给被管理的 Pod 加上了这个特殊的 Toleration，就使得这些 Pod 可以忽略这个限制，继而保证每个节点上都会被调度一个 Pod。当然，如果这个节点有故障的话，这个 Pod 可能会启动失败，而 DaemonSet 则会始终尝试下去，直到 Pod 启动成功。\n\n这时，你应该可以猜到，我在前面介绍到的DaemonSet 的“过人之处”，其实就是依靠 Toleration 实现的。\n\n假如当前 DaemonSet 管理的，是一个网络插件的 Agent Pod，那么你就必须在这个 DaemonSet 的 YAML 文件里，给它的 Pod 模板加上一个能够“容忍”node.kubernetes.io/network-unavailable“污点”的 Toleration。正如下面这个例子所示：\n\n\t...\n\ttemplate:\n\t    metadata:\n\t      labels:\n\t        name: network-plugin-agent\n\t    spec:\n\t      tolerations:\n\t      - key: node.kubernetes.io/network-unavailable\n\t        operator: Exists\n\t        effect: NoSchedule\n在 Kubernetes 项目中，当一个节点的网络插件尚未安装时，这个节点就会被自动加上名为node.kubernetes.io/network-unavailable的“污点”。\n\n**而通过这样一个 Toleration，调度器在调度这个 Pod 的时候，就会忽略当前节点上的“污点”，从而成功地将网络插件的 Agent 组件调度到这台机器上启动起来。**\n\n这种机制，正是我们在部署 Kubernetes 集群的时候，能够先部署 Kubernetes 本身、再部署网络插件的根本原因：因为当时我们所创建的 Weave 的 YAML，实际上就是一个 DaemonSet。\n\n> 这里，你也可以再回顾一下第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》中的相关内容。\n\n至此，通过上面这些内容，你应该能够明白，DaemonSet 其实是一个非常简单的控制器。在它的控制循环中，只需要遍历所有节点，然后根据节点上是否有被管理 Pod 的情况，来决定是否要创建或者删除一个 Pod。\n\n只不过，在创建每个 Pod 的时候，DaemonSet 会自动给这个 Pod 加上一个 nodeAffinity，从而保证这个 Pod 只会在指定节点上启动。同时，它还会自动给这个 Pod 加上一个 Toleration，从而忽略节点的 unschedulable“污点”。\n\n当然，**你也可以在 Pod 模板里加上更多种类的 Toleration，从而利用 DaemonSet 实现自己的目的**。比如，在这个 fluentd-elasticsearch DaemonSet 里，我就给它加上了这样的 Toleration：\n\n\ttolerations:\n\t- key: node-role.kubernetes.io/master\n\t  effect: NoSchedule\n这是因为在默认情况下，Kubernetes 集群不允许用户在 Master 节点部署 Pod。因为，Master 节点默认携带了一个叫作node-role.kubernetes.io/master的“污点”。所以，为了能在 Master 节点上部署 DaemonSet 的 Pod，我就必须让这个 Pod“容忍”这个“污点”。\n\n在理解了 DaemonSet 的工作原理之后，接下来我就通过一个具体的实践来帮你更深入地掌握 DaemonSet 的使用方法。\n\n> 备注：需要注意的是，在 Kubernetes v1.11 之前，由于调度器尚不完善，DaemonSet 是由 DaemonSet Controller 自行调度的，即它会直接设置 Pod 的 spec.nodename 字段，这样就可以跳过调度器了。但是，这样的做法很快就会被废除，所以在这里我也不推荐你再花时间学习这个流程了。\n\n首先，创建这个 DaemonSet 对象：\n\n\t$ kubectl create -f fluentd-elasticsearch.yaml\n需要注意的是，在 DaemonSet 上，我们一般都应该加上 resources 字段，来限制它的 CPU 和内存使用，防止它占用过多的宿主机资源。\n\n而创建成功后，你就能看到，如果有 N 个节点，就会有 N 个 fluentd-elasticsearch Pod 在运行。比如在我们的例子里，会有两个 Pod，如下所示：\n\n\t$ kubectl get pod -n kube-system -l name=fluentd-elasticsearch\n\tNAME                          READY     STATUS    RESTARTS   AGE\n\tfluentd-elasticsearch-dqfv9   1/1       Running   0          53m\n\tfluentd-elasticsearch-pf9z5   1/1       Running   0          53m\n而如果你此时通过 kubectl get 查看一下 Kubernetes 集群里的 DaemonSet 对象：\n\n\t$ kubectl get ds -n kube-system fluentd-elasticsearch\n\tNAME                    DESIRED   CURRENT   READY     UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE\n\tfluentd-elasticsearch   2         2         2         2            2           <none>          1h\n> 备注：Kubernetes 里比较长的 API 对象都有短名字，比如 DaemonSet 对应的是 ds，Deployment 对应的是 deploy。\n\n就会发现 DaemonSet 和 Deployment 一样，也有 DESIRED、CURRENT 等多个状态字段。这也就意味着，DaemonSet 可以像 Deployment 那样，进行版本管理。这个版本，可以使用 kubectl rollout history 看到：\n\n\t$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system\n\tdaemonsets \"fluentd-elasticsearch\"\n\tREVISION  CHANGE-CAUSE\n\t1         <none>\n接下来，我们来把这个 DaemonSet 的容器镜像版本到 v2.2.0：\n\n\t$ kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record -n=kube-system\n这个 kubectl set image 命令里，第一个 fluentd-elasticsearch 是 DaemonSet 的名字，第二个 fluentd-elasticsearch 是容器的名字。\n\n这时候，我们可以使用 kubectl rollout status 命令看到这个“滚动更新”的过程，如下所示：\n\n\t$ kubectl rollout status ds/fluentd-elasticsearch -n kube-system\n\tWaiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 0 out of 2 new pods have been updated...\n\tWaiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 0 out of 2 new pods have been updated...\n\tWaiting for daemon set \"fluentd-elasticsearch\" rollout to finish: 1 of 2 updated pods are available...\n\tdaemon set \"fluentd-elasticsearch\" successfully rolled out\n注意，由于这一次我在升级命令后面加上了`--record` 参数，所以这次升级使用到的指令就会自动出现在 DaemonSet 的 rollout history 里面，如下所示：\n\n\t$ kubectl rollout history daemonset fluentd-elasticsearch -n kube-system\n\tdaemonsets \"fluentd-elasticsearch\"\n\tREVISION  CHANGE-CAUSE\n\t1         <none>\n\t2         kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --namespace=kube-system --record=true\n有了版本号，你也就可以像 Deployment 一样，将 DaemonSet 回滚到某个指定的历史版本了。\n\n而我在前面的文章中讲解 Deployment 对象的时候，曾经提到过，Deployment 管理这些版本，靠的是“一个版本对应一个 ReplicaSet 对象”。可是，DaemonSet 控制器操作的直接就是 Pod，不可能有 ReplicaSet 这样的对象参与其中。那么，它的这些版本又是如何维护的呢？\n\n所谓，一切皆对象！\n\n在 Kubernetes 项目中，任何你觉得需要记录下来的状态，都可以被用 API 对象的方式实现。当然，“版本”也不例外。\n\nKubernetes v1.7 之后添加了一个 API 对象，名叫ControllerRevision，专门用来记录某种 Controller 对象的版本。比如，你可以通过如下命令查看 fluentd-elasticsearch 对应的 ControllerRevision：\n\n\t$ kubectl get controllerrevision -n kube-system -l name=fluentd-elasticsearch\n\tNAME                               CONTROLLER                             REVISION   AGE\n\tfluentd-elasticsearch-64dc6799c9   daemonset.apps/fluentd-elasticsearch   2          1h\n而如果你使用 kubectl describe 查看这个 ControllerRevision 对象：\n\n\t$ kubectl describe controllerrevision fluentd-elasticsearch-64dc6799c9 -n kube-system\n\tName:         fluentd-elasticsearch-64dc6799c9\n\tNamespace:    kube-system\n\tLabels:       controller-revision-hash=2087235575\n\t              name=fluentd-elasticsearch\n\tAnnotations:  deprecated.daemonset.template.generation=2\n\t              kubernetes.io/change-cause=kubectl set image ds/fluentd-elasticsearch fluentd-elasticsearch=k8s.gcr.io/fluentd-elasticsearch:v2.2.0 --record=true --namespace=kube-system\n\tAPI Version:  apps/v1\n\tData:\n\t  Spec:\n\t    Template:\n\t      $ Patch:  replace\n\t      Metadata:\n\t        Creation Timestamp:  <nil>\n\t        Labels:\n\t          Name:  fluentd-elasticsearch\n\t      Spec:\n\t        Containers:\n\t          Image:              k8s.gcr.io/fluentd-elasticsearch:v2.2.0\n\t          Image Pull Policy:  IfNotPresent\n\t          Name:               fluentd-elasticsearch\n\t...\n\tRevision:                  2\n\tEvents:                    <none>\n就会看到，这个 ControllerRevision 对象，实际上是在 Data 字段保存了该版本对应的完整的 DaemonSet 的 API 对象。并且，在 Annotation 字段保存了创建这个对象所使用的 kubectl 命令。\n\n接下来，我们可以尝试将这个 DaemonSet 回滚到 Revision=1 时的状态：\n\n\t$ kubectl rollout undo daemonset fluentd-elasticsearch --to-revision=1 -n kube-system\n\tdaemonset.extensions/fluentd-elasticsearch rolled back\n这个 kubectl rollout undo 操作，实际上相当于读取到了 Revision=1 的 ControllerRevision 对象保存的 Data 字段。而这个 Data 字段里保存的信息，就是 Revision=1 时这个 DaemonSet 的完整 API 对象。\n\n所以，现在 DaemonSet Controller 就可以使用这个历史 API 对象，对现有的 DaemonSet 做一次 PATCH 操作（等价于执行一次 kubectl apply -f “旧的 DaemonSet 对象”），从而把这个 DaemonSet“更新”到一个旧版本。\n\n这也是为什么，在执行完这次回滚完成后，你会发现，DaemonSet 的 Revision 并不会从 Revision=2 退回到 1，而是会增加成 Revision=3。这是因为，一个新的 ControllerRevision 被创建了出来。\n\n**总结**\n在今天这篇文章中，我首先简单介绍了 StatefulSet 的“滚动更新”，然后重点讲解了本专栏的第三个重要编排对象：DaemonSet。\n\n相比于 Deployment，DaemonSet 只管理 Pod 对象，然后通过 nodeAffinity 和 Toleration 这两个调度器的小功能，保证了每个节点上有且只有一个 Pod。这个控制器的实现原理简单易懂，希望你能够快速掌握。\n\n与此同时，DaemonSet 使用 ControllerRevision，来保存和管理自己对应的“版本”。这种“面向 API 对象”的设计思路，大大简化了控制器本身的逻辑，也正是 Kubernetes 项目“声明式 API”的优势所在。\n\n而且，相信聪明的你此时已经想到了，StatefulSet 也是直接控制 Pod 对象的，那么它是不是也在使用 ControllerRevision 进行版本管理呢？\n\n没错。在 Kubernetes 项目里，ControllerRevision 其实是一个通用的版本管理对象。这样，Kubernetes 项目就巧妙地避免了每种控制器都要维护一套冗余的代码和逻辑的问题。\n\n**思考题**\n我在文中提到，在 Kubernetes v1.11 之前，DaemonSet 所管理的 Pod 的调度过程，实际上都是由 DaemonSet Controller 自己而不是由调度器完成的。你能说出这其中有哪些原因吗？\n\n","categories":["reference","k8s"]},{"title":"22 | 撬动离线业务：Job与CronJob","url":"/2021/03/13/reference/k8s/22.撬动离线业务：Job与CronJob/","content":"\n![](1.jpg)\n\n在前面的几篇文章中，我和你详细分享了 Deployment、StatefulSet，以及 DaemonSet 这三个编排概念。你有没有发现它们的共同之处呢？\n\n实际上，它们主要编排的对象，都是“在线业务”，即：Long Running Task（长作业）。比如，我在前面举例时常用的 Nginx、Tomcat，以及 MySQL 等等。这些应用一旦运行起来，除非出错或者停止，它的容器进程会一直保持在 Running 状态。\n\n但是，有一类作业显然不满足这样的条件，这就是“离线业务”，或者叫作 Batch Job（计算业务）。这种业务在计算完成后就直接退出了，而此时如果你依然用 Deployment 来管理这种业务的话，就会发现 Pod 会在计算结束后退出，然后被 Deployment Controller 不断地重启；而像“滚动更新”这样的编排功能，更无从谈起了。\n\n所以，早在 Borg 项目中，Google 就已经对作业进行了分类处理，提出了 LRS（Long Running Service）和 Batch Jobs 两种作业形态，对它们进行“分别管理”和“混合调度”。\n**(1、LRS（Long Running Service）:Deployment、StatefulSet、DaemonSet; 2、Batch Jobs: Job，CronJob)**\n\n不过，在 2015 年 Borg 论文刚刚发布的时候，Kubernetes 项目并不支持对 Batch Job 的管理。直到 v1.4 版本之后，社区才逐步设计出了一个用来描述离线业务的 API 对象，它的名字就是：Job。\n\nJob API 对象的定义非常简单，我来举个例子，如下所示：\n\n\tapiVersion: batch/v1\n\tkind: Job\n\tmetadata:\n\t  name: pi\n\tspec:\n\t  template:\n\t    spec:\n\t      containers:\n\t      - name: pi\n\t        image: resouer/ubuntu-bc \n\t        command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"]\n\t      restartPolicy: Never\n\t  backoffLimit: 4\n此时，相信你对 Kubernetes 的 API 对象已经不再陌生了。在这个 Job 的 YAML 文件里，你肯定一眼就会看到一位“老熟人”：Pod 模板，即 spec.template 字段。\n\n在这个 Pod 模板中，我定义了一个 Ubuntu 镜像的容器（准确地说，是一个安装了 bc 命令的 Ubuntu 镜像），它运行的程序是：\n\n\techo \"scale=10000; 4*a(1)\" | bc -l \n其中，bc 命令是 Linux 里的“计算器”；-l 表示，我现在要使用标准数学库；而 a(1)，则是调用数学库中的 arctangent 函数，计算 atan(1)。这是什么意思呢？\n\n中学知识告诉我们：tan(π/4) = 1。所以，4*atan(1)正好就是π，也就是 3.1415926…。\n\n> 备注：如果你不熟悉这个知识也不必担心，我也是在查阅资料后才知道的。\n\n所以，这其实就是一个计算π值的容器。而通过 scale=10000，我指定了输出的小数点后的位数是 10000。在我的计算机上，这个计算大概用时 1 分 54 秒。\n\n但是，跟其他控制器不同的是，Job 对象并不要求你定义一个 spec.selector 来描述要控制哪些 Pod。具体原因，我马上会讲解到。\n\n现在，我们就可以创建这个 Job 了：\n\n\t$ kubectl create -f job.yaml\n在成功创建后，我们来查看一下这个 Job 对象，如下所示：\n\n\t$ kubectl describe jobs/pi\n\tName:             pi\n\tNamespace:        default\n\tSelector:         controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495\n\tLabels:           controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495\n\t                  job-name=pi\n\tAnnotations:      <none>\n\tParallelism:      1\n\tCompletions:      1\n\t..\n\tPods Statuses:    0 Running / 1 Succeeded / 0 Failed\n\tPod Template:\n\t  Labels:       controller-uid=c2db599a-2c9d-11e6-b324-0209dc45a495\n\t                job-name=pi\n\t  Containers:\n\t   ...\n\t  Volumes:              <none>\n\tEvents:\n\t  FirstSeen    LastSeen    Count    From            SubobjectPath    Type        Reason            Message\n\t  ---------    --------    -----    ----            -------------    --------    ------            -------\n\t  1m           1m          1        {job-controller }                Normal      SuccessfulCreate  Created pod: pi-rq5rl\n可以看到，这个 Job 对象在创建后，它的 Pod 模板，被自动加上了一个 controller-uid=< 一个随机字符串 > 这样的 Label。而这个 Job 对象本身，则被自动加上了这个 Label 对应的 Selector，从而 保证了 Job 与它所管理的 Pod 之间的匹配关系。\n\n而 Job Controller 之所以要使用这种携带了 UID 的 Label，就是为了避免不同 Job 对象所管理的 Pod 发生重合。需要注意的是，这种自动生成的 Label 对用户来说并不友好，所以不太适合推广到 Deployment 等长作业编排对象上。\n\n接下来，我们可以看到这个 Job 创建的 Pod 进入了 Running 状态，这意味着它正在计算 Pi 的值。\n\n\t$ kubectl get pods\n\tNAME                                READY     STATUS    RESTARTS   AGE\n\tpi-rq5rl                            1/1       Running   0          10s\n而几分钟后计算结束，这个 Pod 就会进入 Completed 状态：\n\n\t$ kubectl get pods\n\tNAME                                READY     STATUS      RESTARTS   AGE\n\tpi-rq5rl                            0/1       Completed   0          4m\n这也是我们需要在 Pod 模板中定义 restartPolicy=Never 的原因：离线计算的 Pod 永远都不应该被重启，否则它们会再重新计算一遍。\n\n> 事实上，restartPolicy 在 Job 对象里只允许被设置为 Never 和 OnFailure；而在 Deployment 对象里，restartPolicy 则只允许被设置为 Always。\n\n此时，我们通过 kubectl logs 查看一下这个 Pod 的日志，就可以看到计算得到的 Pi 值已经被打印了出来：\n\n\t$ kubectl logs pi-rq5rl\n\t3.141592653589793238462643383279...\n这时候，你一定会想到这样一个问题，如果这个离线作业失败了要怎么办？\n\n比如，我们在这个例子中定义了 restartPolicy=Never，那么离线作业失败后 Job Controller 就会不断地尝试创建一个新 Pod，如下所示：\n\n\t$ kubectl get pods\n\tNAME                                READY     STATUS              RESTARTS   AGE\n\tpi-55h89                            0/1       ContainerCreating   0          2s\n\tpi-tqbcz                            0/1       Error               0          5s\n可以看到，这时候会不断地有新 Pod 被创建出来。\n\n当然，这个尝试肯定不能无限进行下去。所以，我们就在 Job 对象的 spec.backoffLimit 字段里定义了重试次数为 4（即，backoffLimit=4），而这个字段的默认值是 6。\n\n需要注意的是，Job Controller 重新创建 Pod 的间隔是呈指数增加的，即下一次重新创建 Pod 的动作会分别发生在 10 s、20 s、40 s …后。\n\n而如果你定义的 **restartPolicy=OnFailure，那么离线作业失败后，Job Controller 就不会去尝试创建新的 Pod。但是，它会不断地尝试重启 Pod 里的容器**。这也正好对应了 restartPolicy 的含义（你也可以借此机会再回顾一下第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中的相关内容）。\n\n如前所述，当一个 Job 的 Pod 运行结束后，它会进入 Completed 状态。但是，如果这个 Pod 因为某种原因一直不肯结束呢？\n\n在 Job 的 API 对象里，有一个 spec.activeDeadlineSeconds 字段可以设置最长运行时间，比如：\n\n\tspec:\n\t backoffLimit: 5\n\t activeDeadlineSeconds: 100\n一旦运行超过了 100 s，这个 Job 的所有 Pod 都会被终止。并且，你可以在 Pod 的状态里看到终止的原因是 reason: DeadlineExceeded。\n\n以上，就是一个 Job API 对象最主要的概念和用法了。不过，离线业务之所以被称为 Batch Job，当然是因为它们可以以“Batch”，也就是并行的方式去运行。\n\n接下来，我就来为你讲解一下Job Controller 对并行作业的控制方法。\n\n在 Job 对象中，负责并行控制的参数有两个：\n\n1. spec.parallelism，它定义的是一个 Job 在任意时间最多可以启动多少个 Pod 同时运行；\n2. spec.completions，它定义的是 Job 至少要完成的 Pod 数目，即 Job 的最小完成数。\n\n这两个参数听起来有点儿抽象，所以我准备了一个例子来帮助你理解。\n\n现在，我在之前计算 Pi 值的 Job 里，添加这两个参数：\n\n\tapiVersion: batch/v1\n\tkind: Job\n\tmetadata:\n\t  name: pi\n\tspec:\n\t  parallelism: 2\n\t  completions: 4\n\t  template:\n\t    spec:\n\t      containers:\n\t      - name: pi\n\t        image: resouer/ubuntu-bc\n\t        command: [\"sh\", \"-c\", \"echo 'scale=5000; 4*a(1)' | bc -l \"]\n\t      restartPolicy: Never\n\t  backoffLimit: 4\n这样，我们就指定了这个 Job 最大的并行数是 2，而最小的完成数是 4。\n\n接下来，我们来创建这个 Job 对象：\n\n\t$ kubectl create -f job.yaml\n可以看到，这个 Job 其实也维护了两个状态字段，即 DESIRED 和 SUCCESSFUL，如下所示：\n\n\t$ kubectl get job\n\tNAME      DESIRED   SUCCESSFUL   AGE\n\tpi        4         0            3s\n其中，DESIRED 的值，正是 completions 定义的最小完成数。\n\n然后，我们可以看到，这个 Job 首先创建了两个并行运行的 Pod 来计算 Pi：\n\n\t$ kubectl get pods\n\tNAME       READY     STATUS    RESTARTS   AGE\n\tpi-5mt88   1/1       Running   0          6s\n\tpi-gmcq5   1/1       Running   0          6s\n而在 40 s 后，这两个 Pod 相继完成计算。\n\n这时我们可以看到，每当有一个 Pod 完成计算进入 Completed 状态时，就会有一个新的 Pod 被自动创建出来，并且快速地从 Pending 状态进入到 ContainerCreating 状态：\n\n\t$ kubectl get pods\n\tNAME       READY     STATUS    RESTARTS   AGE\n\tpi-gmcq5   0/1       Completed   0         40s\n\tpi-84ww8   0/1       Pending   0         0s\n\tpi-5mt88   0/1       Completed   0         41s\n\tpi-62rbt   0/1       Pending   0         0s\n\t\n\t$ kubectl get pods\n\tNAME       READY     STATUS    RESTARTS   AGE\n\tpi-gmcq5   0/1       Completed   0         40s\n\tpi-84ww8   0/1       ContainerCreating   0         0s\n\tpi-5mt88   0/1       Completed   0         41s\n\tpi-62rbt   0/1       ContainerCreating   0         0s\n紧接着，Job Controller 第二次创建出来的两个并行的 Pod 也进入了 Running 状态：\n\n\t$ kubectl get pods \n\tNAME       READY     STATUS      RESTARTS   AGE\n\tpi-5mt88   0/1       Completed   0          54s\n\tpi-62rbt   1/1       Running     0          13s\n\tpi-84ww8   1/1       Running     0          14s\n\tpi-gmcq5   0/1       Completed   0          54s\n最终，后面创建的这两个 Pod 也完成了计算，进入了 Completed 状态。\n\n这时，由于所有的 Pod 均已经成功退出，这个 Job 也就执行完了，所以你会看到它的 SUCCESSFUL 字段的值变成了 4：\n\n\t$ kubectl get pods \n\tNAME       READY     STATUS      RESTARTS   AGE\n\tpi-5mt88   0/1       Completed   0          5m\n\tpi-62rbt   0/1       Completed   0          4m\n\tpi-84ww8   0/1       Completed   0          4m\n\tpi-gmcq5   0/1       Completed   0          5m\n\t\n\t$ kubectl get job\n\tNAME      DESIRED   SUCCESSFUL   AGE\n\tpi        4         4            5m\n通过上述 Job 的 DESIRED 和 SUCCESSFUL 字段的关系，我们就可以很容易地理解`Job Controller 的工作原理了`。\n\n首先，Job Controller 控制的对象，直接就是 Pod。\n\n其次，Job Controller 在控制循环中进行的调谐（Reconcile）操作，是根据实际在 Running 状态 Pod 的数目、已经成功退出的 Pod 的数目，以及 parallelism、completions 参数的值共同计算出在这个周期里，应该创建或者删除的 Pod 数目，然后调用 Kubernetes API 来执行这个操作。\n\n以创建 Pod 为例。在上面计算 Pi 值的这个例子中，当 Job 一开始创建出来时，实际处于 Running 状态的 Pod 数目 =0，已经成功退出的 Pod 数目 =0，而用户定义的 completions，也就是最终用户需要的 Pod 数目 =4。\n\n所以，在这个时刻，需要创建的 Pod 数目 = 最终需要的 Pod 数目 - 实际在 Running 状态 Pod 数目 - 已经成功退出的 Pod 数目 = 4 - 0 - 0= 4。也就是说，Job Controller 需要创建 4 个 Pod 来纠正这个不一致状态。\n\n可是，我们又定义了这个 Job 的 parallelism=2。也就是说，我们规定了每次并发创建的 Pod 个数不能超过 2 个。所以，Job Controller 会对前面的计算结果做一个修正，修正后的期望创建的 Pod 数目应该是：2 个。\n\n这时候，Job Controller 就会并发地向 kube-apiserver 发起两个创建 Pod 的请求。\n\n类似地，如果在这次调谐周期里，Job Controller 发现实际在 Running 状态的 Pod 数目，比 parallelism 还大，那么它就会删除一些 Pod，使两者相等。\n\n综上所述，Job Controller 实际上控制了，作业执行的并行度，以及总共需要完成的任务数这两个重要参数。而在实际使用时，你需要根据作业的特性，来决定并行度（parallelism）和任务数（completions）的合理取值。\n\n接下来，我再和你分享三种常用的、使用 Job 对象的方法。\n\n第一种用法，也是最简单粗暴的用法：外部管理器 +Job 模板。\n\n这种模式的特定用法是：把 Job 的 YAML 文件定义为一个“模板”，然后用一个外部工具控制这些“模板”来生成 Job。这时，Job 的定义方式如下所示：\n\n\tapiVersion: batch/v1\n\tkind: Job\n\tmetadata:\n\t  name: process-item-$ITEM\n\t  labels:\n\t    jobgroup: jobexample\n\tspec:\n\t  template:\n\t    metadata:\n\t      name: jobexample\n\t      labels:\n\t        jobgroup: jobexample\n\t    spec:\n\t      containers:\n\t      - name: c\n\t        image: busybox\n\t        command: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"]\n\t      restartPolicy: Never\n可以看到，我们在这个 Job 的 YAML 里，定义了 $ITEM 这样的“变量”。\n\n所以，在控制这种 Job 时，我们只要注意如下两个方面即可：\n\n1. 创建 Job 时，替换掉 $ITEM 这样的变量；\n2. 所有来自于同一个模板的 Job，都有一个 jobgroup: jobexample 标签，也就是说这一组 Job 使用这样一个相同的标识。\n\n而做到第一点非常简单。比如，你可以通过这样一句 shell 把 $ITEM 替换掉：\n\n\t$ mkdir ./jobs\n\t$ for i in apple banana cherry\n\tdo\n\t  cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" > ./jobs/job-$i.yaml\n\tdone\n这样，一组来自于同一个模板的不同 Job 的 yaml 就生成了。接下来，你就可以通过一句 kubectl create 指令创建这些 Job 了：\n\n\t$ kubectl create -f ./jobs\n\t$ kubectl get pods -l jobgroup=jobexample\n\tNAME                        READY     STATUS      RESTARTS   AGE\n\tprocess-item-apple-kixwv    0/1       Completed   0          4m\n\tprocess-item-banana-wrsf7   0/1       Completed   0          4m\n\tprocess-item-cherry-dnfu9   0/1       Completed   0          4m\n这个模式看起来虽然很“傻”，但却是 Kubernetes 社区里使用 Job 的一个很普遍的模式。\n\n原因很简单：大多数用户在需要管理 Batch Job 的时候，都已经有了一套自己的方案，需要做的往往就是集成工作。这时候，Kubernetes 项目对这些方案来说最有价值的，就是 Job 这个 API 对象。所以，你只需要编写一个外部工具（等同于我们这里的 for 循环）来管理这些 Job 即可。\n\n这种模式最典型的应用，就是 TensorFlow 社区的 KubeFlow 项目。\n\n很容易理解，在这种模式下使用 Job 对象，completions 和 parallelism 这两个字段都应该使用默认值 1，而不应该由我们自行设置。而作业 Pod 的并行控制，应该完全交由外部工具来进行管理（比如，KubeFlow）。\n\n第二种用法：拥有固定任务数目的并行 Job。\n\n这种模式下，我只关心最后是否有指定数目（spec.completions）个任务成功退出。至于执行时的并行度是多少，我并不关心。\n\n比如，我们这个计算 Pi 值的例子，就是这样一个典型的、拥有固定任务数目（completions=4）的应用场景。 它的 parallelism 值是 2；或者，你可以干脆不指定 parallelism，直接使用默认的并行度（即：1）。\n\n此外，你还可以使用一个工作队列（Work Queue）进行任务分发。这时，Job 的 YAML 文件定义如下所示：\n\n\tapiVersion: batch/v1\n\tkind: Job\n\tmetadata:\n\t  name: job-wq-1\n\tspec:\n\t  completions: 8\n\t  parallelism: 2\n\t  template:\n\t    metadata:\n\t      name: job-wq-1\n\t    spec:\n\t      containers:\n\t      - name: c\n\t        image: myrepo/job-wq-1\n\t        env:\n\t        - name: BROKER_URL\n\t          value: amqp://guest:guest@rabbitmq-service:5672\n\t        - name: QUEUE\n\t          value: job1\n\t      restartPolicy: OnFailure\n我们可以看到，它的 completions 的值是：8，这意味着我们总共要处理的任务数目是 8 个。也就是说，总共会有 8 个任务会被逐一放入工作队列里（你可以运行一个外部小程序作为生产者，来提交任务）。\n\n在这个实例中，我选择充当工作队列的是一个运行在 Kubernetes 里的 RabbitMQ。所以，我们需要在 Pod 模板里定义 BROKER_URL，来作为消费者。\n\n所以，一旦你用 kubectl create 创建了这个 Job，它就会以并发度为 2 的方式，每两个 Pod 一组，创建出 8 个 Pod。每个 Pod 都会去连接 BROKER_URL，从 RabbitMQ 里读取任务，然后各自进行处理。这个 Pod 里的执行逻辑，我们可以用这样一段伪代码来表示：\n\n\t/* job-wq-1 的伪代码 */\n\tqueue := newQueue($BROKER_URL, $QUEUE)\n\ttask := queue.Pop()\n\tprocess(task)\n\texit\n可以看到，每个 Pod 只需要将任务信息读取出来，处理完成，然后退出即可。而作为用户，我只关心最终一共有 8 个计算任务启动并且退出，只要这个目标达到，我就认为整个 Job 处理完成了。所以说，这种用法，对应的就是“任务总数固定”的场景。\n\n第三种用法，也是很常用的一个用法：指定并行度（parallelism），但不设置固定的 completions 的值。\n\n此时，你就必须自己想办法，来决定什么时候启动新 Pod，什么时候 Job 才算执行完成。在这种情况下，任务的总数是未知的，所以你不仅需要一个工作队列来负责任务分发，还需要能够判断工作队列已经为空（即：所有的工作已经结束了）。\n\n这时候，Job 的定义基本上没变化，只不过是不再需要定义 completions 的值了而已：\n\n\tapiVersion: batch/v1\n\tkind: Job\n\tmetadata:\n\t  name: job-wq-2\n\tspec:\n\t  parallelism: 2\n\t  template:\n\t    metadata:\n\t      name: job-wq-2\n\t    spec:\n\t      containers:\n\t      - name: c\n\t        image: gcr.io/myproject/job-wq-2\n\t        env:\n\t        - name: BROKER_URL\n\t          value: amqp://guest:guest@rabbitmq-service:5672\n\t        - name: QUEUE\n\t          value: job2\n\t      restartPolicy: OnFailure\n而对应的 Pod 的逻辑会稍微复杂一些，我可以用这样一段伪代码来描述：\n\n\t/* job-wq-2 的伪代码 */\n\tfor !queue.IsEmpty($BROKER_URL, $QUEUE) {\n\t  task := queue.Pop()\n\t  process(task)\n\t}\n\tprint(\"Queue empty, exiting\")\n\texit\n由于任务数目的总数不固定，所以每一个 Pod 必须能够知道，自己什么时候可以退出。比如，在这个例子中，我简单地以“队列为空”，作为任务全部完成的标志。所以说，这种用法，对应的是“任务总数不固定”的场景。\n\n不过，在实际的应用中，你需要处理的条件往往会非常复杂。比如，任务完成后的输出、每个任务 Pod 之间是不是有资源的竞争和协同等等。\n\n所以，在今天这篇文章中，我就不再展开 Job 的用法了。因为，在实际场景里，要么干脆就用第一种用法来自己管理作业；要么，这些任务 Pod 之间的关系就不那么“单纯”，甚至还是“有状态应用”（比如，任务的输入 / 输出是在持久化数据卷里）。在这种情况下，我在后面要重点讲解的 Operator，加上 Job 对象一起，可能才能更好的满足实际离线任务的编排需求。\n\n最后，我再来和你分享一个非常有用的 Job 对象，叫作：`CronJob`。\n\n顾名思义，CronJob 描述的，正是定时任务。它的 API 对象，如下所示：\n\n\tapiVersion: batch/v1beta1\n\tkind: CronJob\n\tmetadata:\n\t  name: hello\n\tspec:\n\t  schedule: \"*/1 * * * *\"\n\t  jobTemplate:\n\t    spec:\n\t      template:\n\t        spec:\n\t          containers:\n\t          - name: hello\n\t            image: busybox\n\t            args:\n\t            - /bin/sh\n\t            - -c\n\t            - date; echo Hello from the Kubernetes cluster\n\t          restartPolicy: OnFailure\n在这个 YAML 文件中，最重要的关键词就是jobTemplate。看到它，你一定恍然大悟，原来 CronJob 是一个 Job 对象的控制器（Controller）！\n\n没错，**`CronJob 与 Job 的关系，正如同 Deployment 与 Pod 的关系一样。CronJob 是一个专门用来管理 Job 对象的控制器`**。只不过，它创建和删除 Job 的依据，是 schedule 字段定义的、一个标准的[Unix Cron](https://en.wikipedia.org/wiki/Cron)格式的表达式。\n\n比如，\"*/1 * * * *\"。\n\n这个 Cron 表达式里 */1 中的 * 表示从 0 开始，/ 表示“每”，1 表示偏移量。所以，它的意思就是：从 0 开始，每 1 个时间单位执行一次。\n\n那么，时间单位又是什么呢？\n\nCron 表达式中的五个部分分别代表：分钟、小时、日、月、星期。\n\n所以，上面这句 Cron 表达式的意思是：从当前开始，每分钟执行一次。\n\n而这里要执行的内容，就是 jobTemplate 定义的 Job 了。\n\n所以，这个 CronJob 对象在创建 1 分钟后，就会有一个 Job 产生了，如下所示：\n\n\t$ kubectl create -f ./cronjob.yaml\n\tcronjob \"hello\" created\n\t\n\t# 一分钟后\n\t$ kubectl get jobs\n\tNAME               DESIRED   SUCCESSFUL   AGE\n\thello-4111706356   1         1         2s\n此时，CronJob 对象会记录下这次 Job 执行的时间：\n\n\t$ kubectl get cronjob hello\n\tNAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE\n\thello     */1 * * * *   False     0         Thu, 6 Sep 2018 14:34:00 -070\n需要注意的是，由于定时任务的特殊性，很可能某个 Job 还没有执行完，另外一个新 Job 就产生了。这时候，你可以通过 spec.concurrencyPolicy 字段来定义具体的处理策略。比如：\n\n1. concurrencyPolicy=Allow，这也是默认情况，这意味着这些 Job 可以同时存在；\n2. concurrencyPolicy=Forbid，这意味着不会创建新的 Pod，该创建周期被跳过；\n3. concurrencyPolicy=Replace，这意味着新产生的 Job 会替换旧的、没有执行完的 Job。\n\n而如果某一次 Job 创建失败，这次创建就会被标记为“miss”。当在指定的时间窗口内，miss 的数目达到 100 时，那么 CronJob 会停止再创建这个 Job。\n\n这个时间窗口，可以由 spec.startingDeadlineSeconds 字段指定。比如 startingDeadlineSeconds=200，意味着在过去 200 s 里，如果 miss 的数目达到了 100 次，那么这个 Job 就不会被创建执行了。\n\n**总结**\n在今天这篇文章中，我主要和你分享了 Job 这个离线业务的编排方法，讲解了 completions 和 parallelism 字段的含义，以及 Job Controller 的执行原理。\n\n紧接着，我通过实例和你分享了 Job 对象三种常见的使用方法。但是，根据我在社区和生产环境中的经验，大多数情况下用户还是更倾向于自己控制 Job 对象。所以，相比于这些固定的“模式”，掌握 Job 的 API 对象，和它各个字段的准确含义会更加重要。\n\n最后，我还介绍了一种 Job 的控制器，叫作：CronJob。这也印证了我在前面的分享中所说的：用一个对象控制另一个对象，是 Kubernetes 编排的精髓所在。\n\n**思考题**\n根据 Job 控制器的工作原理，如果你定义的 parallelism 比 completions 还大的话，比如：\n\n\t parallelism: 4\n\t completions: 2\n那么，这个 Job 最开始创建的时候，会同时启动几个 Pod 呢？原因是什么？\n","categories":["reference","k8s"]},{"title":"23 | 声明式API与Kubernetes编程范式","url":"/2021/03/13/reference/k8s/23.声明式API与Kubernetes编程范式/","content":"\n![](1.jpg)\n\n在前面的几篇文章中，我和你分享了很多 Kubernetes 的 API 对象。这些 API 对象，有的是用来描述应用，有的则是为应用提供各种各样的服务。但是，无一例外地，为了使用这些 API 对象提供的能力，你都需要编写一个对应的 YAML 文件交给 Kubernetes。\n\n这个 YAML 文件，正是 Kubernetes 声明式 API 所必须具备的一个要素。不过，是不是只要用 YAML 文件代替了命令行操作，就是声明式 API 了呢？\n\n举个例子。我们知道，Docker Swarm 的编排操作都是基于命令行的，比如：\n\n\t$ docker service create --name nginx --replicas 2  nginx\n\t$ docker service update --image nginx:1.7.9 nginx\n像这样的两条命令，就是用 Docker Swarm 启动了两个 Nginx 容器实例。其中，第一条 create 命令创建了这两个容器，而第二条 update 命令则把它们“滚动更新”为了一个新的镜像。\n\n对于这种使用方式，我们称为**`命令式命令行操作`**.\n\n那么，像上面这样的创建和更新两个 Nginx 容器的操作，在 Kubernetes 里又该怎么做呢？\n\n这个流程，相信你已经非常熟悉了：我们需要在本地编写一个 Deployment 的 YAML 文件：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx\n\t        ports:\n\t        - containerPort: 80\n然后，我们还需要使用 kubectl create 命令在 Kubernetes 里创建这个 Deployment 对象：\n\n\t$ kubectl create -f nginx.yaml\n这样，两个 Nginx 的 Pod 就会运行起来了。\n\n而如果要更新这两个 Pod 使用的 Nginx 镜像，该怎么办呢？\n\n我们前面曾经使用过 kubectl set image 和 kubectl edit 命令，来直接修改 Kubernetes 里的 API 对象。不过，相信很多人都有这样的想法，我能不能通过修改本地 YAML 文件来完成这个操作呢？这样我的改动就会体现在这个本地 YAML 文件里了。\n\n当然可以。\n\n比如，我们可以修改这个 YAML 文件里的 Pod 模板部分，把 Nginx 容器的镜像改成 1.7.9，如下所示：\n\n\t...\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n而接下来，我们就可以执行一句 kubectl replace 操作，来完成这个 Deployment 的更新：\n\n\t$ kubectl replace -f nginx.yaml\n可是，上面这种基于 YAML 文件的操作方式，是“声明式 API”吗？\n\n并不是。\n\n对于上面这种先 kubectl create，再 replace 的操作，我们称为**`命令式配置文件操作`**.\n\n也就是说，它的处理方式，其实跟前面 Docker Swarm 的两句命令，没什么本质上的区别。只不过，它是把 Docker 命令行里的参数，写在了配置文件里而已。\n\n那么，**`到底什么才是“声明式 API”呢？`**\n\n**`答案是，kubectl apply 命令`**。\n\n在前面的文章中，我曾经提到过这个 kubectl apply 命令，并推荐你使用它来代替 kubectl create 命令（你也可以借此机会再回顾一下第 12 篇文章《牛刀小试：我的第一个容器化应用》中的相关内容）。\n\n现在，我就使用 kubectl apply 命令来创建这个 Deployment：\n\n\t$ kubectl apply -f nginx.yaml\n这样，Nginx 的 Deployment 就被创建了出来，这看起来跟 kubectl create 的效果一样。\n\n然后，我再修改一下 nginx.yaml 里定义的镜像：\n\n\t...\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n这时候，关键来了。\n\n在修改完这个 YAML 文件之后，我不再使用 kubectl replace 命令进行更新，而是继续执行一条 kubectl apply 命令，即：\n\n\t$ kubectl apply -f nginx.yaml\n这时，Kubernetes 就会立即触发这个 Deployment 的“滚动更新”。\n\n可是，它跟 kubectl replace 命令有什么本质区别吗？\n\n实际上，你可以简单地理解为，kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，**替换原有的 API 对象**；而 kubectl apply，则是执行了一个**对原有 API 对象的 PATCH 操作**。\n\n> 类似地，kubectl set image 和 kubectl edit 也是对已有 API 对象的修改。\n\n更进一步地，这意味着 kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），**`一次能处理多个写操作，并且具备 Merge 能力`**。\n\n这种区别，可能乍一听起来没那么重要。而且，正是由于要照顾到这样的 API 设计，做同样一件事情，Kubernetes 需要的步骤往往要比其他项目多不少。\n\n但是，如果你仔细思考一下 Kubernetes 项目的工作流程，就不难体会到这种声明式 API 的独到之处。\n\n接下来，我就以 Istio 项目为例，来为你讲解一下声明式 API 在实际使用时的重要意义。\n\n在 2017 年 5 月，Google、IBM 和 Lyft 公司，共同宣布了 Istio 开源项目的诞生。很快，这个项目就在技术圈儿里，掀起了一阵名叫“微服务”的热潮，把 Service Mesh 这个新的编排概念推到了风口浪尖。\n\n而 Istio 项目，实际上就是一个基于 Kubernetes 项目的微服务治理框架。它的架构非常清晰，如下所示：\n\n![](2.jpg)\n\n在上面这个架构图中，我们不难看到 Istio 项目架构的核心所在。Istio 最根本的组件，是运行在每一个应用 Pod 里的 Envoy 容器。\n\n这个 Envoy 项目是 Lyft 公司推出的一个高性能 C++ 网络代理，也是 Lyft 公司对 Istio 项目的唯一贡献。\n\n而 Istio 项目，则把这个代理服务以 sidecar 容器的方式，运行在了每一个被治理的应用 Pod 中。我们知道，**Pod 里的所有容器都共享同一个 Network Namespace。所以，Envoy 容器就能够通过配置 Pod 里的 iptables 规则，把整个 Pod 的进出流量接管下来。**\n\n这时候，Istio 的控制层（Control Plane）里的 Pilot 组件，就能够通过调用每个 Envoy 容器的 API，对这个 Envoy 代理进行配置，从而实现微服务治理。\n\n我们一起来看一个例子。\n\n假设这个 Istio 架构图左边的 Pod 是已经在运行的应用，而右边的 Pod 则是我们刚刚上线的应用的新版本。这时候，Pilot 通过调节这两 Pod 里的 Envoy 容器的配置，从而将 90% 的流量分配给旧版本的应用，将 10% 的流量分配给新版本应用，并且，还可以在后续的过程中随时调整。这样，一个典型的“灰度发布”的场景就完成了。比如，Istio 可以调节这个流量从 90%-10%，改到 80%-20%，再到 50%-50%，最后到 0%-100%，就完成了这个灰度发布的过程。\n\n更重要的是，在整个微服务治理的过程中，无论是对 Envoy 容器的部署，还是像上面这样对 Envoy 代理的配置，用户和应用都是完全“无感”的。\n\n这时候，你可能会有所疑惑：Istio 项目明明需要在每个 Pod 里安装一个 Envoy 容器，又怎么能做到“无感”的呢？\n\n实际上，**`Istio 项目使用的，是 Kubernetes 中的一个非常重要的功能，叫作 Dynamic Admission Control。`**\n\n在 Kubernetes 项目中，当一个 Pod 或者任何一个 API 对象被提交给 APIServer 之后，总有一些“初始化”性质的工作需要在它们被 Kubernetes 项目正式处理之前进行。比如，自动为所有 Pod 加上某些标签（Labels）。\n\n而这个“初始化”操作的实现，借助的是一个叫作 Admission 的功能。它其实是 Kubernetes 项目里一组被称为 Admission Controller 的代码，可以选择性地被编译进 APIServer 中，在 API 对象创建之后会被立刻调用到。\n\n但这就意味着，如果你现在想要添加一些自己的规则到 Admission Controller，就会比较困难。因为，这要求重新编译并重启 APIServer。显然，这种使用方法对 Istio 来说，影响太大了。\n\n所以，**Kubernetes 项目为我们额外提供了一种“热插拔”式的 Admission 机制，它就是 Dynamic Admission Control，也叫作：Initializer。**\n\n现在，我给你举个例子。比如，我有如下所示的一个应用 Pod：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: myapp-pod\n\t  labels:\n\t    app: myapp\n\tspec:\n\t  containers:\n\t  - name: myapp-container\n\t    image: busybox\n\t    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']\n可以看到，这个 Pod 里面只有一个用户容器，叫作：myapp-container。\n\n接下来，Istio 项目要做的，就是在这个 Pod YAML 被提交给 Kubernetes 之后，在它对应的 API 对象里自动加上 Envoy 容器的配置，使这个对象变成如下所示的样子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: myapp-pod\n\t  labels:\n\t    app: myapp\n\tspec:\n\t  containers:\n\t  - name: myapp-container\n\t    image: busybox\n\t    command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600']\n\t  - name: envoy\n\t    image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1\n\t    command: [\"/usr/local/bin/envoy\"]\n\t    ...\n可以看到，被 Istio 处理后的这个 Pod 里，除了用户自己定义的 myapp-container 容器之外，多出了一个叫作 envoy 的容器，它就是 Istio 要使用的 Envoy 代理。\n\n那么，Istio 又是如何在用户完全不知情的前提下完成这个操作的呢？\n\nIstio 要做的，就是编写一个用来为 Pod“自动注入”Envoy 容器的 Initializer。\n\n**首先，Istio 会将这个 Envoy 容器本身的定义，以 ConfigMap 的方式保存在 Kubernetes 当中。这个 ConfigMap（名叫：envoy-initializer）**的定义如下所示：\n\n\tapiVersion: v1\n\tkind: ConfigMap\n\tmetadata:\n\t  name: envoy-initializer\n\tdata:\n\t  config: |\n\t    containers:\n\t      - name: envoy\n\t        image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1\n\t        command: [\"/usr/local/bin/envoy\"]\n\t        args:\n\t          - \"--concurrency 4\"\n\t          - \"--config-path /etc/envoy/envoy.json\"\n\t          - \"--mode serve\"\n\t        ports:\n\t          - containerPort: 80\n\t            protocol: TCP\n\t        resources:\n\t          limits:\n\t            cpu: \"1000m\"\n\t            memory: \"512Mi\"\n\t          requests:\n\t            cpu: \"100m\"\n\t            memory: \"64Mi\"\n\t        volumeMounts:\n\t          - name: envoy-conf\n\t            mountPath: /etc/envoy\n\t    volumes:\n\t      - name: envoy-conf\n\t        configMap:\n\t          name: envoy\n相信你已经注意到了，这个 ConfigMap 的 data 部分，正是一个 Pod 对象的一部分定义。其中，我们可以看到 Envoy 容器对应的 containers 字段，以及一个用来声明 Envoy 配置文件的 volumes 字段。\n\n不难想到，Initializer 要做的工作，就是把这部分 Envoy 相关的字段，自动添加到用户提交的 Pod 的 API 对象里。可是，用户提交的 Pod 里本来就有 containers 字段和 volumes 字段，所以 Kubernetes 在处理这样的更新请求时，就必须使用类似于 git merge 这样的操作，才能将这两部分内容合并在一起。\n\n所以说，在 Initializer 更新用户的 Pod 对象的时候，必须使用 PATCH API 来完成。而这种 PATCH API，正是声明式 API 最主要的能力。\n\n接下来，Istio 将一个编写好的 Initializer，作为一个 Pod 部署在 Kubernetes 中。这个 Pod 的定义非常简单，如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  labels:\n\t    app: envoy-initializer\n\t  name: envoy-initializer\n\tspec:\n\t  containers:\n\t    - name: envoy-initializer\n\t      image: envoy-initializer:0.0.1\n\t      imagePullPolicy: Always\n我们可以看到，这个 envoy-initializer 使用的 envoy-initializer:0.0.1 镜像，就是一个事先编写好的“自定义控制器”（Custom Controller），我将会在下一篇文章中讲解它的编写方法。而在这里，我要先为你解释一下这个控制器的主要功能。\n\n我曾在第 16 篇文章《编排其实很简单：谈谈“控制器”模型》中和你分享过，一个 Kubernetes 的控制器，实际上就是一个“死循环”：它不断地获取“实际状态”，然后与“期望状态”作对比，并以此为依据决定下一步的操作。\n\n而 Initializer 的控制器，不断获取到的“实际状态”，就是用户新创建的 Pod。而它的“期望状态”，则是：这个 Pod 里被添加了 Envoy 容器的定义。\n\n我还是用一段 Go 语言风格的伪代码，来为你描述这个控制逻辑，如下所示：\n\n\tfor {\n\t  // 获取新创建的 Pod\n\t  pod := client.GetLatestPod()\n\t  // Diff 一下，检查是否已经初始化过\n\t  if !isInitialized(pod) {\n\t    // 没有？那就来初始化一下\n\t    doSomething(pod)\n\t  }\n\t}\n * 如果这个 Pod 里面已经添加过 Envoy 容器，那么就“放过”这个 Pod，进入下一个检查周期。\n * 而如果还没有添加过 Envoy 容器的话，它就要进行 Initialize 操作了，即：修改该 Pod 的 API 对象（doSomething 函数）。\n这时候，你应该立刻能想到，Istio 要往这个 Pod 里合并的字段，正是我们之前保存在 envoy-initializer 这个 ConfigMap 里的数据（即：它的 data 字段的值）。\n\n所以，在 Initializer 控制器的工作逻辑里，它首先会从 APIServer 中拿到这个 ConfigMap：\n\n\tfunc doSomething(pod) {\n\t  cm := client.Get(ConfigMap, \"envoy-initializer\")\n\t}\n然后，把这个 ConfigMap 里存储的 containers 和 volumes 字段，直接添加进一个空的 Pod 对象里：\n\n\tfunc doSomething(pod) {\n\t  cm := client.Get(ConfigMap, \"envoy-initializer\")\n\t  \n\t  newPod := Pod{}\n\t  newPod.Spec.Containers = cm.Containers\n\t  newPod.Spec.Volumes = cm.Volumes\n\t}\n现在，关键来了。\n\nKubernetes 的 API 库，为我们提供了一个方法，使得我们可以直接使用新旧两个 Pod 对象，生成一个 TwoWayMergePatch：\n\n\tfunc doSomething(pod) {\n\t  cm := client.Get(ConfigMap, \"envoy-initializer\")\n\t \n\t  newPod := Pod{}\n\t  newPod.Spec.Containers = cm.Containers\n\t  newPod.Spec.Volumes = cm.Volumes\n\t \n\t  // 生成 patch 数据\n\t  patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod)\n\t \n\t  // 发起 PATCH 请求，修改这个 pod 对象\n\t  client.Patch(pod.Name, patchBytes)\n\t}\n有了这个 TwoWayMergePatch 之后，Initializer 的代码就可以使用这个 patch 的数据，调用 Kubernetes 的 Client，发起一个 PATCH 请求。\n\n这样，一个用户提交的 Pod 对象里，就会被自动加上 Envoy 容器相关的字段。\n\n当然，Kubernetes 还允许你通过配置，来指定要对什么样的资源进行这个 Initialize 操作，比如下面这个例子：\n\n\tapiVersion: admissionregistration.k8s.io/v1alpha1\n\tkind: InitializerConfiguration\n\tmetadata:\n\t  name: envoy-config\n\tinitializers:\n\t  // 这个名字必须至少包括两个 \".\"\n\t  - name: envoy.initializer.kubernetes.io\n\t    rules:\n\t      - apiGroups:\n\t          - \"\" // 前面说过， \"\" 就是 core API Group 的意思\n\t        apiVersions:\n\t          - v1\n\t        resources:\n\t          - pods\n这个配置，就意味着 Kubernetes 要对所有的 Pod 进行这个 Initialize 操作，并且，我们指定了负责这个操作的 Initializer，名叫：envoy-initializer。\n\n而一旦这个 InitializerConfiguration 被创建，Kubernetes 就会把这个 Initializer 的名字，加在所有新创建的 Pod 的 Metadata 上，格式如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  initializers:\n\t    pending:\n\t      - name: envoy.initializer.kubernetes.io\n\t  name: myapp-pod\n\t  labels:\n\t    app: myapp\n\t...\n可以看到，每一个新创建的 Pod，都会自动携带了 metadata.initializers.pending 的 Metadata 信息。\n\n这个 Metadata，正是接下来 Initializer 的控制器判断这个 Pod 有没有执行过自己所负责的初始化操作的重要依据（也就是前面伪代码中 isInitialized() 方法的含义）。\n\n**这也就意味着，当你在 Initializer 里完成了要做的操作后，一定要记得将这个 metadata.initializers.pending 标志清除掉。这一点，你在编写 Initializer 代码的时候一定要非常注意。**\n\n此外，除了上面的配置方法，你还可以在具体的 Pod 的 Annotation 里添加一个如下所示的字段，从而声明要使用某个 Initializer：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata\n\t  annotations:\n\t    \"initializer.kubernetes.io/envoy\": \"true\"\n\t    ...\n在这个 Pod 里，我们添加了一个 Annotation，写明： initializer.kubernetes.io/envoy=true。这样，就会使用到我们前面所定义的 envoy-initializer 了。\n\n以上，就是关于 Initializer 最基本的工作原理和使用方法了。相信你此时已经明白，Istio 项目的核心，就是由无数个运行在应用 Pod 中的 Envoy 容器组成的服务代理网格。这也正是 Service Mesh 的含义。\n\n> 备注：如果你对这个 Demo 感兴趣，可以在[这个 GitHub 链接](https://github.com/resouer/kubernetes-initializer-tutorial)里找到它的所有源码和文档。这个 Demo，是我 fork 自 Kelsey Hightower 的一个同名的 Demo。\n\n而这个机制得以实现的原理，正是借助了 Kubernetes 能够对 API 对象进行在线更新的能力，这也正是Kubernetes“声明式 API”的独特之处：\n\n * 首先，所谓“声明式”，指的就是我只需要提交一个定义好的 API 对象来“声明”，我所期望的状态是什么样子。\n * 其次，“声明式 API”允许有多个 API 写端，以 PATCH 的方式对 API 对象进行修改，而无需关心本地原始 YAML 文件的内容。\n * 最后，也是最重要的，有了上述两个能力，Kubernetes 项目才可以基于对 API 对象的增、删、改、查，在完全无需外界干预的情况下，完成对“实际状态”和“期望状态”的调谐（Reconcile）过程。\n所以说，**声明式 API，才是 Kubernetes 项目编排能力“赖以生存”的核心所在**，希望你能够认真理解。\n\n此外，不难看到，无论是对 sidecar 容器的巧妙设计，还是对 Initializer 的合理利用，Istio 项目的设计与实现，其实都依托于 Kubernetes 的声明式 API 和它所提供的各种编排能力。可以说，Istio 是在 Kubernetes 项目使用上的一位“集大成者”。\n\n> 要知道，一个 Istio 项目部署完成后，会在 Kubernetes 里创建大约 43 个 API 对象。\n\n所以，Kubernetes 社区也看得很明白：Istio 项目有多火热，就说明 Kubernetes 这套“声明式 API”有多成功。这，既是 Google Cloud 喜闻乐见的事情，也是 Istio 项目一推出就被 Google 公司和整个技术圈儿热捧的重要原因。\n\n而在使用 Initializer 的流程中，最核心的步骤，莫过于 Initializer“自定义控制器”的编写过程。它遵循的，正是标准的“Kubernetes 编程范式”，即：\n\n> 如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。\n\n这，也正是我要在后面文章中为你详细讲解的内容。\n\n**总结**\n在今天这篇文章中，我为你重点讲解了 Kubernetes 声明式 API 的含义。并且，通过对 Istio 项目的剖析，我为你说明了它使用 Kubernetes 的 Initializer 特性，完成 Envoy 容器“自动注入”的原理。\n\n事实上，从“使用 Kubernetes 部署代码”，到“使用 Kubernetes 编写代码”的蜕变过程，正是你从一个 Kubernetes 用户，到 Kubernetes 玩家的晋级之路。\n\n而，如何理解“Kubernetes 编程范式”，如何为 Kubernetes 添加自定义 API 对象，编写自定义控制器，正是这个晋级过程中的关键点，也是我要在后面几篇文章中分享的核心内容。\n\n此外，基于今天这篇文章所讲述的 Istio 的工作原理，尽管 Istio 项目一直宣称它可以运行在非 Kubernetes 环境中，但我并不建议你花太多时间去做这个尝试。\n\n毕竟，无论是从技术实现还是在社区运作上，Istio 与 Kubernetes 项目之间都是紧密的、唇齿相依的关系。如果脱离了 Kubernetes 项目这个基础，那么这条原本就不算平坦的“微服务”之路，恐怕会更加困难重重。\n\n**思考题**\n你是否对 Envoy 项目做过了解？你觉得为什么它能够击败 Nginx 以及 HAProxy 等竞品，成为 Service Mesh 体系的核心？\n","categories":["reference","k8s"]},{"title":"17 | 经典PaaS的记忆：作业副本与水平扩展","url":"/2021/03/13/reference/k8s/20.深入理解StatefulSet(三)：有状态应用实践/","content":"\n![](1.jpg)\n\n在前面的两篇文章中，我详细讲解了 StatefulSet 的工作原理，以及处理拓扑状态和存储状态的方法。而在今天这篇文章中，我将通过一个实际的例子，再次为你深入解读一下部署一个 StatefulSet 的完整流程。\n\n今天我选择的实例是部署一个 MySQL 集群，这也是 Kubernetes 官方文档里的一个经典案例。但是，很多工程师都曾向我吐槽说这个例子“完全看不懂”。\n\n其实，这样的吐槽也可以理解：相比于 Etcd、Cassandra 等“原生”就考虑了分布式需求的项目，MySQL 以及很多其他的数据库项目，在分布式集群的搭建上并不友好，甚至有点“原始”。\n\n所以，这次我就直接选择了这个具有挑战性的例子，和你分享如何使用 StatefulSet 将它的集群搭建过程“容器化”。\n\n> 备注：在开始实践之前，请确保我们之前一起部署的那个 Kubernetes 集群还是可用的，并且网络插件和存储插件都能正常运行。具体的做法，请参考第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》的内容。\n\n首先，用自然语言来描述一下我们想要部署的“有状态应用”。\n\n1. 是一个“主从复制”（Maser-Slave Replication）的 MySQL 集群；\n2. 有 1 个主节点（Master）；\n3. 有多个从节点（Slave）；\n4. 从节点需要能水平扩展；\n5. 所有的写操作，只能在主节点上执行；\n6. 读操作可以在所有节点上执行。\n\n这就是一个非常典型的主从模式的 MySQL 集群了。我们可以把上面描述的“有状态应用”的需求，通过一张图来表示。\n\n![](2.png)\n\n在常规环境里，部署这样一个主从模式的 MySQL 集群的主要难点在于：如何让从节点能够拥有主节点的数据，即：如何配置主（Master）从（Slave）节点的复制与同步。\n\n所以，在安装好 MySQL 的 Master 节点之后，你需要做的第一步工作，就是**通过 XtraBackup 将 Master 节点的数据备份到指定目录。**\n\n> 备注：XtraBackup 是业界主要使用的开源 MySQL 备份和恢复工具。\n\n这一步会自动在目标目录里生成一个备份信息文件，名叫：xtrabackup_binlog_info。这个文件一般会包含如下两个信息：\n\n\t$ cat xtrabackup_binlog_info\n\tTheMaster-bin.000001     481\n这两个信息会在接下来配置 Slave 节点的时候用到。\n\n第二步：配置 Slave 节点。Slave 节点在第一次启动前，需要先把 Master 节点的备份数据，连同备份信息文件，一起拷贝到自己的数据目录（/var/lib/mysql）下。然后，我们执行这样一句 SQL：\n\n\tTheSlave|mysql> CHANGE MASTER TO\n\t                MASTER_HOST='$masterip',\n\t                MASTER_USER='xxx',\n\t                MASTER_PASSWORD='xxx',\n\t                MASTER_LOG_FILE='TheMaster-bin.000001',\n\t                MASTER_LOG_POS=481;\n其中，MASTER_LOG_FILE 和 MASTER_LOG_POS，就是该备份对应的二进制日志（Binary Log）文件的名称和开始的位置（偏移量），也正是 xtrabackup_binlog_info 文件里的那两部分内容（即：TheMaster-bin.000001 和 481）。\n\n第三步，启动 Slave 节点。在这一步，我们需要执行这样一句 SQL：\n\n\tTheSlave|mysql> START SLAVE;\n这样，Slave 节点就启动了。它会使用备份信息文件中的二进制日志文件和偏移量，与主节点进行数据同步。\n\n第四步，在这个集群中添加更多的 Slave 节点。\n\n需要注意的是，新添加的 Slave 节点的备份数据，来自于已经存在的 Slave 节点。\n\n所以，在这一步，我们需要将 Slave 节点的数据备份在指定目录。而这个备份操作会自动生成另一种备份信息文件，名叫：xtrabackup_slave_info。同样地，这个文件也包含了 MASTER_LOG_FILE 和 MASTER_LOG_POS 两个字段。\n\n然后，我们就可以执行跟前面一样的“CHANGE MASTER TO”和“START SLAVE” 指令，来初始化并启动这个新的 Slave 节点了。\n\n通过上面的叙述，我们不难看到，将部署 MySQL 集群的流程迁移到 Kubernetes 项目上，需要能够“容器化”地解决下面的“三座大山”：\n\n1. Master 节点和 Slave 节点需要有不同的配置文件（即：不同的 my.cnf）；\n2. Master 节点和 Salve 节点需要能够传输备份信息文件；\n3. 在 Slave 节点第一次启动之前，需要执行一些初始化 SQL 操作；\n\n而由于 MySQL 本身同时拥有拓扑状态（主从节点的区别）和存储状态（MySQL 保存在本地的数据），我们自然要通过 StatefulSet 来解决这“三座大山”的问题。\n\n其中，**“第一座大山：Master 节点和 Slave 节点需要有不同的配置文件”**，很容易处理：我们只需要给主从节点分别准备两份不同的 MySQL 配置文件，然后根据 Pod 的序号（Index）挂载进去即可。\n\n正如我在前面文章中介绍过的，这样的配置文件信息，应该保存在 ConfigMap 里供 Pod 使用。它的定义如下所示：\n\n\tapiVersion: v1\n\tkind: ConfigMap\n\tmetadata:\n\t  name: mysql\n\t  labels:\n\t    app: mysql\n\tdata:\n\t  master.cnf: |\n\t    # 主节点 MySQL 的配置文件\n\t    [mysqld]\n\t    log-bin\n\t  slave.cnf: |\n\t    # 从节点 MySQL 的配置文件\n\t    [mysqld]\n\t    super-read-only\n在这里，我们定义了 master.cnf 和 slave.cnf 两个 MySQL 的配置文件。\n\n * master.cnf 开启了 log-bin，即：使用二进制日志文件的方式进行主从复制，这是一个标准的设置。\n * slave.cnf 的开启了 super-read-only，代表的是从节点会拒绝除了主节点的数据同步操作之外的所有写操作，即：它对用户是只读的。\n而上述 ConfigMap 定义里的 data 部分，是 Key-Value 格式的。比如，master.cnf 就是这份配置数据的 Key，而“|”后面的内容，就是这份配置数据的 Value。这份数据将来挂载进 Master 节点对应的 Pod 后，就会在 Volume 目录里生成一个叫作 master.cnf 的文件。\n\n> 备注：如果你对 ConfigMap 的用法感到陌生的话，可以稍微复习一下第 15 篇文章《深入解析 Pod 对象（二）：使用进阶》中，我讲解 Secret 对象部分的内容。因为，ConfigMap 跟 Secret，无论是使用方法还是实现原理，几乎都是一样的。\n\n接下来，我们需要创建两个 Service 来供 StatefulSet 以及用户使用。这两个 Service 的定义如下所示：\n\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: mysql\n\t  labels:\n\t    app: mysql\n\tspec:\n\t  ports:\n\t  - name: mysql\n\t    port: 3306\n\t  clusterIP: None\n\t  selector:\n\t    app: mysql\n\t---\n\tapiVersion: v1\n\tkind: Service\n\tmetadata:\n\t  name: mysql-read\n\t  labels:\n\t    app: mysql\n\tspec:\n\t  ports:\n\t  - name: mysql\n\t    port: 3306\n\t  selector:\n\t    app: mysql\n可以看到，这两个 Service 都代理了所有携带 app=mysql 标签的 Pod，也就是所有的 MySQL Pod。端口映射都是用 Service 的 3306 端口对应 Pod 的 3306 端口。\n\n不同的是，第一个名叫“mysql”的 Service 是一个 Headless Service（即：clusterIP= None）。所以它的作用，是通过为 Pod 分配 DNS 记录来固定它的拓扑状态，比如“mysql-0.mysql”和“mysql-1.mysql”这样的 DNS 名字。其中，编号为 0 的节点就是我们的主节点。\n\n而第二个名叫“mysql-read”的 Service，则是一个常规的 Service。\n\n并且我们规定，所有用户的读请求，都必须访问第二个 Service 被自动分配的 DNS 记录，即：“mysql-read”（当然，也可以访问这个 Service 的 VIP）。这样，读请求就可以被转发到任意一个 MySQL 的主节点或者从节点上。\n\n> 备注：Kubernetes 中的所有 Service、Pod 对象，都会被自动分配同名的 DNS 记录。具体细节，我会在后面 Service 部分做重点讲解。\n\n而所有用户的写请求，则必须直接以 DNS 记录的方式访问到 MySQL 的主节点，也就是：“mysql-0.mysql“这条 DNS 记录。\n\n接下来，我们再一起解决“第二座大山：Master 节点和 Salve 节点需要能够传输备份文件”的问题。\n\n**翻越这座大山的思路，我比较推荐的做法是：先搭建框架，再完善细节。其中，Pod 部分如何定义，是完善细节时的重点。**\n\n所以首先，我们先为 StatefulSet 对象规划一个大致的框架，如下图所示：\n\n![](3.png)\n\n在这一步，我们可以先为 StatefulSet 定义一些通用的字段。\n\n比如：selector 表示，这个 StatefulSet 要管理的 Pod 必须携带 app=mysql 标签；它声明要使用的 Headless Service 的名字是：mysql。\n\n这个 StatefulSet 的 replicas 值是 3，表示它定义的 MySQL 集群有三个节点：一个 Master 节点，两个 Slave 节点。\n\n可以看到，StatefulSet 管理的“有状态应用”的多个实例，也都是通过同一份 Pod 模板创建出来的，使用的是同一个 Docker 镜像。这也就意味着：如果你的应用要求不同节点的镜像不一样，那就不能再使用 StatefulSet 了。对于这种情况，应该考虑我后面会讲解到的 Operator。\n\n除了这些基本的字段外，作为一个有存储状态的 MySQL 集群，StatefulSet 还需要管理存储状态。所以，我们需要通过 volumeClaimTemplate（PVC 模板）来为每个 Pod 定义 PVC。比如，这个 PVC 模板的 resources.requests.strorage 指定了存储的大小为 10 GiB；ReadWriteOnce 指定了该存储的属性为可读写，并且一个 PV 只允许挂载在一个宿主机上。将来，这个 PV 对应的的 Volume 就会充当 MySQL Pod 的存储数据目录。\n\n然后，我们来重点设计一下这个 StatefulSet 的 Pod 模板，也就是 template 字段。\n\n由于 StatefulSet 管理的 Pod 都来自于同一个镜像，这就要求我们在编写 Pod 时，一定要保持清醒，用“人格分裂”的方式进行思考：\n\n1. 如果这个 Pod 是 Master 节点，我们要怎么做；\n2. 如果这个 Pod 是 Slave 节点，我们又要怎么做。\n\n想清楚这两个问题，我们就可以按照 Pod 的启动过程来一步步定义它们了。\n\n**第一步：从 ConfigMap 中，获取 MySQL 的 Pod 对应的配置文件。**\n\n为此，我们需要进行一个初始化操作，根据节点的角色是 Master 还是 Slave 节点，为 Pod 分配对应的配置文件。此外，MySQL 还要求集群里的每个节点都有一个唯一的 ID 文件，名叫 server-id.cnf。\n\n而根据我们已经掌握的 Pod 知识，这些初始化操作显然适合通过 InitContainer 来完成。所以，我们首先定义了一个 InitContainer，如下所示：\n\n\t      ...\n\t      # template.spec\n\t      initContainers:\n\t      - name: init-mysql\n\t        image: mysql:5.7\n\t        command:\n\t        - bash\n\t        - \"-c\"\n\t        - |\n\t          set -ex\n\t          # 从 Pod 的序号，生成 server-id\n\t          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\t          ordinal=${BASH_REMATCH[1]}\n\t          echo [mysqld] > /mnt/conf.d/server-id.cnf\n\t          # 由于 server-id=0 有特殊含义，我们给 ID 加一个 100 来避开它\n\t          echo server-id=$((100 + $ordinal)) >> /mnt/conf.d/server-id.cnf\n\t          # 如果 Pod 序号是 0，说明它是 Master 节点，从 ConfigMap 里把 Master 的配置文件拷贝到 /mnt/conf.d/ 目录；\n\t          # 否则，拷贝 Slave 的配置文件\n\t          if [[ $ordinal -eq 0 ]]; then\n\t            cp /mnt/config-map/master.cnf /mnt/conf.d/\n\t          else\n\t            cp /mnt/config-map/slave.cnf /mnt/conf.d/\n\t          fi\n\t        volumeMounts:\n\t        - name: conf\n\t          mountPath: /mnt/conf.d\n\t        - name: config-map\n\t          mountPath: /mnt/config-map\n在这个名叫 init-mysql 的 InitContainer 的配置中，它从 Pod 的 hostname 里，读取到了 Pod 的序号，以此作为 MySQL 节点的 server-id。\n\n然后，init-mysql 通过这个序号，判断当前 Pod 到底是 Master 节点（即：序号为 0）还是 Slave 节点（即：序号不为 0），从而把对应的配置文件从 /mnt/config-map 目录拷贝到 /mnt/conf.d/ 目录下。\n\n其中，文件拷贝的源目录 /mnt/config-map，正是 ConfigMap 在这个 Pod 的 Volume，如下所示：\n\n\t      ...\n\t      # template.spec\n\t      volumes:\n\t      - name: conf\n\t        emptyDir: {}\n\t      - name: config-map\n\t        configMap:\n\t          name: mysql\n通过这个定义，init-mysql 在声明了挂载 config-map 这个 Volume 之后，ConfigMap 里保存的内容，就会以文件的方式出现在它的 /mnt/config-map 目录当中。\n\n而文件拷贝的目标目录，即容器里的 /mnt/conf.d/ 目录，对应的则是一个名叫 conf 的、emptyDir 类型的 Volume。基于 Pod Volume 共享的原理，当 InitContainer 复制完配置文件退出后，后面启动的 MySQL 容器只需要直接声明挂载这个名叫 conf 的 Volume，它所需要的.cnf 配置文件已经出现在里面了。这跟我们之前介绍的 Tomcat 和 WAR 包的处理方法是完全一样的。\n\n**第二步：在 Slave Pod 启动前，从 Master 或者其他 Slave Pod 里拷贝数据库数据到自己的目录下。**\n\n为了实现这个操作，我们就需要再定义第二个 InitContainer，如下所示：\n\n\t      ...\n\t      # template.spec.initContainers\n\t      - name: clone-mysql\n\t        image: gcr.io/google-samples/xtrabackup:1.0\n\t        command:\n\t        - bash\n\t        - \"-c\"\n\t        - |\n\t          set -ex\n\t          # 拷贝操作只需要在第一次启动时进行，所以如果数据已经存在，跳过\n\t          [[ -d /var/lib/mysql/mysql ]] && exit 0\n\t          # Master 节点 (序号为 0) 不需要做这个操作\n\t          [[ `hostname` =~ -([0-9]+)$ ]] || exit 1\n\t          ordinal=${BASH_REMATCH[1]}\n\t          [[ $ordinal -eq 0 ]] && exit 0\n\t          # 使用 ncat 指令，远程地从前一个节点拷贝数据到本地\n\t          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C /var/lib/mysql\n\t          # 执行 --prepare，这样拷贝来的数据就可以用作恢复了\n\t          xtrabackup --prepare --target-dir=/var/lib/mysql\n\t        volumeMounts:\n\t        - name: data\n\t          mountPath: /var/lib/mysql\n\t          subPath: mysql\n\t        - name: conf\n\t          mountPath: /etc/mysql/conf.d\n在这个名叫 clone-mysql 的 InitContainer 里，我们使用的是 xtrabackup 镜像（它里面安装了 xtrabackup 工具）。\n\n而在它的启动命令里，我们首先做了一个判断。即：当初始化所需的数据（/var/lib/mysql/mysql 目录）已经存在，或者当前 Pod 是 Master 节点的时候，不需要做拷贝操作。\n\n接下来，clone-mysql 会使用 Linux 自带的 ncat 指令，向 DNS 记录为“mysql-< 当前序号减一 >.mysql”的 Pod，也就是当前 Pod 的前一个 Pod，发起数据传输请求，并且直接用 xbstream 指令将收到的备份数据保存在 /var/lib/mysql 目录下。\n\n> 备注：3307 是一个特殊端口，运行着一个专门负责备份 MySQL 数据的辅助进程。我们后面马上会讲到它。\n\n当然，这一步你可以随意选择用自己喜欢的方法来传输数据。比如，用 scp 或者 rsync，都没问题。\n\n你可能已经注意到，这个容器里的 /var/lib/mysql 目录，实际上正是一个名为 data 的 PVC，即：我们在前面声明的持久化存储。\n\n这就可以保证，哪怕宿主机宕机了，我们数据库的数据也不会丢失。更重要的是，由于 Pod Volume 是被 Pod 里的容器共享的，所以后面启动的 MySQL 容器，就可以把这个 Volume 挂载到自己的 /var/lib/mysql 目录下，直接使用里面的备份数据进行恢复操作。\n\n不过，clone-mysql 容器还要对 /var/lib/mysql 目录，执行一句 xtrabackup --prepare 操作，目的是让拷贝来的数据进入一致性状态，这样，这些数据才能被用作数据恢复。\n\n至此，我们就通过 InitContainer 完成了对“主、从节点间备份文件传输”操作的处理过程，也就是翻越了“第二座大山”。\n\n接下来，我们可以开始定义 MySQL 容器, 启动 MySQL 服务了。由于 StatefulSet 里的所有 Pod 都来自用同一个 Pod 模板，所以我们还要“人格分裂”地去思考：这个 MySQL 容器的启动命令，在 Master 和 Slave 两种情况下有什么不同。\n\n有了 Docker 镜像，在 Pod 里声明一个 Master 角色的 MySQL 容器并不是什么困难的事情：直接执行 MySQL 启动命令即可。\n\n但是，如果这个 Pod 是一个第一次启动的 Slave 节点，在执行 MySQL 启动命令之前，它就需要使用前面 InitContainer 拷贝来的备份数据进行初始化。\n\n可是，别忘了，容器是一个单进程模型。\n\n所以，一个 Slave 角色的 MySQL 容器启动之前，谁能负责给它执行初始化的 SQL 语句呢？\n\n这就是我们需要解决的“第三座大山”的问题，即：如何在 Slave 节点的 MySQL 容器第一次启动之前，执行初始化 SQL。\n\n你可能已经想到了，我们可以为这个 MySQL 容器额外定义一个 sidecar 容器，来完成这个操作，它的定义如下所示：\n\n\t      ...\n\t      # template.spec.containers\n\t      - name: xtrabackup\n\t        image: gcr.io/google-samples/xtrabackup:1.0\n\t        ports:\n\t        - name: xtrabackup\n\t          containerPort: 3307\n\t        command:\n\t        - bash\n\t        - \"-c\"\n\t        - |\n\t          set -ex\n\t          cd /var/lib/mysql\n\t          \n\t          # 从备份信息文件里读取 MASTER_LOG_FILEM 和 MASTER_LOG_POS 这两个字段的值，用来拼装集群初始化 SQL\n\t          if [[ -f xtrabackup_slave_info ]]; then\n\t            # 如果 xtrabackup_slave_info 文件存在，说明这个备份数据来自于另一个 Slave 节点。这种情况下，XtraBackup 工具在备份的时候，就已经在这个文件里自动生成了 \"CHANGE MASTER TO\" SQL 语句。所以，我们只需要把这个文件重命名为 change_master_to.sql.in，后面直接使用即可\n\t            mv xtrabackup_slave_info change_master_to.sql.in\n\t            # 所以，也就用不着 xtrabackup_binlog_info 了\n\t            rm -f xtrabackup_binlog_info\n\t          elif [[ -f xtrabackup_binlog_info ]]; then\n\t            # 如果只存在 xtrabackup_binlog_inf 文件，那说明备份来自于 Master 节点，我们就需要解析这个备份信息文件，读取所需的两个字段的值\n\t            [[ `cat xtrabackup_binlog_info` =~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1\n\t            rm xtrabackup_binlog_info\n\t            # 把两个字段的值拼装成 SQL，写入 change_master_to.sql.in 文件\n\t            echo \"CHANGE MASTER TO MASTER_LOG_FILE='${BASH_REMATCH[1]}',\\\n\t                  MASTER_LOG_POS=${BASH_REMATCH[2]}\" > change_master_to.sql.in\n\t          fi\n\t          \n\t          # 如果 change_master_to.sql.in，就意味着需要做集群初始化工作\n\t          if [[ -f change_master_to.sql.in ]]; then\n\t            # 但一定要先等 MySQL 容器启动之后才能进行下一步连接 MySQL 的操作\n\t            echo \"Waiting for mysqld to be ready (accepting connections)\"\n\t            until mysql -h 127.0.0.1 -e \"SELECT 1\"; do sleep 1; done\n\t            \n\t            echo \"Initializing replication from clone position\"\n\t            # 将文件 change_master_to.sql.in 改个名字，防止这个 Container 重启的时候，因为又找到了 change_master_to.sql.in，从而重复执行一遍这个初始化流程\n\t            mv change_master_to.sql.in change_master_to.sql.orig\n\t            # 使用 change_master_to.sql.orig 的内容，也是就是前面拼装的 SQL，组成一个完整的初始化和启动 Slave 的 SQL 语句\n\t            mysql -h 127.0.0.1 <<EOF\n\t          $(<change_master_to.sql.orig),\n\t            MASTER_HOST='mysql-0.mysql',\n\t            MASTER_USER='root',\n\t            MASTER_PASSWORD='',\n\t            MASTER_CONNECT_RETRY=10;\n\t          START SLAVE;\n\t          EOF\n\t          fi\n\t          \n\t          # 使用 ncat 监听 3307 端口。它的作用是，在收到传输请求的时候，直接执行 \"xtrabackup --backup\" 命令，备份 MySQL 的数据并发送给请求者\n\t          exec ncat --listen --keep-open --send-only --max-conns=1 3307 -c \\\n\t            \"xtrabackup --backup --slave-info --stream=xbstream --host=127.0.0.1 --user=root\"\n\t        volumeMounts:\n\t        - name: data\n\t          mountPath: /var/lib/mysql\n\t          subPath: mysql\n\t        - name: conf\n\t          mountPath: /etc/mysql/conf.d\n可以看到，在这个名叫 xtrabackup 的 sidecar 容器的启动命令里，其实实现了两部分工作。\n\n第一部分工作，当然是 MySQL 节点的初始化工作。这个初始化需要使用的 SQL，是 sidecar 容器拼装出来、保存在一个名为 change_master_to.sql.in 的文件里的，具体过程如下所示：\n\nsidecar 容器首先会判断当前 Pod 的 /var/lib/mysql 目录下，是否有 xtrabackup_slave_info 这个备份信息文件。\n\n如果有，则说明这个目录下的备份数据是由一个 Slave 节点生成的。这种情况下，XtraBackup 工具在备份的时候，就已经在这个文件里自动生成了 \"CHANGE MASTER TO\" SQL 语句。所以，我们只需要把这个文件重命名为 change_master_to.sql.in，后面直接使用即可。\n如果没有 xtrabackup_slave_info 文件、但是存在 xtrabackup_binlog_info 文件，那就说明备份数据来自于 Master 节点。这种情况下，sidecar 容器就需要解析这个备份信息文件，读取 MASTER_LOG_FILE 和 MASTER_LOG_POS 这两个字段的值，用它们拼装出初始化 SQL 语句，然后把这句 SQL 写入到 change_master_to.sql.in 文件中。\n接下来，sidecar 容器就可以执行初始化了。从上面的叙述中可以看到，只要这个 change_master_to.sql.in 文件存在，那就说明接下来需要进行集群初始化操作。\n\n所以，这时候，sidecar 容器只需要读取并执行 change_master_to.sql.in 里面的“CHANGE MASTER TO”指令，再执行一句 START SLAVE 命令，一个 Slave 节点就被成功启动了。\n\n> 需要注意的是：Pod 里的容器并没有先后顺序，所以在执行初始化 SQL 之前，必须先执行一句 SQL（select 1）来检查一下 MySQL 服务是否已经可用。\n\n当然，上述这些初始化操作完成后，我们还要删除掉前面用到的这些备份信息文件。否则，下次这个容器重启时，就会发现这些文件存在，所以又会重新执行一次数据恢复和集群初始化的操作，这是不对的。\n\n同理，change_master_to.sql.in 在使用后也要被重命名，以免容器重启时因为发现这个文件存在又执行一遍初始化。\n\n在完成 MySQL 节点的初始化后，这个 sidecar 容器的第二个工作，则是启动一个数据传输服务。\n\n具体做法是：sidecar 容器会使用 ncat 命令启动一个工作在 3307 端口上的网络发送服务。一旦收到数据传输请求时，sidecar 容器就会调用 xtrabackup --backup 指令备份当前 MySQL 的数据，然后把这些备份数据返回给请求者。这就是为什么我们在 InitContainer 里定义数据拷贝的时候，访问的是“上一个 MySQL 节点”的 3307 端口。\n\n值得一提的是，由于 sidecar 容器和 MySQL 容器同处于一个 Pod 里，所以它是直接通过 Localhost 来访问和备份 MySQL 容器里的数据的，非常方便。\n\n同样地，我在这里举例用的只是一种备份方法而已，你完全可以选择其他自己喜欢的方案。比如，你可以使用 innobackupex 命令做数据备份和准备，它的使用方法几乎与本文的备份方法一样。\n\n至此，我们也就翻越了“第三座大山”，完成了 Slave 节点第一次启动前的初始化工作。\n\n扳倒了这“三座大山”后，我们终于可以定义 Pod 里的主角，MySQL 容器了。有了前面这些定义和初始化工作，MySQL 容器本身的定义就非常简单了，如下所示：\n\n\t      ...\n\t      # template.spec\n\t      containers:\n\t      - name: mysql\n\t        image: mysql:5.7\n\t        env:\n\t        - name: MYSQL_ALLOW_EMPTY_PASSWORD\n\t          value: \"1\"\n\t        ports:\n\t        - name: mysql\n\t          containerPort: 3306\n\t        volumeMounts:\n\t        - name: data\n\t          mountPath: /var/lib/mysql\n\t          subPath: mysql\n\t        - name: conf\n\t          mountPath: /etc/mysql/conf.d\n\t        resources:\n\t          requests:\n\t            cpu: 500m\n\t            memory: 1Gi\n\t        livenessProbe:\n\t          exec:\n\t            command: [\"mysqladmin\", \"ping\"]\n\t          initialDelaySeconds: 30\n\t          periodSeconds: 10\n\t          timeoutSeconds: 5\n\t        readinessProbe:\n\t          exec:\n\t            # 通过 TCP 连接的方式进行健康检查\n\t            command: [\"mysql\", \"-h\", \"127.0.0.1\", \"-e\", \"SELECT 1\"]\n\t          initialDelaySeconds: 5\n\t          periodSeconds: 2\n\t          timeoutSeconds: 1\n在这个容器的定义里，我们使用了一个标准的 MySQL 5.7 的官方镜像。它的数据目录是 /var/lib/mysql，配置文件目录是 /etc/mysql/conf.d。\n\n这时候，你应该能够明白，如果 MySQL 容器是 Slave 节点的话，它的数据目录里的数据，就来自于 InitContainer 从其他节点里拷贝而来的备份。它的配置文件目录 /etc/mysql/conf.d 里的内容，则来自于 ConfigMap 对应的 Volume。而它的初始化工作，则是由同一个 Pod 里的 sidecar 容器完成的。这些操作，正是我刚刚为你讲述的大部分内容。\n\n另外，我们为它定义了一个 livenessProbe，通过 mysqladmin ping 命令来检查它是否健康；还定义了一个 readinessProbe，通过查询 SQL（select 1）来检查 MySQL 服务是否可用。当然，凡是 readinessProbe 检查失败的 MySQL Pod，都会从 Service 里被摘除掉。\n\n至此，一个完整的主从复制模式的 MySQL 集群就定义完了。\n\n现在，我们就可以使用 kubectl 命令，尝试运行一下这个 StatefulSet 了。\n\n首先，我们需要在 Kubernetes 集群里创建满足条件的 PV。如果你使用的是我们在第 11 篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》里部署的 Kubernetes 集群的话，你可以按照如下方式使用存储插件 Rook：\n\n\t$ kubectl create -f rook-storage.yaml\n\t$ cat rook-storage.yaml\n\tapiVersion: ceph.rook.io/v1beta1\n\tkind: Pool\n\tmetadata:\n\t  name: replicapool\n\t  namespace: rook-ceph\n\tspec:\n\t  replicated:\n\t    size: 3\n\t---\n\tapiVersion: storage.k8s.io/v1\n\tkind: StorageClass\n\tmetadata:\n\t  name: rook-ceph-block\n\tprovisioner: ceph.rook.io/block\n\tparameters:\n\t  pool: replicapool\n\t  clusterNamespace: rook-ceph\n在这里，我用到了 StorageClass 来完成这个操作。它的作用，是自动地为集群里存在的每一个 PVC，调用存储插件（Rook）创建对应的 PV，从而省去了我们手动创建 PV 的机械劳动。我在后续讲解容器存储的时候，会再详细介绍这个机制。\n\n\t备注：在使用 Rook 的情况下，mysql-statefulset.yaml 里的 volumeClaimTemplates 字段需要加上声明 storageClassName=rook-ceph-block，才能使用到这个 Rook 提供的持久化存储。\n\n然后，我们就可以创建这个 StatefulSet 了，如下所示：\n\n\t$ kubectl create -f mysql-statefulset.yaml\n\t$ kubectl get pod -l app=mysql\n\tNAME      READY     STATUS    RESTARTS   AGE\n\tmysql-0   2/2       Running   0          2m\n\tmysql-1   2/2       Running   0          1m\n\tmysql-2   2/2       Running   0          1m\n可以看到，StatefulSet 启动成功后，会有三个 Pod 运行。\n\n接下来，我们可以尝试向这个 MySQL 集群发起请求，执行一些 SQL 操作来验证它是否正常：\n\n\t$ kubectl run mysql-client --image=mysql:5.7 -i --rm --restart=Never --\\\n\t  mysql -h mysql-0.mysql <<EOF\n\tCREATE DATABASE test;\n\tCREATE TABLE test.messages (message VARCHAR(250));\n\tINSERT INTO test.messages VALUES ('hello');\n\tEOF\n如上所示，我们通过启动一个容器，使用 MySQL client 执行了创建数据库和表、以及插入数据的操作。需要注意的是，我们连接的 MySQL 的地址必须是 mysql-0.mysql（即：Master 节点的 DNS 记录）。因为，只有 Master 节点才能处理写操作。\n\n而通过连接 mysql-read 这个 Service，我们就可以用 SQL 进行读操作，如下所示：\n\n\t$ kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\\n\t mysql -h mysql-read -e \"SELECT * FROM test.messages\"\n\tWaiting for pod default/mysql-client to be running, status is Pending, pod ready: false\n\t+---------+\n\t| message |\n\t+---------+\n\t| hello   |\n\t+---------+\n\tpod \"mysql-client\" deleted\n在有了 StatefulSet 以后，你就可以像 Deployment 那样，非常方便地扩展这个 MySQL 集群，比如：\n\n\t$ kubectl scale statefulset mysql  --replicas=5\n这时候，你就会发现新的 Slave Pod mysql-3 和 mysql-4 被自动创建了出来。\n\n而如果你像如下所示的这样，直接连接 mysql-3.mysql，即 mysql-3 这个 Pod 的 DNS 名字来进行查询操作：\n\n\t$ kubectl run mysql-client --image=mysql:5.7 -i -t --rm --restart=Never --\\\n\t  mysql -h mysql-3.mysql -e \"SELECT * FROM test.messages\"\n\tWaiting for pod default/mysql-client to be running, status is Pending, pod ready: false\n\t+---------+\n\t| message |\n\t+---------+\n\t| hello   |\n\t+---------+\n\tpod \"mysql-client\" deleted\n就会看到，从 StatefulSet 为我们新创建的 mysql-3 上，同样可以读取到之前插入的记录。也就是说，我们的数据备份和恢复，都是有效的。\n\n**总结**\n在今天这篇文章中，我以 MySQL 集群为例，和你详细分享了一个实际的 StatefulSet 的编写过程。这个 YAML 文件的链接在这里，希望你能多花一些时间认真消化。\n\n在这个过程中，有以下几个关键点（坑）特别值得你注意和体会。\n\n1. “人格分裂”：在解决需求的过程中，一定要记得思考，该 Pod 在扮演不同角色时的不同操作。\n2. “阅后即焚”：很多“有状态应用”的节点，只是在第一次启动的时候才需要做额外处理。所以，在编写 YAML 文件时，你一定要考虑“容器重启”的情况，不要让这一次的操作干扰到下一次的容器启动。\n3. “容器之间平等无序”：除非是 InitContainer，否则一个 Pod 里的多个容器之间，是完全平等的。所以，你精心设计的 sidecar，绝不能对容器的顺序做出假设，否则就需要进行前置检查。\n\n最后，相信你也已经能够理解，StatefulSet 其实是一种特殊的 Deployment，只不过这个“Deployment”的每个 Pod 实例的名字里，都携带了一个唯一并且固定的编号。这个编号的顺序，固定了 Pod 的拓扑关系；这个编号对应的 DNS 记录，固定了 Pod 的访问方式；这个编号对应的 PV，绑定了 Pod 与持久化存储的关系。所以，当 Pod 被删除重建时，这些“状态”都会保持不变。\n\n而一旦你的应用没办法通过上述方式进行状态的管理，那就代表了 StatefulSet 已经不能解决它的部署问题了。这时候，我后面讲到的 Operator，可能才是一个更好的选择。\n\n**思考题**\n如果我们现在的需求是：所有的读请求，只由 Slave 节点处理；所有的写请求，只由 Master 节点处理。那么，你需要在今天这篇文章的基础上再做哪些改动呢？\n\n","categories":["reference","k8s"]},{"title":"16 | 编排其实很简单：谈谈“控制器”模型","url":"/2021/03/13/reference/k8s/16.编排其实很简单：谈谈“控制器”模型/","content":"\n![](1.jpg)\n\n在上一篇文章中，我和你详细介绍了 Pod 的用法，讲解了 Pod 这个 API 对象的各个字段。而接下来，我们就一起来看看“编排”这个 Kubernetes 项目最核心的功能吧。\n\n实际上，你可能已经有所感悟：Pod 这个看似复杂的 API 对象，实际上就是对容器的进一步抽象和封装而已。\n\n说得更形象些，“容器”镜像虽然好用，但是容器这样一个“沙盒”的概念，对于描述应用来说，还是太过简单了。这就好比，集装箱固然好用，但是如果它四面都光秃秃的，吊车还怎么把这个集装箱吊起来并摆放好呢？\n\n所以，Pod 对象，其实就是容器的升级版。它对容器进行了组合，添加了更多的属性和字段。这就好比给集装箱四面安装了吊环，使得 Kubernetes 这架“吊车”，可以更轻松地操作它。\n\n而 Kubernetes 操作这些“集装箱”的逻辑，都由控制器（Controller）完成。在前面的第 12 篇文章《牛刀小试：我的第一个容器化应用》中，我们曾经使用过 Deployment 这个最基本的控制器对象。\n\n现在，我们一起来回顾一下这个名叫 nginx-deployment 的例子：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n\t        ports:\n\t        - containerPort: 80\n这个 Deployment 定义的编排动作非常简单，即：确保携带了 app=nginx 标签的 Pod 的个数，永远等于 spec.replicas 指定的个数，即 2 个。\n\n这就意味着，如果在这个集群中，携带 app=nginx 标签的 Pod 的个数大于 2 的时候，就会有旧的 Pod 被删除；反之，就会有新的 Pod 被创建。\n\n这时，你也许就会好奇：究竟是 Kubernetes 项目中的哪个组件，在执行这些操作呢？\n\n我在前面介绍 Kubernetes 架构的时候，曾经提到过一个叫作 kube-controller-manager 的组件。\n\n实际上，这个组件，就是一系列控制器的集合。我们可以查看一下 Kubernetes 项目的 pkg/controller 目录：\n\n\t$ cd kubernetes/pkg/controller/\n\t$ ls -d */              \n\tdeployment/             job/                    podautoscaler/          \n\tcloud/                  disruption/             namespace/              \n\treplicaset/             serviceaccount/         volume/\n\tcronjob/                garbagecollector/       nodelifecycle/          replication/            statefulset/            daemon/\n\t...\n这个目录下面的每一个控制器，都以独有的方式负责某种编排功能。而我们的 Deployment，正是这些控制器中的一种。\n\n实际上，这些控制器之所以被统一放在 pkg/controller 目录下，就是因为它们都遵循 Kubernetes 项目中的一个通用编排模式，即：控制循环（control loop）。\n\n比如，现在有一种待编排的对象 X，它有一个对应的控制器。那么，我就可以用一段 Go 语言风格的伪代码，为你描述这个控制循环：\n\n\tfor {\n\t  实际状态 := 获取集群中对象 X 的实际状态（Actual State）\n\t  期望状态 := 获取集群中对象 X 的期望状态（Desired State）\n\t  if 实际状态 == 期望状态{\n\t    什么都不做\n\t  } else {\n\t    执行编排动作，将实际状态调整为期望状态\n\t  }\n\t}\n在具体实现中，实际状态往往来自于 Kubernetes 集群本身。\n\n比如，kubelet 通过心跳汇报的容器状态和节点状态，或者监控系统中保存的应用监控数据，或者控制器主动收集的它自己感兴趣的信息，这些都是常见的实际状态的来源。\n\n而期望状态，一般来自于用户提交的 YAML 文件。\n\n比如，Deployment 对象中 Replicas 字段的值。很明显，这些信息往往都保存在 Etcd 中。\n\n接下来，以 Deployment 为例，我和你简单描述一下它对控制器模型的实现：\n\n1. Deployment 控制器从 Etcd 中获取到所有携带了“app: nginx”标签的 Pod，然后统计它们的数量，这就是实际状态；\n2. Deployment 对象的 Replicas 字段的值就是期望状态；\n3. Deployment 控制器将两个状态做比较，然后根据比较结果，确定是创建 Pod，还是删除已有的 Pod（具体如何操作 Pod 对象，我会在下一篇文章详细介绍）。\n\n可以看到，一个 Kubernetes 对象的主要编排逻辑，实际上是在第三步的“对比”阶段完成的。\n\n这个操作，通常被叫作调谐（Reconcile）。这个调谐的过程，则被称作“Reconcile Loop”（调谐循环）或者“Sync Loop”（同步循环）。\n\n所以，如果你以后在文档或者社区中碰到这些词，都不要担心，它们其实指的都是同一个东西：控制循环。\n\n而调谐的最终结果，往往都是对被控制对象的某种写操作。\n\n比如，增加 Pod，删除已有的 Pod，或者更新 Pod 的某个字段。这也是 Kubernetes 项目“面向 API 对象编程”的一个直观体现。\n\n其实，像 Deployment 这种控制器的设计原理，就是我们前面提到过的，“用一种对象管理另一种对象”的“艺术”。\n\n其中，这个控制器对象本身，负责定义被管理对象的期望状态。比如，Deployment 里的 replicas=2 这个字段。\n\n而被控制对象的定义，则来自于一个“模板”。比如，Deployment 里的 template 字段。\n\n可以看到，Deployment 这个 template 字段里的内容，跟一个标准的 Pod 对象的 API 定义，丝毫不差。而所有被这个 Deployment 管理的 Pod 实例，其实都是根据这个 template 字段的内容创建出来的。\n\n像 Deployment 定义的 template 字段，在 Kubernetes 项目中有一个专有的名字，叫作 PodTemplate（Pod 模板）。\n\n这个概念非常重要，因为后面我要讲解到的大多数控制器，都会使用 PodTemplate 来统一定义它所要管理的 Pod。更有意思的是，我们还会看到其他类型的对象模板，比如 Volume 的模板。\n\n至此，我们就可以对 Deployment 以及其他类似的控制器，做一个简单总结了：\n\n![](2.png)\n\n如上图所示，**`类似 Deployment 这样的一个控制器，实际上都是由上半部分的控制器定义（包括期望状态），加上下半部分的被控制对象的模板组成的。`**\n\n这就是为什么，在所有 API 对象的 Metadata 里，都有一个字段叫作 ownerReference，用于保存当前这个 API 对象的拥有者（Owner）的信息。\n\n那么，对于我们这个 nginx-deployment 来说，它创建出来的 Pod 的 ownerReference 就是 nginx-deployment 吗？或者说，nginx-deployment 所直接控制的，就是 Pod 对象么？\n\n这个问题的答案，我就留到下一篇文章时再做详细解释吧。\n\n**总结**\n在今天这篇文章中，我以 Deployment 为例，和你详细分享了 Kubernetes 项目如何通过一个称作“控制器模式”（controller pattern）的设计方法，来统一地实现对各种不同的对象或者资源进行的编排操作。\n\n在后面的讲解中，我还会讲到很多不同类型的容器编排功能，比如 StatefulSet、DaemonSet 等等，它们无一例外地都有这样一个甚至多个控制器的存在，并遵循控制循环（control loop）的流程，完成各自的编排逻辑。\n\n实际上，跟 Deployment 相似，这些控制循环最后的执行结果，要么就是创建、更新一些 Pod（或者其他的 API 对象、资源），要么就是删除一些已经存在的 Pod（或者其他的 API 对象、资源）。\n\n但也正是在这个统一的编排框架下，不同的控制器可以在具体执行过程中，设计不同的业务逻辑，从而达到不同的编排效果。\n\n这个实现思路，正是 Kubernetes 项目进行容器编排的核心原理。在此后讲解 Kubernetes 编排功能的文章中，我都会遵循这个逻辑展开，并且带你逐步领悟控制器模式在不同的容器化作业中的实现方式。\n\n**思考题**\n你能否说出，Kubernetes 使用的这个“控制器模式”，跟我们平常所说的“事件驱动”，有什么区别和联系吗？\n\n","categories":["reference","k8s"]},{"title":"17 | 经典PaaS的记忆：作业副本与水平扩展","url":"/2021/03/13/reference/k8s/17.经典PaaS的记忆：作业副本与水平扩展/","content":"\n![](1.jpg)\n\n在上一篇文章中，我为你详细介绍了 Kubernetes 项目中第一个重要的设计思想：控制器模式。\n\n而在今天这篇文章中，我就来为你详细讲解一下，Kubernetes 里第一个控制器模式的完整实现：Deployment。\n\nDeployment 看似简单，但实际上，它实现了 Kubernetes 项目中一个非常重要的功能：`Pod 的“水平扩展 / 收缩”（horizontal scaling out/in）`。这个功能，是从 PaaS 时代开始，一个平台级项目就必须具备的编排能力。\n\n举个例子，如果你更新了 Deployment 的 Pod 模板（比如，修改了容器的镜像），那么 Deployment 就需要遵循一种叫作“滚动更新”（rolling update）的方式，来升级现有的容器。\n\n而这个能力的实现，依赖的是 Kubernetes 项目中的一个非常重要的概念（API 对象）：ReplicaSet。\n\nReplicaSet 的结构非常简单，我们可以通过这个 YAML 文件查看一下：\n\n\tapiVersion: apps/v1\n\tkind: ReplicaSet\n\tmetadata:\n\t  name: nginx-set\n\t  labels:\n\t    app: nginx\n\tspec:\n\t  replicas: 3\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n从这个 YAML 文件中，我们可以看到，**一个 ReplicaSet 对象，其实就是由副本数目的定义和一个 Pod 模板组成的**。不难发现，它的定义其实是 Deployment 的一个子集。\n\n**更重要的是，Deployment 控制器实际操纵的，正是这样的 ReplicaSet 对象，而不是 Pod 对象。**\n\n还记不记得我在上一篇文章《编排其实很简单：谈谈“控制器”模型》中曾经提出过这样一个问题：对于一个 Deployment 所管理的 Pod，它的 ownerReference 是谁？\n\n所以，这个问题的答案就是：ReplicaSet。\n**(ReplicaSet是幕后英雄，Deployment只是代理人。)**\n\n明白了这个原理，我再来和你一起分析一个如下所示的 Deployment：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\t  labels:\n\t    app: nginx\n\tspec:\n\t  replicas: 3\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n\t        ports:\n\t        - containerPort: 80\n可以看到，这就是一个我们常用的 nginx-deployment，它定义的 Pod 副本个数是 3（spec.replicas=3）。\n\n那么，在具体的实现上，这个 Deployment，与 ReplicaSet，以及 Pod 的关系是怎样的呢？\n\n我们可以用一张图把它描述出来：\n\n![](2.png)\n\n通过这张图，我们就很清楚的看到，一个定义了 replicas=3 的 Deployment，与它的 ReplicaSet，以及 Pod 的关系，实际上是一种“层层控制”的关系。\n\n其中，ReplicaSet 负责通过“控制器模式”，保证系统中 Pod 的个数永远等于指定的个数（比如，3 个）。这也正是 Deployment 只允许容器的 restartPolicy=Always 的主要原因：只有在容器能保证自己始终是 Running 状态的前提下，ReplicaSet 调整 Pod 的个数才有意义。\n\n而在此基础上，Deployment 同样通过“控制器模式”，来操作 ReplicaSet 的个数和属性，进而实现“水平扩展 / 收缩”和“滚动更新”这两个编排动作。\n\n其中，“水平扩展 / 收缩”非常容易实现，Deployment Controller 只需要修改它所控制的 ReplicaSet 的 Pod 副本个数就可以了。\n\n比如，把这个值从 3 改成 4，那么 Deployment 所对应的 ReplicaSet，就会根据修改后的值自动创建一个新的 Pod。这就是“水平扩展”了；“水平收缩”则反之。\n\n而用户想要执行这个操作的指令也非常简单，就是 kubectl scale，比如：\n\n\t$ kubectl scale deployment nginx-deployment --replicas=4\n\tdeployment.apps/nginx-deployment scaled\n那么，“滚动更新”又是什么意思，是如何实现的呢？\n\n接下来，我还以这个 Deployment 为例，来为你讲解“滚动更新”的过程。\n\n首先，我们来创建这个 nginx-deployment：\n\n\t$ kubectl create -f nginx-deployment.yaml --record\n**注意，在这里，我额外加了一个`--record 参数`。它的作用，是记录下你每次操作所执行的命令，以方便后面查看。**\n\n然后，我们来检查一下 nginx-deployment 创建后的状态信息：\n\n\t$ kubectl get deployments\n\tNAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n\tnginx-deployment   3         0         0            0           1s\n在返回结果中，我们可以看到四个状态字段，它们的含义如下所示。\n\n1. DESIRED：用户期望的 Pod 副本个数（spec.replicas 的值）；\n2. CURRENT：当前处于 Running 状态的 Pod 的个数；\n3. UP-TO-DATE：当前处于最新版本的 Pod 的个数，所谓最新版本指的是 Pod 的 Spec 部分与 Deployment 里 Pod 模板里定义的完全一致；\n4. AVAILABLE：当前已经可用的 Pod 的个数，即：既是 Running 状态，又是最新版本，并且已经处于 Ready（健康检查正确）状态的 Pod 的个数。\n\n可以看到，只有这个 AVAILABLE 字段，描述的才是用户所期望的最终状态。\n\n而 Kubernetes 项目还为我们提供了一条指令，让我们可以实时查看 Deployment 对象的状态变化。这个指令就是 kubectl rollout status：\n\n\t$ kubectl rollout status deployment/nginx-deployment\n\tWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n\tdeployment.apps/nginx-deployment successfully rolled out\n在这个返回结果中，“2 out of 3 new replicas have been updated”意味着已经有 2 个 Pod 进入了 UP-TO-DATE 状态。\n\n继续等待一会儿，我们就能看到这个 Deployment 的 3 个 Pod，就进入到了 AVAILABLE 状态：\n\n\tNAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\n\tnginx-deployment   3         3         3            3           20s\n此时，你可以尝试查看一下这个 Deployment 所控制的 ReplicaSet：\n\n\t$ kubectl get rs\n\tNAME                          DESIRED   CURRENT   READY   AGE\n\tnginx-deployment-3167673210   3         3         3       20s\n如上所示，在用户提交了一个 Deployment 对象后，Deployment Controller 就会立即创建一个 Pod 副本个数为 3 的 ReplicaSet。这个 ReplicaSet 的名字，则是由 Deployment 的名字和一个随机字符串共同组成。\n\n这个随机字符串叫作 pod-template-hash，在我们这个例子里就是：3167673210。ReplicaSet 会把这个随机字符串加在它所控制的所有 Pod 的标签里，从而保证这些 Pod 不会与集群里的其他 Pod 混淆。\n\n而 ReplicaSet 的 DESIRED、CURRENT 和 READY 字段的含义，和 Deployment 中是一致的。所以，**相比之下，Deployment 只是在 ReplicaSet 的基础上，添加了 UP-TO-DATE 这个跟版本有关的状态字段**。\n\n这个时候，如果我们修改了 Deployment 的 Pod 模板，“滚动更新”就会被自动触发。\n\n修改 Deployment 有很多方法。比如，我可以直接使用 kubectl edit 指令编辑 Etcd 里的 API 对象。\n\n\t$ kubectl edit deployment/nginx-deployment\n\t... \n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.9.1 # 1.7.9 -> 1.9.1\n\t        ports:\n\t        - containerPort: 80\n\t...\n\tdeployment.extensions/nginx-deployment edited\n这个 kubectl edit 指令，会帮你直接打开 nginx-deployment 的 API 对象。然后，你就可以修改这里的 Pod 模板部分了。比如，在这里，我将 nginx 镜像的版本升级到了 1.9.1。\n\n> 备注：kubectl edit 并不神秘，它不过是把 API 对象的内容下载到了本地文件，让你修改完成后再提交上去。\n\nkubectl edit 指令编辑完成后，保存退出，Kubernetes 就会立刻触发“滚动更新”的过程。你还可以通过 kubectl rollout status 指令查看 nginx-deployment 的状态变化：\n\n\t$ kubectl rollout status deployment/nginx-deployment\n\tWaiting for rollout to finish: 2 out of 3 new replicas have been updated...\n\tdeployment.extensions/nginx-deployment successfully rolled out\n这时，你可以通过查看 Deployment 的 Events，看到这个“滚动更新”的流程：\n\n\t$ kubectl describe deployment nginx-deployment\n\t...\n\tEvents:\n\t  Type    Reason             Age   From                   Message\n\t  ----    ------             ----  ----                   -------\n\t...\n\t  Normal  ScalingReplicaSet  24s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 1\n\t  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 2\n\t  Normal  ScalingReplicaSet  22s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 2\n\t  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 1\n\t  Normal  ScalingReplicaSet  19s   deployment-controller  Scaled up replica set nginx-deployment-1764197365 to 3\n\t  Normal  ScalingReplicaSet  14s   deployment-controller  Scaled down replica set nginx-deployment-3167673210 to 0\n可以看到，首先，当你修改了 Deployment 里的 Pod 定义之后，Deployment Controller 会使用这个修改后的 Pod 模板，创建一个新的 ReplicaSet（hash=1764197365），这个新的 ReplicaSet 的初始 Pod 副本数是：0。\n\n然后，在 Age=24 s 的位置，Deployment Controller 开始将这个新的 ReplicaSet 所控制的 Pod 副本数从 0 个变成 1 个，即：“水平扩展”出一个副本。\n\n紧接着，在 Age=22 s 的位置，Deployment Controller 又将旧的 ReplicaSet（hash=3167673210）所控制的旧 Pod 副本数减少一个，即：“水平收缩”成两个副本。\n\n如此交替进行，新 ReplicaSet 管理的 Pod 副本数，从 0 个变成 1 个，再变成 2 个，最后变成 3 个。而旧的 ReplicaSet 管理的 Pod 副本数则从 3 个变成 2 个，再变成 1 个，最后变成 0 个。这样，就完成了这一组 Pod 的版本升级过程。\n\n像这样，**将一个集群中正在运行的多个 Pod 版本，交替地逐一升级的过程，就是“滚动更新”。**\n**(replicaSet 操作 pod实现水平扩展和水平收缩，deployment 操作replicaSet实现滚动更新)**\n\n在这个“滚动更新”过程完成之后，你可以查看一下新、旧两个 ReplicaSet 的最终状态：\n\n\t$ kubectl get rs\n\tNAME                          DESIRED   CURRENT   READY   AGE\n\tnginx-deployment-1764197365   3         3         3       6s\n\tnginx-deployment-3167673210   0         0         0       30s\n其中，旧 ReplicaSet（hash=3167673210）已经被“水平收缩”成了 0 个副本。\n\n这种“滚动更新”的好处是显而易见的。\n\n比如，在升级刚开始的时候，集群里只有 1 个新版本的 Pod。如果这时，新版本 Pod 有问题启动不起来，那么“滚动更新”就会停止，从而允许开发和运维人员介入。而在这个过程中，由于应用本身还有两个旧版本的 Pod 在线，所以服务并不会受到太大的影响。\n\n当然，这也就要求你一定要使用 Pod 的 Health Check 机制检查应用的运行状态，而不是简单地依赖于容器的 Running 状态。要不然的话，虽然容器已经变成 Running 了，但服务很有可能尚未启动，“滚动更新”的效果也就达不到了。\n\n而为了进一步保证服务的连续性，Deployment Controller 还会确保，在任何时间窗口内，只有指定比例的 Pod 处于离线状态。同时，它也会确保，在任何时间窗口内，只有指定比例的新 Pod 被创建出来。这两个比例的值都是可以配置的，默认都是 DESIRED 值的 25%。\n\n所以，在上面这个 Deployment 的例子中，它有 3 个 Pod 副本，那么控制器在“滚动更新”的过程中永远都会确保至少有 2 个 Pod 处于可用状态，至多只有 4 个 Pod 同时存在于集群中。这个策略，是 Deployment 对象的一个字段，名叫 RollingUpdateStrategy，如下所示：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\t  labels:\n\t    app: nginx\n\tspec:\n\t...\n\t  strategy:\n\t    type: RollingUpdate\n\t    rollingUpdate:\n\t      maxSurge: 1\n\t      maxUnavailable: 1\n在上面这个 RollingUpdateStrategy 的配置中，maxSurge 指定的是除了 DESIRED 数量之外，在一次“滚动”中，Deployment 控制器还可以创建多少个新 Pod；而 maxUnavailable 指的是，在一次“滚动”中，Deployment 控制器可以删除多少个旧 Pod。\n\n同时，这两个配置还可以用前面我们介绍的百分比形式来表示，比如：maxUnavailable=50%，指的是我们最多可以一次删除“50%*DESIRED 数量”个 Pod。\n\n结合以上讲述，现在我们可以扩展一下 Deployment、ReplicaSet 和 Pod 的关系图了。\n\n![](3.png)\n\n如上所示，**`Deployment 的控制器，实际上控制的是 ReplicaSet 的数目，以及每个 ReplicaSet 的属性。`**\n\n而一个应用的版本，对应的正是一个 ReplicaSet；这个版本应用的 Pod 数量，则由 ReplicaSet 通过它自己的控制器（ReplicaSet Controller）来保证。\n\n通过这样的多个 ReplicaSet 对象，Kubernetes 项目就实现了对多个“应用版本”的描述。\n\n而明白了“应用版本和 ReplicaSet 一一对应”的设计思想之后，我就可以为你讲解一下Deployment 对应用进行版本控制的具体原理了。\n\n这一次，我会使用一个叫kubectl set image的指令，直接修改 nginx-deployment 所使用的镜像。这个命令的好处就是，你可以不用像 kubectl edit 那样需要打开编辑器。\n\n不过这一次，我把这个镜像名字修改成为了一个错误的名字，比如：nginx:1.91。这样，这个 Deployment 就会出现一个升级失败的版本。\n\n我们一起来实践一下：\n\n\t$ kubectl set image deployment/nginx-deployment nginx=nginx:1.91\n\tdeployment.extensions/nginx-deployment image updated\n由于这个 nginx:1.91 镜像在 Docker Hub 中并不存在，所以这个 Deployment 的“滚动更新”被触发后，会立刻报错并停止。\n\n这时，我们来检查一下 ReplicaSet 的状态，如下所示：\n\n\t$ kubectl get rs\n\tNAME                          DESIRED   CURRENT   READY   AGE\n\tnginx-deployment-1764197365   2         2         2       24s\n\tnginx-deployment-3167673210   0         0         0       35s\n\tnginx-deployment-2156724341   2         2         0       7s\n通过这个返回结果，我们可以看到，新版本的 ReplicaSet（hash=2156724341）的“水平扩展”已经停止。而且此时，它已经创建了两个 Pod，但是它们都没有进入 READY 状态。这当然是因为这两个 Pod 都拉取不到有效的镜像。\n\n与此同时，旧版本的 ReplicaSet（hash=1764197365）的“水平收缩”，也自动停止了。此时，已经有一个旧 Pod 被删除，还剩下两个旧 Pod。\n\n那么问题来了， 我们如何让这个 Deployment 的 3 个 Pod，都回滚到以前的旧版本呢？\n\n我们只需要执行一条 kubectl rollout undo 命令，就能把整个 Deployment 回滚到上一个版本：\n\n\t$ kubectl rollout undo deployment/nginx-deployment\n\tdeployment.extensions/nginx-deployment\n很容易想到，在具体操作上，Deployment 的控制器，其实就是让这个旧 ReplicaSet（hash=1764197365）再次“扩展”成 3 个 Pod，而让新的 ReplicaSet（hash=2156724341）重新“收缩”到 0 个 Pod。\n\n更进一步地，如果我想回滚到更早之前的版本，要怎么办呢？\n\n首先，**我需要使用 kubectl rollout history 命令，查看每次 Deployment 变更对应的版本。而由于我们在创建这个 Deployment 的时候，指定了–record 参数，所以我们创建这些版本时执行的 kubectl 命令，都会被记录下来。** 这个操作的输出如下所示：\n\n\t$ kubectl rollout history deployment/nginx-deployment\n\tdeployments \"nginx-deployment\"\n\tREVISION    CHANGE-CAUSE\n\t1           kubectl create -f nginx-deployment.yaml --record\n\t2           kubectl edit deployment/nginx-deployment\n\t3           kubectl set image deployment/nginx-deployment nginx=nginx:1.91\n可以看到，我们前面执行的创建和更新操作，分别对应了版本 1 和版本 2，而那次失败的更新操作，则对应的是版本 3。\n\n当然，你还可以通过这个 kubectl rollout history 指令，看到每个版本对应的 Deployment 的 API 对象的细节，具体命令如下所示：\n\n\t$ kubectl rollout history deployment/nginx-deployment --revision=2\n**然后，我们就可以在 kubectl rollout undo 命令行最后，加上要回滚到的指定版本的版本号，就可以回滚到指定版本了。**这个指令的用法如下：\n\n\t$ kubectl rollout undo deployment/nginx-deployment --to-revision=2\n\tdeployment.extensions/nginx-deployment\n这样，Deployment Controller 还会按照“滚动更新”的方式，完成对 Deployment 的降级操作。\n\n不过，你可能已经想到了一个问题：**我们对 Deployment 进行的每一次更新操作，都会生成一个新的 ReplicaSet 对象，是不是有些多余，甚至浪费资源**呢？\n\n没错。\n\n所以，**Kubernetes 项目还提供了一个指令，使得我们对 Deployment 的多次更新操作，最后 只生成一个 ReplicaSet。**\n\n具体的做法是，在更新 Deployment 前，你要先执行一条 kubectl rollout pause 指令。它的用法如下所示：\n\n\t$ kubectl rollout pause deployment/nginx-deployment\n\tdeployment.extensions/nginx-deployment paused\n这个 kubectl rollout pause 的作用，是让这个 Deployment 进入了一个“暂停”状态。\n\n所以接下来，你就可以随意使用 kubectl edit 或者 kubectl set image 指令，修改这个 Deployment 的内容了。\n\n由于此时 Deployment 正处于“暂停”状态，所以我们对 Deployment 的所有修改，都不会触发新的“滚动更新”，也不会创建新的 ReplicaSet。\n\n而等到我们对 Deployment 修改操作都完成之后，只需要再执行一条 kubectl rollout resume 指令，就可以把这个 Deployment“恢复”回来，如下所示：\n\n\t$ kubectl rollout resume deploy/nginx-deployment\n\tdeployment.extensions/nginx-deployment resumed\n而在这个 **kubectl rollout resume 指令执行之前，在 kubectl rollout pause 指令之后的这段时间里，我们对 Deployment 进行的所有修改，最后只会触发一次“滚动更新”**。\n\n当然，我们可以通过检查 ReplicaSet 状态的变化，来验证一下 kubectl rollout pause 和 kubectl rollout resume 指令的执行效果，如下所示：\n\n\t$ kubectl get rs\n\tNAME               DESIRED   CURRENT   READY     AGE\n\tnginx-1764197365   0         0         0         2m\n\tnginx-3196763511   3         3         3         28s\n通过返回结果，我们可以看到，只有一个 hash=3196763511 的 ReplicaSet 被创建了出来。\n\n不过，即使你像上面这样小心翼翼地控制了 ReplicaSet 的生成数量，随着应用版本的不断增加，Kubernetes 中还是会为同一个 Deployment 保存很多很多不同的 ReplicaSet。\n\n那么，我们又该如何控制这些“历史”ReplicaSet 的数量呢？\n\n很简单，Deployment 对象有一个字段，叫作 spec.revisionHistoryLimit，就是 Kubernetes 为 Deployment 保留的“历史版本”个数。所以，如果把它设置为 0，你就再也不能做回滚操作了。\n\n**总结**\n在今天这篇文章中，我为你详细讲解了 Deployment 这个 Kubernetes 项目中最基本的编排控制器的实现原理和使用方法。\n\n通过这些讲解，你应该了解到： **`Deployment 实际上是一个两层控制器。首先，它通过ReplicaSet 的个数来描述应用的版本；然后，它再通过ReplicaSet 的属性（比如 replicas 的值），来保证 Pod 的副本数量。`**\n\n> 备注：Deployment 控制 ReplicaSet（版本），ReplicaSet 控制 Pod（副本数）。这个两层控制关系一定要牢记。\n\n不过，相信你也能够感受到，Kubernetes 项目对 Deployment 的设计，实际上是代替我们完成了对“应用”的抽象，使得我们可以使用这个 Deployment 对象来描述应用，使用 kubectl rollout 命令控制应用的版本。\n\n可是，在实际使用场景中，应用发布的流程往往千差万别，也可能有很多的定制化需求。比如，我的应用可能有会话黏连（session sticky），这就意味着“滚动更新”的时候，哪个 Pod 能下线，是不能随便选择的。\n\n这种场景，光靠 Deployment 自己就很难应对了。对于这种需求，我在专栏后续文章中重点介绍的“自定义控制器”，就可以帮我们实现一个功能更加强大的 Deployment Controller。\n\n当然，Kubernetes 项目本身，也提供了另外一种抽象方式，帮我们应对其他一些用 Deployment 无法处理的应用编排场景。这个设计，就是对有状态应用的管理，也是我在下一篇文章中要重点讲解的内容。\n\n**思考题**\n你听说过金丝雀发布（Canary Deployment）和蓝绿发布（Blue-Green Deployment）吗？你能说出它们是什么意思吗？\n\n实际上，有了 Deployment 的能力之后，你可以非常轻松地用它来实现金丝雀发布、蓝绿发布，以及 A/B 测试等很多应用发布模式。这些问题的答案都在这个 [GitHub 库](https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/canary)，建议你在课后实践一下。\n","categories":["reference","k8s"]},{"title":"10 | Kubernetes一键部署利器：kubeadm","url":"/2021/03/13/reference/k8s/10.Kubernetes一键部署利器kubeadm/","content":"\n![](1.jpg)\n\n通过前面几篇文章的内容，我其实阐述了这样一个思想：**`要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。`**\n\n这些知识更适合作为你的技术储备，以便在需要的时候可以帮你更快的定位问题，并解决问题。\n\n而更深入的学习容器技术的关键在于，**`如何使用这些技术来“容器化”你的应用。`**\n\n比如，我们的应用既可能是 Java Web 和 MySQL 这样的组合，也可能是 Cassandra 这样的分布式系统。而要使用容器把后者运行起来，你单单通过 Docker 把一个 Cassandra 镜像跑起来是没用的。\n\n要把 Cassandra 应用容器化的关键，在于如何处理好这些 Cassandra 容器之间的编排关系。比如，哪些 Cassandra 容器是主，哪些是从？主从容器如何区分？它们之间又如何进行自动发现和通信？Cassandra 容器的持久化数据又如何保持，等等。\n\n这也是为什么我们要反复强调 Kubernetes 项目的主要原因：这个项目体现出来的容器化“表达能力”，具有独有的先进性和完备性。这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合，还能够处理 Cassandra 容器集群等复杂编排问题。所以，对这种编排能力的剖析、解读和最佳实践，将是本专栏最重要的一部分内容。\n\n不过，万事开头难。\n\n作为一个典型的分布式项目，Kubernetes 的部署一直以来都是挡在初学者前面的一只“拦路虎”。尤其是在 Kubernetes 项目发布初期，它的部署完全要依靠一堆由社区维护的脚本。\n\n其实，Kubernetes 作为一个 Golang 项目，已经免去了很多类似于 Python 项目要安装语言级别依赖的麻烦。但是，除了将各个组件编译成二进制文件外，用户还要负责为这些二进制文件编写对应的配置文件、配置自启动脚本，以及为 kube-apiserver 配置授权文件等等诸多运维工作。\n\n目前，各大云厂商最常用的部署的方法，是使用 SaltStack、Ansible 等运维工具自动化地执行这些步骤。\n\n但即使这样，这个部署过程依然非常繁琐。因为，SaltStack 这类专业运维工具本身的学习成本，就可能比 Kubernetes 项目还要高。\n\n难道 Kubernetes 项目就没有简单的部署方法了吗？\n\n这个问题，在 Kubernetes 社区里一直没有得到足够重视。直到 2017 年，在志愿者的推动下，社区才终于发起了一个独立的部署工具，名叫：[kubeadm](https://github.com/kubernetes/kubeadm)。\n\n这个项目的目的，就是要让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署：\n\n\t# 创建一个 Master 节点\n\t$ kubeadm init\n\t\n\t# 将一个 Node 节点加入到当前集群中\n\t$ kubeadm join <Master 节点的 IP 和端口 >\n是不是非常方便呢？\n\n不过，你可能也会有所顾虑：Kubernetes 的功能那么多，这样一键部署出来的集群，能用于生产环境吗？\n\n为了回答这个问题，在今天这篇文章，我就先和你介绍一下 kubeadm 的工作原理吧。\n\nkubeadm 的工作原理\n在上一篇文章《从容器到容器云：谈谈 Kubernetes 的本质》中，我已经详细介绍了 Kubernetes 的架构和它的组件。**`在部署时，它的每一个组件都是一个需要被执行的、单独的二进制文件。`**所以不难想象，SaltStack 这样的运维工具或者由社区维护的脚本的功能，就是要把这些二进制文件传输到指定的机器当中，然后编写控制脚本来启停这些组件。\n\n不过，在理解了容器技术之后，你可能已经萌生出了这样一个想法，为什么不用容器部署 Kubernetes 呢？\n\n这样，我只要给每个 Kubernetes 组件做一个容器镜像，然后在每台宿主机上用 docker run 指令启动这些组件容器，部署不就完成了吗？\n\n事实上，在 Kubernetes 早期的部署脚本里，确实有一个脚本就是用 Docker 部署 Kubernetes 项目的，这个脚本相比于 SaltStack 等的部署方式，也的确简单了不少。\n\n但是，这样做会带来一个很麻烦的问题，即：如何容器化 kubelet。\n\n我在上一篇文章中，已经提到 kubelet 是 Kubernetes 项目用来操作 Docker 等容器运行时的核心组件。可是，除了跟容器运行时打交道外，kubelet 在配置容器网络、管理容器数据卷时，都需要直接操作宿主机。\n\n而如果现在 kubelet 本身就运行在一个容器里，那么直接操作宿主机就会变得很麻烦。对于网络配置来说还好，kubelet 容器可以通过不开启 Network Namespace（即 Docker 的 host network 模式）的方式，直接共享宿主机的网络栈。可是，要让 kubelet 隔着容器的 Mount Namespace 和文件系统，操作宿主机的文件系统，就有点儿困难了。\n\n比如，如果用户想要使用 NFS 做容器的持久化数据卷，那么 kubelet 就需要在容器进行绑定挂载前，在宿主机的指定目录上，先挂载 NFS 的远程目录。\n\n可是，这时候问题来了。由于现在 kubelet 是运行在容器里的，这就意味着它要做的这个“mount -F nfs”命令，被隔离在了一个单独的 Mount Namespace 中。即，kubelet 做的挂载操作，不能被“传播”到宿主机上。\n\n对于这个问题，有人说，可以使用 setns() 系统调用，在宿主机的 Mount Namespace 中执行这些挂载操作；也有人说，应该让 Docker 支持一个–mnt=host 的参数。\n\n但是，到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，我也不推荐你用容器去部署 Kubernetes 项目。\n\n正因为如此，kubeadm 选择了一种妥协方案：\n\n> 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。\n\n所以，你使用 kubeadm 的第一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行：\n\n\t$ apt-get install kubeadm\n就可以了。\n\n接下来，你就可以使用“kubeadm init”部署 Master 节点了。\n\n\tkubeadm init 的工作流程\n**`一. 当你执行 kubeadm init 指令后，kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。`**这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。\n\n其实，Preflight Checks 包括了很多方面，比如：\n\nLinux 内核的版本必须是否是 3.10 以上？**`(linux内核3.10及以上支持overlay fs)`**\nLinux Cgroups 模块是否可用？\n机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。\n用户安装的 kubeadm 和 kubelet 的版本是否匹配？\n机器上是不是已经安装了 Kubernetes 的二进制文件？\nKubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用？\nip、mount 等 Linux 指令是否存在？\nDocker 是否已经安装？\n……\n\n**`二. 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。`**\n\nKubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。\n**`(kubeadm 生成的证书配置文件)`**\n\nkubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。\n\n此外，用户使用 kubectl 获取容器日志等 streaming 操作时**`(kubectl的操作需要双向认证)`**，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。\n\n除此之外，Kubernetes 集群中还有 Aggregate APIServer 等特性，也需要用到专门的证书，这里我就不再一一列举了。**`需要指出的是，你可以选择不让 kubeadm 为你生成这些证书，而是拷贝现有的证书到如下证书的目录里:`**\n\n\t/etc/kubernetes/pki/ca.{crt,key}\n**`这时，kubeadm 就会跳过证书生成的步骤，把它完全交给用户处理。`**\n\n**`三. 证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf：`**\n\n\tls /etc/kubernetes/\n\tadmin.conf  controller-manager.conf  kubelet.conf  scheduler.conf\n这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube-apiserver 建立安全连接。\n\n**`四. 接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。`**我已经在上一篇文章中和你介绍过 Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。\n\n你可能会有些疑问：这时，Kubernetes 集群尚不存在，难道 kubeadm 会直接执行 docker run 来启动这些容器吗？\n\n当然不是。\n\n在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。\n\n从这一点也可以看出，kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。\n**`(k8s组件除了kubelet外用容器化方式启动)`**\n\n在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。比如，kube-apiserver.yaml：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  annotations:\n\t    scheduler.alpha.kubernetes.io/critical-pod: \"\"\n\t  creationTimestamp: null\n\t  labels:\n\t    component: kube-apiserver\n\t    tier: control-plane\n\t  name: kube-apiserver\n\t  namespace: kube-system\n\tspec:\n\t  containers:\n\t  - command:\n\t    - kube-apiserver\n\t    - --authorization-mode=Node,RBAC\n\t    - --runtime-config=api/all=true\n\t    - --advertise-address=10.168.0.2\n\t    ...\n\t    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n\t    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n\t    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1\n\t    imagePullPolicy: IfNotPresent\n\t    livenessProbe:\n\t      ...\n\t    name: kube-apiserver\n\t    resources:\n\t      requests:\n\t        cpu: 250m\n\t    volumeMounts:\n\t    - mountPath: /usr/share/ca-certificates\n\t      name: usr-share-ca-certificates\n\t      readOnly: true\n\t    ...\n\t  hostNetwork: true\n\t  priorityClassName: system-cluster-critical\n\t  volumes:\n\t  - hostPath:\n\t      path: /etc/ca-certificates\n\t      type: DirectoryOrCreate\n\t    name: etc-ca-certificates\n\t  ...\n关于一个 Pod 的 YAML 文件怎么写、里面的字段如何解读，我会在后续专门的文章中为你详细分析。在这里，你只需要关注这样几个信息：\n\n1. 这个 Pod 里只定义了一个容器，它使用的镜像是：k8s.gcr.io/kube-apiserver-amd64:v1.11.1 。这个镜像是 Kubernetes 官方维护的一个组件镜像。\n\n2. 这个容器的启动命令（commands）是 kube-apiserver --authorization-mode=Node,RBAC …，这样一句非常长的命令。其实，它就是容器里 kube-apiserver 这个二进制文件再加上指定的配置参数而已。\n\n3. 如果你要修改一个已有集群的 kube-apiserver 的配置，需要修改这个 YAML 文件。\n\n4. 这些组件的参数也可以在部署时指定，我很快就会讲解到。\n\n**`五. 在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。`** 所以，最后 Master 组件的 Pod YAML 文件如下所示：\n\n\t$ ls /etc/kubernetes/manifests/\n\tetcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml\n而一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。\n\nMaster 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。\n\n**`六. 然后，kubeadm 就会为集群生成一个 bootstrap token.`** 在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。\n\n这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。\n\n**`在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用. 这个 ConfigMap 的名字是 cluster-info.`**\n\n**`七. kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的.`** 它们分别用来提供整个集群的服务发现和 DNS 功能。其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了。\n\nkubeadm join 的工作流程\n这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。\n\n可是，为什么执行 kubeadm join 需要这样一个 token 呢？\n\n因为，任何一台机器想要成为 Kubernetes 集群中的一个节点，就必须在集群的 kube-apiserver 上注册。可是，要想跟 apiserver 打交道，这台机器就必须要获取到相应的证书文件（CA 文件）。可是，为了能够一键安装，我们就不能让用户去 Master 节点上手动拷贝这些文件。\n\n所以，kubeadm 至少需要发起一次“不安全模式”的访问到 kube-apiserver，从而拿到保存在 ConfigMap 中的 cluster-info（它保存了 APIServer 的授权信息）。而 bootstrap token，扮演的就是这个过程中的安全验证的角色。\n\n只要有了 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。\n\n接下来，你只要在其他节点上重复这个指令就可以了。\n\n配置 kubeadm 的部署参数\n我在前面讲解了 kubeadm 部署 Kubernetes 集群最关键的两个步骤，kubeadm init 和 kubeadm join。相信你一定会有这样的疑问：kubeadm 确实简单易用，可是我又该如何定制我的集群组件参数呢？\n\n比如，我要指定 kube-apiserver 的启动参数，该怎么办？\n\n在这里，我强烈推荐你在使用 kubeadm init 部署 Master 节点时，使用下面这条指令：\n\n\t$ kubeadm init --config kubeadm.yaml\n这时，你就可以给 kubeadm 提供一个 YAML 文件（比如，kubeadm.yaml），它的内容如下所示（我仅列举了主要部分）：\n\n\tapiVersion: kubeadm.k8s.io/v1alpha2\n\tkind: MasterConfiguration\n\tkubernetesVersion: v1.11.0\n\tapi:\n\t  advertiseAddress: 192.168.0.102\n\t  bindPort: 6443\n\t  ...\n\tetcd:\n\t  local:\n\t    dataDir: /var/lib/etcd\n\t    image: \"\"\n\timageRepository: k8s.gcr.io\n\tkubeProxy:\n\t  config:\n\t    bindAddress: 0.0.0.0\n\t    ...\n\tkubeletConfiguration:\n\t  baseConfig:\n\t    address: 0.0.0.0\n\t    ...\n\tnetworking:\n\t  dnsDomain: cluster.local\n\t  podSubnet: \"\"\n\t  serviceSubnet: 10.96.0.0/12\n\tnodeRegistration:\n\t  criSocket: /var/run/dockershim.sock\n\t  ...\n通过制定这样一个部署参数配置文件，你就可以很方便地在这个文件里填写各种自定义的部署参数了。比如，我现在要指定 kube-apiserver 的参数，那么我只要在这个文件里加上这样一段信息：\n\n\t...\n\tapiServerExtraArgs:\n\t  advertise-address: 192.168.0.103\n\t  anonymous-auth: false\n\t  enable-admission-plugins: AlwaysPullImages,DefaultStorageClass\n\t  audit-log-path: /home/johndoe/audit.log\n然后，kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了。\n\n而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以**`修改 kubelet 和 kube-proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的k8s.gcr.io/xxx镜像 URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等.`** 这些配置项，就留给你在后续实践中探索了。\n\n**总结**\n在今天的这次分享中，我重点介绍了 kubeadm 这个部署工具的工作原理和使用方法。紧接着，我会在下一篇文章中，使用它一步步地部署一个完整的 Kubernetes 集群。\n\n从今天的分享中，你可以看到，kubeadm 的设计非常简洁。并且，它在实现每一步部署功能时，都在最大程度地重用 Kubernetes 已有的功能，这也就使得我们在使用 kubeadm 部署 Kubernetes 项目时，非常有“原生”的感觉，一点都不会感到突兀。\n\n而 kubeadm 的源代码，直接就在 kubernetes/cmd/kubeadm 目录下，是 Kubernetes 项目的一部分。其中，app/phases 文件夹下的代码，对应的就是我在这篇文章中详细介绍的每一个具体步骤。\n\n看到这里，你可能会猜想，kubeadm 的作者一定是 Google 公司的某个“大神”吧。\n\n实际上，kubeadm 几乎完全是一位高中生的作品。他叫 Lucas Käldström，芬兰人，今年只有 18 岁。kubeadm，是他 17 岁时用业余时间完成的一个社区项目。\n\n所以说，开源社区的魅力也在于此：一个成功的开源项目，总能够吸引到全世界最厉害的贡献者参与其中。尽管参与者的总体水平参差不齐，而且频繁的开源活动又显得杂乱无章难以管控，但一个有足够热度的社区最终的收敛方向，却一定是代码越来越完善、Bug 越来越少、功能越来越强大。\n\n最后，我再来回答一下我在今天这次分享开始提到的问题：**`kubeadm 能够用于生产环境吗？`**\n\n到目前为止（2018 年 9 月），这个问题的**`答案是：不能。`**\n\n**`因为 kubeadm 目前最欠缺的是，一键部署一个高可用的 Kubernetes 集群，即：Etcd、Master 组件都应该是多节点集群，而不是现在这样的单点。`**这，当然也正是 kubeadm 接下来发展的主要方向。\n\n另一方面，Lucas 也正在积极地把 kubeadm phases 开放给用户，即：用户可以更加自由地定制 kubeadm 的每一个部署步骤。这些举措，都可以让这个项目更加完善，我对它的发展走向也充满了信心。\n\n当然，如果你有部署规模化生产环境的需求，我推荐使用[kops](https://github.com/kubernetes/kops)或者 SaltStack 这样更复杂的部署工具。但，在本专栏接下来的讲解中，我都会以 kubeadm 为依据进行讲述。\n\n * 一方面，作为 Kubernetes 项目的原生部署工具，kubeadm 对 Kubernetes 项目特性的使用和集成，确实要比其他项目“技高一筹”，非常值得我们学习和借鉴；\n * 另一方面，kubeadm 的部署方法，不会涉及到太多的运维工作，也不需要我们额外学习复杂的部署工具。而它部署的 Kubernetes 集群，跟一个完全使用二进制文件搭建起来的集群几乎没有任何区别。\n因此，使用 kubeadm 去部署一个 Kubernetes 集群，对于你理解 Kubernetes 组件的工作方式和架构，最好不过了。\n\n**思考题**\n1. 在 Linux 上为一个类似 kube-apiserver 的 Web Server 制作证书，你知道可以用哪些工具实现吗？\n\n2. 回忆一下我在前面文章中分享的 Kubernetes 架构，你能够说出 Kubernetes 各个功能组件之间（包含 Etcd），都有哪些建立连接或者调用的方式吗？（比如：HTTP/HTTPS，远程调用等等）\n\n","categories":["reference","k8s"]},{"title":"12 | 牛刀小试：我的第一个容器化应用","url":"/2021/03/13/reference/k8s/12.我的第一个容器化应用/","content":"\n![](1.jpg)\n\n在上一篇文章《从 0 到 1：搭建一个完整的 Kubernetes 集群》中，我和你一起部署了一套完整的 Kubernetes 集群。这个集群虽然离生产环境的要求还有一定差距（比如，没有一键高可用部署），但也可以当作是一个准生产级别的 Kubernetes 集群了。\n\n而在这篇文章中，我们就来扮演一个应用开发者的角色，使用这个 Kubernetes 集群发布第一个容器化应用。\n\n在开始实践之前，我先给你讲解一下 Kubernetes 里面与开发者关系最密切的几个概念。\n\n作为一个应用开发者，你首先要做的，是制作容器的镜像。这一部分内容，我已经在容器基础部分《白话容器基础（三）：深入理解容器镜像》重点讲解过了。\n\n而有了容器镜像之后，你需要按照 Kubernetes 项目的规范和要求，将你的镜像组织为它能够“认识”的方式，然后提交上去。\n\n那么，什么才是 Kubernetes 项目能“认识”的方式呢？\n\n这就是使用 Kubernetes 的必备技能：编写配置文件。\n\n> 备注：这些配置文件可以是 YAML 或者 JSON 格式的。为方便阅读与理解，在后面的讲解中，我会统一使用 YAML 文件来指代它们。\n\nKubernetes 跟 Docker 等很多项目最大的不同，就在于它不推荐你使用命令行的方式直接运行容器（虽然 Kubernetes 项目也支持这种方式，比如：kubectl run），而是希望你用 YAML 文件的方式，即：把容器的定义、参数、配置，统统记录在一个 YAML 文件中，然后用这样一句指令把它运行起来：\n\n\t$ kubectl create -f 我的配置文件\n这么做最直接的好处是，你会有一个文件能记录下 Kubernetes 到底“run”了什么。比如下面这个例子：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n\t        ports:\n\t        - containerPort: 80\n像这样的一个 YAML 文件，对应到 Kubernetes 中，就是一个 API Object（API 对象）。当你为这个对象的各个字段填好值并提交给 Kubernetes 之后，Kubernetes 就会负责创建出这些对象所定义的容器或者其他类型的 API 资源。\n\n可以看到，这个 YAML 文件中的 Kind 字段，指定了这个 API 对象的类型（Type），是一个 Deployment。\n\n所谓 Deployment，是一个定义多副本应用（即多个副本 Pod）的对象，我在前面的文章中（也是第 9 篇文章《从容器到容器云：谈谈 Kubernetes 的本质》）曾经简单提到过它的用法。此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。\n\n在上面这个 YAML 文件中，我给它定义的 Pod 副本个数 (spec.replicas) 是：2。\n\n而这些 Pod 具体的又长什么样子呢？\n\n为此，我定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。在上面的例子里，这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 nginx:1.7.9，这个容器监听端口（containerPort）是 80。\n\n关于 Pod 的设计和用法我已经在第 9 篇文章《从容器到容器云：谈谈 Kubernetes 的本质》中简单的介绍过。而在这里，你需要记住这样一句话：\n\n> Pod 就是 Kubernetes 世界里的“应用”；而一个应用，可以由多个容器组成。\n\n需要注意的是，像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。在我们的例子中，Deployment 扮演的正是 Pod 的控制器的角色。关于 Pod 和控制器模式的更多细节，我会在后续编排部分做进一步讲解。\n\n你可能还注意到，这样的每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。\n\n顾名思义，Labels 就是一组 key-value 格式的标签。而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。\n\n比如，在上面这个 YAML 文件中，Deployment 会把所有正在运行的、携带“app: nginx”标签的 Pod 识别为被管理的对象，并确保这些 Pod 的总数严格等于两个。\n\n而这个过滤规则的定义，是在 Deployment 的“spec.selector.matchLabels”字段。我们一般称之为：Label Selector。\n\n另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息。所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。\n\n一个 Kubernetes 的 API 对象的定义，大多可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据，对所有 API 对象来说，这一部分的字段和格式基本上是一样的；而后者存放的，则是属于这个对象独有的定义，用来描述它所要表达的功能。\n\n在了解了上述 Kubernetes 配置文件的基本知识之后，我们现在就可以把这个 YAML 文件“运行”起来。正如前所述，你可以使用 kubectl create 指令完成这个操作：\n\n\t$ kubectl create -f nginx-deployment.yaml\n然后，通过 kubectl get 命令检查这个 YAML 运行起来的状态是不是与我们预期的一致：\n\n\t$ kubectl get pods -l app=nginx\n\tNAME                                READY     STATUS    RESTARTS   AGE\n\tnginx-deployment-67594d6bf6-9gdvr   1/1       Running   0          10m\n\tnginx-deployment-67594d6bf6-v6j7w   1/1       Running   0          10m\n\tkubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，**`所有 key-value 格式的参数，都使用“=”而非“:”表示。`**\n\n从这条指令返回的结果中，我们可以看到现在有两个 Pod 处于 Running 状态，也就意味着我们这个 Deployment 所管理的 Pod 都处于预期的状态。\n\n此外， 你还可以使用 kubectl describe 命令，查看一个 API 对象的细节，比如：\n\n\t$ kubectl describe pod nginx-deployment-67594d6bf6-9gdvr\n\tName:               nginx-deployment-67594d6bf6-9gdvr\n\tNamespace:          default\n\tPriority:           0\n\tPriorityClassName:  <none>\n\tNode:               node-1/10.168.0.3\n\tStart Time:         Thu, 16 Aug 2018 08:48:42 +0000\n\tLabels:             app=nginx\n\t                    pod-template-hash=2315082692\n\tAnnotations:        <none>\n\tStatus:             Running\n\tIP:                 10.32.0.23\n\tControlled By:      ReplicaSet/nginx-deployment-67594d6bf6\n\t...\n\tEvents:\n\t \n\t  Type     Reason                  Age                From               Message\n\t \n\t  ----     ------                  ----               ----               -------\n\t  \n\t  Normal   Scheduled               1m                 default-scheduler  Successfully assigned default/nginx-deployment-67594d6bf6-9gdvr to node-1\n\t  Normal   Pulling                 25s                kubelet, node-1    pulling image \"nginx:1.7.9\"\n\t  Normal   Pulled                  17s                kubelet, node-1    Successfully pulled image \"nginx:1.7.9\"\n\t  Normal   Created                 17s                kubelet, node-1    Created container\n\t  Normal   Started                 17s                kubelet, node-1    Started container\n在 kubectl describe 命令返回的结果中，你可以清楚地看到这个 Pod 的详细信息，比如它的 IP 地址等等。其中，有一个部分值得你特别关注，它就是**`Events（事件）.`**\n\n在 Kubernetes 执行的过程中，对 API 对象的所有重要操作，都会被记录在这个对象的 Events 里，并且显示在 kubectl describe 指令返回的结果中。\n\n比如，对于这个 Pod，我们可以看到它被创建之后，被调度器调度（Successfully assigned）到了 node-1，拉取了指定的镜像（pulling image），然后启动了 Pod 里定义的容器（Started container）。\n\n所以，这个部分正是我们将来进行 Debug 的重要依据。**`如果有异常发生，你一定要第一时间查看这些 Events，往往可以看到非常详细的错误信息。`**\n\n接下来，如果我们要对这个 Nginx 服务进行升级，把它的镜像版本从 1.7.9 升级为 1.8，要怎么做呢？\n\n很简单，我们只要修改这个 YAML 文件即可。\n\n\t...    \n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.8 # 这里被从 1.7.9 修改为 1.8\n\t        ports:\n\t      - containerPort: 80\n可是，这个修改目前只发生在本地，如何让这个更新在 Kubernetes 里也生效呢？\n\n我们可以使用 kubectl replace 指令来完成这个更新：\n\n\t$ kubectl replace -f nginx-deployment.yaml\n不过，在本专栏里，我推荐你使用 kubectl apply 命令，来统一进行 Kubernetes 对象的创建和更新操作，具体做法如下所示：\n\n\t$ kubectl apply -f nginx-deployment.yaml\n\t\n\t# 修改 nginx-deployment.yaml 的内容\n\t\n\t$ kubectl apply -f nginx-deployment.yaml\n这样的操作方法，是 Kubernetes“声明式 API”所推荐的使用方法。也就是说，作为用户，你不必关心当前的操作是创建，还是更新，你执行的命令始终是 kubectl apply，而 Kubernetes 则会根据 YAML 文件的内容变化，自动进行具体的处理。\n\n而这个流程的好处是，它有助于帮助开发和运维人员，围绕着可以版本化管理的 YAML 文件，而不是“行踪不定”的命令行进行协作，从而大大降低开发人员和运维人员之间的沟通成本。\n\n举个例子，一位开发人员开发好一个应用，制作好了容器镜像。那么他就可以在应用的发布目录里附带上一个 Deployment 的 YAML 文件。\n\n而运维人员，拿到这个应用的发布目录后，就可以直接用这个 YAML 文件执行 kubectl apply 操作把它运行起来。\n\n这时候，如果开发人员修改了应用，生成了新的发布内容，那么这个 YAML 文件，也就需要被修改，并且成为这次变更的一部分。\n\n而接下来，运维人员可以使用 git diff 命令查看到这个 YAML 文件本身的变化，然后继续用 kubectl apply 命令更新这个应用。\n\n所以说，如果通过容器镜像，我们能够保证应用本身在开发与部署环境里的一致性的话，那么现在，Kubernetes 项目通过这些 YAML 文件，就保证了应用的“部署参数”在开发与部署环境中的一致性。\n\n**`而当应用本身发生变化时，开发人员和运维人员可以依靠容器镜像来进行同步；当应用部署参数发生变化时，这些 YAML 文件就是他们相互沟通和信任的媒介。`**\n\n以上，就是 Kubernetes 发布应用的最基本操作了。\n\n接下来，我们再在这个 Deployment 中尝试声明一个 Volume。\n\n在 Kubernetes 中，Volume 是属于 Pod 对象的一部分。所以，我们就需要修改这个 YAML 文件里的 template.spec 字段，如下所示：\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  replicas: 2\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.8\n\t        ports:\n\t        - containerPort: 80\n\t        volumeMounts:\n\t        - mountPath: \"/usr/share/nginx/html\"\n\t          name: nginx-vol\n\t      volumes:\n\t      - name: nginx-vol\n\t        emptyDir: {}\n可以看到，我们在 Deployment 的 Pod 模板部分添加了一个 volumes 字段，定义了这个 Pod 声明的所有 Volume。它的名字叫作 nginx-vol，类型是 emptyDir。\n\n那什么是 emptyDir 类型呢？\n\n它其实就等同于我们之前讲过的 Docker 的隐式 Volume 参数，即：不显式声明宿主机目录的 Volume。所以，Kubernetes 也会在宿主机上创建一个临时目录，这个目录将来就会被绑定挂载到容器所声明的 Volume 目录上。\n\n> 备注：不难看到，Kubernetes 的 emptyDir 类型，只是把 Kubernetes 创建的临时目录作为 Volume 的宿主机目录，交给了 Docker。这么做的原因，是 Kubernetes 不想依赖 Docker 自己创建的那个 _data 目录。\n\n而 Pod 中的容器，使用的是 volumeMounts 字段来声明自己要挂载哪个 Volume，并通过 mountPath 字段来定义容器内的 Volume 目录，比如：/usr/share/nginx/html。\n\n当然，Kubernetes 也提供了显式的 Volume 定义，它叫做 hostPath。比如下面的这个 YAML 文件：\n\n\t ...   \n\t    volumes:\n\t      - name: nginx-vol\n\t        hostPath: \n\t          path: /var/data\n这样，容器 Volume 挂载的宿主机目录，就变成了 /var/data。\n\n在上述修改完成后，我们还是使用 kubectl apply 指令，更新这个 Deployment:\n\n\t$ kubectl apply -f nginx-deployment.yaml\n接下来，你可以通过 kubectl get 指令，查看两个 Pod 被逐一更新的过程：\n\n\t$ kubectl get pods\n\tNAME                                READY     STATUS              RESTARTS   AGE\n\tnginx-deployment-5c678cfb6d-v5dlh   0/1       ContainerCreating   0          4s\n\tnginx-deployment-67594d6bf6-9gdvr   1/1       Running             0          10m\n\tnginx-deployment-67594d6bf6-v6j7w   1/1       Running             0          10m\n\t$ kubectl get pods\n\tNAME                                READY     STATUS    RESTARTS   AGE\n\tnginx-deployment-5c678cfb6d-lg9lw   1/1       Running   0          8s\n\tnginx-deployment-5c678cfb6d-v5dlh   1/1       Running   0          19s\n从返回结果中，我们可以看到，新旧两个 Pod，被交替创建、删除，最后剩下的就是新版本的 Pod。这个滚动更新的过程，我也会在后续进行详细的讲解。\n\n然后，你可以使用 kubectl describe 查看一下最新的 Pod，就会发现 Volume 的信息已经出现在了 Container 描述部分：\n\n\t...\n\tContainers:\n\t  nginx:\n\t    Container ID:   docker://07b4f89248791c2aa47787e3da3cc94b48576cd173018356a6ec8db2b6041343\n\t    Image:          nginx:1.8\n\t    ...\n\t    Environment:    <none>\n\t    Mounts:\n\t      /usr/share/nginx/html from nginx-vol (rw)\n\t...\n\tVolumes:\n\t  nginx-vol:\n\t    Type:    EmptyDir (a temporary directory that shares a pod's lifetime)\n> 备注：作为一个完整的容器化平台项目，Kubernetes 为我们提供的 Volume 类型远远不止这些，在容器存储章节里，我将会为你详细介绍这部分内容。\n\n最后，你还可以使用 kubectl exec 指令，进入到这个 Pod 当中（即容器的 Namespace 中）查看这个 Volume 目录：\n\n\t$ kubectl exec -it nginx-deployment-5c678cfb6d-lg9lw -- /bin/bash\n\t# ls /usr/share/nginx/html\n此外，你想要从 Kubernetes 集群中删除这个 Nginx Deployment 的话，直接执行：\n\n\t$ kubectl delete -f nginx-deployment.yaml\n就可以了。\n\n**总结**\n在今天的分享中，我通过一个小案例，和你近距离体验了 Kubernetes 的使用方法。\n\n可以看到，Kubernetes 推荐的使用方式，是用一个 YAML 文件来描述你所要部署的 API 对象。然后，统一使用 kubectl apply 命令完成对这个对象的创建和更新操作。\n\n而 Kubernetes 里“最小”的 API 对象是 Pod。Pod 可以等价为一个应用，所以，Pod 可以由多个紧密协作的容器组成。\n\n在 Kubernetes 中，我们经常会看到它通过一种 API 对象来管理另一种 API 对象，比如 Deployment 和 Pod 之间的关系；而由于 Pod 是“最小”的对象，所以它往往都是被其他对象控制的。这种组合方式，正是 Kubernetes 进行容器编排的重要模式。\n\n而像这样的 Kubernetes API 对象，往往由 Metadata 和 Spec 两部分组成，其中 Metadata 里的 Labels 字段是 Kubernetes 过滤对象的主要手段。\n\n在这些字段里面，容器想要使用的数据卷，也就是 Volume，正是 Pod 的 Spec 字段的一部分。而 Pod 里的每个容器，则需要显式的声明自己要挂载哪个 Volume。\n\n上面这些基于 YAML 文件的容器管理方式，跟 Docker、Mesos 的使用习惯都是不一样的，而从 docker run 这样的命令行操作，向 kubectl apply YAML 文件这样的声明式 API 的转变，是每一个容器技术学习者，必须要跨过的第一道门槛。\n\n所以，如果你想要快速熟悉 Kubernetes，请按照下面的流程进行练习：\n\n * 首先，在本地通过 Docker 测试代码，制作镜像；\n * 然后，选择合适的 Kubernetes API 对象，编写对应 YAML 文件（比如，Pod，Deployment）；\n * 最后，在 Kubernetes 上部署这个 YAML 文件。\n更重要的是，在部署到 Kubernetes 之后，接下来的所有操作，要么通过 kubectl 来执行，要么通过修改 YAML 文件来实现，就尽量不要再碰 Docker 的命令行了。\n\n**思考题**\n在实际使用 Kubernetes 的过程中，相比于编写一个单独的 Pod 的 YAML 文件，我一定会推荐你使用一个 replicas=1 的 Deployment。请问，这两者有什么区别呢？\n\n","categories":["reference","k8s"]},{"title":"11 | 从0到1：搭建一个完整的Kubernetes集群","url":"/2021/03/13/reference/k8s/11.搭建一个完整的Kubernetes集群/","content":"\n![](1.jpg)\n\n不过，首先需要指出的是，本篇搭建指南是完全的手工操作，细节比较多，并且有些外部链接可能还会遇到特殊的“网络问题”。所以，对于只关心学习 Kubernetes 本身知识点、不太关注如何手工部署 Kubernetes 集群的同学，可以略过本节，直接使用 [MiniKube](https://github.com/kubernetes/minikube) 或者 [Kind](https://github.com/kubernetes-sigs/kind)，来在本地启动简单的 Kubernetes 集群进行后面的学习即可。如果是使用 MiniKube 的话，阿里云还维护了一个[国内版的 MiniKube](https://github.com/AliyunContainerService/minikube)，这对于在国内的同学来说会比较友好。\n\n在上一篇文章中，我介绍了 kubeadm 这个 Kubernetes 半官方管理工具的工作原理。既然 kubeadm 的初衷是让 Kubernetes 集群的部署不再让人头疼，那么这篇文章，我们就来使用它部署一个完整的 Kubernetes 集群吧。\n\n> 备注：这里所说的“完整”，指的是这个集群具备 Kubernetes 项目在 GitHub 上已经发布的所有功能，并能够模拟生产环境的所有使用需求。但并不代表这个集群是生产级别可用的：类似于高可用、授权、多租户、灾难备份等生产级别集群的功能暂时不在本篇文章的讨论范围。\n> 目前，kubeadm 的高可用部署[已经有了第一个发布](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/)。但是，这个特性还没有 GA（生产可用），所以包括了大量的手动工作，跟我们所预期的一键部署还有一定距离。GA 的日期预计是 2018 年底到 2019 年初。届时，如果有机会我会再和你分享这部分内容。\n\n这次部署，我不会依赖于任何公有云或私有云的能力，而是完全在 Bare-metal 环境中完成。这样的部署经验会更有普适性。而在后续的讲解中，如非特殊强调，我也都会以本次搭建的这个集群为基础。\n\n**准备工作**\n首先，准备机器。最直接的办法，自然是到公有云上申请几个虚拟机。当然，如果条件允许的话，拿几台本地的物理服务器来组集群是最好不过了。这些机器只要满足如下几个条件即可：\n\n1. 满足安装 Docker 项目所需的要求，比如 64 位的 Linux 操作系统、3.10 及以上的内核版本；\n\n2. x86 或者 ARM 架构均可；\n\n3. 机器之间网络互通，这是将来容器之间网络互通的前提；\n\n4. 有外网访问权限，因为需要拉取镜像；\n\n5. 能够访问到gcr.io、quay.io这两个 docker registry，因为有小部分镜像需要在这里拉取；\n\n6. 单机可用资源建议 2 核 CPU、8 GB 内存或以上，再小的话问题也不大，但是能调度的 Pod 数量就比较有限了；\n\n7. 30 GB 或以上的可用磁盘空间，这主要是留给 Docker 镜像和日志文件用的。\n\n在本次部署中，我准备的机器配置如下：\n\n1. 2 核 CPU、 7.5 GB 内存；\n\n2. 30 GB 磁盘；\n\n3. Ubuntu 16.04；\n\n4. 内网互通；\n\n5. 外网访问权限不受限制；\n\n> 备注：在开始部署前，我推荐你先花几分钟时间，回忆一下 Kubernetes 的架构。\n\n然后，我再和你介绍一下今天实践的目标：\n\n1. 在所有节点上安装 Docker 和 kubeadm；\n\n2. 部署 Kubernetes Master；\n\n3. 部署容器网络插件；\n\n4. 部署 Kubernetes Worker；\n\n5. 部署 Dashboard 可视化插件；\n\n6. 部署容器存储插件。\n\n好了，现在，就来开始这次集群部署之旅吧！\n\n**安装 kubeadm 和 Docker**\n我在上一篇文章《 Kubernetes 一键部署利器：kubeadm》中，已经介绍过 kubeadm 的基础用法，它的一键安装非常方便，我们只需要添加 kubeadm 的源，然后直接使用 apt-get 安装即可，具体流程如下所示：\n\n> 备注：为了方便讲解，我后续都直接会在 root 用户下进行操作\n\n\t$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n\t$ cat <<EOF > /etc/apt/sources.list.d/kubernetes.list\n\tdeb http://apt.kubernetes.io/ kubernetes-xenial main\n\tEOF\n\t$ apt-get update\n\t$ apt-get install -y docker.io kubeadm\n> 提示：如果 apt.kubernetes.io 因为网络问题访问不到，可以换成中科大的 Ubuntu 镜像源 deb http://mirrors.ustc.edu.cn/kubernetes/apt kubernetes-xenial main。\n在上述安装 kubeadm 的过程中，kubeadm 和 kubelet、kubectl、kubernetes-cni 这几个二进制文件都会被自动安装好。\n\n另外，这里我直接使用 Ubuntu 的 docker.io 的安装源，原因是 Docker 公司每次发布的最新的 Docker CE（社区版）产品往往还没有经过 Kubernetes 项目的验证，可能会有兼容性方面的问题。\n\n部署 Kubernetes 的 Master 节点\n在上一篇文章中，我已经介绍过 kubeadm 可以一键部署 Master 节点。不过，在本篇文章中既然要部署一个“完整”的 Kubernetes 集群，那我们不妨稍微提高一下难度：通过配置文件来开启一些实验性功能。\n\n所以，这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）：\n\n\tapiVersion: kubeadm.k8s.io/v1alpha1\n\tkind: MasterConfiguration\n\tcontrollerManagerExtraArgs:\n\t  horizontal-pod-autoscaler-use-rest-clients: \"true\"\n\t  horizontal-pod-autoscaler-sync-period: \"10s\"\n\t  node-monitor-grace-period: \"10s\"\n\tapiServerExtraArgs:\n\t  runtime-config: \"api/all=true\"\n\tkubernetesVersion: \"stable-1.11\"\n这个配置中，我给 kube-controller-manager 设置了：\n\n\thorizontal-pod-autoscaler-use-rest-clients: \"true\"\n这意味着，将来部署的 kube-controller-manager 能够使用自定义资源（Custom Metrics）进行自动水平扩展。这是我后面文章中会重点介绍的一个内容。\n\n其中，“stable-1.11”就是 kubeadm 帮我们部署的 Kubernetes 版本号，即：Kubernetes release 1.11 最新的稳定版，在我的环境下，它是 v1.11.1。你也可以直接指定这个版本，比如：kubernetesVersion: “v1.11.1”\n\n然后，我们只需要执行一句指令：\n\n\t$ kubeadm init --config kubeadm.yaml\n就可以完成 Kubernetes Master 的部署了，这个过程只需要几分钟。部署完成后，kubeadm 会生成一行指令：\n\n\tkubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711\n这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。\n\n此外，kubeadm 还会提示我们第一次使用 Kubernetes 集群所需要的配置命令：\n\n\tmkdir -p $HOME/.kube\n\tsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n\tsudo chown $(id -u):$(id -g) $HOME/.kube/config\n而需要这些配置命令的原因是：Kubernetes 集群默认需要加密方式访问。所以，这几条命令，就是将刚刚部署生成的 Kubernetes 集群的安全配置文件，保存到当前用户的.kube 目录下，kubectl 默认会使用这个目录下的授权信息访问 Kubernetes 集群。\n\n如果不这么做的话，我们每次都需要通过 export KUBECONFIG 环境变量告诉 kubectl 这个安全配置文件的位置。\n\n现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了：\n\n\t$ kubectl get nodes\n\t\n\tNAME      STATUS     ROLES     AGE       VERSION\n\tmaster    NotReady   master    1d        v1.11.1\n可以看到，这个 get 指令输出的结果里，Master 节点的状态是 NotReady，这是为什么呢？\n\n在调试 Kubernetes 集群时，最重要的手段就是用 kubectl describe 来查看这个节点（Node）对象的详细信息、状态和事件（Event），我们来试一下：\n\n\t$ kubectl describe node master\n\t \n\t...\n\tConditions:\n\t...\n\t \n\tReady   False ... KubeletNotReady  runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized\n通过 kubectl describe 指令的输出，我们可以看到 NodeNotReady 的原因在于，我们尚未部署任何网络插件。\n\n另外，我们还可以通过 kubectl 检查这个节点上各个系统 Pod 的状态，其中，kube-system 是 Kubernetes 项目预留的系统 Pod 的工作空间（Namepsace，注意它并不是 Linux Namespace，它只是 Kubernetes 划分不同工作空间的单位）：\n\n\t$ kubectl get pods -n kube-system\n\t\n\tNAME               READY   STATUS   RESTARTS  AGE\n\tcoredns-78fcdf6894-j9s52     0/1    Pending  0     1h\n\tcoredns-78fcdf6894-jm4wf     0/1    Pending  0     1h\n\tetcd-master           1/1    Running  0     2s\n\tkube-apiserver-master      1/1    Running  0     1s\n\tkube-controller-manager-master  0/1    Pending  0     1s\n\tkube-proxy-xbd47         1/1    NodeLost  0     1h\n\tkube-scheduler-master      1/1    Running  0     1s\n可以看到，CoreDNS、kube-controller-manager 等依赖于网络的 Pod 都处于 Pending 状态，即调度失败。这当然是符合预期的：因为这个 Master 节点的网络尚未就绪。\n\n**部署网络插件**\n在 Kubernetes 项目“一切皆容器”的设计理念指导下，部署网络插件非常简单，只需要执行一句 kubectl apply 指令，以 Weave 为例：\n\n\t$ kubectl apply -f https://git.io/weave-kube-1.6\n部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态：\n\n\t$ kubectl get pods -n kube-system\n\t\n\tNAME                             READY     STATUS    RESTARTS   AGE\n\tcoredns-78fcdf6894-j9s52         1/1       Running   0          1d\n\tcoredns-78fcdf6894-jm4wf         1/1       Running   0          1d\n\tetcd-master                      1/1       Running   0          9s\n\tkube-apiserver-master            1/1       Running   0          9s\n\tkube-controller-manager-master   1/1       Running   0          9s\n\tkube-proxy-xbd47                 1/1       Running   0          1d\n\tkube-scheduler-master            1/1       Running   0          9s\n\tweave-net-cmk27                  2/2       Running   0          19s\n可以看到，所有的系统 Pod 都成功启动了，而刚刚部署的 Weave 网络插件则在 kube-system 下面新建了一个名叫 weave-net-cmk27 的 Pod，一般来说，这些 Pod 就是容器网络插件在每个节点上的控制组件。\n\n**`Kubernetes 支持容器网络插件，使用的是一个名叫 CNI 的通用接口，它也是当前容器网络的事实标准，市面上的所有容器网络开源项目都可以通过 CNI 接入 Kubernetes，比如 Flannel、Calico、Canal、Romana 等等.`** 它们的部署方式也都是类似的“一键部署”。关于这些开源项目的实现细节和差异，我会在后续的网络部分详细介绍。\n\n至此，Kubernetes 的 Master 节点就部署完成了。如果你只需要一个单节点的 Kubernetes，现在你就可以使用了。不过，在默认情况下，Kubernetes 的 Master 节点是不能运行用户 Pod 的，所以还需要额外做一个小操作。在本篇的最后部分，我会介绍到它。\n\n**部署 Kubernetes 的 Worker 节点**\nKubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。\n\n所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。\n\n第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。\n\n第二步，执行部署 Master 节点时生成的 kubeadm join 指令：\n\n\t$ kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711\n**通过 Taint/Toleration 调整 Master 执行 Pod 的策略**\n我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。\n\n它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”。\n\n除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。\n\n其中，为节点打上“污点”（Taint）的命令是：\n\n\t$ kubectl taint nodes node1 foo=bar:NoSchedule\n这时，**`该 node1 节点上就会增加一个键值对格式的 Taint，即：foo=bar:NoSchedule。其中值里面的 NoSchedule，意味着这个 Taint 只会在调度新 Pod 时产生作用，而不会影响已经在 node1 上运行的 Pod，哪怕它们没有 Toleration。`**\n\n那么 Pod 又如何声明 Toleration 呢？\n\n我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可：\n\n\tapiVersion: v1\n\tkind: Pod\n\t...\n\tspec:\n\t  tolerations:\n\t  - key: \"foo\"\n\t    operator: \"Equal\"\n\t    value: \"bar\"\n\t    effect: \"NoSchedule\"\n这个 Toleration 的含义是，这个 Pod 能“容忍”所有键值对为 foo=bar 的 Taint（ operator: “Equal”，“等于”操作）。\n\n现在回到我们已经搭建的集群上来。这时，如果你通过 kubectl describe 检查一下 Master 节点的 Taint 字段，就会有所发现了：\n\n\t$ kubectl describe node master\n\t\n\tName:               master\n\tRoles:              master\n\tTaints:             node-role.kubernetes.io/master:NoSchedule\n可以看到，Master 节点默认被加上了node-role.kubernetes.io/master:NoSchedule这样一个“污点”，其中“键”是node-role.kubernetes.io/master，而没有提供“值”。\n\n此时，你就需要像下面这样用“Exists”操作符（operator: “Exists”，“存在”即可）来说明，该 Pod 能够容忍所有以 foo 为键的 Taint，才能让这个 Pod 运行在该 Master 节点上：\n\n\tapiVersion: v1\n\tkind: Pod\n\t...\n\tspec:\n\t  tolerations:\n\t  - key: \"foo\"\n\t    operator: \"Exists\"\n\t    effect: \"NoSchedule\"\n当然，如果你就是想要一个单节点的 Kubernetes，删除这个 Taint 才是正确的选择：\n\n\t$ kubectl taint nodes --all node-role.kubernetes.io/master-\n如上所示，我们在“node-role.kubernetes.io/master”这个键后面加上了一个短横线“-”，这个格式就意味着移除所有以“node-role.kubernetes.io/master”为键的 Taint。\n\n到了这一步，一个基本完整的 Kubernetes 集群就部署完毕了。是不是很简单呢？\n\n有了 kubeadm 这样的原生管理工具，Kubernetes 的部署已经被大大简化。更重要的是，像证书、授权、各个组件的配置等部署中最麻烦的操作，kubeadm 都已经帮你完成了。\n\n接下来，我们再在这个 Kubernetes 集群上安装一些其他的辅助插件，比如 Dashboard 和存储插件。\n\n**部署 Dashboard 可视化插件**\n在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。毫不意外，它的部署也相当简单：\n\n\t$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml\n部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：\n\n\t$ kubectl get pods -n kube-system\n\t\n\tkubernetes-dashboard-6948bdb78-f67xk   1/1       Running   0          1m\n需要注意的是，由于 Dashboard 是一个 Web Server，很多人经常会在自己的公有云上无意地暴露 Dashboard 的端口，从而造成安全隐患。所以，1.7 版本之后的 Dashboard 项目部署完成后，默认只能通过 Proxy 的方式在本地访问。具体的操作，你可以查看 Dashboard 项目的[官方文档](https://github.com/kubernetes/dashboard)。\n\n而如果你想从集群外访问这个 Dashboard 的话，就需要用到 Ingress，我会在后面的文章中专门介绍这部分内容。\n\n**部署容器存储插件**\n接下来，让我们完成这个 Kubernetes 集群的最后一块拼图：容器持久化存储。\n\n我在前面介绍容器原理时已经提到过，很多时候我们需要用数据卷（Volume）把外面宿主机上的目录或者文件挂载进容器的 Mount Namespace 中，从而达到容器和宿主机共享这些目录或者文件的目的。容器里的应用，也就可以在这些数据卷中新建和写入文件。\n\n可是，**`如果你在某一台机器上启动的一个容器，显然无法看到其他机器上的容器在它们的数据卷里写入的文件。这是容器最典型的特征之一：无状态。`**\n\n而容器的持久化存储，就是用来保存容器存储状态的重要手段：存储插件会在容器里挂载一个基于网络或者其他机制的远程数据卷，使得在容器里创建的文件，实际上是保存在远程存储服务器上，或者以分布式的方式保存在多个节点上，而与当前宿主机没有任何绑定关系。这样，无论你在其他哪个宿主机上启动新的容器，都可以请求挂载指定的持久化存储卷，从而访问到数据卷里保存的内容。**这就是“持久化”的含义。**\n\n由于 Kubernetes 本身的松耦合设计，绝大多数存储项目，比如 Ceph、GlusterFS、NFS 等，都可以为 Kubernetes 提供持久化存储能力。在这次的部署实战中，我会选择部署一个很重要的 Kubernetes 存储插件项目：Rook。\n\nRook 项目是一个基于 Ceph 的 Kubernetes 存储插件（它后期也在加入对更多存储实现的支持）。不过，不同于对 Ceph 的简单封装，Rook 在自己的实现中加入了水平扩展、迁移、灾难备份、监控等大量的企业级功能，使得这个项目变成了一个完整的、生产级别可用的容器存储插件。\n\n得益于容器化技术，用两条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来：\n\n\t$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml\n\t\n\t$ kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml\n在部署完成后，你就可以看到 Rook 项目会将自己的 Pod 放置在由它自己管理的两个 Namespace 当中：\n\n\t$ kubectl get pods -n rook-ceph-system\n\tNAME                                  READY     STATUS    RESTARTS   AGE\n\trook-ceph-agent-7cv62                 1/1       Running   0          15s\n\trook-ceph-operator-78d498c68c-7fj72   1/1       Running   0          44s\n\trook-discover-2ctcv                   1/1       Running   0          15s\n\t\n\t$ kubectl get pods -n rook-ceph\n\tNAME                   READY     STATUS    RESTARTS   AGE\n\trook-ceph-mon0-kxnzh   1/1       Running   0          13s\n\trook-ceph-mon1-7dn2t   1/1       Running   0          2s\n这样，一个基于 Rook 持久化存储集群就以容器的方式运行起来了，而接下来在 Kubernetes 项目上创建的所有 Pod 就能够通过 Persistent Volume（PV）和 Persistent Volume Claim（PVC）的方式，在容器里挂载由 Ceph 提供的数据卷了。\n\n而 Rook 项目，则会负责这些数据卷的生命周期管理、灾难备份等运维工作。关于这些容器持久化存储的知识，我会在后续章节中专门讲解。\n\n这时候，你可能会有个疑问：为什么我要选择 Rook 项目呢？\n\n其实，是因为这个项目很有前途。\n\n如果你去研究一下 Rook 项目的实现，就会发现它巧妙地依赖了 Kubernetes 提供的编排能力，合理的使用了很多诸如 Operator、CRD 等重要的扩展特性（这些特性我都会在后面的文章中逐一讲解到）。这使得 Rook 项目，成为了目前社区中基于 Kubernetes API 构建的最完善也最成熟的容器存储插件。我相信，这样的发展路线，很快就会得到整个社区的推崇。\n\n> 备注：其实，在很多时候，大家说的所谓“云原生”，就是“Kubernetes 原生”的意思。而像 Rook、Istio 这样的项目，正是贯彻这个思路的典范。在我们后面讲解了声明式 API 之后，相信你对这些项目的设计思想会有更深刻的体会。\n\n**总结**\n在本篇文章中，我们完全从 0 开始，在 Bare-metal 环境下使用 kubeadm 工具部署了一个完整的 Kubernetes 集群：这个集群有一个 Master 节点和多个 Worker 节点；使用 Weave 作为容器网络插件；使用 Rook 作为容器持久化存储插件；使用 Dashboard 插件提供了可视化的 Web 界面。\n\n这个集群，也将会是我进行后续讲解所依赖的集群环境，并且在后面的讲解中，我还会给它安装更多的插件，添加更多的新能力。\n\n另外，这个集群的部署过程并不像传说中那么繁琐，这主要得益于：\n\n1. kubeadm 项目大大简化了部署 Kubernetes 的准备工作，尤其是配置文件、证书、二进制文件的准备和制作，以及集群版本管理等操作，都被 kubeadm 接管了。\n\n2. Kubernetes 本身“一切皆容器”的设计思想，加上良好的可扩展机制，使得插件的部署非常简便。\n\n上述思想，也是开发和使用 Kubernetes 的重要指导思想，即：基于 Kubernetes 开展工作时，你一定要优先考虑这两个问题：\n\n1. 我的工作是不是可以容器化？\n\n2. 我的工作是不是可以借助 Kubernetes API 和可扩展机制来完成？\n\n而一旦这项工作能够基于 Kubernetes 实现容器化，就很有可能像上面的部署过程一样，大幅简化原本复杂的运维工作。对于时间宝贵的技术人员来说，这个变化的重要性是不言而喻的。\n\n**思考题**\n1. 你是否使用其他工具部署过 Kubernetes 项目？经历如何？\n\n2. 你是否知道 Kubernetes 项目当前（v1.11）能够有效管理的集群规模是多少个节点？你在生产环境中希望部署或者正在部署的集群规模又是多少个节点呢？\n\n","categories":["reference","k8s"]},{"title":"14 | 深入解析Pod对象（一）：基本概念","url":"/2021/03/13/reference/k8s/14.深入解析Pod对象(一)基本概念/","content":"\n![](1.jpg)\n\n在上一篇文章中，我详细介绍了 Pod 这个 Kubernetes 项目中最重要的概念。而在今天这篇文章中，我会和你分享 Pod 对象的更多细节。\n\n现在，你已经非常清楚：Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？\n\n要彻底理解这个问题，你就一定要牢记我在上一篇文章中提到的一个结论：Pod 扮演的是传统部署环境里“虚拟机”的角色。这样的设计，是为了使用户从传统环境（虚拟机环境）向 Kubernetes（容器环境）的迁移，更加平滑。\n\n而如果你能把 Pod 看成传统环境里的“机器”、把容器看作是运行在这个“机器”里的“用户程序”，那么很多关于 Pod 对象的设计就非常容易理解了。\n\n比如，凡是**`调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。`**\n\n这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。比如，配置这个“机器”的网卡（即：Pod 的网络定义），配置这个“机器”的磁盘（即：Pod 的存储定义），配置这个“机器”的防火墙（即：Pod 的安全定义）。更不用说，这台“机器”运行在哪个服务器之上（即：Pod 的调度）。\n\n接下来，我就先为你介绍 Pod 中几个重要字段的含义和用法。\n\n**`NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段`**，用法如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\t...\n\tspec:\n\t nodeSelector:\n\t   disktype: ssd\n这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。\n\n**`NodeName`**：一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。\n\n**`HostAliases`**：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下：\n\n\tapiVersion: v1\n\tkind: Pod\n\t...\n\tspec:\n\t  hostAliases:\n\t  - ip: \"10.1.2.3\"\n\t    hostnames:\n\t    - \"foo.remote\"\n\t    - \"bar.remote\"\n\t...\n在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。这样，这个 Pod 启动后，/etc/hosts 文件的内容将如下所示：\n\n\tcat /etc/hosts\n\t# Kubernetes-managed hosts file.\n\t127.0.0.1 localhost\n\t...\n\t10.244.135.10 hostaliases-pod\n\t10.1.2.3 foo.remote\n\t10.1.2.3 bar.remote\n其中，最下面两行记录，就是我通过 HostAliases 字段为 Pod 设置的。需要指出的是，在 Kubernetes 项目中，如果要设置 hosts 文件里的内容，一定要通过这种方法。否则，如果直接修改了 hosts 文件的话，在 Pod 被删除重建之后，kubelet 会自动覆盖掉被修改的内容。\n\n除了上述跟“机器”相关的配置外，你可能也会发现，**`凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的`**。这个原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。\n\n举个例子，在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: nginx\n\tspec:\n\t  shareProcessNamespace: true\n\t  containers:\n\t  - name: nginx\n\t    image: nginx\n\t  - name: shell\n\t    image: busybox\n\t    stdin: true\n\t    tty: true\n这就意味着这个 Pod 里的容器要共享 PID Namespace。\n\n而在这个 YAML 文件中，我还定义了两个容器：一个是 nginx 容器，一个是开启了 tty 和 stdin 的 shell 容器。\n\n我在前面介绍容器基础时，曾经讲解过什么是 tty 和 stdin。而在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。\n**`(docker run -it ，-it交互是窗口，i表示的是stdin,t表示的是tty，后者表示的是终端交互窗口，可以接收用户的输入，并且输出结果，而若需要实现输入信息，需要同时开启标准输入流，stdin正是用来做这个操作)`**\n\n如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。\n\n于是，这个 Pod 被创建后，你就可以使用 shell 容器的 tty 跟这个容器进行交互了。我们一起实践一下：\n\n\t$ kubectl create -f nginx.yaml\n接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上：\n\n\t$ kubectl attach -it nginx -c shell\n这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程：\n\n\t$ kubectl attach -it nginx -c shell\n\t/ # ps ax\n\tPID   USER     TIME  COMMAND\n\t    1 root      0:00 /pause\n\t    8 root      0:00 nginx: master process nginx -g daemon off;\n\t   14 101       0:00 nginx: worker process\n\t   15 root      0:00 sh\n\t   21 root      0:00 ps ax\n可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。\n\n类似地，**`凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义`**，比如：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: nginx\n\tspec:\n\t  hostNetwork: true\n\t  hostIPC: true\n\t  hostPID: true\n\t  containers:\n\t  - name: nginx\n\t    image: nginx\n\t  - name: shell\n\t    image: busybox\n\t    stdin: true\n\t    tty: true\n在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。\n\n当然，除了这些属性，Pod 里最重要的字段当属“Containers”了。而在上一篇文章中，我还介绍过“Init Containers”。其实，这两个字段都属于 Pod 对容器的定义，内容也完全相同，只是 Init Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行。\n\nKubernetes 项目中对 Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。\n\n首先，是 **`ImagePullPolicy`** 字段。它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。\n\nImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。\n\n而如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。\n\n其次，是 **`Lifecycle`** 字段。它定义的是 Container Lifecycle Hooks。顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: lifecycle-demo\n\tspec:\n\t  containers:\n\t  - name: lifecycle-demo-container\n\t    image: nginx\n\t    lifecycle:\n\t      postStart:\n\t        exec:\n\t          command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"]\n\t      preStop:\n\t        exec:\n\t          command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"]\n这是一个来自 Kubernetes 官方文档的 Pod 的 YAML 文件。它其实非常简单，只是定义了一个 nginx 镜像的容器。不过，在这个 YAML 文件的容器（Containers）部分，你会看到这个容器分别设置了一个 postStart 和 preStop 参数。这是什么意思呢？\n\n先说 postStart 吧。它指的是，在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。\n**`(postStart 和ENTRYPOINT异步并发执行)`**\n\n当然，如果 postStart 执行超时或者错误，Kubernetes 会在该 Pod 的 Events 中报出该容器启动失败的错误信息，导致 Pod 也处于失败的状态。\n\n而类似地，preStop 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。\n**`(prestop和poststart不同之处在于poststart与entrypoint启动顺序是entrypoint先于poststart，但是poststart并不会等待entrypoint完成之后再执行。而prestop与容器退出是同步的，必须执行完成prestop容器才会退出)`**\n\n所以，在这个例子中，我们在容器成功启动之后，在 /usr/share/message 里写入了一句“欢迎信息”（即 postStart 定义的操作）。而在这个容器被删除之前，我们则先调用了 nginx 的退出指令（即 preStop 定义的操作），从而实现了容器的“优雅退出”。\n\n在熟悉了 Pod 以及它的 Container 部分的主要字段之后，我再和你分享一下这样一个的 Pod 对象在 Kubernetes 中的生命周期。\n\nPod 生命周期的变化，主要体现在 Pod API 对象的Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况：\n\n1. Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。\n\n2. Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。\n\n3. Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。\n\n4. Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。\n\n5. Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。\n\n更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。\n\n比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。\n\n而其中，Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。这两者之间（Running 和 Ready）是有区别的，你不妨仔细思考一下。\n\nPod 的这些状态信息，是我们判断应用运行情况的重要标准，尤其是 Pod 进入了非“Running”状态后，你一定要能迅速做出反应，根据它所代表的异常情况开始跟踪和定位，而不是去手忙脚乱地查阅文档。\n\n**总结**\n在今天这篇文章中，我详细讲解了 Pod API 对象，介绍了 Pod 的核心使用方法，并分析了 Pod 和 Container 在字段上的异同。希望这些讲解能够帮你更好地理解和记忆 Pod YAML 中的核心字段，以及这些字段的准确含义。\n\n实际上，Pod API 对象是整个 Kubernetes 体系中最核心的一个概念，也是后面我讲解各种控制器时都要用到的。\n\n在学习完这篇文章后，我希望你能仔细阅读 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。争取做到下次看到一个 Pod 的 YAML 文件时，不再需要查阅文档，就能做到把常用字段及其作用信手拈来。\n\n而在下一篇文章中，我会通过大量的实践，帮助你巩固和进阶关于 Pod API 对象核心字段的使用方法，敬请期待吧。\n\n**思考题**\n你能否举出一些 Pod（即容器）的状态是 Running，但是应用其实已经停止服务的例子？相信 Java Web 开发者的亲身体会会比较多吧。\n\n","categories":["reference","k8s"]},{"title":"13 | 为什么我们需要Pod？","url":"/2021/03/13/reference/k8s/13.为什么我们需要Pod/","content":"\n![](1.jpg)\n\n在前面的文章中，我详细介绍了在 Kubernetes 里部署一个应用的过程。在这些讲解中，我提到了这样一个知识点：Pod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。\n\n不过，我相信你在学习和使用 Kubernetes 项目的过程中，已经不止一次地想要问这样一个问题：为什么我们会需要 Pod？\n\n是啊，我们在前面已经花了很多精力去解读 Linux 容器的原理、分析了 Docker 容器的本质，终于，**`“Namespace 做隔离，Cgroups 做限制，rootfs 做文件系统”`**这样的“三句箴言”可以朗朗上口了，为什么 Kubernetes 项目又突然搞出一个 Pod 来呢？\n\n要回答这个问题，我们还是要一起回忆一下我曾经反复强调的一个问题：容器的本质到底是什么？\n\n你现在应该可以不假思索地回答出来：**`容器的本质是进程.`**\n\n没错。容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？\n\n你应该也能立刻回答上来：Kubernetes 就是\"操作系统\"！**`(Kubernetes是管理容器的“操作系统”)`**\n\n非常正确。\n\n现在，就让我们登录到一台 Linux 机器里，执行一条如下所示的命令：\n\n\t$ pstree -g\n这条命令的作用，是展示当前系统中正在运行的进程的树状结构。它的返回结果如下所示：\n\n\tsystemd(1)-+-accounts-daemon(1984)-+-{gdbus}(1984)\n\t           | `-{gmain}(1984)\n\t           |-acpid(2044)\n\t          ...      \n\t           |-lxcfs(1936)-+-{lxcfs}(1936)\n\t           | `-{lxcfs}(1936)\n\t           |-mdadm(2135)\n\t           |-ntpd(2358)\n\t           |-polkitd(2128)-+-{gdbus}(2128)\n\t           | `-{gmain}(2128)\n\t           |-rsyslogd(1632)-+-{in:imklog}(1632)\n\t           |  |-{in:imuxsock) S 1(1632)\n\t           | `-{rs:main Q:Reg}(1632)\n\t           |-snapd(1942)-+-{snapd}(1942)\n\t           |  |-{snapd}(1942)\n\t           |  |-{snapd}(1942)\n\t           |  |-{snapd}(1942)\n\t           |  |-{snapd}(1942)\n不难发现，在一个真正的操作系统里，进程并不是“孤苦伶仃”地独自运行的，而是以进程组的方式，“有原则的”组织在一起。在这个进程的树状图中，每一个进程后面括号里的数字，就是它的进程组 ID（Process Group ID, PGID） \n\n比如，这里有一个叫作 rsyslogd 的程序，它负责的是 Linux 操作系统里的日志处理。可以看到，rsyslogd 的主程序 main，和它要用到的内核日志模块 imklog 等，同属于 1632 进程组。这些进程相互协作，共同完成 rsyslogd 程序的职责。\n\n> 注意：我在本篇中提到的“进程”，比如，rsyslogd 对应的 imklog，imuxsock 和 main，严格意义上来说，其实是 Linux 操作系统语境下的“线程”。这些线程，或者说，轻量级进程之间，可以共享文件、信号、数据内存、甚至部分代码，从而紧密协作共同完成一个程序的职责。所以同理，我提到的“进程组”，对应的也是 Linux 操作系统语境下的“线程组”。这种命名关系与实际情况的不一致，是 Linux 发展历史中的一个遗留问题。对这个话题感兴趣的同学，可以阅读[这篇技术文章](https://www.ibm.com/developerworks/cn/linux/kernel/l-thread/index.html)来了解一下。\n\n对于操作系统来说，这样的进程组更方便管理。举个例子，Linux 操作系统只需要将信号，比如，SIGKILL 信号，发送给一个进程组，那么该进程组中的所有进程就都会收到这个信号而终止运行。\n\n而 Kubernetes 项目所做的，其实就是将“进程组”的概念映射到了容器技术中，并使其成为了这个云计算“操作系统”里的“一等公民”。\n\nKubernetes 项目之所以要这么做的原因，我在前面介绍 Kubernetes 和 Borg 的关系时曾经提到过：在 Borg 项目的开发和实践过程中，Google 公司的工程师们发现，他们部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。\n\n而如果事先没有“组”的概念，像这样的运维关系就会非常难以处理。\n\n我还是以前面的 rsyslogd 为例子。已知 rsyslogd 由三个进程组成：一个 imklog 模块，一个 imuxsock 模块，一个 rsyslogd 自己的 main 函数主进程。这三个进程一定要运行在同一台机器上，否则，它们之间基于 Socket 的通信和文件交换，都会出现问题。\n\n现在，我要把 rsyslogd 这个应用给容器化，由于受限于容器的“单进程模型”，这三个模块必须被分别制作成三个不同的容器。而在这三个容器运行的时候，它们设置的内存配额都是 1 GB。\n\n> 再次强调一下：容器的“单进程模型”，并不是指容器里只能运行“一个”进程，而是指容器没有管理多个进程的能力。这是因为容器里 PID=1 的进程就是应用本身，其他的进程都是这个 PID=1 进程的子进程。可是，用户编写的应用，并不能够像正常操作系统里的 init 进程或者 systemd 那样拥有进程管理的功能。比如，你的应用是一个 Java Web 程序（PID=1），然后你执行 docker exec 在后台启动了一个 Nginx 进程（PID=3）。可是，当这个 Nginx 进程异常退出的时候，你该怎么知道呢？这个进程退出后的垃圾收集工作，又应该由谁去做呢？\n\n假设我们的 Kubernetes 集群上有两个节点：node-1 上有 3 GB 可用内存，node-2 有 2.5 GB 可用内存。\n\n这时，假设我要用 Docker Swarm 来运行这个 rsyslogd 程序。为了能够让这三个容器都运行在同一台机器上，我就必须在另外两个容器上设置一个 affinity=main（与 main 容器有亲密性）的约束，即：它们俩必须和 main 容器运行在同一台机器上。\n\n然后，我顺序执行：“docker run main”“docker run imklog”和“docker run imuxsock”，创建这三个容器。\n\n这样，这三个容器都会进入 Swarm 的待调度队列。然后，main 容器和 imklog 容器都先后出队并被调度到了 node-2 上（这个情况是完全有可能的）。\n\n可是，当 imuxsock 容器出队开始被调度时，Swarm 就有点懵了：node-2 上的可用资源只有 0.5 GB 了，并不足以运行 imuxsock 容器；可是，根据 affinity=main 的约束，imuxsock 容器又只能运行在 node-2 上。\n\n这就是一个典型的成组调度（gang scheduling）没有被妥善处理的例子。\n\n在工业界和学术界，关于这个问题的讨论可谓旷日持久，也产生了很多可供选择的解决方案。\n\n比如，Mesos 中就有一个资源囤积（resource hoarding）的机制，会在所有设置了 Affinity 约束的任务都达到时，才开始对它们统一进行调度。而在 Google Omega 论文中，则提出了使用乐观调度处理冲突的方法，即：先不管这些冲突，而是通过精心设计的回滚机制在出现了冲突之后解决问题。\n\n可是这些方法都谈不上完美。资源囤积带来了不可避免的调度效率损失和死锁的可能性；而乐观调度的复杂程度，则不是常规技术团队所能驾驭的。\n\n但是，到了 Kubernetes 项目里，这样的问题就迎刃而解了：Pod 是 Kubernetes 里的原子调度单位。这就意味着，Kubernetes 项目的调度器，是统一按照 Pod 而非容器的资源需求进行计算的。\n\n所以，像 imklog、imuxsock 和 main 函数主进程这样的三个容器，正是一个典型的由三个容器组成的 Pod。Kubernetes 项目在调度时，自然就会去选择可用内存等于 3 GB 的 node-1 节点进行绑定，而根本不会考虑 node-2。\n\n像这样容器间的紧密协作，我们可以称为“超亲密关系”。这些具有“超亲密关系”容器的典型特征包括但不限于：互相之间会发生直接的文件交换、使用 localhost 或者 Socket 文件进行本地通信、会发生非常频繁的远程调用、需要共享某些 Linux Namespace（比如，一个容器要加入另一个容器的 Network Namespace）等等。\n\n这也就意味着，并不是所有有“关系”的容器都属于同一个 Pod。比如，PHP 应用容器和 MySQL 虽然会发生访问关系，但并没有必要、也不应该部署在同一台机器上，它们更适合做成两个 Pod。\n\n不过，相信此时你可能会有第二个疑问：\n\n对于初学者来说，一般都是先学会了用 Docker 这种单容器的工具，才会开始接触 Pod。\n\n而如果 Pod 的设计只是出于调度上的考虑，那么 Kubernetes 项目似乎完全没有必要非得把 Pod 作为“一等公民”吧？这不是故意增加用户的学习门槛吗？\n\n没错，如果只是处理“超亲密关系”这样的调度问题，有 Borg 和 Omega 论文珠玉在前，Kubernetes 项目肯定可以在调度器层面给它解决掉。\n\n不过，Pod 在 Kubernetes 项目里还有更重要的意义，那就是：容器设计模式。\n\n为了理解这一层含义，我就必须先给你介绍一下Pod 的实现原理。\n\n首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。\n\n也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。\n\n那么，Pod 又是怎么被“创建”出来的呢？\n\n答案是：**`Pod，其实是一组共享了某些资源的容器。`**\n\n具体的说：**`Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。`**\n\n那这么来看的话，一个有 A、B 两个容器的 Pod，不就是等同于一个容器（容器 A）共享另外一个容器（容器 B）的网络和 Volume 的玩儿法么？\n\n这好像通过 docker run --net --volumes-from 这样的命令就能实现嘛，比如：\n\n\t$ docker run --net=B --volumes-from=B --name=A image-A ...\n但是，你有没有考虑过，如果真这样做的话，容器 B 就必须比容器 A 先启动，这样一个 Pod 里的多个容器就不是对等关系，而是拓扑关系了。\n\n所以，在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。这样的组织关系，可以用下面这样一个示意图来表达：\n![](2.png)\n\n如上图所示，这个 Pod 里有两个用户容器 A 和 B，还有一个 **`Infra 容器`**。很容易理解，在 Kubernetes 项目里，Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：**`k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。`**\n\n而在 Infra 容器“Hold 住”Network Namespace 后，用户容器就可以加入到 Infra 容器的 Network Namespace 当中了。所以，如果你查看这些容器在宿主机上的 Namespace 文件（这个 Namespace 文件的路径，我已经在前面的内容中介绍过），它们指向的值一定是完全一样的。\n\n这也就意味着，对于 Pod 里的容器 A 和容器 B 来说：\n\n * 它们可以直接使用 localhost 进行通信；\n * 它们看到的网络设备跟 Infra 容器看到的完全一样；\n * 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；\n * 当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；\n * **`Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。`**\n而对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，也可以认为都是通过 Infra 容器完成的。这一点很重要，因为**`将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。`**\n\n这就意味着，如果你的网络插件需要在容器里安装某些包或者配置才能完成的话，是不可取的：Infra 容器镜像的 rootfs 里几乎什么都没有，没有你随意发挥的空间。当然，这同时也意味着你的网络插件完全不必关心用户容器的启动与否，而只需要关注如何配置 Pod，也就是 Infra 容器的 Network Namespace 即可。\n\n有了这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。\n\n这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: two-containers\n\tspec:\n\t  restartPolicy: Never\n\t  volumes:\n\t  - name: shared-data\n\t    hostPath:      \n\t      path: /data\n\t  containers:\n\t  - name: nginx-container\n\t    image: nginx\n\t    volumeMounts:\n\t    - name: shared-data\n\t      mountPath: /usr/share/nginx/html\n\t  - name: debian-container\n\t    image: debian\n\t    volumeMounts:\n\t    - name: shared-data\n\t      mountPath: /pod-data\n\t    command: [\"/bin/sh\"]\n\t    args: [\"-c\", \"echo Hello from the debian container > /pod-data/index.html\"]\n在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。\n\n这就是为什么，nginx-container 可以从它的 /usr/share/nginx/html 目录中，读取到 debian-container 生成的 index.html 文件的原因。\n\n明白了 Pod 的实现原理后，我们再来讨论“容器设计模式”，就容易多了。\n\nPod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。\n\n为了能够掌握这种思考方式，你就应该尽量尝试使用它来描述一些用单个容器难以解决的问题。\n\n第一个最典型的例子是：WAR 包与 Web 服务器。\n\n我们现在有一个 Java Web 应用的 WAR 包，它需要被放在 Tomcat 的 webapps 目录下运行起来。\n\n假如，你现在只能用 Docker 来做这件事情，那该如何处理这个组合关系呢？\n\n * 一种方法是，把 WAR 包直接放在 Tomcat 镜像的 webapps 目录下，做成一个新的镜像运行起来。可是，这时候，如果你要更新 WAR 包的内容，或者要升级 Tomcat 镜像，就要重新制作一个新的发布镜像，非常麻烦。\n * 另一种方法是，你压根儿不管 WAR 包，永远只发布一个 Tomcat 容器。不过，这个容器的 webapps 目录，就必须声明一个 hostPath 类型的 Volume，从而把宿主机上的 WAR 包挂载进 Tomcat 容器当中运行起来。不过，这样你就必须要解决一个问题，即：如何让每一台宿主机，都预先准备好这个存储有 WAR 包的目录呢？这样来看，你只能独立维护一套分布式存储系统了。\n实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: javaweb-2\n\tspec:\n\t  initContainers:\n\t  - image: geektime/sample:v2\n\t    name: war\n\t    command: [\"cp\", \"/sample.war\", \"/app\"]\n\t    volumeMounts:\n\t    - mountPath: /app\n\t      name: app-volume\n\t  containers:\n\t  - image: geektime/tomcat:7.0\n\t    name: tomcat\n\t    command: [\"sh\",\"-c\",\"/root/apache-tomcat-7.0.42-v2/bin/start.sh\"]\n\t    volumeMounts:\n\t    - mountPath: /root/apache-tomcat-7.0.42-v2/webapps\n\t      name: app-volume\n\t    ports:\n\t    - containerPort: 8080\n\t      hostPort: 8001 \n\t  volumes:\n\t  - name: app-volume\n\t    emptyDir: {}\n在这个 Pod 中，我们定义了两个容器，第一个容器使用的镜像是 geektime/sample:v2，这个镜像里只有一个 WAR 包（sample.war）放在根目录下。而第二个容器则使用的是一个标准的 Tomcat 镜像。\n\n不过，你可能已经注意到，WAR 包容器的类型不再是一个普通容器，而是一个 Init Container 类型的容器。\n\n在 Pod 中，**`所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。`**\n\n所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句 \"cp /sample.war /app\"，把应用的 WAR 包拷贝到 /app 目录下，然后退出。\n\n而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。\n\n接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。\n\n所以，等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。\n\n像这样，我们就用一种“组合”方式，解决了 WAR 包与 Tomcat 容器之间耦合关系的问题。\n\n实际上，这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。\n\n顾名思义，sidecar 指的就是我们可以在一个 Pod 中，启动一个辅助容器，来完成一些独立于主进程（主容器）之外的工作。\n\n比如，在我们的这个应用 Pod 中，Tomcat 容器是我们要使用的主容器，而 WAR 包容器的存在，只是为了给它提供一个 WAR 包而已。所以，我们用 Init Container 的方式优先运行 WAR 包容器，扮演了一个 sidecar 的角色。\n\n**第二个例子，则是容器的日志收集。**\n\n比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。\n\n这时，我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。\n\n然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。\n\n这样，接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了。\n\n跟第一个例子一样，这个例子中的 sidecar 的主要工作也是使用共享的 Volume 来完成对文件的操作。\n\n但不要忘记，Pod 的另一个重要特性是，它的所有容器都共享同一个 Network Namespace。这就使得很多与 Pod 网络相关的配置和管理，也都可以交给 sidecar 完成，而完全无须干涉用户容器。这里最典型的例子莫过于 Istio 这个微服务治理项目了。\n\nIstio 项目使用 sidecar 容器完成微服务治理的原理，我在后面很快会讲解到。\n\n备注：Kubernetes 社区曾经把“容器设计模式”这个理论，整理成了[一篇小论文](https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns)，你可以点击链接浏览。\n\n总结\n在本篇文章中我重点分享了 Kubernetes 项目中 Pod 的实现原理。\n\nPod 是 Kubernetes 项目与其他单容器项目相比最大的不同，也是一位容器技术初学者需要面对的第一个与常规认知不一致的知识点。\n\n事实上，直到现在，仍有很多人把容器跟虚拟机相提并论，他们把容器当做性能更好的虚拟机，喜欢讨论如何把应用从虚拟机无缝地迁移到容器中。\n\n但实际上，**`无论是从具体的实现原理，还是从使用方法、特性、功能等方面，容器与虚拟机几乎没有任何相似的地方；也不存在一种普遍的方法，能够把虚拟机里的应用无缝迁移到容器中。因为，容器的性能优势，必然伴随着相应缺陷，即：它不能像虚拟机那样，完全模拟本地物理机环境中的部署方法。`**\n\n所以，**`这个“上云”工作的完成，最终还是要靠深入理解容器的本质，即：进程。`**\n\n实际上，一个运行在虚拟机里的应用，哪怕再简单，也是被管理在 systemd 或者 supervisord 之下的一组进程，而不是一个进程。这跟本地物理机上应用的运行方式其实是一样的。这也是为什么，从物理机到虚拟机之间的应用迁移，往往并不困难。\n\n可是对于容器来说，**`一个容器永远只能管理一个进程。更确切地说，一个容器，就是一个进程。这是容器技术的“天性”，不可能被修改。所以，将一个原本运行在虚拟机里的应用，“无缝迁移”到容器中的想法，实际上跟容器的本质是相悖的。`**\n\n这也是当初 Swarm 项目无法成长起来的重要原因之一：一旦到了真正的生产环境上，Swarm 这种单容器的工作方式，就难以描述真实世界里复杂的应用架构了。\n\n所以，你现在可以这么理解 Pod 的本质：\n\n> Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。\n\n所以下一次，当你需要把一个运行在虚拟机里的应用迁移到 Docker 容器中时，一定要仔细分析到底有哪些进程（组件）运行在这个虚拟机里。\n\n然后，你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。\n\n> 注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的[virtlet 项目](https://github.com/Mirantis/virtlet)就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。\n\n相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。\n\n思考题\n除了 Network Namespace 外，Pod 里的容器还可以共享哪些 Namespace 呢？你能说出共享这些 Namesapce 的具体应用场景吗？\n\n\n\n\n","categories":["reference","k8s"]},{"title":"15 | 深入解析Pod对象（二）：使用进阶","url":"/2021/03/13/reference/k8s/15.深入解析Pod对象(二)使用进阶/","content":"\n![](1.jpg)\n\n在上一篇文章中，我深入解析了 Pod 的 API 对象，讲解了 Pod 和 Container 的关系。\n\n作为 Kubernetes 项目里最核心的编排对象，Pod 携带的信息非常丰富。其中，资源定义（比如 CPU、内存等），以及调度相关的字段，我会在后面专门讲解调度器时再进行深入的分析。在本篇，我们就先从一种特殊的 Volume 开始，来帮助你更加深入地理解 Pod 对象各个重要字段的含义。\n\n这种特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”。\n\n> 备注：Projected Volume 是 Kubernetes v1.11 之后的新特性\n\n这是什么意思呢？\n\n在 Kubernetes 中，有几种特殊的 Volume，它们存在的意义不是为了存放容器里的数据，也不是用来进行容器和宿主机之间的数据交换。这些特殊 Volume 的作用，是为容器提供预先定义好的数据。所以，从容器的角度来看，这些 Volume 里的信息就是仿佛是`被 Kubernetes“投射”（Project）进入容器当中的。`这正是 Projected Volume 的含义。\n\n到目前为止，Kubernetes 支持的 Projected Volume 一共有四种：\n\n1.Secret;\n2.ConfigMap;\n3.Downward API;\n4.ServiceAccountToken.\n\n在今天这篇文章中，我首先和你分享的是 Secret。它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。\n\nSecret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: test-projected-volume \n\tspec:\n\t  containers:\n\t  - name: test-secret-volume\n\t    image: busybox\n\t    args:\n\t    - sleep\n\t    - \"86400\"\n\t    volumeMounts:\n\t    - name: mysql-cred\n\t      mountPath: \"/projected-volume\"\n\t      readOnly: true\n\t  volumes:\n\t  - name: mysql-cred\n\t    projected:\n\t      sources:\n\t      - secret:\n\t          name: user\n\t      - secret:\n\t          name: pass\n在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。\n\n这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示：\n\n\t$ cat ./username.txt\n\tadmin\n\t$ cat ./password.txt\n\tc1oudc0w!\n\t\n\t$ kubectl create secret generic user --from-file=./username.txt\n\t$ kubectl create secret generic pass --from-file=./password.txt\n其中，username.txt 和 password.txt 文件里，存放的就是用户名和密码；而 user 和 pass，则是我为 Secret 对象指定的名字。而我想要查看这些 Secret 对象的话，只要执行一条 kubectl get 命令就可以了：\n\n\t$ kubectl get secrets\n\tNAME           TYPE                                DATA      AGE\n\tuser          Opaque                                1         51s\n\tpass          Opaque                                1         51s\n当然，除了使用 kubectl create secret 指令外，我也可以直接通过编写 YAML 文件的方式来创建这个 Secret 对象，比如：\n\n\tapiVersion: v1\n\tkind: Secret\n\tmetadata:\n\t  name: mysecret\n\ttype: Opaque\n\tdata:\n\t  user: YWRtaW4=\n\t  pass: MWYyZDFlMmU2N2Rm\n可以看到，通过编写 YAML 文件创建出来的 Secret 对象只有一个。但它的 data 字段，却以 Key-Value 的格式保存了两份 Secret 数据。其中，“user”就是第一份数据的 Key，“pass”是第二份数据的 Key。\n\n需要注意的是，Secret 对象要求这些数据必须是经过 Base64 转码的，以免出现明文密码的安全隐患。这个转码操作也很简单，比如：\n**(secret对象必须是经过base64，转码方式为： echo -n '明文' | base64 但是这种方式仅仅是进行了编码，但是却没有进行加密)**\n\n\t$ echo -n 'admin' | base64\n\tYWRtaW4=\n\t$ echo -n '1f2d1e2e67df' | base64\n\tMWYyZDFlMmU2N2Rm\n这里需要注意的是，像这样创建的 Secret 对象，它里面的内容仅仅是经过了转码，而并没有被加密。在真正的生产环境中，你需要在 Kubernetes 中开启 Secret 的加密插件，增强数据的安全性。关于开启 Secret 加密插件的内容，我会在后续专门讲解 Secret 的时候，再做进一步说明。\n\n接下来，我们尝试一下创建这个 Pod：\n\n\t$ kubectl create -f test-projected-volume.yaml\n当 Pod 变成 Running 状态之后，我们再验证一下这些 Secret 对象是不是已经在容器里了：\n\n\t$ kubectl exec -it test-projected-volume -- /bin/sh\n\t$ ls /projected-volume/\n\tuser\n\tpass\n\t$ cat /projected-volume/user\n\troot\n\t$ cat /projected-volume/pass\n\t1f2d1e2e67df\n从返回结果中，我们可以看到，保存在 Etcd 里的用户名和密码信息，已经以文件的形式出现在了容器的 Volume 目录里。而这个文件的名字，就是 kubectl create secret 指定的 Key，或者说是 Secret 对象的 data 字段指定的 Key。\n\n更重要的是，像这样通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。\n\n**需要注意的是，这个更新可能会有一定的延时。所以在编写应用程序时，在发起数据库连接的代码处写好重试和超时的逻辑，绝对是个好习惯。**\n\n与 Secret 类似的是 ConfigMap，它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。\n\n比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里：\n\n\t# .properties 文件的内容\n\t$ cat example/ui.properties\n\tcolor.good=purple\n\tcolor.bad=yellow\n\tallow.textmode=true\n\thow.nice.to.look=fairlyNice\n\t \n\t# 从.properties 文件创建 ConfigMap\n\t$ kubectl create configmap ui-config --from-file=example/ui.properties\n\t \n\t# 查看这个 ConfigMap 里保存的信息 (data)\n\t$ kubectl get configmaps ui-config -o yaml\n\tapiVersion: v1\n\tdata:\n\t  ui.properties: |\n\t    color.good=purple\n\t    color.bad=yellow\n\t    allow.textmode=true\n\t    how.nice.to.look=fairlyNice\n\tkind: ConfigMap\n\tmetadata:\n\t  name: ui-config\n\t  ...\n> 备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。\n\n接下来是 Downward API，它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。\n**(有两种方式可以将 Pod 和 Container 字段呈现给运行中的容器： 1、Environment variables 2、Volume Files 这两种呈现 Pod 和 Container 字段的方式都称为 Downward API。)**\n\n举个例子：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: test-downwardapi-volume\n\t  labels:\n\t    zone: us-est-coast\n\t    cluster: test-cluster1\n\t    rack: rack-22\n\tspec:\n\t  containers:\n\t    - name: client-container\n\t      image: k8s.gcr.io/busybox\n\t      command: [\"sh\", \"-c\"]\n\t      args:\n\t      - while true; do\n\t          if [[ -e /etc/podinfo/labels ]]; then\n\t            echo -en '\\n\\n'; cat /etc/podinfo/labels; fi;\n\t          sleep 5;\n\t        done;\n\t      volumeMounts:\n\t        - name: podinfo\n\t          mountPath: /etc/podinfo\n\t          readOnly: false\n\t  volumes:\n\t    - name: podinfo\n\t      projected:\n\t        sources:\n\t        - downwardAPI:\n\t            items:\n\t              - path: \"labels\"\n\t                fieldRef:\n\t                  fieldPath: metadata.labels\n在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。\n\n通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。\n\n而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示：\n\n\t$ kubectl create -f dapi-volume.yaml\n\t$ kubectl logs test-downwardapi-volume\n\tcluster=\"test-cluster1\"\n\track=\"rack-22\"\n\tzone=\"us-est-coast\"\n目前，Downward API 支持的字段已经非常丰富了，比如：\n\n\t1. 使用 fieldRef 可以声明使用:\n\tspec.nodeName - 宿主机名字\n\tstatus.hostIP - 宿主机 IP\n\tmetadata.name - Pod 的名字\n\tmetadata.namespace - Pod 的 Namespace\n\tstatus.podIP - Pod 的 IP\n\tspec.serviceAccountName - Pod 的 Service Account 的名字\n\tmetadata.uid - Pod 的 UID\n\tmetadata.labels['<KEY>'] - 指定 <KEY> 的 Label 值\n\tmetadata.annotations['<KEY>'] - 指定 <KEY> 的 Annotation 值\n\tmetadata.labels - Pod 的所有 Label\n\tmetadata.annotations - Pod 的所有 Annotation\n\t\n\t2. 使用 resourceFieldRef 可以声明使用:\n\t容器的 CPU limit\n\t容器的 CPU request\n\t容器的 memory limit\n\t容器的 memory request\n上面这个列表的内容，随着 Kubernetes 项目的发展肯定还会不断增加。所以这里列出来的信息仅供参考，你在使用 Downward API 时，还是要记得去查阅一下官方文档。\n\n不过，需要注意的是，**Downward API 能够获取到的信息，一定是 Pod 里的容器进程启动之前就能够确定下来的信息**。而如果你想要获取 Pod 容器运行后才会出现的信息，比如，容器进程的 PID，那就肯定不能使用 Downward API 了，而应该考虑在 Pod 里定义一个 sidecar 容器。\n\n其实，Secret、ConfigMap，以及 Downward API 这三种 Projected Volume 定义的信息，大多还可以通过环境变量的方式出现在容器里。但是，通过环境变量获取这些信息的方式，不具备自动更新的能力。所以，一般情况下，我都建议你使用 Volume 文件的方式获取这些信息。\n**(之所以使用Projected Volume来代替环境变量，是因为环境变量不具备自动更新的能力)**\n\n在明白了 Secret 之后，我再为你讲解 Pod 中一个与它密切相关的概念：**`Service Account`**.\n\n相信你一定有过这样的想法：我现在有了一个 Pod，我能不能在这个 Pod 里安装一个 Kubernetes 的 Client，这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了呢？\n\n这当然是可以的。\n\n不过，你首先要解决 API Server 的授权问题。\n\nService Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。比如，Service Account A，可以只被允许对 Kubernetes API 进行 GET 操作，而 Service Account B，则可以有 Kubernetes API 的所有操作的权限。\n\n像这样的 Service Account 的授权信息和文件，实际上保存在它所绑定的一个特殊的 Secret 对象里的。这个特殊的 Secret 对象，就叫作ServiceAccountToken。任何运行在 Kubernetes 集群上的应用，都必须使用这个 ServiceAccountToken 里保存的授权信息，也就是 Token，才可以合法地访问 API Server。\n**(通过serviceAccountToken可实现对API server访问权限管理，不同的service Account可设置不同的访问能力，如只开放GET的能力，开放Edit的能力，可根据需要进行自定义，部分资源默认已经配置了service Account，如Pod在volume中默认设置了此类型secret，它是一种secret.)**\n\n所以说，Kubernetes 项目的 Projected Volume 其实只有三种，因为第四种 ServiceAccountToken，只是一种特殊的 Secret 而已。\n**(1、Secret，特例ServiceAccountToken 2、ConfigMap 3、Downward API)**\n\n另外，为了方便使用，Kubernetes 已经为你提供了一个的默认“服务账户”（default Service Account）。并且，任何一个运行在 Kubernetes 里的 Pod，都可以直接使用这个默认的 Service Account，而无需显示地声明挂载它。\n\n这是如何做到的呢？\n\n当然还是靠 Projected Volume 机制。\n\n如果你查看一下任意一个运行在 Kubernetes 集群里的 Pod，就会发现，每一个 Pod，都已经自动声明一个类型是 Secret、名为 default-token-xxxx 的 Volume，然后 自动挂载在每个容器的一个固定目录上。比如：\n\n\t$ kubectl describe pod nginx-deployment-5c678cfb6d-lg9lw\n\tContainers:\n\t...\n\t  Mounts:\n\t    /var/run/secrets/kubernetes.io/serviceaccount from default-token-s8rbq (ro)\n\tVolumes:\n\t  default-token-s8rbq:\n\t  Type:       Secret (a volume populated by a Secret)\n\t  SecretName:  default-token-s8rbq\n\t  Optional:    false\n这个 Secret 类型的 Volume，正是默认 Service Account 对应的 ServiceAccountToken。所以说，Kubernetes 其实在每个 Pod 创建的时候，自动在它的 spec.volumes 部分添加上了默认 ServiceAccountToken 的定义，然后自动给每个容器加上了对应的 volumeMounts 字段。这个过程对于用户来说是完全透明的。\n\n这样，一旦 Pod 创建完成，容器里的应用就可以直接从这个默认 ServiceAccountToken 的挂载目录里访问到授权信息和文件。这个容器内的路径在 Kubernetes 里是固定的，即：/var/run/secrets/kubernetes.io/serviceaccount ，而这个 Secret 类型的 Volume 里面的内容如下所示：\n\n\t$ ls /var/run/secrets/kubernetes.io/serviceaccount \n\tca.crt namespace  token\nca.crt跟宿主机上/etc/kubernetes/pki/ca.crt内容一致.\n所以，你的应用程序只要直接加载这些授权文件，就可以访问并操作 Kubernetes API 了。而且，如果你使用的是 Kubernetes 官方的 Client 包（k8s.io/client-go）的话，它还可以自动加载这个目录下的文件，你不需要做任何配置或者编码操作。\n\n**这种把 Kubernetes 客户端以容器的方式运行在集群里，然后使用 default Service Account 自动授权的方式，被称作“InClusterConfig”，也是我最推荐的进行 Kubernetes API 编程的授权方式。**\n\n当然，考虑到自动挂载默认 ServiceAccountToken 的潜在风险，Kubernetes 允许你设置默认不为 Pod 里的容器自动挂载这个 Volume。\n\n除了这个默认的 Service Account 外，我们很多时候还需要创建一些我们自己定义的 Service Account，来对应不同的权限设置。这样，我们的 Pod 里的容器就可以通过挂载这些 Service Account 对应的 ServiceAccountToken，来使用这些自定义的授权信息。在后面讲解为 Kubernetes 开发插件的时候，我们将会实践到这个操作。\n\n接下来，我们再来看 Pod 的另一个重要的配置：**`容器健康检查和恢复机制。`**\n\n在 Kubernetes 中，你可以为 Pod 里的容器定义一个健康检查“探针”（Probe）。这样，kubelet 就会根据这个 Probe 的返回值决定这个容器的状态，而不是直接以容器进行是否运行（来自 Docker 返回的信息）作为依据。这种机制，是生产环境中保证应用健康存活的重要手段。\n\n我们一起来看一个 Kubernetes 文档中的例子。\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  labels:\n\t    test: liveness\n\t  name: test-liveness-exec\n\tspec:\n\t  containers:\n\t  - name: liveness\n\t    image: busybox\n\t    args:\n\t    - /bin/sh\n\t    - -c\n\t    - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600\n\t    livenessProbe:\n\t      exec:\n\t        command:\n\t        - cat\n\t        - /tmp/healthy\n\t      initialDelaySeconds: 5\n\t      periodSeconds: 5\n在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。\n\n与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一句我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）。\n\n现在，让我们来具体实践一下这个过程。\n\n首先，创建这个 Pod：\n\n\t$ kubectl create -f test-liveness-exec.yaml\n然后，查看这个 Pod 的状态：\n\n\t$ kubectl get pod\n\tNAME                READY     STATUS    RESTARTS   AGE\n\ttest-liveness-exec   1/1       Running   0          10s\n可以看到，由于已经通过了健康检查，这个 Pod 就进入了 Running 状态。\n\n而 30 s 之后，我们再查看一下 Pod 的 Events：\n\n\t$ kubectl describe pod test-liveness-exec\n你会发现，这个 Pod 在 Events 报告了一个异常：\n\n\tFirstSeen LastSeen    Count   From            SubobjectPath           Type        Reason      Message\n\t--------- --------    -----   ----            -------------           --------    ------      -------\n\t2s        2s      1   {kubelet worker0}   spec.containers{liveness}   Warning     Unhealthy   Liveness probe failed: cat: can't open '/tmp/healthy': No such file or directory\n显然，这个健康检查探查到 /tmp/healthy 已经不存在了，所以它报告容器是不健康的。那么接下来会发生什么呢？\n\n我们不妨再次查看一下这个 Pod 的状态：\n\n\t$ kubectl get pod test-liveness-exec\n\tNAME           READY     STATUS    RESTARTS   AGE\n\tliveness-exec   1/1       Running   1          1m\n这时我们发现，Pod 并没有进入 Failed 状态，而是保持了 Running 状态。这是为什么呢？\n\n其实，如果你注意到 RESTARTS 字段从 0 到 1 的变化，就明白原因了：这个异常的容器已经被 Kubernetes 重启了。在这个过程中，Pod 保持 Running 状态不变。\n\n需要注意的是：Kubernetes 中并没有 Docker 的 Stop 语义。所以虽然是 Restart（重启），但实际却是重新创建了容器。\n\n这个功能就是 Kubernetes 里的Pod 恢复机制，也叫 restartPolicy。它是 Pod 的 Spec 部分的一个标准字段（pod.spec.restartPolicy），默认值是 Always，即：任何时候这个容器发生了异常，它一定会被重新创建。\n\n但一定要强调的是，Pod 的恢复过程，永远都是发生在当前节点上，而不会跑到别的节点上去。事实上，一旦一个 Pod 与一个节点（Node）绑定，除非这个绑定发生了变化（pod.spec.node 字段被修改），否则它永远都不会离开这个节点。这也就意味着，如果这个宿主机宕机了，这个 Pod 也不会主动迁移到其他节点上去。\n\n而如果你想让 Pod 出现在其他的可用节点上，就必须使用 Deployment 这样的“控制器”来管理 Pod，哪怕你只需要一个 Pod 副本。这就是我在第 12 篇文章《牛刀小试：我的第一个容器化应用》最后给你留的思考题的答案，即一个单 Pod 的 Deployment 与一个 Pod 最主要的区别。\n\n而作为用户，你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况：\n\n * Always：在任何情况下，只要容器不在运行状态，就自动重启容器；\n * OnFailure: 只在容器 异常时才自动重启容器；\n * Never: 从来不重启容器。\n\n在实际使用时，我们需要根据应用运行的特性，合理设置这三种恢复策略。\n\n比如，一个 Pod，它只计算 1+1=2，计算完成输出结果后退出，变成 Succeeded 状态。这时，你如果再用 restartPolicy=Always 强制重启这个 Pod 的容器，就没有任何意义了。\n\n而如果你要关心这个容器退出后的上下文环境，比如容器退出后的日志、文件和目录，就需要将 restartPolicy 设置为 Never。因为一旦容器被自动重新创建，这些内容就有可能丢失掉了（被垃圾回收了）。\n\n值得一提的是，Kubernetes 的官方文档，把 restartPolicy 和 Pod 里容器的状态，以及 Pod 状态的对应关系，[总结了非常复杂的一大堆情况](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states)。实际上，你根本不需要死记硬背这些对应关系，只要记住如下两个基本的设计原理即可：\n\n1. **只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。**\n\n2. **对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。在此之前，Pod 都是 Running 状态。此时，Pod 的 READY 字段会显示正常容器的个数，**比如：\n\n\n\t$ kubectl get pod test-liveness-exec\n\tNAME           READY     STATUS    RESTARTS   AGE\n\tliveness-exec   0/1       Running   1          1m\n所以，假如一个 Pod 里只有一个容器，然后这个容器异常退出了。那么，只有当 restartPolicy=Never 时，这个 Pod 才会进入 Failed 状态。而其他情况下，由于 Kubernetes 都可以重启这个容器，所以 Pod 的状态保持 Running 不变。\n\n而如果这个 Pod 有多个容器，仅有一个容器异常退出，它就始终保持 Running 状态，哪怕即使 restartPolicy=Never。只有当所有容器也异常退出之后，这个 Pod 才会进入 Failed 状态。\n\n其他情况，都可以以此类推出来。\n\n现在，我们一起回到前面提到的 livenessProbe 上来。\n\n除了在容器中执行命令外，livenessProbe 也可以定义为发起 HTTP 或者 TCP 请求的方式，定义格式如下：\n\n\t...\n\tlivenessProbe:\n\t     httpGet:\n\t       path: /healthz\n\t       port: 8080\n\t       httpHeaders:\n\t       - name: X-Custom-Header\n\t         value: Awesome\n\t       initialDelaySeconds: 3\n\t       periodSeconds: 3\n\n\n\t    ...\n\t    livenessProbe:\n\t      tcpSocket:\n\t        port: 8080\n\t      initialDelaySeconds: 15\n\t      periodSeconds: 20\n所以，你的 Pod 其实可以暴露一个健康检查 URL（比如 /healthz），或者直接让健康检查去检测应用的监听端口。这两种配置方法，在 Web 服务类的应用中非常常用。\n\n在 Kubernetes 的 Pod 中，还有一个叫 readinessProbe 的字段。虽然它的用法与 livenessProbe 类似，但作用却大不一样。readinessProbe 检查结果的成功与否，决定的这个 Pod 是不是能被通过 Service 的方式访问到，而并不影响 Pod 的生命周期。这部分内容，我会留在讲解 Service 时再重点介绍。\n\n在讲解了这么多字段之后，想必你对 Pod 对象的语义和描述能力，已经有了一个初步的感觉。\n\n这时，你有没有产生这样一个想法：Pod 的字段这么多，我又不可能全记住，`Kubernetes 能不能自动给 Pod 填充某些字段呢？`\n\n这个需求实际上非常实用。比如，开发人员只需要提交一个基本的、非常简单的 Pod YAML，Kubernetes 就可以自动给对应的 Pod 对象加上其他必要的信息，比如 labels，annotations，volumes 等等。而这些信息，可以是运维人员事先定义好的。\n\n这么一来，开发人员编写 Pod YAML 的门槛，就被大大降低了。\n\n所以，这个叫作 PodPreset（Pod 预设置）的功能 已经出现在了 v1.11 版本的 Kubernetes 中。\n\n举个例子，现在开发人员编写了如下一个 pod.yaml 文件：\n\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: website\n\t  labels:\n\t    app: website\n\t    role: frontend\n\tspec:\n\t  containers:\n\t    - name: website\n\t      image: nginx\n\t      ports:\n\t        - containerPort: 80\n作为 Kubernetes 的初学者，你肯定眼前一亮：这不就是我最擅长编写的、最简单的 Pod 嘛。没错，这个 YAML 文件里的字段，想必你现在闭着眼睛也能写出来。\n\n可是，如果运维人员看到了这个 Pod，他一定会连连摇头：这种 Pod 在生产环境里根本不能用啊！\n\n所以，这个时候，运维人员就可以定义一个 PodPreset 对象。在这个对象中，凡是他想在开发人员编写的 Pod 里追加的字段，都可以预先定义好。比如这个 preset.yaml：\n\n\tapiVersion: settings.k8s.io/v1alpha1\n\tkind: PodPreset\n\tmetadata:\n\t  name: allow-database\n\tspec:\n\t  selector:\n\t    matchLabels:\n\t      role: frontend\n\t  env:\n\t    - name: DB_PORT\n\t      value: \"6379\"\n\t  volumeMounts:\n\t    - mountPath: /cache\n\t      name: cache-volume\n\t  volumes:\n\t    - name: cache-volume\n\t      emptyDir: {}\n在这个 PodPreset 的定义中，首先是一个 selector。这就意味着后面这些追加的定义，只会作用于 selector 所定义的、带有“role: frontend”标签的 Pod 对象，这就可以防止“误伤”。\n\n然后，我们定义了一组 Pod 的 Spec 里的标准字段，以及对应的值。比如，env 里定义了 DB_PORT 这个环境变量，volumeMounts 定义了容器 Volume 的挂载目录，volumes 定义了一个 emptyDir 的 Volume。\n\n接下来，我们假定运维人员先创建了这个 PodPreset，然后开发人员才创建 Pod：\n\n\t$ kubectl create -f preset.yaml\n\t$ kubectl create -f pod.yaml\n这时，Pod 运行起来之后，我们查看一下这个 Pod 的 API 对象：\n\n\t$ kubectl get pod website -o yaml\n\tapiVersion: v1\n\tkind: Pod\n\tmetadata:\n\t  name: website\n\t  labels:\n\t    app: website\n\t    role: frontend\n\t  annotations:\n\t    podpreset.admission.kubernetes.io/podpreset-allow-database: \"resource version\"\n\tspec:\n\t  containers:\n\t    - name: website\n\t      image: nginx\n\t      volumeMounts:\n\t        - mountPath: /cache\n\t          name: cache-volume\n\t      ports:\n\t        - containerPort: 80\n\t      env:\n\t        - name: DB_PORT\n\t          value: \"6379\"\n\t  volumes:\n\t    - name: cache-volume\n\t      emptyDir: {}\n这个时候，我们就可以清楚地看到，这个 Pod 里多了新添加的 labels、env、volumes 和 volumeMount 的定义，它们的配置跟 PodPreset 的内容一样。此外，这个 Pod 还被自动加上了一个 annotation 表示这个 Pod 对象被 PodPreset 改动过。\n\n需要说明的是，PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。\n\n比如，我们现在提交的是一个 nginx-deployment，那么这个 Deployment 对象本身是永远不会被 PodPreset 改变的，被修改的只是这个 Deployment 创建出来的所有 Pod。这一点请务必区分清楚。\n\n这里有一个问题：如果你定义了同时作用于一个 Pod 对象的多个 PodPreset，会发生什么呢？\n\n实际上，Kubernetes 项目会帮你合并（Merge）这两个 PodPreset 要做的修改。而如果它们要做的修改有冲突的话，这些冲突字段就不会被修改。\n\n**总结**\n在今天这篇文章中，我和你详细介绍了 Pod 对象更高阶的使用方法，希望通过对这些实例的讲解，你可以更深入地理解 Pod API 对象的各个字段。\n\n而在学习这些字段的同时，你还应该认真体会一下 Kubernetes“一切皆对象”的设计思想：比如应用是 Pod 对象，应用的配置是 ConfigMap 对象，应用要访问的密码则是 Secret 对象。\n\n所以，也就自然而然地有了 PodPreset 这样专门用来对 Pod 进行批量化、自动化修改的工具对象。在后面的内容中，我会为你讲解更多的这种对象，还会和你介绍 Kubernetes 项目如何围绕着这些对象进行容器编排。\n\n在本专栏中，Pod 对象相关的知识点非常重要，它是接下来 Kubernetes 能够描述和编排各种复杂应用的基石所在，希望你能够继续多实践、多体会。\n\n**思考题**\n在没有 Kubernetes 的时候，你是通过什么方法进行应用的健康检查的？Kubernetes 的 livenessProbe 和 readinessProbe 提供的几种探测机制，是否能满足你的需求？\n\n","categories":["reference","k8s"]},{"title":"08 | 白话容器基础（四）：重新认识Docker容器","url":"/2021/03/13/reference/k8s/08.白话容器基础(四)重新认识Docker容器/","content":"\n![](1.jpg)\n\n在前面的三次分享中，我分别从 Linux Namespace 的隔离能力、Linux Cgroups 的限制能力，以及基于 rootfs 的文件系统三个角度，为你剖析了一个 Linux 容器的核心实现原理。\n\n> 备注：之所以要强调 Linux 容器，是因为比如 Docker on Mac，以及 Windows Docker（Hyper-V 实现），实际上是基于虚拟化技术实现的，跟我们这个专栏着重介绍的 Linux 容器完全不同。\n\n而在今天的分享中，我会通过一个实际案例，对“白话容器基础”系列的所有内容做一次深入的总结和扩展。希望通过这次的讲解，能够让你更透彻地理解 Docker 容器的本质。\n\n在开始实践之前，你需要准备一台 Linux 机器，并安装 Docker。这个流程我就不再赘述了。\n\n这一次，我要用 Docker 部署一个用 Python 编写的 Web 应用。这个应用的代码部分（app.py）非常简单：\n\n\tfrom flask import Flask\n\timport socket\n\timport os\n\t \n\tapp = Flask(__name__)\n\t \n\t@app.route('/')\n\tdef hello():\n\t    html = \"<h3>Hello {name}!</h3>\" \\\n\t           \"<b>Hostname:</b> {hostname}<br/>\"           \n\t    return html.format(name=os.getenv(\"NAME\", \"world\"), hostname=socket.gethostname())\n\t    \n\tif __name__ == \"__main__\":\n\t    app.run(host='0.0.0.0', port=80)\n在这段代码中，我使用 Flask 框架启动了一个 Web 服务器，而它唯一的功能是：如果当前环境中有“NAME”这个环境变量，就把它打印在“Hello”后，否则就打印“Hello world”，最后再打印出当前环境的 hostname。\n\n这个应用的依赖，则被定义在了同目录下的 requirements.txt 文件里，内容如下所示：\n\n\t$ cat requirements.txt\n\tFlask\n而将这样一个应用容器化的第一步，是制作容器镜像。\n\n不过，相较于我之前介绍的制作 rootfs 的过程，Docker 为你提供了一种更便捷的方式，叫作 Dockerfile，如下所示。\n\n\t# 使用官方提供的 Python 开发镜像作为基础镜像\n\tFROM python:2.7-slim\n\t\n\t# 将工作目录切换为 /app\n\tWORKDIR /app\n\t\n\t# 将当前目录下的所有内容复制到 /app 下\n\tADD . /app\n\t\n\t# 使用 pip 命令安装这个应用所需要的依赖\n\tRUN pip install --trusted-host pypi.python.org -r requirements.txt\n\t\n\t# 允许外界访问容器的 80 端口\n\tEXPOSE 80\n\t\n\t# 设置环境变量\n\tENV NAME World\n\t\n\t# 设置容器进程为：python app.py，即：这个 Python 应用的启动命令\n\tCMD [\"python\", \"app.py\"]\n通过这个文件的内容，你可以看到**`Dockerfile 的设计思想，是使用一些标准的原语（即大写高亮的词语），描述我们所要构建的 Docker 镜像。并且这些原语，都是按顺序处理的。`**\n\n比如 FROM 原语，指定了“python:2.7-slim”这个官方维护的基础镜像，从而免去了安装 Python 等语言环境的操作。否则，这一段我们就得这么写了：\n\n\tFROM ubuntu:latest\n\tRUN apt-get update -yRUN apt-get install -y python-pip python-dev build-essential\n\t...\n其中，RUN 原语就是在容器里执行 shell 命令的意思。\n\n而 WORKDIR，意思是在这一句之后，Dockerfile 后面的操作都以这一句指定的 /app 目录作为当前目录。\n\n所以，到了最后的 CMD，意思是 Dockerfile 指定 python app.py 为这个容器的进程。这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”] 等价于 \"docker runpython app.py\"。\n\n另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。\n\n但是，默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。\n\n备注：基于以上原因，**`我们后面会统一称 Docker 容器的启动进程为 ENTRYPOINT，而不是 CMD。`**\n\n需要注意的是，Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。\n\n读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中：\n\n\t$ ls\n\tDockerfile  app.py   requirements.txt\n接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行：\n\n\t$ docker build -t helloworld .\n其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。而这个过程，实际上可以等同于 Docker 使用基础镜像启动了一个容器，然后在容器中依次执行 Dockerfile 中的原语。\n\n**`需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。`** 即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。\n\ndocker build 操作完成后，我可以通过 docker images 命令查看结果：\n\n\t$ docker image ls\n\t\n\tREPOSITORY            TAG                 IMAGE ID\n\thelloworld         latest              653287cdf998\n通过这个镜像 ID，你就可以使用在《白话容器基础（三）：深入理解容器镜像》中讲过的方法，查看这些新增的层在 AuFS 路径下对应的文件和目录了。\n\n接下来，我使用这个镜像，通过 docker run 命令启动容器：\n\n\t$ docker run -p 4000:80 helloworld\n在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。否则，我就得把进程的启动命令加在后面：\n\n\t$ docker run -p 4000:80 helloworld python app.py\n容器启动之后，我可以使用 docker ps 命令看到：\n\n\t$ docker ps\n\tCONTAINER ID        IMAGE               COMMAND             CREATED\n\t4ddf4638572d        helloworld       \"python app.py\"     10 seconds ago\n同时，我已经通过 -p 4000:80 告诉了 Docker，**`请把容器内的 80 端口映射在宿主机的 4000 端口上。`**\n\n这样做的目的是，只要访问宿主机的 4000 端口，我就可以看到容器里应用返回的结果：\n\n\t$ curl http://localhost:4000\n\t<h3>Hello World!</h3><b>Hostname:</b> 4ddf4638572d<br/>\n否则，我就得先用 docker inspect 命令查看容器的 IP 地址，然后访问“http://< 容器 IP 地址 >:80”才可以看到容器内应用的返回。\n\n至此，我已经使用容器完成了一个应用的开发与测试，如果现在想要把这个容器的镜像上传到 DockerHub 上分享给更多的人，我要怎么做呢？\n\n为了能够上传镜像，我首先需要注册一个 Docker Hub 账号，然后使用 docker login 命令登录。\n\n接下来，我要用 docker tag 命令给容器镜像起一个完整的名字：\n\n\t$ docker tag helloworld geektime/helloworld:v1\n注意：你自己做实验时，请将 \"geektime\" 替换成你自己的 Docker Hub 账户名称，比如 zhangsan/helloworld:v1\n\n其中，geektime 是我在 Docker Hub 上的用户名，它的“学名”叫镜像仓库（Repository）；“/”后面的 helloworld 是这个镜像的名字，而“v1”则是我给这个镜像分配的版本号。\n\n然后，我执行 docker push：\n\n\t$ docker push geektime/helloworld:v1\n这样，我就可以把这个镜像上传到 Docker Hub 上了。\n\n此外，我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。一般来说，需要这么操作原因是：这个容器运行起来后，我又在里面做了一些操作，并且要把操作结果保存到镜像里，比如：\n\n\t$ docker exec -it 4ddf4638572d /bin/sh\n\t# 在容器内部新建了一个文件\n\troot@4ddf4638572d:/app# touch test.txt\n\troot@4ddf4638572d:/app# exit\n\t\n\t# 将这个新建的文件提交到镜像中保存\n\t$ docker commit 4ddf4638572d geektime/helloworld:v2\n这里，我使用了 docker exec 命令进入到了容器当中。在了解了 Linux Namespace 的隔离机制后，你应该会很自然地想到一个问题：docker exec 是怎么做到进入容器里的呢？\n\n实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。\n\n比如，通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686：\n\n\t$ docker inspect --format '{{ .State.Pid }}'  4ddf4638572d\n\t25686\n这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件：\n\n\t$ ls -l  /proc/25686/ns\n\ttotal 0\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 cgroup -> cgroup:[4026531835]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 ipc -> ipc:[4026532278]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 mnt -> mnt:[4026532276]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 net -> net:[4026532281]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 pid -> pid:[4026532279]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 pid_for_children -> pid:[4026532279]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 user -> user:[4026531837]\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 uts -> uts:[4026532277]\n可以看到，一个进程的每种 Linux Namespace，都在它对应的 /proc/[进程号]/ns 下有一个对应的虚拟文件，并且链接到一个真实的 Namespace 文件上。\n\n有了这样一个可以“hold 住”所有 Linux Namespace 的文件，我们就可以对 Namespace 做一些很有意义事情了，比如：加入到一个已经存在的 Namespace 当中。\n\n**`这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。`**\n\n而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。它的调用方法，我可以用如下一段小程序为你说明：\n\n\t#define _GNU_SOURCE\n\t#include <fcntl.h>\n\t#include <sched.h>\n\t#include <unistd.h>\n\t#include <stdlib.h>\n\t#include <stdio.h>\n\t\n\t#define errExit(msg) do { perror(msg); exit(EXIT_FAILURE);} while (0)\n\t\n\tint main(int argc, char *argv[]) {\n\t    int fd;\n\t    \n\t    fd = open(argv[1], O_RDONLY);\n\t    if (setns(fd, 0) == -1) {\n\t        errExit(\"setns\");\n\t    }\n\t    execvp(argv[2], &argv[2]); \n\t    errExit(\"execvp\");\n\t}\n这段代码功能非常简单：它一共接收两个参数，第一个参数是 argv[1]，即当前进程要加入的 Namespace 文件的路径，比如 /proc/25686/ns/net；而第二个参数，则是你要在这个 Namespace 里运行的进程，比如 /bin/bash。\n\n这段代码的的核心操作，则是通过 open() 系统调用打开了指定的 Namespace 文件，并把这个文件的描述符 fd 交给 setns() 使用。在 setns() 执行后，当前进程就加入了这个文件对应的 Linux Namespace 当中了。\n\n现在，你可以编译执行一下这个程序，加入到容器进程（PID=25686）的 Network Namespace 中：\n\n\t$ gcc -o set_ns set_ns.c \n\t$ ./set_ns /proc/25686/ns/net /bin/bash \n\t$ ifconfig\n\teth0      Link encap:Ethernet  HWaddr 02:42:ac:11:00:02  \n\t          inet addr:172.17.0.2  Bcast:0.0.0.0  Mask:255.255.0.0\n\t          inet6 addr: fe80::42:acff:fe11:2/64 Scope:Link\n\t          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\n\t          RX packets:12 errors:0 dropped:0 overruns:0 frame:0\n\t          TX packets:10 errors:0 dropped:0 overruns:0 carrier:0\n\t\t   collisions:0 txqueuelen:0 \n\t          RX bytes:976 (976.0 B)  TX bytes:796 (796.0 B)\n\t \n\tlo        Link encap:Local Loopback  \n\t          inet addr:127.0.0.1  Mask:255.0.0.0\n\t          inet6 addr: ::1/128 Scope:Host\n\t          UP LOOPBACK RUNNING  MTU:65536  Metric:1\n\t          RX packets:0 errors:0 dropped:0 overruns:0 frame:0\n\t          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0\n\t\t  collisions:0 txqueuelen:1000 \n\t          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)\n正如上所示，当我们执行 ifconfig 命令查看网络设备时，我会发现能看到的网卡“变少”了：只有两个。而我的宿主机则至少有四个网卡。这是怎么回事呢？\n\n实际上，在 setns() 之后我看到的这两个网卡，正是我在前面启动的 Docker 容器里的网卡。也就是说，我新创建的这个 /bin/bash 进程，由于加入了该容器进程（PID=25686）的 Network Namepace，它看到的网络设备与这个容器里是一样的，即：/bin/bash 进程的网络设备视图，也被修改了。\n\n而一旦一个进程加入到了另一个 Namespace 当中，在宿主机的 Namespace 文件上，也会有所体现。\n\n在宿主机上，你可以用 ps 指令找到这个 set_ns 程序执行的 /bin/bash 进程，其真实的 PID 是 28499：\n\n\t# 在宿主机上\n\tps aux | grep /bin/bash\n\troot     28499  0.0  0.0 19944  3612 pts/0    S    14:15   0:00 /bin/bash\n这时，如果按照前面介绍过的方法，查看一下这个 PID=28499 的进程的 Namespace，你就会发现这样一个事实：\n\n\t$ ls -l /proc/28499/ns/net\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:18 /proc/28499/ns/net -> net:[4026532281]\n\t\n\t$ ls -l  /proc/25686/ns/net\n\tlrwxrwxrwx 1 root root 0 Aug 13 14:05 /proc/25686/ns/net -> net:[4026532281]\n在 /proc/[PID]/ns/net 目录下，这个 PID=28499 进程，与我们前面的 Docker 容器进程（PID=25686）指向的 Network Namespace 文件完全一样。这说明这两个进程，共享了这个名叫 net:[4026532281] 的 Network Namespace。\n\n此外，Docker 还专门提供了一个参数，可以让你启动一个容器并“加入”到另一个容器的 Network Namespace 里，这个参数就是 -net，比如:\n\n\t$ docker run -it --net container:4ddf4638572d busybox ifconfig\n这样，我们新启动的这个容器，就会直接加入到 ID=4ddf4638572d 的容器，也就是我们前面的创建的 Python 应用容器（PID=25686）的 Network Namespace 中。所以，这里 ifconfig 返回的网卡信息，跟我前面那个小程序返回的结果一模一样，你也可以尝试一下。\n\n而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。这就意味着，这个容器拆除了 Network Namespace 的“隔离墙”，所以，它会和宿主机上的其他普通进程一样，直接共享宿主机的网络栈。这就为容器直接操作和使用宿主机网络提供了一个渠道。\n\n转了一个大圈子，我其实是为你详细解读了 docker exec 这个操作背后，Linux Namespace 更具体的工作原理。\n\n这种通过操作系统进程相关的知识，逐步剖析 Docker 容器的方法，是理解容器的一个关键思路，希望你一定要掌握。\n\n现在，我们再一起回到前面提交镜像的操作 docker commit 上来吧。\n\ndocker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。\n\n而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。\n\n而正如前所说，Init 层的存在，就是为了避免你执行 docker commit 时，把 Docker 自己对 /etc/hosts 等文件做的修改，也一起提交掉。\n\n有了新的镜像，我们就可以把它推送到 Docker Hub 上了：\n\n\t$ docker push geektime/helloworld:v2\n你可能还会有这样的问题：我在企业内部，能不能也搭建一个跟 Docker Hub 类似的镜像上传系统呢？\n\n当然可以，这个统一存放镜像的系统，就叫作 Docker Registry。感兴趣的话，你可以查看Docker 的官方文档，以及VMware 的 Harbor 项目。\n\n最后，我再来讲解一下 Docker 项目另一个重要的内容：Volume（数据卷）。\n\n前面我已经介绍过，容器技术使用了 rootfs 机制和 Mount Namespace，构建出了一个同宿主机完全隔离开的文件系统环境。这时候，我们就需要考虑这样两个问题：\n\n1. 容器里进程新建的文件，怎么才能让宿主机获取到？\n\n2. 宿主机上的文件和目录，怎么才能让容器里的进程访问到？\n\n这正是 Docker Volume 要解决的问题：**`Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。`**\n\n在 Docker 项目里，它支持两种 Volume 声明方式，可以把宿主机目录挂载进容器的 /test 目录当中：\n\n\t$ docker run -v /test ...\n\t$ docker run -v /home:/test ...\n而这两种声明方式的本质，实际上是相同的：都是把一个宿主机的目录挂载进了容器的 /test 目录。\n\n只不过，在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。\n\n那么，Docker 又是如何做到把一个宿主机上的目录或者文件，挂载到容器里面去呢？难道又是 Mount Namespace 的黑科技吗？\n\n实际上，并不需要这么麻烦。\n\n在《白话容器基础（三）：深入理解容器镜像》的分享中，我已经介绍过，当容器进程被创建之后，尽管开启了 Mount Namespace，但是在它执行 chroot（或者 pivot_root）之前，容器进程一直可以看到宿主机上的整个文件系统。\n\n而宿主机上的文件系统，也自然包括了我们要使用的容器镜像。这个镜像的各个层，保存在 /var/lib/docker/aufs/diff 目录下，在容器进程启动后，它们会被联合挂载在 /var/lib/docker/aufs/mnt/ 目录中，这样容器所需的 rootfs 就准备好了。\n\n所以，我们只需要在 rootfs 准备好之后，在执行 chroot 之前，把 Volume 指定的宿主机目录（比如 /home 目录），挂载到指定的容器目录（比如 /test 目录）在宿主机上对应的目录（即 /var/lib/docker/aufs/mnt/[可读写层 ID]/test）上，这个 Volume 的挂载工作就完成了。\n\n更重要的是，由于执行这个挂载操作时，“容器进程”已经创建了，也就意味着此时 Mount Namespace 已经开启了。所以，这个挂载事件只在这个容器里可见。你在宿主机上，是看不见容器内部的这个挂载点的。这就保证了容器的隔离性不会被 Volume 打破。\n\n> 注意：这里提到的 \" 容器进程 \"，是 Docker 创建的一个容器初始化进程 (dockerinit)，而不是应用进程 (ENTRYPOINT + CMD)。dockerinit 会负责完成根目录的准备、挂载设备和目录、配置 hostname 等一系列需要在容器内进行的初始化操作。最后，它通过 execv() 系统调用，让应用进程取代自己，成为容器里的 PID=1 的进程。\n\n而这里要使用到的挂载技术，就是 **`Linux 的绑定挂载（bind mount）机制。`** 它的主要作用就是，允许你将一个目录或者文件，而不是整个设备，挂载到一个指定的目录上。并且，这时你在该挂载点上进行的任何操作，只是发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来且不受影响。\n\n其实，如果你了解 Linux 内核的话，就会明白，绑定挂载实际上是一个 inode 替换的过程。在 Linux 操作系统中，inode 可以理解为存放文件内容的“对象”，而 dentry，也叫目录项，就是访问这个 inode 所使用的“指针”。\n**`(dentry结构最重要的作用就是指向这个文件对应的 inode。)`**\n\n正如上图所示，**`mount --bind /home /test，会将 /home 挂载到 /test 上。其实相当于将 /test 的 dentry，重定向到了 /home 的 inode。这样当我们修改 /test 目录时，实际修改的是 /home 目录的 inode。这也就是为何，一旦执行 umount 命令，/test 目录原先的内容就会恢复：因为修改真正发生在的，是 /home 目录里。挂载时机，挂载到读写层，把/home 挂载到 /var/lib/docker/aufs/mnt/[可读写层 ID]/test.`**\n\n所以，在一个正确的时机，进行一次绑定挂载，Docker 就可以成功地将一个宿主机上的目录或文件，不动声色地挂载到容器中。\n\n这样，进程在容器里对这个 /test 目录进行的所有操作，都实际发生在宿主机的对应目录（比如，/home，或者 /var/lib/docker/volumes/[VOLUME_ID]/_data）里，而不会影响容器镜像的内容。\n\n那么，这个 /test 目录里的内容，既然挂载在容器 rootfs 的可读写层，它会不会被 docker commit 提交掉呢？\n\n也不会。\n\n这个原因其实我们前面已经提到过。容器的镜像操作，比如 docker commit，都是发生在宿主机空间的。而由于 Mount Namespace 的隔离作用，宿主机并不知道这个绑定挂载的存在。所以，在宿主机看来，容器中可读写层的 /test 目录（/var/lib/docker/aufs/mnt/[可读写层 ID]/test），始终是空的。\n\n不过，由于 Docker 一开始还是要创建 /test 这个目录作为挂载点，所以执行了 docker commit 之后，你会发现新产生的镜像里，会多出来一个空的 /test 目录。毕竟，新建目录操作，又不是挂载操作，Mount Namespace 对它可起不到“障眼法”的作用。\n\n结合以上的讲解，我们现在来亲自验证一下：\n\n首先，启动一个 helloworld 容器，给它声明一个 Volume，挂载在容器里的 /test 目录上：\n\n\t$ docker run -d -v /test helloworld\n\tcf53b766fa6f\n容器启动之后，我们来查看一下这个 Volume 的 ID：\n\n\t$ docker volume ls\n\tDRIVER              VOLUME NAME\n\tlocal               cb1c2f7221fa9b0971cc35f68aa1034824755ac44a034c0c0a1dd318838d3a6d\n然后，使用这个 ID，可以找到它在 Docker 工作目录下的 volumes 路径：\n\n\t$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/\n这个 _data 文件夹，就是这个容器的 Volume 在宿主机上对应的临时目录了。\n\n接下来，我们在容器的 Volume 里，添加一个文件 text.txt：\n\n\t$ docker exec -it cf53b766fa6f /bin/sh\n\tcd test/\n\ttouch text.txt\n这时，我们再回到宿主机，就会发现 text.txt 已经出现在了宿主机上对应的临时目录里：\n\n\t$ ls /var/lib/docker/volumes/cb1c2f7221fa/_data/\n\ttext.txt\n可是，如果你在宿主机上查看该容器的可读写层，虽然可以看到这个 /test 目录，但其内容是空的（关于如何找到这个 AuFS 文件系统的路径，请参考我上一次分享的内容）：\n\n\t$ ls /var/lib/docker/aufs/mnt/6780d0778b8a/test\n可以确认，容器 Volume 里的信息，并不会被 docker commit 提交掉；但这个挂载点目录 /test 本身，则会出现在新的镜像当中。\n\n以上内容，就是 Docker Volume 的核心原理了。\n\n**总结**\n在今天的这次分享中，我用了一个非常经典的 Python 应用作为案例，讲解了 Docke 容器使用的主要场景。熟悉了这些操作，你也就基本上摸清了 Docker 容器的核心功能。\n\n更重要的是，我着重介绍了如何使用 Linux Namespace、Cgroups，以及 rootfs 的知识，对容器进行了一次庖丁解牛似的解读。\n\n借助这种思考问题的方法，最后的 Docker 容器，我们实际上就可以用下面这个“全景图”描述出来：\n![](2.jpg)\n\n\n这个容器进程“python app.py”，运行在由 Linux Namespace 和 Cgroups 构成的隔离环境里；而它运行所需要的各种文件，比如 python，app.py，以及整个操作系统文件，则由多个联合挂载在一起的 rootfs 层提供。\n\n这些 **`rootfs 层的最下层，是来自 Docker 镜像的只读层。`**\n\n**`在只读层之上，是 Docker 自己添加的 Init 层`**，用来存放被临时修改过的 /etc/hosts 等文件。\n\n而 **`rootfs 的最上层是一个可读写层，它以 Copy-on-Write 的方式存放任何对只读层的修改，容器声明的 Volume 的挂载点，也出现在这一层。`**\n\n通过这样的剖析，对于曾经“神秘莫测”的容器技术，你是不是感觉清晰了很多呢？\n\n思考题\n你在查看 Docker 容器的 Namespace 时，是否注意到有一个叫 cgroup 的 Namespace？它是 Linux 4.6 之后新增加的一个 Namespace，你知道它的作用吗？\n\n如果你执行 docker run -v /home:/test 的时候，容器镜像里的 /test 目录下本来就有内容的话，你会发现，在宿主机的 /home 目录下，也会出现这些内容。这是怎么回事？为什么它们没有被绑定挂载隐藏起来呢？（提示：Docker 的“copyData”功能）\n\n请尝试给这个 Python 应用加上 CPU 和 Memory 限制，然后启动它。根据我们前面介绍的 Cgroups 的知识，请你查看一下这个容器的 Cgroups 文件系统的设置，是不是跟我前面的讲解一致。\n\n\n","categories":["reference","k8s"]},{"title":"07 | 白话容器基础（三）：深入理解容器镜像","url":"/2021/03/13/reference/k8s/07.白话容器基础(三)深入理解容器镜像/","content":"![](2.jpg)\n\n在前两次的分享中，我讲解了 Linux 容器最基础的两种技术：Namespace 和 Cgroups。希望此时，你已经彻底理解了“容器的本质是一种特殊的进程”这个最重要的概念。\n\n而正如我前面所说的，Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。这么一折腾，进程就真的被“装”在了一个与世隔绝的房间里，而这些房间就是 PaaS 项目赖以生存的应用“沙盒”。\n\n可是，还有一个问题不知道你有没有仔细思考过：这个房间四周虽然有了墙，但是如果容器进程低头一看地面，又是怎样一副景象呢？\n\n换句话说，**`容器里的进程看到的文件系统又是什么样子的呢？`**\n\n可能你立刻就能想到，这一定是一个关于 Mount Namespace 的问题：容器里的应用进程，理应看到一份完全独立的文件系统。这样，它就可以在自己的容器目录（比如 /tmp）下进行操作，而完全不会受宿主机以及其他容器的影响。\n\n那么，真实情况是这样吗？\n\n“左耳朵耗子”叔在多年前写的一篇[关于 Docker 基础知识的博客](https://coolshell.cn/articles/17010.html)里，曾经介绍过一段小程序。这段小程序的作用是，在创建子进程时开启指定的 Namespace。\n\n下面，我们不妨使用它来验证一下刚刚提到的问题。\n\n\t#define _GNU_SOURCE\n\t#include <sys/mount.h> \n\t#include <sys/types.h>\n\t#include <sys/wait.h>\n\t#include <stdio.h>\n\t#include <sched.h>\n\t#include <signal.h>\n\t#include <unistd.h>\n\t#define STACK_SIZE (1024 * 1024)\n\tstatic char container_stack[STACK_SIZE];\n\tchar* const container_args[] = {\n\t  \"/bin/bash\",\n\t  NULL\n\t};\n\t \n\tint container_main(void* arg)\n\t{  \n\t  printf(\"Container - inside the container!\\n\");\n\t  execv(container_args[0], container_args);\n\t  printf(\"Something's wrong!\\n\");\n\t  return 1;\n\t}\n\t \n\tint main()\n\t{\n\t  printf(\"Parent - start a container!\\n\");\n\t  int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWNS | SIGCHLD , NULL);\n\t  waitpid(container_pid, NULL, 0);\n\t  printf(\"Parent - container stopped!\\n\");\n\t  return 0;\n\t}\n这段代码的功能非常简单：在 main 函数里，我们通过 clone() 系统调用创建了一个新的子进程 container_main，并且声明要为它启用 Mount Namespace（即：CLONE_NEWNS 标志）。\n\n而这个子进程执行的，是一个“/bin/bash”程序，也就是一个 shell。所以这个 shell 就运行在了 Mount Namespace 的隔离环境中。\n\n我们来一起编译一下这个程序：\n\n\t$ gcc -o ns ns.c\n\t$ ./ns\n\tParent - start a container!\n\tContainer - inside the container!\n这样，我们就进入了这个“容器”当中。可是，如果在“容器”里执行一下 ls 指令的话，我们就会发现一个有趣的现象： /tmp 目录下的内容跟宿主机的内容是一样的。\n\n\t$ ls /tmp\n\t# 你会看到好多宿主机的文件\n也就是说：\n\n> 即使开启了 Mount Namespace，容器进程看到的文件系统也跟宿主机完全一样。\n\n这是怎么回事呢？\n\n仔细思考一下，你会发现这其实并不难理解：Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。但是，这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。而在此之前，新创建的容器会直接继承宿主机的各个挂载点。\n\n这时，你可能已经想到了一个解决办法：创建新进程时，除了声明要启用 Mount Namespace 之外，我们还可以告诉容器进程，有哪些目录需要重新挂载，就比如这个 /tmp 目录。于是，我们在容器进程执行前可以添加一步重新挂载 /tmp 目录的操作：\n\n\tint container_main(void* arg)\n\t{\n\t  printf(\"Container - inside the container!\\n\");\n\t  // 如果你的机器的根目录的挂载类型是 shared，那必须先重新挂载根目录\n\t  // mount(\"\", \"/\", NULL, MS_PRIVATE, \"\");\n\t  mount(\"none\", \"/tmp\", \"tmpfs\", 0, \"\");\n\t  execv(container_args[0], container_args);\n\t  printf(\"Something's wrong!\\n\");\n\t  return 1;\n\t}\n可以看到，在修改后的代码里，我在容器进程启动之前，加上了一句 mount(“none”, “/tmp”, “tmpfs”, 0, “”) 语句。就这样，我告诉了容器以 tmpfs（内存盘）格式，重新挂载了 /tmp 目录。\n\n这段修改后的代码，编译执行后的结果又如何呢？我们可以试验一下：\n\n\t$ gcc -o ns ns.c\n\t$ ./ns\n\tParent - start a container!\n\tContainer - inside the container!\n\t$ ls /tmp\n可以看到，这次 /tmp 变成了一个空目录，这意味着重新挂载生效了。我们可以用 mount -l 检查一下：\n\n\t$ mount -l | grep tmpfs\n\tnone on /tmp type tmpfs (rw,relatime)\n可以看到，容器里的 /tmp 目录是以 tmpfs 方式单独挂载的。\n\n更重要的是，因为我们创建的新进程启用了 Mount Namespace，所以这次重新挂载的操作，只在容器进程的 Mount Namespace 中有效。如果在宿主机上用 mount -l 来检查一下这个挂载，你会发现它是不存在的：\n\n\t# 在宿主机上\n\t$ mount -l | grep tmpfs\n**`这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。`**\n\n可是，作为一个普通用户，我们希望的是一个更友好的情况：每当创建一个新容器时，我希望容器进程看到的文件系统就是一个独立的隔离环境，而不是继承自宿主机的文件系统。怎么才能做到这一点呢？\n\n不难想到，我们可以在容器进程启动之前重新挂载它的整个根目录“/”。而由于 Mount Namespace 的存在，这个挂载对宿主机不可见，所以容器进程就可以在里面随便折腾了。\n\n在 Linux 操作系统里，有一个名为 chroot 的命令可以帮助你在 shell 中方便地完成这个工作。顾名思义，它的作用就是帮你“change root file system”，即改变进程的根目录到你指定的位置。它的用法也非常简单。\n\n假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。\n\n首先，创建一个 test 目录和几个 lib 文件夹：\n\n\t$ mkdir -p $HOME/test\n\t$ mkdir -p $HOME/test/{bin,lib64,lib}\n\t$ cd $T\n然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下：\n\n\t$ cp -v /bin/{bash,ls} $HOME/test/bin\n接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令：\n\n\t$ T=$HOME/test\n\t$ list=\"$(ldd /bin/ls | egrep -o '/lib.*\\.[0-9]')\"\n\t$ for i in $list; do cp -v \"$i\" \"${T}${i}\"; done\n最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录：\n\n\t$ chroot $HOME/test /bin/bash\n这时，你如果执行 \"ls /\"，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。\n\n更重要的是，对于被 chroot 的进程来说，它并不会感受到自己的根目录已经被“修改”成 $HOME/test 了。\n\n这种视图被修改的原理，是不是跟我之前介绍的 Linux Namespace 很类似呢？\n\n没错！\n\n**`实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。`**\n\n当然，为了能够让容器的这个根目录看起来更“真实”，我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 \"ls /\" 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。\n\n**`而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs(根文件系统).`**\n**`我们将程序的代码、依赖、环境打包成一个镜像，这里可以看出容器镜像是一种静态的，事实上，容器镜像是为容器进程提供可执行隔离环境的文件系统，如何实现的隔离，就是通过切换与宿主机不同的跟根目录，这个根目录也就是rootfs根文件系统.`**\n所以，一个最常见的 rootfs，或者说容器镜像，会包括如下所示的一些目录和文件，比如 /bin，/etc，/proc 等等：\n\n\t$ ls /\n\tbin dev etc home lib lib64 mnt opt proc root run sbin sys tmp usr var\n而你进入容器之后执行的 /bin/bash，就是 /bin 目录下的可执行文件，与宿主机的 /bin/bash 完全不同。\n\n现在，你应该可以理解，对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：\n\n1.启用 Linux Namespace 配置;\n\n2.设置指定的 Cgroups 参数;\n\n3.切换进程的根目录(Change Root);\n> **`牛逼:容器是一个 创建时指定了namespace的进程,并且启动参数里设置了cpu,内存的限制,使用的文件系统目录也被提前设置了,挂载到了其他的目录(这个就是镜像) 2.所以容器没有像虚拟机一样虚拟出内核等,容器使用的是宿主机的内核,那么问题来了,为什么我在Windows上也可以使用docker呢? 其实是虚拟机....那为什么有windows的镜像呢,原理本身也是会启动一个虚拟机.`**\n\n这样，一个完整的容器就诞生了。不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。这两个系统调用虽然功能类似，但是也有细微的区别，这一部分小知识就交给你课后去探索了。\n\n另外，需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。\n\n所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。\n\n那么，对于容器来说，这个操作系统的“灵魂”又在哪里呢？\n\n实际上，**`同一台机器上的所有容器，都共享宿主机操作系统的内核.`**\n\n这就意味着，**如果你的应用程序需要配置内核参数、加载额外的内核模块，以及跟内核进行直接的交互，你就需要注意了：这些操作和依赖的对象，都是宿主机操作系统的内核，它对于该机器上的所有容器来说是一个“全局变量”，牵一发而动全身。**\n\n这也是容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。\n\n不过，正是由于 rootfs 的存在，容器才有了一个被反复宣传至今的重要特性:**`一致性.`**\n\n什么是容器的“一致性”呢？\n\n我在专栏的第一篇文章《小鲸鱼大事记（一）：初出茅庐》中曾经提到过：由于云端与本地服务器环境不同，应用的打包过程，一直是使用 PaaS 时最“痛苦”的一个步骤。\n\n但有了容器之后，更准确地说，有了容器镜像（即 rootfs）之后，这个问题被非常优雅地解决了。\n\n**`由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起.`**\n\n事实上，对于大多数开发者而言，他们对应用依赖的理解，一直局限在编程语言层面。比如 Golang 的 Godeps.json。但实际上，一个一直以来很容易被忽视的事实是，**`对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”.`**\n\n有了容器镜像“打包操作系统”的能力，这个最基础的依赖环境也终于变成了应用沙盒的一部分。这就赋予了容器所谓的一致性：无论在本地、云端，还是在一台任何地方的机器上，用户只需要解压打包好的容器镜像，那么这个应用运行所需要的完整的执行环境就被重现出来了。\n\n**`这种深入到操作系统级别的运行环境一致性，打通了应用在本地开发和远端执行环境之间难以逾越的鸿沟.`**\n\n不过，这时你可能已经发现了另一个非常棘手的问题：难道我每开发一个应用，或者升级一下现有的应用，都要重复制作一次 rootfs 吗？\n\n比如，我现在用 Ubuntu 操作系统的 ISO 做了一个 rootfs，然后又在里面安装了 Java 环境，用来部署我的 Java 应用。那么，我的另一个同事在发布他的 Java 应用时，显然希望能够直接使用我安装过 Java 环境的 rootfs，而不是重复这个流程。\n\n一种比较直观的解决办法是，我在制作 rootfs 的时候，每做一步“有意义”的操作，就保存一个 rootfs 出来，这样其他同事就可以按需求去用他需要的 rootfs 了。\n\n但是，这个解决办法并不具备推广性。原因在于，一旦你的同事们修改了这个 rootfs，新旧两个 rootfs 之间就没有任何关系了。这样做的结果就是极度的碎片化。\n\n那么，既然这些修改都基于一个旧的 rootfs，我们能不能以增量的方式去做这些修改呢？这样做的好处是，所有人都只需要维护相对于 base rootfs 修改的增量内容，而不是每次修改都制造一个“fork”。\n\n答案当然是肯定的。\n\n这也正是为何，Docker 公司在实现 Docker 镜像时并没有沿用以前制作 rootfs 的标准流程，而是做了一个小小的创新：\n\n> **`Docker 在镜像的设计中，引入了层（layer）的概念。也就是说，用户制作镜像的每一步操作，都会生成一个层，也就是一个增量 rootfs.`**\n\n当然，这个想法不是凭空臆造出来的，而是用到了一种叫作联合文件系统（Union File System）的能力。\n\nUnion File System 也叫 UnionFS，最主要的功能是将多个不同位置的目录联合挂载（union mount）到同一个目录下。比如，我现在有两个目录 A 和 B，它们分别有两个文件：\n\n\t$ tree\n\t.\n\t├── A\n\t│  ├── a\n\t│  └── x\n\t└── B\n\t  ├── b\n\t  └── x\n然后，我使用联合挂载的方式，将这两个目录挂载到一个公共的目录 C 上：\n\n\t$ mkdir C\n\t$ mount -t aufs -o dirs=./A:./B none ./C\n这时，我再查看目录 C 的内容，就能看到目录 A 和 B 下的文件被合并到了一起：\n\n\t$ tree ./C\n\t./C\n\t├── a\n\t├── b\n\t└── x\n可以看到，在这个合并后的目录 C 里，有 a、b、x 三个文件，并且 x 文件只有一份。这，就是“合并”的含义。此外，如果你在目录 C 里对 a、b、x 文件做修改，这些修改也会在对应的目录 A、B 中生效。\n\n那么，在 Docker 项目中，又是如何使用这种 Union File System 的呢？\n\n我的环境是 Ubuntu 16.04 和 Docker CE 18.05，这对组合默认使用的是 AuFS 这个联合文件系统的实现。你可以通过 docker info 命令，查看到这个信息。\n\nAuFS 的全称是 Another UnionFS，后改名为 Alternative UnionFS，再后来干脆改名叫作 Advance UnionFS，从这些名字中你应该能看出这样两个事实：\n\n1. 它是对 Linux 原生 UnionFS 的重写和改进；\n\n2. 它的作者怨气好像很大。我猜是 Linus Torvalds（Linux 之父）一直不让 AuFS 进入 Linux 内核主干的缘故，所以我们只能在 Ubuntu 和 Debian 这些发行版上使用它。\n\n对于 AuFS 来说，它最关键的目录结构在 /var/lib/docker 路径下的 diff 目录：\n\n\t/var/lib/docker/aufs/diff/<layer_id>\n而这个目录的作用，我们不妨通过一个具体例子来看一下。\n\n现在，我们启动一个容器，比如：\n\n\t$ docker run -d ubuntu:latest sleep 3600\n这时候，Docker 就会从 Docker Hub 上拉取一个 Ubuntu 镜像到本地。\n\n这个所谓的“镜像”，实际上就是一个 Ubuntu 操作系统的 rootfs，它的内容是 Ubuntu 操作系统的所有文件和目录。不过，与之前我们讲述的 rootfs 稍微不同的是，Docker 镜像使用的 rootfs，往往由多个“层”组成：\n\n\t$ docker image inspect ubuntu:latest\n\t...\n\t     \"RootFS\": {\n\t      \"Type\": \"layers\",\n\t      \"Layers\": [\n\t        \"sha256:f49017d4d5ce9c0f544c...\",\n\t        \"sha256:8f2b771487e9d6354080...\",\n\t        \"sha256:ccd4d61916aaa2159429...\",\n\t        \"sha256:c01d74f99de40e097c73...\",\n\t        \"sha256:268a067217b5fe78e000...\"\n\t      ]\n\t    }\n可以看到，这个 Ubuntu 镜像，实际上由五个层组成。这五个层就是五个增量 rootfs，每一层都是 Ubuntu 操作系统文件与目录的一部分；而在使用镜像时，Docker 会把这些增量联合挂载在一个统一的挂载点上（等价于前面例子里的“/C”目录）。\n\n这个挂载点就是 /var/lib/docker/aufs/mnt/，比如：\n\n\t/var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e\n不出意外的，这个目录里面正是一个完整的 Ubuntu 操作系统：\n\n\t$ ls /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fcfa2a2f5c89dc21ee30e166be823ceaeba15dce645b3e\n\tbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var\n那么，前面提到的五个镜像层，又是如何被联合挂载成这样一个完整的 Ubuntu 文件系统的呢？\n\n这个信息记录在 AuFS 的系统目录 /sys/fs/aufs 下面。\n\n首先，通过查看 AuFS 的挂载信息，我们可以找到这个目录对应的 AuFS 的内部 ID（也叫：si）：\n\n\t$ cat /proc/mounts| grep aufs\n\tnone /var/lib/docker/aufs/mnt/6e3be5d2ecccae7cc0fc... aufs rw,relatime,si=972c6d361e6b32ba,dio,dirperm1 0 0\n即，si=972c6d361e6b32ba。\n\n然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息：\n\n\t$ cat /sys/fs/aufs/si_972c6d361e6b32ba/br[0-9]*\n\t/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...=rw\n\t/var/lib/docker/aufs/diff/6e3be5d2ecccae7cc...-init=ro+wh\n\t/var/lib/docker/aufs/diff/32e8e20064858c0f2...=ro+wh\n\t/var/lib/docker/aufs/diff/2b8858809bce62e62...=ro+wh\n\t/var/lib/docker/aufs/diff/20707dce8efc0d267...=ro+wh\n\t/var/lib/docker/aufs/diff/72b0744e06247c7d0...=ro+wh\n\t/var/lib/docker/aufs/diff/a524a729adadedb90...=ro+wh\n从这些信息里，我们可以看到，镜像的层都放置在 /var/lib/docker/aufs/diff 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。\n\n而且，从这个结构可以看出来，这个容器的 rootfs 由如下图所示的三部分组成：\n\n![](1.png)\n\n**第一部分，只读层。**\n\n它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。\n\n这时，我们可以分别查看一下这些层的内容：\n\n\t$ ls /var/lib/docker/aufs/diff/72b0744e06247c7d0...\n\tetc sbin usr var\n\t$ ls /var/lib/docker/aufs/diff/32e8e20064858c0f2...\n\trun\n\t$ ls /var/lib/docker/aufs/diff/a524a729adadedb900...\nbin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var\n可以看到，这些层，都以增量的方式分别包含了 Ubuntu 操作系统的一部分。\n\n**第二部分，可读写层。**\n\n它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。\n\n可是，你有没有想到这样一个问题：如果我现在要做的，是删除只读层里的一个文件呢？\n\n为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。\n\n比如，你要删除只读层里一个名叫 foo 的文件，那么这个删除操作实际上是在可读写层创建了一个名叫.wh.foo 的文件。这样，当这两个层被联合挂载之后，foo 文件就会被.wh.foo 文件“遮挡”起来，“消失”了。这个功能，就是“ro+wh”的挂载方式，即只读 +whiteout 的含义。我喜欢把 whiteout 形象地翻译为：“白障”。\n\n所以，最上面这个可读写层的作用，就是专门用来存放你修改 rootfs 后产生的增量，无论是增、删、改，都发生在这里。而当我们使用完了这个被修改过的容器之后，还可以使用 docker commit 和 push 指令，保存这个被修改过的可读写层，并上传到 Docker Hub 上，供其他人使用；而与此同时，原先的只读层里的内容则不会有任何变化。这，就是增量 rootfs 的好处。\n\n**第三部分，Init 层。**\n\n它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。\n\n需要这样一层的原因是，这些文件本来属于只读的 Ubuntu 镜像的一部分，但是用户往往需要在启动容器时写入一些指定的值比如 hostname，所以就需要在可读写层对它们进行修改。\n\n可是，这些修改往往只对当前的容器有效，我们并不希望执行 docker commit 时，把这些信息连同可读写层一起提交掉。\n\n所以，Docker 做法是，在修改了这些文件之后，以一个单独的层挂载了出来。而用户执行 docker commit 只会提交可读写层，所以是不包含这些内容的。\n\n最终，这 7 个层都被联合挂载到 /var/lib/docker/aufs/mnt 目录下，表现为一个完整的 Ubuntu 操作系统供容器使用。\n\n**总结**\n在今天的分享中，我着重介绍了 Linux 容器文件系统的实现方式。而这种机制，正是我们经常提到的容器镜像，也叫作：rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。\n\n**`通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。`**\n\n**`而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。`**\n\n通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。\n\n更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。\n\n而这种价值正是支撑 Docker 公司在 2014~2016 年间迅猛发展的核心动力。容器镜像的发明，不仅打通了“开发 - 测试 - 部署”流程的每一个环节，更重要的是：\n\n> 容器镜像将会成为未来软件的主流发布方式。\n\n思考题\n1. 既然容器的 rootfs（比如，Ubuntu 镜像），是以只读方式挂载的，那么又如何在容器里修改 Ubuntu 镜像的内容呢？（提示：Copy-on-Write）\n\n2. 除了 AuFS，你知道 Docker 项目还支持哪些 UnionFS 实现吗？你能说出不同宿主机环境下推荐使用哪种实现吗？\n\n","categories":["reference","k8s"]},{"title":"06 | 白话容器基础（二）：隔离与限制","url":"/2021/03/13/reference/k8s/06.白话容器基础(二)隔离与限制/","content":"\n![](2.jpg)\n在上一篇文章中，我详细介绍了 Linux 容器中用来实现“隔离”的技术手段：Namespace。而通过这些讲解，你应该能够明白，**`Namespace 技术实际上修改了应用进程看待整个计算机“视图”，即它的“视线”被操作系统做了限制，只能“看到”某些指定的内容.`** 但对于宿主机来说，这些被“隔离”了的进程跟其他进程并没有太大区别。\n\n说到这一点，相信你也能够知道我在上一篇文章最后给你留下的第一个思考题的答案了：在之前虚拟机与容器技术的对比图里，不应该把 Docker Engine 或者任何容器管理工具放在跟 Hypervisor 相同的位置，因为它们并不像 Hypervisor 那样对应用进程的隔离环境负责，也不会创建任何实体的“容器”，真正对隔离环境负责的是宿主机操作系统本身：\n![](1.jpg)\n\n\n所以，在这个对比图里，我们应该把 Docker 画在跟应用同级别并且靠边的位置。这意味着，**`用户运行在容器里的应用进程，跟宿主机上的其他进程一样，都由宿主机操作系统统一管理，只不过这些被隔离的进程拥有额外设置过的 Namespace 参数.`** 而 Docker 项目在这里扮演的角色，更多的是旁路式的辅助和管理工作。\n\n我在后续分享 CRI 和容器运行时的时候还会专门介绍到，其实像 Docker 这样的角色甚至可以去掉。\n\n这样的架构也解释了为什么 Docker 项目比虚拟机更受欢迎的原因。\n\n这是因为，使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。\n\n根据实验，一个运行着 CentOS 的 KVM 虚拟机启动后，在不做优化的情况下，虚拟机自己就需要占用 100~200 MB 内存。此外，用户应用运行在虚拟机里面，它对宿主机操作系统的调用就不可避免地要经过虚拟化软件的拦截和处理，这本身又是一层性能损耗，尤其对计算资源、网络和磁盘 I/O 的损耗非常大。\n\n而相比之下，**`容器化后的用户应用，却依然还是一个宿主机上的普通进程，这就意味着这些因为虚拟化而带来的性能损耗都是不存在的;`**而另一方面，使用 Namespace 作为隔离手段的容器并不需要单独的 Guest OS，这就使得容器额外的资源占用几乎可以忽略不计。\n\n所以说，**`“敏捷”和“高性能”是容器相较于虚拟机最大的优势，也是它能够在 PaaS 这种更细粒度的资源管理平台上大行其道的重要原因。`**\n\n不过，有利就有弊，基于 Linux Namespace 的隔离机制相比于虚拟化技术也有很多不足之处，其中最主要的问题就是：**`隔离得不彻底。`**\n\n首先，**`既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。`**\n\n尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。这意味着，**`如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。`**\n\n而相比之下，拥有硬件虚拟化技术和独立 Guest OS 的虚拟机就要方便得多了。**`最极端的例子是，Microsoft 的云计算平台 Azure，实际上就是运行在 Windows 服务器集群上的，但这并不妨碍你在它上面创建各种 Linux 虚拟机出来。`**\n\n其次，**`在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。`**\n\n这就意味着，**`如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。`**相比于在虚拟机里面可以随便折腾的自由度，在容器里部署应用的时候，“什么能做，什么不能做”，就是用户必须考虑的一个问题。\n\n此外，由于上述问题，尤其是共享宿主机内核的事实，容器给应用暴露出来的攻击面是相当大的，应用“越狱”的难度自然也比虚拟机低得多。\n\n更为棘手的是，尽管在实践中我们确实可以使用 Seccomp 等技术，对容器内部发起的所有系统调用进行过滤和甄别来进行安全加固，但这种方法因为多了一层对系统调用的过滤，一定会拖累容器的性能。何况，默认情况下，谁也不知道到底该开启哪些系统调用，禁止哪些系统调用。\n\n所以，**`在生产环境中，没有人敢把运行在物理机上的 Linux 容器直接暴露到公网上。当然，我后续会讲到的基于虚拟化或者独立内核技术的容器实现，则可以比较好地在隔离与性能之间做出平衡。`**\n\n在介绍完容器的“隔离”技术之后，我们再来研究一下容器的“限制”问题。\n\n也许你会好奇，我们不是已经通过 Linux Namespace 创建了一个“容器”吗，为什么还需要对容器做**`“限制”`**呢？\n\n我还是以 PID Namespace 为例，来给你解释这个问题。\n\n虽然容器内的第 1 号进程在“障眼法”的干扰下只能看到容器里的情况，但是宿主机上，它作为第 100 号进程与其他所有进程之间依然是平等的竞争关系。这就意味着，虽然第 100 号进程表面上被隔离了起来，但是它所能够使用到的资源（比如 CPU、内存），却是可以随时被宿主机上的其他进程（或者其他容器）占用的。当然，这个 100 号进程自己也可能把所有资源吃光。这些情况，显然都不是一个“沙盒”应该表现出来的合理行为。\n\n而**`Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。`**\n\n有意思的是，Google 的工程师在 2006 年发起这项特性的时候，曾将它命名为“进程容器”（process container）。实际上，在 Google 内部，“容器”这个术语长期以来都被用于形容被 Cgroups 限制过的进程组。后来 Google 的工程师们说，他们的 KVM 虚拟机也运行在 Borg 所管理的“容器”里，其实也是运行在 Cgroups“容器”当中。这和我们今天说的 Docker 容器差别很大。\n\n**`Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。`**\n\n此外，Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。在今天的分享中，我只和你重点探讨它与容器关系最紧密的“限制”能力，并通过一组实践来带你认识一下 Cgroups。\n\n在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 **`/sys/fs/cgroup`** 路径下。在 Ubuntu 16.04 机器里，我可以用 mount 指令把它们展示出来，这条命令是：\n\n\t$ mount -t cgroup \n\tcpuset on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\n\tcpu on /sys/fs/cgroup/cpu type cgroup (rw,nosuid,nodev,noexec,relatime,cpu)\n\tcpuacct on /sys/fs/cgroup/cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpuacct)\n\tblkio on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\n\tmemory on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\n\t...\n它的输出结果，是一系列文件系统目录。如果你在自己的机器上没有看到这些目录，那你就需要自己去挂载 Cgroups，具体做法可以自行 Google。\n\n可以看到，在 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。这些都是我这台机器当前可以被 Cgroups 进行限制的资源种类。而在子系统对应的资源种类下，你就可以看到该类资源具体可以被限制的方法。比如，对 CPU 子系统来说，我们就可以看到如下几个配置文件，这个指令是：\n\n\t$ ls /sys/fs/cgroup/cpu\n\tcgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release\n\tcgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks\n如果熟悉 Linux CPU 管理的话，你就会在它的输出里注意到 cfs_period 和 cfs_quota 这样的关键词。这两个参数需要组合使用，可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。\n\n而这样的配置文件又如何使用呢？\n\n你需要在对应的子系统下面创建一个目录，比如，我们现在进入 **`/sys/fs/cgroup/cpu`** 目录下：\n\n\troot@ubuntu:/sys/fs/cgroup/cpu$ mkdir container\n\troot@ubuntu:/sys/fs/cgroup/cpu$ ls container/\n\tcgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release\n\tcgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks\n这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。\n\n现在，我们在后台执行这样一条脚本：\n\n\t$ while : ; do : ; done &\n\t[1] 226\n显然，它执行了一个死循环，可以把计算机的 CPU 吃到 100%，根据它的输出，我们可以看到这个脚本在后台运行的进程号（PID）是 226。\n\n这样，我们可以用 top 指令来确认一下 CPU 有没有被打满：\n\n\t$ top\n\t%Cpu0 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\n在输出里可以看到，CPU 的使用率已经 100% 了（%Cpu0 :100.0 us）。\n\n而此时，我们可以通过查看 container 目录下的文件，看到 container 控制组里的 CPU quota 还没有任何限制（即：-1），CPU period 则是默认的 100 ms（100000 us）：\n\n\t$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us \n\t-1\n\t$ cat /sys/fs/cgroup/cpu/container/cpu.cfs_period_us \n\t100000\n接下来，我们可以通过修改这些文件的内容来设置限制。\n\n比如，向 container 组里的 cfs_quota 文件写入 20 ms（20000 us）：\n\n\t$ echo 20000 > /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us\n结合前面的介绍，你应该能明白这个操作的含义，它意味着在每 100 ms 的时间里，被该控制组限制的进程只能使用 20 ms 的 CPU 时间，也就是说这个进程只能使用到 20% 的 CPU 带宽。\n\n接下来，我们把被限制的进程的 PID 写入 container 组里的 tasks 文件，上面的设置就会对该进程生效了：\n\n\t$ echo 226 > /sys/fs/cgroup/cpu/container/tasks \n我们可以用 top 指令查看一下：\n\n\t$ top\n\t%Cpu0 : 20.3 us, 0.0 sy, 0.0 ni, 79.7 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st\n可以看到，计算机的 CPU 使用率立刻降到了 20%（%Cpu0 : 20.3 us）。\n\n除 CPU 子系统外，Cgroups 的每一项子系统都有其独有的资源限制能力，比如：\n\n * blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制，一般用于磁盘等设备；\n * cpuset，为进程分配单独的 CPU 核和对应的内存节点；\n * memory，为进程设定内存使用的限制。\n**`Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合.`** 而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。\n\n而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：\n\n\t$ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash\n在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：\n\n\t$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d<container-ID>/cpu.cfs_period_us \n\t100000\n\t$ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d<container-ID>/cpu.cfs_quota_us \n\t20000\n这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。\n\n总结\n在这篇文章中，我首先介绍了容器使用 Linux Namespace 作为隔离手段的优势和劣势，对比了 Linux 容器跟虚拟机技术的不同，进一步明确了“容器只是一种特殊的进程”这个结论。\n\n除了创建 Namespace 之外，在后续关于容器网络的分享中，我还会介绍一些其他 Namespace 的操作，比如看不见摸不着的 Linux Namespace 在计算机中到底如何表示、一个进程如何“加入”到其他进程的 Namespace 当中，等等。\n\n紧接着，我详细介绍了容器在做好了隔离工作之后，又如何通过 Linux Cgroups 实现资源的限制，并通过一系列简单的实验，模拟了 Docker 项目创建容器限制的过程。\n\n通过以上讲述，你现在应该能够理解，一个正在运行的 Docker 容器，其实就是一个启用了多个 Linux Namespace 的应用进程，而这个进程能够使用的资源量，则受 Cgroups 配置的限制。\n\n这也是容器技术中一个非常重要的概念，即：**`容器是一个“单进程”模型.容器的单进程概念，一个容器绑定一个进程，这个进程是和容器有一样的生命周期，进程作为启动容器的进程在 容器中PID=1,也就是说这个进程是容器中所有进程的父进程，如果在容器中在创建一个进程是不行的.`**\n\n由于一个容器的本质就是一个进程，用户的应用进程实际上就是容器里 PID=1 的进程，也是其他后续创建的所有进程的父进程。这就意味着，在一个容器中，你没办法同时运行两个不同的应用，除非你能事先找到一个公共的 PID=1 的程序来充当两个不同应用的父进程，这也是为什么很多人都会用 systemd 或者 supervisord 这样的软件来代替应用本身作为容器的启动进程。\n\n但是，在后面分享容器设计模式时，我还会推荐其他更好的解决办法。**`这是因为容器本身的设计，就是希望容器和应用能够同生命周期，这个概念对后续的容器编排非常重要。否则，一旦出现类似于“容器是正常运行的，但是里面的应用早已经挂了”的情况，编排系统处理起来就非常麻烦了。`**\n\n另外，跟 Namespace 的情况类似，Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。\n\n众所周知，Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。\n\n但是，你如果在**`容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。`**\n\n造成这个问题的原因就是，**`/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在.`**\n\n**`在生产环境中，这个问题必须进行修正，否则应用程序在容器里读取到的 CPU 核数、可用内存等信息都是宿主机上的数据，这会给应用的运行带来非常大的困惑和风险.`** 这也是在企业中，容器化应用碰到的一个常见问题，也是容器相较于虚拟机另一个不尽如人意的地方。\n\n思考题\n你是否知道如何修复容器中的 top 指令以及 /proc 文件系统中的信息呢？（提示：lxcfs）\n\n在从虚拟机向容器环境迁移应用的过程中，你还遇到哪些容器与虚拟机的不一致问题？","categories":["reference","k8s"]},{"title":"09 | 从容器到容器云：谈谈Kubernetes的本质","url":"/2021/03/13/reference/k8s/09.从容器到容器云：谈谈Kubernetes的本质/","content":"\n![](1.jpg)\n\n在前面的四篇文章中，我以 Docker 项目为例，一步步剖析了 Linux 容器的具体实现方式。通过这些讲解你应该能够明白：一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。\n\n从这个结构中我们不难看出，一个正在运行的 Linux 容器，其实可以被“一分为二”地看待：\n\n1. 一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图；\n\n2. 一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。\n\n更进一步地说，作为一名开发者，我并不关心容器运行时的差异。因为，在整个“开发 - 测试 - 发布”的流程中，真正承载着容器信息进行传递的，是容器镜像，而不是容器运行时。\n\n这个重要假设，正是容器技术圈在 Docker 项目成功后不久，就迅速走向了“容器编排”这个“上层建筑”的主要原因：作为一家云服务商或者基础设施提供商，我只要能够将用户提交的 Docker 镜像以容器的方式运行起来，就能成为这个非常热闹的容器生态图上的一个承载点，从而将整个容器技术栈上的价值，沉淀在我的这个节点上。\n\n更重要的是，只要从我这个承载点向 Docker 镜像制作者和使用者方向回溯，整条路径上的各个服务节点，比如 CI/CD、监控、安全、网络、存储等等，都有我可以发挥和盈利的余地。这个逻辑，正是所有云计算提供商如此热衷于容器技术的重要原因：通过容器镜像，它们可以和潜在用户（即，开发者）直接关联起来。\n\n从一个开发者和单一的容器镜像，到无数开发者和庞大的容器集群，容器技术实现了从“容器”到“容器云”的飞跃，标志着它真正得到了市场和生态的认可。\n\n这样，**`容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”。`**\n\n这其中，最具代表性的容器编排工具，当属 Docker 公司的 Compose+Swarm 组合，以及 Google 与 RedHat 公司共同主导的 Kubernetes 项目。\n\n我在前面介绍容器技术发展历史的四篇预习文章中，已经对这两个开源项目做了详细地剖析和评述。所以，在今天的这次分享中，我会专注于本专栏的主角 Kubernetes 项目，谈一谈它的设计与架构。\n\n跟很多基础设施领域先有工程实践、后有方法论的发展路线不同，Kubernetes 项目的理论基础则要比工程实践走得靠前得多，这当然要归功于 Google 公司在 2015 年 4 月发布的 Borg 论文了。\n\nBorg 系统，一直以来都被誉为 Google 公司内部最强大的“秘密武器”。虽然略显夸张，但这个说法倒不算是吹牛。\n\n因为，相比于 Spanner、BigTable 等相对上层的项目，Borg 要承担的责任，是承载 Google 公司整个基础设施的核心依赖。在 Google 公司已经公开发表的基础设施体系论文中，Borg 项目当仁不让地位居整个基础设施技术栈的最底层。\n![](2.png)\n\n图片来源：[Malte Schwarzkopf. “Operating system support for warehouse-scale computing”. PhD thesis. University of Cambridge Computer Laboratory (to appear), 2015, Chapter 2.](http://malteschwarzkopf.de/research/assets/google-stack.pdf)\n\n上面这幅图，来自于 Google Omega 论文的第一作者的博士毕业论文。它描绘了当时 Google 已经公开发表的整个基础设施栈。在这个图里，你既可以找到 MapReduce、BigTable 等知名项目，也能看到 Borg 和它的继任者 Omega 位于整个技术栈的最底层。\n\n正是由于这样的定位，Borg 可以说是 Google 最不可能开源的一个项目。而幸运地是，得益于 Docker 项目和容器技术的风靡，它却终于得以以另一种方式与开源社区见面，这个方式就是 Kubernetes 项目。\n\n所以，相比于“小打小闹”的 Docker 公司、“旧瓶装新酒”的 Mesos 社区，Kubernetes 项目从一开始就比较幸运地站上了一个他人难以企及的高度：在它的成长阶段，这个项目每一个核心特性的提出，几乎都脱胎于 Borg/Omega 系统的设计与经验。更重要的是，这些特性在开源社区落地的过程中，又在整个社区的合力之下得到了极大的改进，修复了很多当年遗留在 Borg 体系中的缺陷和问题。\n\n所以，尽管在发布之初被批评是“曲高和寡”，但是在逐渐觉察到 Docker 技术栈的“稚嫩”和 Mesos 社区的“老迈”之后，这个社区很快就明白了：Kubernetes 项目在 Borg 体系的指导下，体现出了一种独有的“先进性”与“完备性”，而这些特质才是一个基础设施领域开源项目赖以生存的核心价值。\n\n为了更好地理解这两种特质，我们不妨从 Kubernetes 的顶层设计说起。\n\n首先，Kubernetes 项目要解决的问题是什么？\n\n编排？调度？容器云？还是集群管理？\n\n实际上，这个问题到目前为止都没有固定的答案。因为在不同的发展阶段，Kubernetes 需要着重解决的问题是不同的。\n\n但是，对于大多数用户来说，他们希望 Kubernetes 项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群上把这个应用运行起来。\n\n更进一步地说，我还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。\n\n等一下，这些功能听起来好像有些耳熟？这不就是经典 PaaS（比如，Cloud Foundry）项目的能力吗？\n\n而且，有了 Docker 之后，我根本不需要什么 Kubernetes、PaaS，只要使用 Docker 公司的 Compose+Swarm 项目，就完全可以很方便地 DIY 出这些功能了！\n\n所以说，如果 Kubernetes 项目只是停留在拉取用户镜像、运行容器，以及提供常见的运维功能的话，那么别说跟“原生”的 Docker Swarm 项目竞争了，哪怕跟经典的 PaaS 项目相比也难有什么优势可言。\n\n而实际上，在定义核心功能的过程中，Kubernetes 项目正是依托着 Borg 项目的理论优势，才在短短几个月内迅速站稳了脚跟，进而确定了一个如下图所示的全局架构：\n![](3.png)\n\n\n我们可以看到，Kubernetes 项目的架构，跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成，而这两种角色分别对应着控制节点和计算节点。\n\n其中，控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Ectd 中。\n\n而计算节点上最核心的部分，则是一个叫作 kubelet 的组件。\n\n**`在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。`** 而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。\n\n这也是为何，Kubernetes 项目并不关心你部署的是什么容器运行时、使用的什么技术实现，只要你的这个容器运行时能够运行标准的容器镜像，它就可以通过实现 CRI 接入到 Kubernetes 项目当中。\n**`(一个docker相当于一个线程，一个pod相当于一组紧密协作的线程。把k8s看作数据中心操作系统，它的调度对象是可以完成某种任务的实体，docker只是一种实现而已。相当于又引入了一个抽象中间层)`**\n\n而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。\n\n此外，**`kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互.`** 这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。\n\n而 **`kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储.`** 这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。\n\n实际上，kubelet 这个奇怪的名字，来自于 Borg 项目里的同源组件 Borglet。不过，如果你浏览过 Borg 论文的话，就会发现，这个命名方式可能是 kubelet 组件与 Borglet 组件的唯一相似之处。因为 Borg 项目，并不支持我们这里所讲的容器技术，而只是简单地使用了 Linux Cgroups 对进程进行限制。\n\n这就意味着，像 Docker 这样的“容器镜像”在 Borg 中是不存在的，Borglet 组件也自然不需要像 kubelet 这样考虑如何同 Docker 进行交互、如何对容器镜像进行管理的问题，也不需要支持 CRI、CNI、CSI 等诸多容器技术接口。\n\n可以说，kubelet 完全就是为了实现 Kubernetes 项目对容器的管理能力而重新实现的一个组件，与 Borg 之间并没有直接的传承关系。\n\n> 备注：虽然不使用 Docker，但 Google 内部确实在使用一个包管理工具，名叫 Midas Package Manager (MPM)，其实它可以部分取代 Docker 镜像的角色。\n\n那么，Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢？\n\n答案是，Master 节点。\n\n虽然在 Master 节点的实现细节上 Borg 项目与 Kubernetes 项目不尽相同，但它们的出发点却高度一致，即：如何编排、管理、调度用户提交的作业？\n\n所以，Borg 项目完全可以把 Docker 镜像看做是一种新的应用打包方式。这样，Borg 团队过去在大规模作业管理与编排上的经验就可以直接“套”在 Kubernetes 项目上了。\n\n这些经验最主要的表现就是，**`从一开始，Kubernetes 项目就没有像同时期的各种“容器云”项目那样，把 Docker 作为整个架构的核心，而仅仅把它作为最底层的一个容器运行时实现。`**\n\n而 Kubernetes 项目要着重解决的问题，则来自于 Borg 的研究人员在论文中提到的一个非常重要的观点：\n\n> 运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。\n\n事实也正是如此。\n\n其实，这种任务与任务之间的关系，在我们平常的各种技术场景中随处可见。比如，**`一个 Web 应用与数据库之间的访问关系，一个负载均衡器和它的后端服务之间的代理关系，一个门户应用与授权组件之间的调用关系。`**\n\n更进一步地说，同属于一个服务单位的不同功能之间，也完全可能存在这样的关系。比如，一个 Web 应用与日志搜集组件之间的文件交换关系。\n\n而在容器技术普及之前，传统虚拟机环境对这种关系的处理方法都是比较“粗粒度”的。你会经常发现很多功能并不相关的应用被一股脑儿地部署在同一台虚拟机中，只是因为它们之间偶尔会互相发起几个 HTTP 请求。\n\n更常见的情况则是，一个应用被部署在虚拟机里之后，你还得手动维护很多跟它协作的守护进程（Daemon），用来处理它的日志搜集、灾难恢复、数据备份等辅助工作。\n\n但容器技术出现以后，你就不难发现，在“功能单位”的划分上，容器有着独一无二的“细粒度”优势：毕竟容器的本质，只是一个进程而已。\n\n也就是说，只要你愿意，那些原先拥挤在同一个虚拟机里的各个应用、组件、守护进程，都可以被分别做成镜像，然后运行在一个个专属的容器中。它们之间互不干涉，拥有各自的资源配额，可以被调度在整个集群里的任何一台机器上。而这，正是一个 PaaS 系统最理想的工作状态，也是所谓“微服务”思想得以落地的先决条件。\n\n当然，如果只做到“封装微服务、调度单容器”这一层次，Docker Swarm 项目就已经绰绰有余了。如果再加上 Compose 项目，你甚至还具备了处理一些简单依赖关系的能力，比如：一个“Web 容器”和它要访问的数据库“DB 容器”。\n\n在 Compose 项目中，你可以为这样的两个容器定义一个“link”，而 Docker 项目则会负责维护这个“link”关系，其具体做法是：Docker 会在 Web 容器中，将 DB 容器的 IP 地址、端口等信息以环境变量的方式注入进去，供应用进程使用，比如：\n\n\t    DB_NAME=/web/db\n\t    DB_PORT=tcp://172.17.0.5:5432\n\t    DB_PORT_5432_TCP=tcp://172.17.0.5:5432\n\t    DB_PORT_5432_TCP_PROTO=tcp\n\t    DB_PORT_5432_TCP_PORT=5432\n\t    DB_PORT_5432_TCP_ADDR=172.17.0.5\n而当 DB 容器发生变化时（比如，镜像更新，被迁移到其他宿主机上等等），这些环境变量的值会由 Docker 项目自动更新。**`这就是平台项目自动地处理容器间关系的典型例子。`**\n\n可是，如果我们现在的需求是，要求这个项目能够处理前面提到的所有类型的关系，甚至还要能够支持未来可能出现的更多种类的关系呢？\n\n这时，“link”这种单独针对一种案例设计的解决方案就太过简单了。如果你做过架构方面的工作，就会深有感触：一旦要追求项目的普适性，那就一定要从顶层开始做好设计。\n\n所以，**`Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。`**\n\n比如，Kubernetes 项目对容器间的“访问”进行了分类，首先总结出了一类非常常见的“紧密交互”的关系，即：这些应用之间需要非常频繁的交互和访问；又或者，它们会直接通过本地文件进行信息交换。\n\n在常规环境下，这些应用往往会被直接部署在同一台机器上，通过 Localhost 通信，通过本地磁盘目录交换文件。而在 Kubernetes 项目中，这些容器则会被划分为一个“Pod”，Pod 里的容器共享同一个 Network Namespace、同一组数据卷，从而达到高效率交换信息的目的。\n\nPod 是 Kubernetes 项目中最基础的一个对象，源自于 Google Borg 论文中一个名叫 Alloc 的设计。在后续的章节中，我们会对 Pod 做更进一步地阐述。\n\n而对于另外一种更为常见的需求，比如 Web 应用与数据库之间的访问关系，Kubernetes 项目则提供了一种叫作“Service”的服务。像这样的两个应用，往往故意不部署在同一台机器上，这样即使 Web 应用所在的机器宕机了，数据库也完全不受影响。可是，我们知道，对于一个容器来说，它的 IP 地址等信息不是固定的，那么 Web 应用又怎么找到数据库容器的 Pod 呢？\n\n所以，Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务，而 Service 服务声明的 IP 地址等信息是“终生不变”的。**`这个Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。`**\n**`(service是pod的LB?)`**\n\n这样，对于 Web 应用的 Pod 来说，它需要关心的就是数据库 Pod 的 Service 信息。不难想象，Service 后端真正代理的 Pod 的 IP 地址、端口等信息的自动更新、维护，则是 Kubernetes 项目的职责。\n\n像这样，围绕着容器和 Pod 不断向真实的技术场景扩展，我们就能够摸索出一幅如下所示的 Kubernetes 项目核心功能的“全景图”。\n![](4.png)\n\n\n按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。\n\n可是，如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。最典型的例子就是 Web 应用对数据库访问时需要 Credential（数据库的用户名和密码）信息。那么，在 Kubernetes 中这样的关系又如何处理呢？\n\nKubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。这样，你把 Credential 信息以 Secret 的方式存在 Etcd 里，Kubernetes 就会在你指定的 Pod（比如，Web 应用的 Pod）启动时，自动把 Secret 里的数据以 Volume 的方式挂载到容器里。这样，这个 Web 应用就可以访问数据库了。\n\n**`除了应用与应用之间的关系外，应用运行的形态是影响“如何容器化这个应用”的第二个重要因素。`**\n\n为此，Kubernetes 定义了新的、基于 Pod 改进后的对象。比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。\n\n如此种种，正是 Kubernetes 项目定义容器间关系和形态的主要方法。\n\n可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。\n\n相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是：\n\n * 首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用；\n\n * 然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。\n\n**`这种使用方法，就是所谓的“声明式 API”。这种 API 对应的“编排对象”和“服务对象”，都是 Kubernetes 项目中的 API 对象（API Object）。`**\n\n这就是 Kubernetes 最核心的设计理念，也是接下来我会重点剖析的关键技术点。\n\n最后，我来回答一个更直接的问题：Kubernetes 项目如何启动一个容器化任务呢？\n\n比如，我现在已经制作好了一个 Nginx 容器镜像，希望让平台帮我启动这个镜像。并且，我要求平台帮我运行两个完全相同的 Nginx 副本，以负载均衡的方式共同对外提供服务。\n\n * 如果是自己 DIY 的话，可能需要启动两台虚拟机，分别安装两个 Nginx，然后使用 keepalived 为这两个虚拟机做一个虚拟 IP。\n\n * 而如果使用 Kubernetes 项目呢？你需要做的则是编写如下这样一个 YAML 文件（比如名叫 nginx-deployment.yaml）：\n\n\n\tapiVersion: apps/v1\n\tkind: Deployment\n\tmetadata:\n\t  name: nginx-deployment\n\t  labels:\n\t    app: nginx\n\tspec:\n\t  replicas: 2\n\t  selector:\n\t    matchLabels:\n\t      app: nginx\n\t  template:\n\t    metadata:\n\t      labels:\n\t        app: nginx\n\t    spec:\n\t      containers:\n\t      - name: nginx\n\t        image: nginx:1.7.9\n\t        ports:\n\t        - containerPort: 80\n在上面这个 YAML 文件中，我们定义了一个 Deployment 对象，它的主体部分（spec.template 部分）是一个使用 Nginx 镜像的 Pod，而这个 Pod 的副本数是 2（replicas=2）。\n\n然后执行：\n\n\t$ kubectl create -f nginx-deployment.yaml\n这样，两个完全相同的 Nginx 容器副本就被启动了。\n\n不过，这么看来，做同样一件事情，Kubernetes 用户要做的工作也不少嘛。\n\n别急，在后续的讲解中，我会陆续介绍 Kubernetes 项目这种“声明式 API”的种种好处，以及基于它实现的强大的编排能力。\n\n拭目以待吧。\n\n**总结**\n首先，我和你一起回顾了容器的核心知识，说明了容器其实可以分为两个部分：容器运行时和容器镜像。\n\n然后，我重点介绍了 Kubernetes 项目的架构，详细讲解了它如何使用“声明式 API”来描述容器化业务和容器间关系的设计思想。\n\n实际上，过去很多的集群管理项目（比如 Yarn、Mesos，以及 Swarm）所擅长的，都是把一个容器，按照某种规则，放置在某个最佳节点上运行起来。这种功能，我们称为“调度”。\n\n而 Kubernetes 项目所擅长的，是按照用户的意愿和整个系统的规则，完全自动化地处理好容器之间的各种关系。**`这种功能，就是我们经常听到的一个概念：编排。`**\n\n所以说，Kubernetes 项目的本质，是为用户提供一个具有普遍意义的容器编排工具。\n\n不过，更重要的是，Kubernetes 项目为用户提供的不仅限于一个工具。它真正的价值，乃在于提供了一套基于容器构建分布式系统的基础依赖。关于这一点，相信你会在今后的学习中，体会的越来越深。\n\n**思考题**\n这今天的分享中，我介绍了 Kubernetes 项目的架构。你是否了解了 Docker Swarm（SwarmKit 项目）跟 Kubernetes 在架构上和使用方法上的异同呢？\n\n在 Kubernetes 之前，很多项目都没办法管理“有状态”的容器，即，不能从一台宿主机“迁移”到另一台宿主机上的容器。你是否能列举出，阻止这种“迁移”的原因都有哪些呢？\n\n\n\n","categories":["reference","k8s"]},{"title":"00 | 开篇词-打通“容器技术”的任督二脉","url":"/2021/03/13/reference/k8s/00.开篇词-打通“容器技术”的任督二脉/","content":"\n![](1.jpg)\n\n2012 年，我还在浙大读书的时候，就有幸组建了一个云计算与 PaaS 基础设施相关的科研团队，就这样，我从早期的 Cloud Foundry 社区开始，正式与容器结缘。\n\n这几年里，我大多数时间都在 Kubernetes 项目里从事上游技术工作，也得以作为一名从业者和社区成员的身份，参与和亲历了容器技术从“初出茅庐”到“尘埃落定”的全过程。\n\n而即使从 2013 年 Docker 项目发布开始算起，这次变革也不过短短 5 年时间，可在现如今的技术圈儿里，不懂容器，没听过 Kubernetes，你还真不好意思跟人打招呼。\n\n容器技术这样一个新生事物，完全重塑了整个云计算市场的形态。它不仅催生出了一批年轻有为的容器技术人，更培育出了一个具有相当规模的开源基础设施技术市场。\n\n在这个市场里，不仅有 Google、Microsoft 等技术巨擘们厮杀至今，更有无数的国内外创业公司前仆后继。而在国内，甚至连以前对开源基础设施领域涉足不多的 BAT、蚂蚁、滴滴这样的巨头们，也都从 AI、云计算、微服务、基础设施等维度多管齐下，争相把容器和 Kubernetes 项目树立为战略重心之一。\n\n就在这场因“容器”而起的技术变革中，Kubernetes 项目已然成为容器技术的事实标准，重新定义了基础设施领域对应用编排与管理的种种可能。\n\n2014 年后，我开始以远程的方式，全职在 Kubernetes 和 Kata Containers 社区从事上游开发工作，先后发起了容器镜像亲密性调度、基于等价类的调度优化等多个核心特性，参与了容器运行时接口、安全容器沙盒等多个基础特性的设计和研发。还有幸作为主要的研发人员和维护者之一，亲历了 Serverless Container 概念的诞生与崛起。\n\n在 2015 年，我发起和组织撰写了《Docker 容器与容器云》一书，希望帮助更多的人利用容器解决实际场景中的问题。时至今日，这本书的第 2 版也已经出版快 2 年了，受到了广大容器技术读者们的好评。\n\n2018 年，我又赴西雅图，在微软研究院（MSR）云计算与存储研究组，专门从事基于 Kubernetes 的深度学习基础设施相关的研究工作。\n\n我与容器打交道的这些年，一直在与关注容器生态的工程师们交流，并经常探讨容器在落地过程中遇到的问题。从这些交流中，我发现总有很多相似的问题被反复提及，比如：\n\n1. 为什么容器里只能跑“一个进程”？\n2. 为什么我原先一直在用的某个 JVM 参数，在容器里就不好使了？\n3. 为什么 Kubernetes 就不能固定 IP 地址？容器网络连不通又该如何去 Debug？\n4. Kubernetes 中 StatefulSet 和 Operator 到底什么区别？PV 和 PVC 这些概念又该怎么用？\n\n这些问题乍一看与我们平常的认知非常矛盾，但它们的答案和原理却并不复杂。不过很遗憾，对于刚刚开始学习容器的技术人员来说，它们却很难用一两句话就能解释清楚。\n\n究其原因在于，从过去以物理机和虚拟机为主体的开发运维环境，向以容器为核心的基础设施的转变过程，并不是一次温和的改革，而是涵盖了对网络、存储、调度、操作系统、分布式原理等各个方面的容器化理解和改造。\n\n这就导致了很多初学者，对于容器技术栈表现出来的这些难题，要么知识储备不足，要么杂乱无章、无法形成体系。这，也是很多初次参与 PaaS 项目的从业者们共同面临的一个困境。\n\n其实，容器技术体系看似纷乱繁杂，却存在着很多可以“牵一发而动全身”的主线。比如，Linux 的进程模型对于容器本身的重要意义；或者，“控制器”模式对整个 Kubernetes 项目提纲挈领的作用。\n\n但是，这些关于 Linux 内核、分布式系统、网络、存储等方方面面的积累，并不会在 Docker 或者 Kubernetes 的文档中交代清楚。可偏偏就是它们，才是真正掌握容器技术体系的精髓所在，是每一位技术从业者需要悉心修炼的“内功”。\n\n而这，也正是我开设这个专栏的初衷。\n\n我希望借由这个专栏，给你讲清楚容器背后的这些技术本质与设计思想，并结合着对核心特性的剖析与实践，加深你对容器技术的理解。为此，我把专栏划分成了 4 大模块：\n\n1. “白话”容器技术基础： 我希望用饶有趣味的解说，给你梳理容器技术生态的发展脉络，用最通俗易懂的语言描述容器底层技术的实现方式，让你知其然，也知其所以然。\n2. Kubernetes 集群的搭建与实践： Kubernetes 集群号称“非常复杂”，但是如果明白了其中的架构和原理，选择了正确的工具和方法，它的搭建却也可以“一键安装”，它的应用部署也可以浅显易懂。\n3. 容器编排与 Kubernetes 核心特性剖析： 这是这个专栏最重要的内容。“编排”永远都是容器云项目的灵魂所在，也是 Kubernetes 社区持久生命力的源泉。在这一模块，我会从分布式系统设计的视角出发，抽象和归纳出这些特性中体现出来的普遍方法，然后带着这些指导思想去逐一阐述 Kubernetes 项目关于编排、调度和作业管理的各项核心特性。“不识庐山真面目，只缘身在此山中”，希望这样一个与众不同的角度，能够给你以全新的启发。\n4. Kubernetes 开源社区与生态：“开源生态”永远都是容器技术和 Kubernetes 项目成功的关键。在这个模块，我会和你一起探讨，容器社区在开源软件工程指导下的演进之路；带你思考，如何同团队一起平衡内外部需求，让自己逐渐成为社区中不可或缺的一员。\n\n我希望通过这些对容器与 Kubernetes 项目的逐层剖析，能够让你面对容器化浪潮时不再踌躇无措，有一种拨云见日的酣畅淋漓。\n\n最后，我想再和你分享一个故事。\n\n2015 年我在 InfoQ 举办的第一届容器技术大会上，结识了当时 CoreOS 的布道师 Kelsey Hightower，他热情地和大家一起安装和体验微信，谈笑风生间，还时不时地安利一番自家产品。\n\n但两年后也就是 2017 年，Kelsey 已经是全世界容器圈儿的意见领袖，是 Google 公司 Kubernetes 项目的首席布道师，而他的座右铭也变为了“只布道，不推销”。此时，就算你漂洋过海想要亲自拜会 Kelsey ，恐怕也得先预约下时间了。\n\n诚然，Kelsey 的“一夜成名”，与他的勤奋和天赋密不可分，但他对这次“容器”变革走向的准确把握却也是功不可没。这也正应了一句名言：一个人的命运啊，当然要靠自我奋斗，但是也要考虑到历史的行程。\n\n眼下，你我可能已经错过了互联网技术大爆炸的时代，也没有在数字货币早期的狂热里分到一杯羹。可就在此时此刻，在沉寂了多年的云计算与基础设施领域，一次以“容器”为名的历史变革，正呼之欲出。这一次，我们又有什么理由作壁上观呢？\n\n如果你也想登上“容器”这趟高速前进的列车，我相信这个专栏，可以帮助你打通学习容器技术的“任督二脉”。在专栏开始，我首先为你准备了 4 篇预习文章，详细地梳理了容器技术自兴起到现在的发展历程，同时也回答了“Kubernetes 为什么会赢”这个重要的问题，算是我额外为你准备的一份开学礼物吧。\n\n机会总是留给有准备的人，现在就让我们一起开启这次充满挑战的容器之旅！\n\n\n\n\n","categories":["reference","k8s"]},{"title":"02 | 预习篇 · 小鲸鱼大事记（二）：崭露头角","url":"/2021/03/13/reference/k8s/02.预习篇-小鲸鱼大事记(二)崭露头角/","content":"\n![](1.jpg)\n\n在上一篇文章中，我说到，伴随着 PaaS 概念的逐步普及，以 Cloud Foundry 为代表的经典 PaaS 项目，开始进入基础设施领域的视野，平台化和 PaaS 化成了这个生态中的一个最为重要的进化趋势。\n\n就在对开源 PaaS 项目落地的不断尝试中，这个领域的从业者们发现了 PaaS 中最为棘手也最亟待解决的一个问题：究竟如何给应用打包？\n\n遗憾的是，无论是 Cloud Foundry、OpenShift，还是 Clodify，面对这个问题都没能给出一个完美的答案，反而在竞争中走向了碎片化的歧途。\n\n而就在这时，一个并不引人瞩目的 PaaS 创业公司 dotCloud，却选择了开源自家的一个容器项目 Docker。更出人意料的是，**就是这样一个普通到不能再普通的技术，却开启了一个名为“Docker”的全新时代。**\n\n你可能会有疑问，Docker 项目的崛起，是不是偶然呢？\n\n事实上，**这个以“鲸鱼”为注册商标的技术创业公司，最重要的战略之一就是：坚持把“开发者”群体放在至高无上的位置。**\n\n相比于其他正在企业级市场里厮杀得头破血流的经典 PaaS 项目们，Docker 项目的推广策略从一开始就呈现出一副“憨态可掬”的亲人姿态，把每一位后端技术人员（而不是他们的老板）作为主要的传播对象。\n\n简洁的 UI，有趣的 demo，“1 分钟部署一个 WordPress 网站”“3 分钟部署一个 Nginx 集群”，这种同开发者之间与生俱来的亲近关系，使 Docker 项目迅速成为了全世界 Meetup 上最受欢迎的一颗新星。\n\n在过去的很长一段时间里，相较于前端和互联网技术社区，服务器端技术社区一直是一个相对沉闷而小众的圈子。在这里，从事 Linux 内核开发的极客们自带“不合群”的“光环”，后端开发者们啃着多年不变的 TCP/IP 发着牢骚，运维更是天生注定的幕后英雄。\n\n而 Docker 项目，却给后端开发者提供了走向聚光灯的机会。就比如 Cgroups 和 Namespace 这种已经存在多年却很少被人们关心的特性，在 2014 年和 2015 年竟然频繁入选各大技术会议的分享议题，就因为听众们想要知道 Docker 这个东西到底是怎么一回事儿。\n\n**而 Docker 项目之所以能取得如此高的关注，一方面正如前面我所说的那样，它解决了应用打包和发布这一困扰运维人员多年的技术难题；而另一方面，就是因为它第一次把一个纯后端的技术概念，通过非常友好的设计和封装，交到了最广大的开发者群体手里。**\n\n在这种独特的氛围烘托下，你不需要精通 TCP/IP，也无需深谙 Linux 内核原理，哪怕只是一个前端或者网站的 PHP 工程师，都会对如何把自己的代码打包成一个随处可以运行的 Docker 镜像充满好奇和兴趣。\n\n这种受众群体的变革，正是 Docker 这样一个后端开源项目取得巨大成功的关键。这也是经典 PaaS 项目想做却没有做好的一件事情：PaaS 的最终用户和受益者，一定是为这个 PaaS 编写应用的开发者们，而在 Docker 项目开源之前，PaaS 与开发者之间的关系却从未如此紧密过。\n\n**解决了应用打包这个根本性的问题，同开发者与生俱来的的亲密关系，再加上 PaaS 概念已经深入人心的完美契机，成为 Docker 这个技术上看似平淡无奇的项目一举走红的重要原因。**\n\n一时之间，“容器化”取代“PaaS 化”成为了基础设施领域最炙手可热的关键词，一个以“容器”为中心的、全新的云计算市场，正呼之欲出。而作为这个生态的一手缔造者，此时的 dotCloud 公司突然宣布将公司名称改为“Docker”。\n\n这个举动，在当时颇受质疑。在大家印象中，Docker 只是一个开源项目的名字。可是现在，这个单词却成了 Docker 公司的注册商标，任何人在商业活动中使用这个单词，以及鲸鱼的 Logo，都会立刻受到法律警告。\n\n那么，Docker 公司这个举动到底卖的什么药？这个问题，我不妨后面再做解读，因为相较于这件“小事儿”，Docker 公司在 2014 年发布 Swarm 项目才是真正的“大事儿”。\n\n那么，Docker 公司为什么一定要发布 Swarm 项目呢？\n\n通过我对 Docker 项目崛起背后原因的分析，你应该能发现这样一个有意思的事实：虽然通过“容器”这个概念完成了对经典 PaaS 项目的“降维打击”，但是 Docker 项目和 Docker 公司，兜兜转转了一年多，却还是回到了 PaaS 项目原本深耕了多年的那个战场：**如何让开发者把应用部署在我的项目上。**\n\n没错，Docker 项目从发布之初就全面发力，从技术、社区、商业、市场全方位争取到的开发者群体，实际上是为此后吸引整个生态到自家“PaaS”上的一个铺垫。**只不过这时，“PaaS”的定义已经全然不是 Cloud Foundry 描述的那个样子，而是变成了一套以 Docker 容器为技术核心，以 Docker 镜像为打包标准的、全新的“容器化”思路。**\n\n**这，正是 Docker 项目从一开始悉心运作“容器化”理念和经营整个 Docker 生态的主要目的。**\n\n而 Swarm 项目，正是接下来承接 Docker 公司所有这些努力的关键所在。\n\n总结\n今天，我着重介绍了 Docker 项目在短时间内迅速崛起的三个重要原因：\n\n1. Docker 镜像通过技术手段解决了 PaaS 的根本性问题；\n\n2. Docker 容器同开发者之间有着与生俱来的密切关系；\n\n3. PaaS 概念已经深入人心的完美契机。\n\n崭露头角的 Docker 公司，也终于能够以一个更加强硬的姿态来面对这个曾经无比强势，但现在却完全不知所措的云计算市场。而 2014 年底的 DockerCon 欧洲峰会，则正式拉开了 Docker 公司扩张的序幕。\n\n思考题\n1. 你是否认同 dotCloud 公司改名并开启扩张道路的战略选择？\n\n2. Docker 公司凭借“开源”和“开发者社群”这两个关键词完成崛起的过程，对你和你所在的团队有什么启发？\n","categories":["reference","k8s"]},{"title":"01 | 预习篇 · 小鲸鱼大事记（一）：初出茅庐","url":"/2021/03/13/reference/k8s/01.预习篇-小鲸鱼大事记(一)初出茅庐/","content":"\n![](1.jpg)\n\n如果我问你，现今最热门的服务器端技术是什么？想必你不假思索就能回答上来：当然是容器！可是，如果现在不是 2018 年而是 2013 年，你的回答还能这么斩钉截铁么？\n\n现在就让我们把时间拨回到五年前去看看吧。\n\n2013 年的后端技术领域，已经太久没有出现过令人兴奋的东西了。曾经被人们寄予厚望的云计算技术，也已经从当初虚无缥缈的概念蜕变成了实实在在的虚拟机和账单。而相比于的如日中天 AWS 和盛极一时的 OpenStack，以 Cloud Foundry 为代表的开源 PaaS 项目，却成为了当时云计算技术中的一股清流。\n\n这时，Cloud Foundry 项目已经基本度过了最艰难的概念普及和用户教育阶段，吸引了包括百度、京东、华为、IBM 等一大批国内外技术厂商，开启了以开源 PaaS 为核心构建平台层服务能力的变革。如果你有机会问问当时的云计算从业者们，他们十有八九都会告诉你：PaaS 的时代就要来了！\n\n这个说法其实一点儿没错，如果不是后来一个叫 Docker 的开源项目突然冒出来的话。\n\n事实上，当时还名叫 dotCloud 的 Docker 公司，也是这股 PaaS 热潮中的一份子。只不过相比于 Heroku、Pivotal、Red Hat 等 PaaS 弄潮儿们，dotCloud 公司实在是太微不足道了，而它的主打产品由于跟主流的 Cloud Foundry 社区脱节，长期以来也无人问津。眼看就要被如火如荼的 PaaS 风潮抛弃，dotCloud 公司却做出了这样一个决定：开源自己的容器项目 Docker。\n\n显然，这个决定在当时根本没人在乎。\n\n“容器”这个概念从来就不是什么新鲜的东西，也不是 Docker 公司发明的。即使在当时最热门的 PaaS 项目 Cloud Foundry 中，容器也只是其最底层、最没人关注的那一部分。说到这里，我正好以当时的事实标准 Cloud Foundry 为例，来解说一下 PaaS 技术。\n\nPaaS 项目被大家接纳的一个主要原因，就是它提供了一种名叫“应用托管”的能力。 在当时，虚拟机和云计算已经是比较普遍的技术和服务了，那时主流用户的普遍用法，就是租一批 AWS 或者 OpenStack 的虚拟机，然后像以前管理物理服务器那样，用脚本或者手工的方式在这些机器上部署应用。\n\n当然，这个部署过程难免会碰到云端虚拟机和本地环境不一致的问题，所以当时的云计算服务，比的就是谁能更好地模拟本地服务器环境，能带来更好的“上云”体验。而 PaaS 开源项目的出现，就是当时解决这个问题的一个最佳方案。\n\n举个例子，虚拟机创建好之后，运维人员只需要在这些机器上部署一个 Cloud Foundry 项目，然后开发者只要执行一条命令就能把本地的应用部署到云上，这条命令就是：\n\n\t$ cf push \" 我的应用 \"\n是不是很神奇？\n\n事实上，**像 Cloud Foundry 这样的 PaaS 项目，最核心的组件就是一套应用的打包和分发机制。** Cloud Foundry 为每种主流编程语言都定义了一种打包格式，而“cf push”的作用，基本上等同于用户把应用的可执行文件和启动脚本打进一个压缩包内，上传到云上 Cloud Foundry 的存储中。接着，Cloud Foundry 会通过调度器选择一个可以运行这个应用的虚拟机，然后通知这个机器上的 Agent 把应用压缩包下载下来启动。\n\n这时候关键来了，由于需要在一个虚拟机上启动很多个来自不同用户的应用，Cloud Foundry 会调用操作系统的 Cgroups 和 Namespace 机制为每一个应用单独创建一个称作“沙盒”的隔离环境，然后在“沙盒”中启动这些应用进程。这样，就实现了把多个用户的应用互不干涉地在虚拟机里批量地、自动地运行起来的目的。\n\n**这，正是 PaaS 项目最核心的能力。** 而这些 Cloud Foundry 用来运行应用的隔离环境，或者说“沙盒”，就是所谓的“容器”。\n\n而 Docker 项目，实际上跟 Cloud Foundry 的容器并没有太大不同，所以在它发布后不久，Cloud Foundry 的首席产品经理 James Bayer 就在社区里做了一次详细对比，告诉用户 Docker 实际上只是一个同样使用 Cgroups 和 Namespace 实现的“沙盒”而已，没有什么特别的黑科技，也不需要特别关注。\n\n然而，短短几个月，Docker 项目就迅速崛起了。它的崛起速度如此之快，以至于 Cloud Foundry 以及所有的 PaaS 社区还没来得及成为它的竞争对手，就直接被宣告出局了。那时候，一位多年的 PaaS 从业者曾经如此感慨道：这简直就是一场“降维打击”啊。\n\n难道这一次，连闯荡多年的“老江湖”James Bayer 也看走眼了么？\n\n并没有。\n\n事实上，Docker 项目确实与 Cloud Foundry 的容器在大部分功能和实现原理上都是一样的，可偏偏就是这剩下的一小部分不一样的功能，成了 Docker 项目接下来“呼风唤雨”的不二法宝。\n\n**这个功能，就是 Docker 镜像。**\n\n恐怕连 Docker 项目的作者 Solomon Hykes 自己当时都没想到，这个小小的创新，在短短几年内就如此迅速地改变了整个云计算领域的发展历程。\n\n我前面已经介绍过，PaaS 之所以能够帮助用户大规模部署应用到集群里，是因为它提供了一套应用打包的功能。可偏偏就是这个打包功能，却成了 PaaS 日后不断遭到用户诟病的一个“软肋”。\n\n出现这个问题的根本原因是，一旦用上了 PaaS，用户就必须为每种语言、每种框架，甚至每个版本的应用维护一个打好的包。这个打包过程，没有任何章法可循，更麻烦的是，明明在本地运行得好好的应用，却需要做很多修改和配置工作才能在 PaaS 里运行起来。而这些修改和配置，并没有什么经验可以借鉴，基本上得靠不断试错，直到你摸清楚了本地应用和远端 PaaS 匹配的“脾气”才能够搞定。\n\n最后结局就是，“cf push”确实是能一键部署了，但是为了实现这个一键部署，用户为每个应用打包的工作可谓一波三折，费尽心机。\n\n而**Docker 镜像解决的，恰恰就是打包这个根本性的问题。** 所谓 Docker 镜像，其实就是一个压缩包。但是这个压缩包里的内容，比 PaaS 的应用可执行文件 + 启停脚本的组合就要丰富多了。实际上，大多数 Docker 镜像是直接由一个完整操作系统的所有文件和目录构成的，所以这个压缩包里的内容跟你本地开发和测试环境用的操作系统是完全一样的。\n\n这就有意思了：假设你的应用在本地运行时，能看见的环境是 CentOS 7.2 操作系统的所有文件和目录，那么只要用 CentOS 7.2 的 ISO 做一个压缩包，再把你的应用可执行文件也压缩进去，那么无论在哪里解压这个压缩包，都可以得到与你本地测试时一样的环境。当然，你的应用也在里面！\n\n这就是 Docker 镜像最厉害的地方：只要有这个压缩包在手，你就可以使用某种技术创建一个“沙盒”，在“沙盒”中解压这个压缩包，然后就可以运行你的程序了。\n\n更重要的是，这个压缩包包含了完整的操作系统文件和目录，也就是包含了这个应用运行所需要的所有依赖，所以你可以先用这个压缩包在本地进行开发和测试，完成之后，再把这个压缩包上传到云端运行。\n\n在这个过程中，你完全不需要进行任何配置或者修改，因为这个压缩包赋予了你一种极其宝贵的能力：**本地环境和云端环境的高度一致！**\n\n这，正是 Docker 镜像的精髓。\n\n那么，有了 Docker 镜像这个利器，PaaS 里最核心的打包系统一下子就没了用武之地，最让用户抓狂的打包过程也随之消失了。相比之下，在当今的互联网里，Docker 镜像需要的操作系统文件和目录，可谓唾手可得。\n\n所以，你只需要提供一个下载好的操作系统文件与目录，然后使用它制作一个压缩包即可，这个命令就是：\n\n\t$ docker build \" 我的镜像 \"\n一旦镜像制作完成，用户就可以让 Docker 创建一个“沙盒”来解压这个镜像，然后在“沙盒”中运行自己的应用，这个命令就是：\n\n\t$ docker run \" 我的镜像 \"\n当然，docker run 创建的“沙盒”，也是使用 Cgroups 和 Namespace 机制创建出来的隔离环境。我会在后面的文章中，详细介绍这个机制的实现原理。\n\n所以，**Docker 项目给 PaaS 世界带来的“降维打击”，其实是提供了一种非常便利的打包机制。这种机制直接打包了应用运行所需要的整个操作系统，从而保证了本地环境和云端环境的高度一致，避免了用户通过“试错”来匹配两种不同运行环境之间差异的痛苦过程。**\n\n而对于开发者们来说，在终于体验到了生产力解放所带来的痛快之后，他们自然选择了用脚投票，直接宣告了 PaaS 时代的结束。\n\n不过，Docker 项目固然解决了应用打包的难题，但正如前面所介绍的那样，它并不能代替 PaaS 完成大规模部署应用的职责。\n\n遗憾的是，考虑到 Docker 公司是一个与自己有潜在竞争关系的商业实体，再加上对 Docker 项目普及程度的错误判断，Cloud Foundry 项目并没有第一时间使用 Docker 作为自己的核心依赖，去替换自己那套饱受诟病的打包流程。\n\n反倒是一些机敏的创业公司，纷纷在第一时间推出了 Docker 容器集群管理的开源项目（比如 Deis 和 Flynn），它们一般称自己为 CaaS，即 Container-as-a-Service，用来跟“过时”的 PaaS 们划清界限。\n\n而在 2014 年底的 DockerCon 上，Docker 公司雄心勃勃地对外发布了自家研发的“Docker 原生”容器集群管理项目 Swarm，不仅将这波“CaaS”热推向了一个前所未有的高潮，更是寄托了整个 Docker 公司重新定义 PaaS 的宏伟愿望。\n\n在 2014 年的这段巅峰岁月里，Docker 公司离自己的理想真的只有一步之遥。\n\n总结\n2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。\n\nDocker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。这正是 Docker 项目刚刚开源不久，就能够带领一家原本默默无闻的 PaaS 创业公司脱颖而出，然后迅速占领了所有云计算领域头条的技术原因。\n\n而在成为了基础设施领域近十年难得一见的技术明星之后，dotCloud 公司则在 2013 年底大胆改名为 Docker 公司。不过，这个在当时就颇具争议的改名举动，也成为了日后容器技术圈风云变幻的一个关键伏笔。\n\n思考题\n你是否曾经研发过类似 PaaS 的项目？你碰到过应用打包的问题吗，又是如何解决的呢？\n\n感谢收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。","categories":["reference","k8s"]},{"title":"03 | 预习篇 · 小鲸鱼大事记（三）：群雄并起","url":"/2021/03/13/reference/k8s/03.预习篇-小鲸鱼大事记(三)群雄并起/","content":"\n![](1.jpg)\n\n在上一篇文章中，我剖析了 Docker 项目迅速走红背后的技术与非技术原因，也介绍了 Docker 公司开启平台化战略的野心。可是，Docker 公司为什么在 Docker 项目已经取得巨大成功之后，却执意要重新走回那条已经让无数先驱们尘沙折戟的 PaaS 之路呢？\n\n实际上，Docker 项目一日千里的发展势头，一直伴随着公司管理层和股东们的阵阵担忧。他们心里明白，虽然 Docker 项目备受追捧，但用户们最终要部署的，还是他们的网站、服务、数据库，甚至是云计算业务。\n\n这就意味着，只有那些能够为用户提供平台层能力的工具，才会真正成为开发者们关心和愿意付费的产品。而 Docker 项目这样一个只能用来创建和启停容器的小工具，最终只能充当这些平台项目的“幕后英雄”。\n\n而谈到 Docker 项目的定位问题，就不得不说说 Docker 公司的老朋友和老对手 CoreOS 了。\n\nCoreOS 是一个基础设施领域创业公司。 它的核心产品是一个定制化的操作系统，用户可以按照分布式集群的方式，管理所有安装了这个操作系统的节点。从而，用户在集群里部署和管理应用就像使用单机一样方便了。\n\nDocker 项目发布后，CoreOS 公司很快就认识到可以把“容器”的概念无缝集成到自己的这套方案中，从而为用户提供更高层次的 PaaS 能力。所以，CoreOS 很早就成了 Docker 项目的贡献者，并在短时间内成为了 Docker 项目中第二重要的力量。\n\n然而，这段短暂的蜜月期到 2014 年底就草草结束了。CoreOS 公司以强烈的措辞宣布与 Docker 公司停止合作，并直接推出了自己研制的 Rocket（后来叫 rkt）容器。\n\n这次决裂的根本原因，正是源于 Docker 公司对 Docker 项目定位的不满足。Docker 公司解决这种不满足的方法就是，让 Docker 项目提供更多的平台层能力，即向 PaaS 项目进化。而这，显然与 CoreOS 公司的核心产品和战略发生了严重冲突。\n\n也就是说，Docker 公司在 2014 年就已经定好了平台化的发展方向，并且绝对不会跟 CoreOS 在平台层面开展任何合作。这样看来，Docker 公司在 2014 年 12 月的 DockerCon 上发布 Swarm 的举动，也就一点都不突然了。\n\n相较于 CoreOS 是依托于一系列开源项目（比如 Container Linux 操作系统、Fleet 作业调度工具、systemd 进程管理和 rkt 容器），一层层搭建起来的平台产品，Swarm 项目则是以一个完整的整体来对外提供集群管理功能。而 Swarm 的最大亮点，则是它完全使用 Docker 项目原本的容器管理 API 来完成集群管理，比如：\n\n单机 Docker 项目：\n\n\t$ docker run \" 我的容器\n多机 Docker 项目：\n\n\t$ docker run -H \" 我的 Swarm 集群 API 地址 \" \" 我的容器 \"\n所以在部署了 Swarm 的多机环境下，用户只需要使用原先的 Docker 指令创建一个容器，这个请求就会被 Swarm 拦截下来处理，然后通过具体的调度算法找到一个合适的 Docker Daemon 运行起来。\n\n这个操作方式简洁明了，对于已经了解过 Docker 命令行的开发者们也很容易掌握。所以，这样一个“原生”的 Docker 容器集群管理项目一经发布，就受到了已有 Docker 用户群的热捧。而相比之下，CoreOS 的解决方案就显得非常另类，更不用说用户还要去接受完全让人摸不着头脑、新造的容器项目 rkt 了。\n\n当然，Swarm 项目只是 Docker 公司重新定义“PaaS”的关键一环而已。在 2014 年到 2015 年这段时间里，Docker 项目的迅速走红催生出了一个非常繁荣的“Docker 生态”。在这个生态里，围绕着 Docker 在各个层次进行集成和创新的项目层出不穷。\n\n而此时已经大红大紫到“不差钱”**的Docker 公司，开始及时地借助这波浪潮通过并购来完善自己的平台层能力。**其中一个最成功的案例，莫过于对 Fig 项目的收购。\n\n要知道，Fig 项目基本上只是靠两个人全职开发和维护的，可它却是当时 GitHub 上热度堪比 Docker 项目的明星。\n\n**Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念。**\n\n其实，“编排”（Orchestration）在云计算行业里不算是新词汇，它主要是指用户如何通过某些工具或者配置来完成一组虚拟机以及关联资源的定义、配置、创建、删除等工作，然后由云计算平台按照这些指定的逻辑来完成的过程。\n\n而容器时代，“编排”显然就是对 Docker 容器的一系列定义、配置和创建动作的管理。而 Fig 的工作实际上非常简单：假如现在用户需要部署的是应用容器 A、数据库容器 B、负载均衡容器 C，那么 Fig 就允许用户把 A、B、C 三个容器定义在一个配置文件中，并且可以指定它们之间的关联关系，比如容器 A 需要访问数据库容器 B。\n\n接下来，你只需要执行一条非常简单的指令：\n\n\t$ fig up\nFig 就会把这些容器的定义和配置交给 Docker API 按照访问逻辑依次创建，你的一系列容器就都启动了；而容器 A 与 B 之间的关联关系，也会交给 Docker 的 Link 功能通过写入 hosts 文件的方式进行配置。更重要的是，你还可以在 Fig 的配置文件里定义各种容器的副本个数等编排参数，再加上 Swarm 的集群管理能力，一个活脱脱的 PaaS 呼之欲出。\n\n**Fig 项目被收购后改名为 Compose，它成了 Docker 公司到目前为止第二大受欢迎的项目，一直到今天也依然被很多人使用。**\n\n当时的这个容器生态里，还有很多令人眼前一亮的开源项目或公司。比如，专门负责处理容器网络的 SocketPlane 项目（后来被 Docker 公司收购），专门负责处理容器存储的 Flocker 项目（后来被 EMC 公司收购），专门给 Docker 集群做图形化管理界面和对外提供云服务的 Tutum 项目（后来被 Docker 公司收购）等等。\n\n一时之间，整个后端和云计算领域的聪明才俊都汇集在了这个“小鲸鱼”的周围，为 Docker 生态的蓬勃发展献上了自己的智慧。\n\n而除了这个异常繁荣的、围绕着 Docker 项目和公司的生态之外，还有一个势力在当时也是风头无两，这就是老牌集群管理项目 Mesos 和它背后的创业公司 Mesosphere。\n\nMesos 作为 Berkeley 主导的大数据套件之一，是大数据火热时最受欢迎的资源管理项目，也是跟 Yarn 项目杀得难舍难分的实力派选手。\n\n不过，**大数据所关注的计算密集型离线业务，其实并不像常规的 Web 服务那样适合用容器进行托管和扩容，也没有对应用打包的强烈需求，所以 Hadoop、Spark 等项目到现在也没在容器技术上投下更大的赌注.** 但是对于 Mesos 来说，天生的两层调度机制让它非常容易从大数据领域抽身，转而去支持受众更加广泛的 PaaS 业务。\n\n在这种思路的指导下，Mesosphere 公司发布了一个名为 Marathon 的项目，而这个项目很快就成为了 Docker Swarm 的一个有力竞争对手。\n\n**虽然不能提供像 Swarm 那样的原生 Docker API，Mesos 社区却拥有一个独特的竞争力：超大规模集群的管理经验。**\n\n早在几年前，Mesos 就已经通过了万台节点的验证，2014 年之后又被广泛使用在 eBay 等大型互联网公司的生产环境中。而这次通过 Marathon 实现了诸如应用托管和负载均衡的 PaaS 功能之后，Mesos+Marathon 的组合实际上进化成了一个高度成熟的 PaaS 项目，同时还能很好地支持大数据业务。\n\n所以，在这波容器化浪潮中，Mesosphere 公司不失时机地提出了一个名叫“DC/OS”（数据中心操作系统）的口号和产品，旨在使用户能够像管理一台机器那样管理一个万级别的物理机集群，并且使用 Docker 容器在这个集群里自由地部署应用。而这，对很多大型企业来说具有着非同寻常的吸引力。\n\n这时，如果你再去审视当时的容器技术生态，就不难发现 CoreOS 公司竟然显得有些尴尬了。它的 rkt 容器完全打不开局面，Fleet 集群管理项目更是少有人问津，CoreOS 完全被 Docker 公司压制了。\n\n而处境同样不容乐观的似乎还有 RedHat，作为 Docker 项目早期的重要贡献者，RedHat 也是因为对 Docker 公司平台化战略不满而愤愤退出。但此时，它竟只剩下 OpenShift 这个跟 Cloud Foundry 同时代的经典 PaaS 一张牌可以打，跟 Docker Swarm 和转型后的 Mesos 完全不在同一个“竞技水平”之上。\n\n那么，事实果真如此吗？\n\n2014 年注定是一个神奇的年份。就在这一年的 6 月，**基础设施领域的翘楚 Google 公司突然发力，正式宣告了一个名叫 Kubernetes 项目的诞生。而这个项目，不仅挽救了当时的 CoreOS 和 RedHat，还如同当年 Docker 项目的横空出世一样，再一次改变了整个容器市场的格局。**\n\n总结\n我分享了 Docker 公司平台化战略的来龙去脉，阐述了 Docker Swarm 项目发布的意义和它背后的设计思想，介绍了 Fig（后来的 Compose）项目如何成为了继 Docker 之后最受瞩目的新星。\n\n同时，我也和你一起回顾了 2014~2015 年间如火如荼的容器化浪潮里群雄并起的繁荣姿态。在这次生态大爆发中，Docker 公司和 Mesosphere 公司，依托自身优势率先占据了有利位置。\n\n但是，更强大的挑战者们，即将在不久后纷至沓来。\n\n思考题\n你所在团队有没有在 2014~2015 年 Docker 热潮中，推出过相关的容器产品或者项目？现在结局如何呢？","categories":["reference","k8s"]},{"title":"05 | 白话容器基础（一）：从进程说开去","url":"/2021/03/13/reference/k8s/05.白话容器基础(一)从进程说开去/","content":"\n![](2.jpg)\n\n在前面的 4 篇预习文章中，我梳理了“容器”这项技术的来龙去脉，通过这些内容，我希望你能理解如下几个事实：\n\n * 容器技术的兴起源于 PaaS 技术的普及；\n * Docker 公司发布的 Docker 项目具有里程碑式的意义；\n * Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。\n紧接着，我详细介绍了容器技术圈在过去五年里的“风云变幻”，而通过这部分内容，我希望你能理解这样一个道理：\n\n>容器本身没有价值，有价值的是“容器编排”。\n\n也正因为如此，容器技术生态才爆发了一场关于“容器编排”的“战争”。**`而这次战争，最终以 Kubernetes 项目和 CNCF 社区的胜利而告终。`**所以，这个专栏后面的内容，我会以 Docker 和 Kubernetes 项目为核心，为你详细介绍容器技术的各项实践与其中的原理。\n\n不过在此之前，你还需要搞清楚一个更为基础的问题：\n\n>容器，到底是怎么一回事儿？\n\n在第一篇预习文章《小鲸鱼大事记（一）：初出茅庐》中，我已经提到过，容器其实是一种沙盒技术。顾名思义，沙盒就是能够像一个集装箱一样，把你的应用“装”起来的技术。这样，应用与应用之间，就因为有了边界而不至于相互干扰；而被装进集装箱的应用，也可以被方便地搬来搬去，这不就是 PaaS 最理想的状态嘛。\n\n不过，这两个能力说起来简单，但要用技术手段去实现它们，可能大多数人就无从下手了。\n\n所以，我就先来跟你说说这个“边界”的实现手段。\n\n假如，现在你要写一个计算加法的小程序，这个程序需要的输入来自于一个文件，计算完成后的结果则输出到另一个文件中。\n\n由于计算机只认识 0 和 1，所以无论用哪种语言编写这段代码，最后都需要通过某种方式翻译成二进制文件，才能在计算机操作系统中运行起来。\n\n而为了能够让这些代码正常运行，我们往往还要给它提供数据，比如我们这个加法程序所需要的输入文件。这些数据加上代码本身的二进制文件，放在磁盘上，就是我们平常所说的一个“程序”，也叫代码的可执行镜像（executable image）。\n\n然后，我们就可以在计算机上运行这个“程序”了。\n\n首先，操作系统从“程序”中发现输入数据保存在一个文件中，所以这些数据就被会加载到内存中待命。同时，操作系统又读取到了计算加法的指令，这时，它就需要指示 CPU 完成加法操作。而 CPU 与内存协作进行加法计算，又会使用寄存器存放数值、内存堆栈保存执行的命令和变量。同时，计算机里还有被打开的文件，以及各种各样的 I/O 设备在不断地调用中修改自己的状态。\n\n就这样，一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。`像这样一个程序运起来后的计算机执行环境的总和，就是我们今天的主角：进程。`\n\n所以，对于进程来说，它的静态表现就是程序，平常都安安静静地待在磁盘上；而一旦运行起来，它就变成了计算机里的数据和状态的总和，这就是它的动态表现。\n\n而`容器技术的核心功能，就是通过约束和修改进程的动态表现，从而为其创造出一个“边界”。`\n\n对于 Docker 等大多数 Linux 容器来说，`Cgroups` 技术是用来制造约束的主要手段，而 `Namespace` 技术则是用来修改进程视图的主要方法。\n\n你可能会觉得 Cgroups 和 Namespace 这两个概念很抽象，别担心，接下来我们一起动手实践一下，你就很容易理解这两项技术了。\n\n假设你已经有了一个 Linux 操作系统上的 Docker 项目在运行，比如我的环境是 Ubuntu 16.04 和 Docker CE 18.05。\n\n接下来，让我们首先创建一个容器来试试。\n\n\t$ docker run -it busybox /bin/sh\n\t/ #\n这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。\n\n而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。\n\n所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。\n\n这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。\n\n上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情：\n\n\t/ # ps\n\tPID  USER   TIME COMMAND\n\t  1  root   0:00 /bin/sh\n\t  10 root   0:00 ps\n可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。\n\n这究竟是怎么做到呢？\n\n本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。\n\n而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。\n\n`这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。`\n\n`这种技术，就是 Linux 里面的 Namespace 机制。`而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如：\n\n\tint pid = clone(main_function, stack_size, SIGCHLD, NULL); \n这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。\n\n而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如：\n\n\tint pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); \n这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。\n\n当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。\n\n而`除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。`\n\n比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。\n\n这，`就是 Linux 容器最基本的实现原理了。`\n\n所以，`Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。`这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。\n\n所以说，`容器，其实是一种特殊的进程而已。`\n\n**总结**\n谈到为“进程划分一个独立空间”的思想，相信你一定会联想到虚拟机。而且，你应该还看过一张虚拟机和容器的对比图。\n![](1.jpg)\n\n\n这幅图的左边，画出了虚拟机的工作原理。其中，名为 Hypervisor 的软件是虚拟机最主要的部分。它通过硬件虚拟化功能，模拟出了运行一个操作系统需要的各种硬件，比如 CPU、内存、I/O 设备等等。然后，它在这些虚拟的硬件上安装了一个新的操作系统，即 Guest OS。\n\n这样，用户的应用进程就可以运行在这个虚拟的机器中，它能看到的自然也只有 Guest OS 的文件和目录，以及这个机器里的虚拟设备。这就是为什么虚拟机也能起到将不同的应用进程相互隔离的作用。\n\n而这幅图的右边，则用一个名为 Docker Engine 的软件替换了 Hypervisor。这也是为什么，很多人会把 Docker 项目称为“轻量级”虚拟化技术的原因，实际上就是把虚拟机的概念套在了容器上。\n\n可是这样的说法，却并不严谨。\n\n在理解了 Namespace 的工作方式之后，你就会明白，跟真实存在的虚拟机不同，在使用 Docker 的时候，并没有一个真正的“Docker 容器”运行在宿主机里面。Docker 项目帮助用户启动的，还是原来的应用进程，只不过在创建这些进程时，Docker 为它们加上了各种各样的 Namespace 参数。\n\n这时，这些进程就会觉得自己是各自 PID Namespace 里的第 1 号进程，只能看到各自 Mount Namespace 里挂载的目录和文件，只能访问到各自 Network Namespace 里的网络设备，就仿佛运行在一个个“容器”里面，与世隔绝。\n\n不过，相信你此刻已经会心一笑：这些不过都是“障眼法”罢了。\n\n**思考题**\n鉴于我对容器本质的讲解，你觉得上面这张容器和虚拟机对比图右侧关于容器的部分，怎么画才更精确？\n\n你是否知道最新的 Docker 项目默认会为容器启用哪些 Namespace 吗？\n\n感谢你的收听，欢迎你给我留言，也欢迎分享给更多的朋友一起阅读。","categories":["reference","k8s"]},{"title":"04 | 预习篇 · 小鲸鱼大事记（四）：尘埃落定","url":"/2021/03/13/reference/k8s/04.预习篇-小鲸鱼大事记(四)尘埃落定/","content":"\n![](1.jpg)\n\n在上一次的分享中我提到，伴随着 Docker 公司一手打造出来的容器技术生态在云计算市场中站稳了脚跟，围绕着 Docker 项目进行的各个层次的集成与创新产品，也如雨后春笋般出现在这个新兴市场当中。而 Docker 公司，不失时机地发布了 Docker Compose、Swarm 和 Machine“三件套”，在重新定义 PaaS 的方向上走出了最关键的一步。\n\n这段时间，也正是 Docker 生态创业公司们的春天，大量围绕着 Docker 项目的网络、存储、监控、CI/CD，甚至 UI 项目纷纷出台，也涌现出了很多 Rancher、Tutum 这样在开源与商业上均取得了巨大成功的创业公司。\n\n在 2014~2015 年间，整个容器社区可谓热闹非凡。\n\n这令人兴奋的繁荣背后，却浮现出了更多的担忧。这其中最主要的负面情绪，是对 Docker 公司商业化战略的种种顾虑。\n\n事实上，很多从业者也都看得明白，Docker 项目此时已经成为 Docker 公司一个商业产品。而开源，只是 Docker 公司吸引开发者群体的一个重要手段。不过这么多年来，开源社区的商业化其实都是类似的思路，无非是高不高调、心不心急的问题罢了。\n\n而真正令大多数人不满意的是，Docker 公司在 Docker 开源项目的发展上，始终保持着绝对的权威和发言权，并在多个场合用实际行动挑战到了其他玩家（比如，CoreOS、RedHat，甚至谷歌和微软）的切身利益。\n\n那么，这个时候，大家的不满也就不再是在 GitHub 上发发牢骚这么简单了。\n\n相信很多容器领域的老玩家们都听说过，Docker 项目刚刚兴起时，Google 也开源了一个在内部使用多年、经历过生产环境验证的 Linux 容器：lmctfy（Let Me Container That For You）。\n\n然而，面对 Docker 项目的强势崛起，这个对用户没那么友好的 Google 容器项目根本没有招架之力。所以，知难而退的 Google 公司，向 Docker 公司表示了合作的愿望：关停这个项目，和 Docker 公司共同推进一个中立的容器运行时（container runtime）库作为 Docker 项目的核心依赖。\n\n不过，Docker 公司并没有认同这个明显会削弱自己地位的提议，还在不久后，自己发布了一个容器运行时库 Libcontainer。这次匆忙的、由一家主导的、并带有战略性考量的重构，成了 Libcontainer 被社区长期诟病代码可读性差、可维护性不强的一个重要原因。\n\n至此，Docker 公司在容器运行时层面上的强硬态度，以及 Docker 项目在高速迭代中表现出来的不稳定和频繁变更的问题，开始让社区叫苦不迭。\n\n这种情绪在 2015 年达到了一个小高潮，容器领域的其他几位玩家开始商议“切割”Docker 项目的话语权。而“切割”的手段也非常经典，那就是成立一个中立的基金会。\n\n于是，2015 年 6 月 22 日，由 Docker 公司牵头，CoreOS、Google、RedHat 等公司共同宣布，Docker 公司将 Libcontainer 捐出，并改名为 RunC 项目，交由一个完全中立的基金会管理，然后以 RunC 为依据，大家共同制定一套容器和镜像的标准和规范。\n\n这套标准和规范，**就是 OCI（ Open Container Initiative ）。OCI 的提出，意在将容器运行时和镜像的实现从 Docker 项目中完全剥离出来。**这样做，一方面可以改善 Docker 公司在容器技术上一家独大的现状，另一方面也为其他玩家不依赖于 Docker 项目构建各自的平台层能力提供了可能。\n\n不过，不难看出，OCI 的成立更多的是这些容器玩家出于自身利益进行干涉的一个妥协结果。所以，尽管 Docker 是 OCI 的发起者和创始成员，它却很少在 OCI 的技术推进和标准制定等事务上扮演关键角色，也没有动力去积极地推进这些所谓的标准。\n\n这，也正是迄今为止 OCI 组织效率持续低下的根本原因。\n\n眼看着 OCI 并没能改变 Docker 公司在容器领域一家独大的现状，Google 和 RedHat 等公司于是把与第二把武器摆上了台面。\n\nDocker 之所以不担心 OCI 的威胁，原因就在于它的 Docker 项目是容器生态的事实标准，而它所维护的 Docker 社区也足够庞大。可是，一旦这场斗争被转移到容器之上的平台层，或者说 PaaS 层，Docker 公司的竞争优势便立刻捉襟见肘了。\n\n在这个领域里，像 Google 和 RedHat 这样的成熟公司，都拥有着深厚的技术积累；而像 CoreOS 这样的创业公司，也拥有像 Etcd 这样被广泛使用的开源基础设施项目。\n\n可是 Docker 公司呢？它却只有一个 Swarm。\n\n所以这次，**Google、RedHat 等开源基础设施领域玩家们，共同牵头发起了一个名为 CNCF（Cloud Native Computing Foundation）的基金会。这个基金会的目的其实很容易理解：它希望，以 Kubernetes 项目为基础，建立一个由开源基础设施领域厂商主导的、按照独立基金会方式运营的平台级社区，来对抗以 Docker 公司为核心的容器商业生态。**\n\n而为了打造出这样一个围绕 Kubernetes 项目的“护城河”，CNCF 社区就需要至少确保两件事情：\n\n1. Kubernetes 项目必须能够在容器编排领域取得足够大的竞争优势；\n\n2. CNCF 社区必须以 Kubernetes 项目为核心，覆盖足够多的场景。\n\n我们先来看看 CNCF 社区如何解决 Kubernetes 项目在编排领域的竞争力的问题。\n\n在容器编排领域，Kubernetes 项目需要面对来自 Docker 公司和 Mesos 社区两个方向的压力。不难看出，Swarm 和 Mesos 实际上分别从两个不同的方向讲出了自己最擅长的故事：Swarm 擅长的是跟 Docker 生态的无缝集成，而 Mesos 擅长的则是大规模集群的调度与管理。\n\n这两个方向，也是大多数人做容器集群管理项目时最容易想到的两个出发点。也正因为如此，Kubernetes 项目如果继续在这两个方向上做文章恐怕就不太明智了。\n\n所以这一次，Kubernetes 选择的应对方式是：Borg。\n\n如果你看过 Kubernetes 项目早期的 GitHub Issue 和 Feature 的话，就会发现它们大多来自于 Borg 和 Omega 系统的内部特性，这些特性落到 Kubernetes 项目上，就是 Pod、Sidecar 等功能和设计模式。\n\n这就解释了，为什么 Kubernetes 发布后，很多人“抱怨”其设计思想过于“超前”的原因：Kubernetes 项目的基础特性，并不是几个工程师突然“拍脑袋”想出来的东西，而是 Google 公司在容器化基础设施领域多年来实践经验的沉淀与升华。这，正是 Kubernetes 项目能够从一开始就避免同 Swarm 和 Mesos 社区同质化的重要手段。\n\n于是，CNCF 接下来的任务就是，如何把这些先进的思想通过技术手段在开源社区落地，并培育出一个认同这些理念的生态？这时，RedHat 就发挥了重要作用。\n\n当时，Kubernetes 团队规模很小，能够投入的工程能力也十分紧张，而这恰恰是 RedHat 的长处。更难得的是，RedHat 是世界上为数不多的、能真正理解开源社区运作和项目研发真谛的合作伙伴。\n\n所以，RedHat 与 Google 联盟的成立，不仅保证了 RedHat 在 Kubernetes 项目上的影响力，也正式开启了容器编排领域“三国鼎立”的局面。\n\n这时，我们再重新审视容器生态的格局，就不难发现 Kubernetes 项目、Docker 公司和 Mesos 社区这三大玩家的关系已经发生了微妙的变化。\n\n其中，Mesos 社区与容器技术的关系，更像是“借势”，而不是这个领域真正的参与者和领导者。这个事实，加上它所属的 Apache 社区固有的封闭性，导致了 Mesos 社区虽然技术最为成熟，却在容器编排领域鲜有创新。\n\n这也是为何，Google 公司很快就把注意力转向了动作更加激进的 Docker 公司。\n\n有意思的是，Docker 公司对 Mesos 社区也是类似的看法。所以从一开始，Docker 公司就把应对 Kubernetes 项目的竞争摆在了首要位置：一方面，不断强调“Docker Native”的“重要性”，另一方面，与 Kubernetes 项目在多个场合进行了直接的碰撞。\n\n不过，这次竞争的发展态势，很快就超过了 Docker 公司的预期。\n\nKubernetes 项目并没有跟 Swarm 项目展开同质化的竞争，所以“Docker Native”的说辞并没有太大的杀伤力。相反地，Kubernetes 项目让人耳目一新的设计理念和号召力，很快就构建出了一个与众不同的容器编排与管理的生态。\n\n就这样，Kubernetes 项目在 GitHub 上的各项指标开始一骑绝尘，将 Swarm 项目远远地甩在了身后。\n\n有了这个基础，CNCF 社区就可以放心地解决第二个问题了。\n\n**在已经囊括了容器监控事实标准的 Prometheus 项目之后，CNCF 社区迅速在成员项目中添加了 Fluentd、OpenTracing、CNI 等一系列容器生态的知名工具和项目。**\n\n**而在看到了 CNCF 社区对用户表现出来的巨大吸引力之后，大量的公司和创业团队也开始专门针对 CNCF 社区而非 Docker 公司制定推广策略。**\n\n面对这样的竞争态势，Docker 公司决定更进一步。在 2016 年，Docker 公司宣布了一个震惊所有人的计划：放弃现有的 Swarm 项目，将容器编排和集群管理功能全部内置到 Docker 项目当中。\n\n显然，Docker 公司意识到了 Swarm 项目目前唯一的竞争优势，就是跟 Docker 项目的无缝集成。那么，如何让这种优势最大化呢？那就是把 Swarm 内置到 Docker 项目当中。\n\n实际上，从工程角度来看，这种做法的风险很大。内置容器编排、集群管理和负载均衡能力，固然可以使得 Docker 项目的边界直接扩大到一个完整的 PaaS 项目的范畴，但这种变更带来的技术复杂度和维护难度，长远来看对 Docker 项目是不利的。\n\n不过，在当时的大环境下，Docker 公司的选择恐怕也带有一丝孤注一掷的意味。\n\n而Kubernetes 的应对策略则是反其道而行之，开始在整个社区推进“民主化”架构，即：**从 API 到容器运行时的每一层，Kubernetes 项目都为开发者暴露出了可以扩展的插件机制，鼓励用户通过代码的方式介入到 Kubernetes 项目的每一个阶段。**\n\nKubernetes 项目的这个变革的效果立竿见影，很快在整个容器社区中催生出了大量的、基于 Kubernetes API 和扩展接口的二次创新工作，比如：\n\n * 目前热度极高的微服务治理项目 Istio；\n * 被广泛采用的有状态应用部署框架 Operator；\n * 还有像 Rook 这样的开源创业项目，它通过 Kubernetes 的可扩展接口，把 Ceph 这样的重量级产品封装成了简单易用的容器存储插件。\n就这样，在这种鼓励二次创新的整体氛围当中，Kubernetes 社区在 2016 年之后得到了空前的发展。更重要的是，不同于之前局限于“打包、发布”这样的 PaaS 化路线，**这一次容器社区的繁荣，是一次完全以 Kubernetes 项目为核心的“百花争鸣”。**\n\n面对 Kubernetes 社区的崛起和壮大，Docker 公司也不得不面对自己豪赌失败的现实。但在**早前拒绝了微软的天价收购**之后，Docker 公司实际上已经没有什么回旋余地，只能选择逐步放弃开源社区而专注于自己的商业化转型。\n\n所以，从 2017 年开始，**Docker 公司先是将 Docker 项目的容器运行时部分 Containerd 捐赠给 CNCF 社区，标志着 Docker 项目已经全面升级成为一个 PaaS 平台；紧接着，Docker 公司宣布将 Docker 项目改名为 Moby，然后交给社区自行维护，而 Docker 公司的商业产品将占有 Docker 这个注册商标。**\n\nDocker 公司这些举措背后的含义非常明确：**它将全面放弃在开源社区同 Kubernetes 生态的竞争，转而专注于自己的商业业务，并且通过将 Docker 项目改名为 Moby 的举动，将原本属于 Docker 社区的用户转化成了自己的客户。**\n\n2017 年 10 月，**Docker 公司出人意料地宣布，将在自己的主打产品 Docker 企业版中内置 Kubernetes 项目，这标志着持续了近两年之久的“编排之争”至此落下帷幕。**\n\n2018 年 1 月 30 日，RedHat 宣布斥资 2.5 亿美元收购 CoreOS。\n\n2018 年 3 月 28 日，这一切纷争的始作俑者，**Docker 公司的 CTO Solomon Hykes 宣布辞职，**曾经纷纷扰扰的容器技术圈子，到此尘埃落定。\n\n**总结**\n容器技术圈子在短短几年里发生了很多变数，但很多事情其实也都在情理之中。就像 Docker 这样一家创业公司，在通过开源社区的运作取得了巨大的成功之后，就不得不面对来自整个云计算产业的竞争和围剿。而这个产业的垄断特性，对于 Docker 这样的技术型创业公司其实天生就不友好。\n\n在这种局势下，接受微软的天价收购，在大多数人看来都是一个非常明智和实际的选择。可是 Solomon Hykes 却多少带有一些理想主义的影子，既然不甘于“寄人篱下”，那他就必须带领 Docker 公司去对抗来自整个云计算产业的压力。\n\n只不过，**Docker 公司最后选择的对抗方式，是将开源项目与商业产品紧密绑定，打造了一个极端封闭的技术生态。而这，其实违背了 Docker 项目与开发者保持亲密关系的初衷。相比之下，Kubernetes 社区，正是以一种更加温和的方式，承接了 Docker 项目的未尽事业，即：以开发者为核心，构建一个相对民主和开放的容器生态。**\n\n这也是为何，Kubernetes 项目的成功其实是必然的。\n\n现在，我们很难想象如果 Docker 公司最初选择了跟 Kubernetes 社区合作，如今的容器生态又将会是怎样的一番景象。不过我们可以肯定的是，Docker 公司在过去五年里的风云变幻，以及 Solomon Hykes 本人的传奇经历，都已经在云计算的长河中留下了浓墨重彩的一笔。\n\n思考题\n你如何评价 Solomon Hykes 在 Docker 公司发展历程中的所作所为？你又是否看好 Docker 公司在今后的发展呢？","categories":["reference","k8s"]},{"title":"02 kubernetes introduction","url":"/2021/03/13/microService/kubernetes/02_kubernetes_roles_introduction/","content":"\n\n## kubernetes组件官网介绍\nhttps://kubernetes.io/docs/reference/\n\n\n### REST\n> REST（representational state transfer）是由roy博士在他的论文中提出的一个术语，rest本身只是为分布式超媒体系统设计的一种架构风格，而不是标准\n> * 无状态性：\n> \t这是在客户-服务器的基础上添加的又一层规范，它要求通信必须在本质上是无状态的，即从客户端到服务器的每个request都必须包含理解该request所必须的所有信息。这个规范改善了系统的可见性（无状态性使得客户端和服务端不必保存对方的详细信息，服务器只需要处理当前的request，而不必了解所有request的历史）、可靠性（无状态性减少了服务器从局部错误中恢复的任务量）、可伸缩性（无状态使得服务器端可以很容易地释放资源，因为服务端不必在多个request中保存状态）。同时，这种规范的缺点也是显而易见的，由于不能将状态数据保存在服务器上，因此增加了在一系列request中发生重复数据的开销，严重降低了效率。\n> * 缓存：\n> \t为了改善无状态性带来的网络低效性，我们添加了缓存约束，缓存约束允许隐式或显式标记一个response中的数据，赋予了客户端缓存response数据的功能，这样就可以为以后的request公用缓存的数据，部分或全部地消除一部分交互，提高了网络效率。但是由于客户端缓存了信息，所以增加了客户端与服务器数据不一致的可能性，从而降低了可靠性。\n> REST中的资源所指的不是数据，而是数据和表现形式的组合，比如“最新访问的10位会员”和“最活跃的10位会员”在数据上可能有重叠或者完全相同，而由于它们的表现形式不同，所以被归于不同的资源，这也就是为什么REST的全名是representational state transfer，资源标识符就是URI（uniform resource identifier），不管是图片，word还是视频文件，也不管是什么格式，全部通过URI对资源进行唯一标识.\n> kubernetes API是集群系统中的重要组成部分，kubernetes中各种资源的数据通过该API接口被提交到后端的持久性存储etcd中，kubernetes集群中的各部分之间通过该API接口实现解耦合，同时kubernetes集群中一个重要且便捷的管理工具kubectl也是通过访问该API接口实现其强大的管理功能的。kubernetes API中的资源对象都拥有通用的元数据，资源对象也可能存在嵌套现象，比如在一个pod里面嵌套多个container。创建一个API对象是指通过API调用一条有意义的记录，该记录一旦被创建，kubernetes将确保对应的资源对象会被自动创建并托管维护\n\n### 标签\n> 我们不会特别说明pod应该调度到哪个节点上， 因为这将会使应用程序与基础架构强耦合， 从而违背了Kubemetes对运行在其上的应用程序隐藏实际 的基础架构的整个构想。 \n> 但如果你想对一个 pod应该调度到哪里拥有发言权， 那就不应该直接指定一个确切的节点， 而应该用某种方式描述对节点的需求， 使Kubemetes选择一个符合这些需求的节点。这恰恰可以通过节点标签和节点标签选择器完成\n> pod并不是唯一可以附加标签的Kubemetes资源。 标签可以附加到任何Kubemetes对象上， 包括节点。 通常来说， 当运维团队向集群添加新节点时，他们将通过附加标签来对节点进行分类， 这些 标签指定节点提供 的硬件类型 ， 或者任何在调度pod 时能提供便利的其他信息\n\n## roles\n\n\n\n\n\n\n\n\n\n","tags":["kubernetes"],"categories":["microService","kubernetes"]},{"title":"Istio_K8s_SpringCloud","url":"/2021/03/13/microService/istio/istio_K8s_SpringCloud/","content":"\nSpringCloud、Istio比较\n> https://blog.csdn.net/qq_33873431/article/details/89524554\n\nSpring Cloud Netflix vs. Kubernetes＆Istio\n> https://my.oschina.net/xiaominmin/blog/1859677\n> https://my.oschina.net/xiaominmin?tab=newest&catalogId=5894416\n\n史上最简单的spark系列教程\n> https://blog.csdn.net/youbitch1/article/details/89925790\nSpark教程视频:\n> https://www.bilibili.com/video/BV1JE411R7Xp?p=55\n> https://www.bilibili.com/video/BV11J41147iP?p=1\n> https://www.bilibili.com/video/BV11J41147iP?p=1\n\n\n\nIstio B站学习视频和对应的Gihub文档\n> https://www.bilibili.com/video/BV1vt411H755/?spm_id_from=333.788.videocard.0\n> https://github.com/Kung-Fu-Master/Document\n\nIstio中文社区:\n> * [中文社区](https://istio.cn/)\n> * [Istio 1.5发布](https://istio.cn/t/topic/270)\n\n\n","tags":["kubernetes"],"categories":["microService","istio"]},{"title":"Istio problems","url":"/2021/03/13/microService/istio/istio_problems/","content":"\n## istio bookinfo部署不成功\n部署istio官网bookinfo出现以下错误\n\n\t$ kubectl describe replicaset.apps/details-v1-769468b8c -n book-info\n\t  Warning  FailedCreate  3s (x13 over 24s)  replicaset-controller  Error creating: Internal error occurred: failed calling webhook \"sidecar-injector.istio.io\": Post \"https://istiod.istio-system.svc:443/inject?timeout=30s\": x509: certificate signed by unknown authority\n官网推荐解决方法: https://istio.io/latest/docs/ops/common-problems/injection/#automatic-sidecar-injection-fails-if-the-kubernetes-api-server-has-proxy-settings\n注释掉  /etc/kubernetes/manifests/kube-apiserver.yaml 文件里的proxy\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio 查看和延长自签名证书","url":"/2021/03/13/microService/istio/istio_查看和延长自签名证书/","content":"\nOfficial website: \nhttps://istio.io/latest/zh/docs/ops/configuration/security/root-transition/\n\n\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"istio 限制访问策略","url":"/2021/03/13/microService/istio/istio_限制访问策略/","content":"\nOfficial website: \nhttps://istio.io/latest/docs/concepts/security/#authorization-policies\n(已经弃用)https://istio.io/latest/docs/tasks/policy-enforcement/denial-and-list/\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"查看SSD","url":"/2021/03/13/linux/查看SSD/","content":"[root@wlp10 ~]# lsscsi -ls\n[6:0:0:0]    disk    ATA      INTEL SSDSC2BB48 0370  /dev/sda    480GB\n  state=running queue_depth=32 scsi_level=6 type=0 device_blocked=0 timeout=30\n[7:0:0:0]    disk    ATA      INTEL SSDSC2BB48 0101  /dev/sdb    480GB\n  state=running queue_depth=32 scsi_level=6 type=0 device_blocked=0 timeout=30\n[8:0:0:0]    disk    ATA      INTEL SSDSC2BB48 0370  /dev/sdc    480GB\n  state=running queue_depth=32 scsi_level=6 type=0 device_blocked=0 timeout=30\n[9:0:0:0]    disk    ATA      INTEL SSDSC2BG01 0015  /dev/sdd   1.20TB\n  state=running queue_depth=32 scsi_level=6 type=0 device_blocked=0 timeout=30\n[root@wlp10 ~]#\n看第四列就知道是否是SSD硬盘了\n\n\n[root@wlp10 ~]# lsscsi -h\nUsage: lsscsi   [--classic] [--device] [--generic] [--help] [--hosts]\n                [--kname] [--list] [--lunhex] [--long] [--protection]\n                [--scsi_id] [--size] [--sysfsroot=PATH] [--transport]\n                [--verbose] [--version] [--wwn] [<h:c:t:l>]\n  where:\n    --classic|-c      alternate output similar to 'cat /proc/scsi/scsi'\n    --device|-d       show device node's major + minor numbers\n    --generic|-g      show scsi generic device name\n    --help|-h         this usage information\n    --hosts|-H        lists scsi hosts rather than scsi devices\n    --kname|-k        show kernel name instead of device node name\n    --list|-L         additional information output one\n                      attribute=value per line\n    --long|-l         additional information output\n    --lunhex|-x       show LUN part of tuple as hex number in T10 format;\n                      use twice to get full 16 digit hexadecimal LUN\n    --protection|-p   show target and initiator protection information\n    --protmode|-P     show negotiated protection information mode\n    --scsi_id|-i      show udev derived /dev/disk/by-id/scsi* entry\n    --size|-s         show disk size\n    --sysfsroot=PATH|-y PATH    set sysfs mount point to PATH (def: /sys)\n    --transport|-t    transport information for target or, if '--hosts'\n                      given, for initiator\n    --verbose|-v      output path names where data is found\n    --version|-V      output version string and exit\n    --wwn|-w          output WWN for disks (from /dev/disk/by-id/wwn*)\n    <h:c:t:l>         filter output list (def: '*:*:*:*' (all))\n\nList SCSI devices or hosts, optionally with additional information\n[root@wlp10 ~]#\n\nFrom <https://www.cnblogs.com/laozhuang/p/7110438.html> \n","categories":["linux"]},{"title":"用户态和内核共享内存----使用 /dev/mem & mmap","url":"/2021/03/13/linux/用户态和内核共享内存-使用dev_mem&mmap/","content":"想法的来源是看到chinaunix上有人转载了wheelz的博客，但是wheelz的代码在我的实验平台上是不能正常工作的，可能是wheelz的代码太过久远，我试验的内核版本是：3.4.13。wheelz的源代码如下：\n// 内核模块\n#include <linux/config.h>\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"Wheelz\");\nMODULE_DESCRIPTION(\"mmap demo\");\nstatic unsigned long p = 0;\nstatic int __init init(void)\n{\n        //分配共享内存（一个页面）\n        p = __get_free_pages(GFP_KERNEL, 0);\n        SetPageReserved(virt_to_page(p));\n        printk(\"<1> p = 0x%08x\\n\", p); \n    //p是内核中的虚拟地址  \n        //在共享内存中写上一个字符串\n        strcpy(p, \"Hello world!\\n\");\n        return 0;\n}\nstatic void __exit fini(void)\n{\n        ClearPageReserved(virt_to_page(p));\n        free_pages(p, 0);        \n}\nmodule_init(init);\nmodule_exit(fini);\n----------------------------------------------------------------------------------------------------------------------------------------\n// 用户态程序\n#include <sys/mman.h> \n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h> \n#include <stdio.h> \n#define PAGE_SIZE (4*1024)\n#define PAGE_OFFSET                0xc0000000\n#define KERNEL_VIRT_ADDR        0xc5e3c000\nint main() \n{ \n        char *buf; \n        int fd; \n        unsigned long phy_addr; \nfd=open(\"/dev/mem\",O_RDWR); \nif(fd == -1)\n                perror(\"open\");\n        phy_addr=KERNEL_VIRT_ADDR - PAGE_OFFSET; \n        //此处不太懂，不能理解物理地址phy_addr的计算方法\n        buf=mmap(0, PAGE_SIZE, \n                PROT_READ|PROT_WRITE, MAP_SHARED, \n                fd, phy_addr); \n        if(buf == MAP_FAILED)\n                perror(\"mmap\");\nputs(buf);//打印共享内存的内容\n        munmap(buf,PAGE_SIZE); \n        close(fd); \n        return 0; \n} \n在网上找了一些资料，导致这段代码不工作的原因可能有一下几个：\n（1）在编译内核时设置了CONFIG_STRICT_DEVMEM（某些版本中是CONFIG_NONPROMISC_DEVMEM），应该将此设置删除。\n（2）请求的地址没有通过内核中devmem_is_allowed函数对/dev/mem的保护。\n（3）物理地址phy_addr计算错误。（PS：wheelz的计算方法是怎么得到的？）\n \n我对上面的几个问题一一做了修改：\n（1）修改了.config文件\n# CONFIG_STRICT_DEVMEM is not set\n（２）重写arch/x86/mm/init.c下的devmem_is_allowed函数，这里我没有做太细致的修改，只是让函数一直返回1。当然这可能会存在一些问题。\n\n/*\n * devmem_is_allowed() checks to see if /dev/mem access to a certain address\n * is valid. The argument is a physical page number.\n *\n *\n * On x86, access has to be given to the first megabyte of ram because that area\n * contains bios code and data regions used by X and dosemu and similar apps.\n * Access has to be given to non-kernel-ram areas as well, these contain the PCI\n * mmio resources as well as potential bios/acpi data regions.\n */\nint devmem_is_allowed(unsigned long pagenr)\n{\nreturn 1;\n        if (pagenr <= 256)\n                return 1;\n        if (iomem_is_exclusive(pagenr << PAGE_SHIFT))\n                return 0;\n        if (!page_is_ram(pagenr))\n                return 1;\n        return 0;\n}\n（3）修改物理地址的计算，这里我们直接使用内核中提供的转换函数virt_to_phy()或者__pa()。\n\n// 内核模块\n#include <linux/module.h>\n#include <linux/kernel.h>\n#include <linux/mm.h>\nMODULE_LICENSE(\"GPL\");\nMODULE_AUTHOR(\"godjesse\");\nMODULE_DESCRIPTION(\"mmap demo\");\nstatic unsigned long p = 0;\nstatic unsigned long pp = 0;\nstatic int __init init(void)\n{\n        p = __get_free_pages(GFP_KERNEL, 0);\nif(!p)\n        {\n                printk(\"Allocate memory failure!/n\");\n        }\n        else\n        {\n                SetPageReserved(virt_to_page(p));\n// 使用virt_to_phys计算物理地址，供用户态程序使用\n                pp = (unsigned long)virt_to_phys((void *)p);\n                printk(\"<1> page : pp = 0x%lx\\n\",pp);\n        }\n        strcpy((char *)p, \"Hello world !\\n\");\n        return 0;\n}\nstatic void __exit fini(void)\n{\nprintk(\"The content written by user is: %s/n\", (unsigned char *)p);\n        ClearPageReserved(virt_to_page(p));\n        free_pages(p, 0);\n        printk(\" exit \\n\");\n}\nmodule_init(init);\nmodule_exit(fini);\n---------------------------------------------------------------------------------------------------------------------------------------\n// 用户态程序\n#include <sys/mman.h>\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <fcntl.h>\n#include <stdio.h>\n#include <string.h>\n//hard coding read after the module installed\n#define KERNEL_PHY_ADDR  0x3737c000\nint main()\n{\nchar *buf;\n        int fd;\n        unsigned long phy_addr;\n        int  pagesize = getpagesize();\nphy_addr=KERNEL_PHY_ADDR;\nfd=open(\"/dev/mem\",O_RDWR);\nif(fd == -1)\n                perror(\"open\");\nbuf=mmap(0, pagesize, PROT_READ|PROT_WRITE, MAP_SHARED, fd, phy_addr);\nif(buf == MAP_FAILED)\n        {\n                perror(\"mmap\");\n        }\nprintf(\"buf : %s\\n\",buf);\n// test the write \n        buf[0] = 'X';\nmunmap(buf,pagesize);\n        close(fd);\n        return 0;\n}\n\n 经过这些修改后，demo可以正常工作。\n上文中提到修改devmem_is_allowed实际上是存在问题的，存在其他一些较为优雅的方法，如某牛人写的博客：bypassing devmem_is_allowed with kernel probes，博客链接：\nhttp://www.libcrack.so/2012/09/02/bypassing-devmem_is_allowed-with-kprobes/\n相关资料：\nhttp://stackoverflow.com/questions/11891979/accessing-mmaped-dev-mem \n\nFrom <https://www.cnblogs.com/godjesse/archive/2012/11/23/2784093.html> \n","categories":["linux"]},{"title":"硬盘存储空间单位GB","url":"/2021/03/13/linux/硬盘存储空间单位GB/","content":"信息技术的存储设备常用B、KB、MB、GB等作为存储设备的单位,例如,我们常说的某计算机的硬盘容量是320GB,某移动硬盘的容量是80GB,某个文件夹的大小是156KB等,其中1GB=210MB,1MB=210KB,1KB=210B(字节),对于一个容量为8GB的内存盘,其容量为____B(字节).\n![](unit_GB.gif)\n\nFrom <http://www.manfen5.com/stinfo/CZ_SX/SYS201808030701323019115696/> \n","categories":["linux"]},{"title":"脚本执行错误 $'\\r':command not found","url":"/2021/03/13/linux/脚本执行错误_r_command not found/","content":"shell脚本执行错误 $'\\r':command not found\n第一种解决方案：\nLinux下有命令dos2unix\n你只要输入dos2unix *.sh就可以完成转换工作了\n如果命令不存在的话就用如下命令安装\nyum install dos2unix -y\n \n第二种解决方案：\n这种情况发生的原因是因为你所处理的文件换行符是dos格式的\"\\r\\n\"\n可以使用cat -v 文件名 来查看换行符是否是，如果是上述的，则行结尾会是^m\n需要转换成linux/unix格式的\"\\n\nsed 's/\\r//' 原文件 >转换后文件\n第三种解决方案：\n首先要确保文件有可执行权限\n#sh>chmod a+x filename\n利用如下命令查看文件格式\n:set ff 或 :set fileformat\n可以看到如下信息\nfileformat=dos 或 fileformat=unix\n利用如下命令修改文件格式\n:set ff=unix 或 :set fileformat=unix\n","categories":["linux"]},{"title":"连接router后机器连不上外网","url":"/2021/03/13/linux/连接router后机器连不上外网/","content":"几台机器连接同一个路由器router后，centos机器连不上外网\n[root@sphm001 ~]# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         192.168.1.1    0.0.0.0         UG    100    0        0 enp0s31f6\n0.0.0.0         10.239.173.1    0.0.0.0         UG    101    0        0 enp0s20f0u8\n10.239.173.0    0.0.0.0         255.255.255.0   U     101    0        0 enp0s20f0u8\n192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp0s31f6\n[root@sphm001 ~]#\n[root@sphm001 ~]# route del -net 0.0.0.0 gw 192.168.1.1\n[root@sphm001 ~]# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         10.239.173.1    0.0.0.0         UG    101    0        0 enp0s20f0u8\n10.239.173.0    0.0.0.0         255.255.255.0   U     101    0        0 enp0s20f0u8\n192.168.1.0     0.0.0.0         255.255.255.0   U     100    0        0 enp0s31f6\n[root@sphm001 ~]#\n[root@sphm001 ~]# wget www.baidu.com\n--1998-01-29 23:28:17--  http://www.baidu.com/\nResolving child-prc.intel.com (child-prc.intel.com)... 10.239.4.101\nConnecting to child-prc.intel.com (child-prc.intel.com)|10.239.4.101|:913... connected.\nProxy request sent, awaiting response... 200 OK\nLength: 2381 (2.3K) [text/html]\nSaving to: ‘index.html.1’\n\n100%[============================================================================================>] 2,381       --.-K/s   in 0s\n\n1998-01-29 23:28:17 (81.6 MB/s) - ‘index.html.1’ saved [2381/2381]\n修改完路由如果还不行就重启机器试试\n[root@sphm001 ~]#\n\n\nlinux上用route添加/删除路由\n1. 查看\nroute -n\n 2. 添加\nroute add -net 9.123.0.0 netmask 255.255.0.0 gw 9.123.0.1\n 3. 删除\nroute del -net 9.123.0.0 netmask 255.255.0.0 gw 9.123.0.1\n \n有些童鞋问怎么加永久路由，很简单，把 “route add -net 9.123.0.0 netmask 255.255.0.0 gw 9.123.0.1” 写到/etc/rc.local最后就行了，\n或者复制编辑好执行也可以：\necho “route add -net 9.123.0.0 netmask 255.255.0.0 gw 9.123.0.1” >> /etc/rc.local\n \n实际中发现，CentOS7.4默认环境下，rc.local中不会执行，需要赋权才可以\nchmod +x /etc/rc.local\n\nFrom <https://www.cnblogs.com/lynsen/p/8027446.html> \n\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nSPH + RVP\nroot@imie:~# route -n\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\n0.0.0.0         10.239.173.1    0.0.0.0         UG    0      0        0 eth1\n10.239.173.0    0.0.0.0         255.255.255.0   U     0      0        0 eth1\n172.17.0.0      0.0.0.0         255.255.0.0     U     0      0        0 docker0\n192.168.1.0     0.0.0.0         255.255.255.0   U     0      0        0 eth0\nroot@imie:~#\n\n\nroot@imie:~# vim /etc/network/interfaces\niface lo inet loopback\n\n# Wired interfaces\nauto eth0 eth1\niface eth0 inet dhcp\niface eth1 inet dhcp\n\n# iface eth0 inet manual\n# iface eth1 inet manual\n\n# Network bridge\n# auto br0\n# iface br0 inet dhcp\n#   bridge_ports eth0 eth1\n","categories":["linux"]},{"title":"istio kubernetes网上学习资源","url":"/2021/03/13/microService/istio/01_istio_kubernetes网上学习资源/","content":"\n## 网上提供的master和work机器\nkatacoda:\nhttps://katacoda.com/courses/kubernetes/playground\nhttps://www.katacoda.com/courses/istio/deploy-istio-on-kubernetes\n\nplay-with-k8s:\nhttps://labs.play-with-k8s.com/\n\n## redhat官网提供的istio学习教程\n\nhttp://developers.redhat.com/topics/service-mesh/\n\n## IBM的Istio示例教程\n\nhttps://developer.ibm.com/code/patterns/manage-microservices-traffic-using-istio\n\n\n## bilibili一些教程视频\n\n[Done]https://www.bilibili.com/video/BV1Xz411b79q?p=7\n\n## 其它网站教程\n\nService Mesh实战\nhttps://time.geekbang.org/course/intro/303?utm_term=zeusL6Y5E&utm_source=bilibili&utm_medium=geektime&utm_content=pinglun\n相应gihub资料: https://github.com/geektime-geekbang/geektime-servicemesh\n\n","tags":["istio"],"categories":["microService","istio"]},{"title":"wget www.baidu.com","url":"/2021/03/13/linux/wget_www.baidu.com/","content":"\nPing www.baidu.com 改为 ：wget www.baidu.com 来测试是否机器可以上网","categories":["linux"]},{"title":"yum下载遇到XZ_5.1.2alpha问题","url":"/2021/03/13/linux/yum下载遇到XZ_5.1.2alpha问题/","content":"遇到如下问题：\n[root@localhost download]# yum -y install libopencv-dev\nThere was a problem importing one of the Python modules\nrequired to run yum. The error leading to this problem was:\n\n   /lib64/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3)\n\nPlease install a package which provides this module, or\nverify that the module is installed correctly.\n\nIt's possible that the above module doesn't match the\ncurrent version of Python, which is:\n2.7.5 (default, Jun 20 2019, 20:27:34)\n[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)]\n\nIf you cannot solve this problem yourself, please go to\nthe yum faq at:\n  http://yum.baseurl.org/wiki/Faq\n[root@localhost download]#\n…\n\n解决方法：\n\n /lib64/liblzma.so.5: version `XZ_5.1.2alpha' not found (required by /lib64/librpmio.so.3)\n\t1. 下载安装 xz-5.2.2.tar.gz\n\t\t进入xz工具官网下载源码包：http://tukaani.org/xz/\n\t\t      下载版本：xz-5.2.2.tar.gz\n\ta. 下载之后，将压缩包解压 tar -vxf xz-5.2.2.tar.gz\n\t\n\t      b. 进入到xz源码目录 cd  xz-5.2.2.tar.gz\n\t\n\t      c. 配置 ./configure--enable-shared\n\t\n\t      d. 编译 make\n\t\n\t      e. 安装 makeinstall\n\t\n\t   如此，则系统安装了xz工具。\n\t   当然，如果用户自己希望安装到自己的特定路径下，可以在配置选项中，设定安装路径，如\n\t     ./configure --enable-shared --prefix=/opt/install/xz/bin\n\t\n\t     这样xz工具就被安装在/tmp/xz目录中，如果要导入到系统，则需要设置环境变量，编辑系统配置文件,\n\t\n\t     vi  /etc/bash.bashrc\n\t\n\t     在系统配置文件的末尾，加入路径：\n\t\n\t     export PATH=$PATH:/opt/install/xz/bin\n\t\n\t     export PATH\n\t\n\t     如果修改了环境变量，需要 \n\t\n\t      4.3 验证是否xz安装成功\n\t\n\t      在终端中，输入命令查看版本号：   xz -V\n\t      得到信息如下，则说明安装成功。\n\t      xz (XZ Utils) 5.2.2\n\t      liblzma 5.2.2\n\t\n\t2. 到/lib64/目录\n\tcp /lib64/liblzma.so.5.2.2 .\n\tsudo ln -s -f liblzma.so.5.2.2 liblzma.so.5\n\t就可以了\n\t\n\t\n","categories":["linux"]},{"title":"yum 遇到的问题","url":"/2021/03/13/linux/yum遇到的问题/","content":"\n## 安装python3后用yum安装软件出错\n\n`yum search` 出现错误1：except KeyboardInterrupt, e:\n`yum install` 出现错误2： except OSError, e:\n\n### 第一种：升级yum\n\n### 第二种: 修改文件里的python为python2\nCentOS系统原带有Python2，后自行安装Python3，并改变/usr/bin/python连接到python3，在执行python的时候直接调用python3.5版本。  \n原因是 `yum does not support Python3`, 该改变导致了yum运行时会报错。  \n解决方法修改/usr/bin下的yum文件的第一行：  \n\n\t$ vim /usr/bin/yum\n\t  1 #!/usr/bin/python -->> #!/usr/bin/python2.7\n\n\t$ vim /usr/libexec/urlgrabber-ext-down\n\t  1 #! /usr/bin/python2  -->>  #!/usr/bin/python2.7\n","categories":["linux"]},{"title":"yum使用和安装cmake 3 版本上","url":"/2021/03/13/linux/yum使用和安装cmake-3版本上/","content":"第一种安装cmake3\n$yum search cmake\n\t……\n\tcmake3.x86_64 : Cross-platform make system\n\t……\n\t\n$yum install cmake3.x86_64\n\t//但使用命令需用cmake3\n\t\n$cmake3 --version      //查看cmake版本\n\n$yum remove cmake3\n\n第二种:\n（1）移除旧版本：\nyum remove cmake\n（2）下载新版本\n1、下载：wget https://cmake.org/files/v3.6/cmake-3.6.0-Linux-x86_64.tar.gz\n2、解压：tar -zxvf cmake-3.6.0-Linux-x86_64.tar.gz\n注意：这个压缩包不是源码包，解压后直接用。\n3、增加环境变量，使其成为全局变量：\nvim /etc/profile  // 或者vim ~/.bashrc\n在文件末尾处增加以下代码\nexport PATH=$PATH:/lnmp/src/cmake-3.6.0-Linux-x86_64/bin\n注意：写自己刚安装cmake的bin的路径\n使修改的文件生效\nsource /etc/profile //或者 source ~/.bashrc\n4、查看环境变量：\necho $PATH\n5、检查cmake版本：\ncmake --version\n\n 扩展知识：\n百度百科的介绍：\nCMake是一个跨平台的安装（编译）工具，可以用简单的语句来描述所有平台的安装(编译过程)。\n他能够输出各种各样的makefile或者project文件，能测试编译器所支持的C++特性,类似UNIX下的automake。\n只是 CMake 的组态档取名为 CMakeLists.txt。Cmake 并不直接建构出最终的软件，而是产生标准的建构档\n（如 Unix 的 Makefile 或 Windows Visual C++ 的 projects/workspaces），然后再依一般的建构方式使用。\n这使得熟悉某个集成开发环境（IDE）的开发者可以用标准的方式建构他的软件，这种可以使用各平台的原生\n建构系统的能力是 CMake 和 SCons 等其他类似系统的区别之处。\n","categories":["linux"]},{"title":"linux 修改系统时间时区","url":"/2021/03/13/linux/修改os时间时区/","content":"\n发现一台服务器时间比北京时间慢 12 个小时，使用 date 命令后发现是：\n\n\t2019年 06月 04日 星期二 21:50:33 EDT\nEDT 时间即美国东部时间。这里要改为北京时间即可：\n\n\tmv /etc/localtime /etc/localtime.bak\n\tln -s /usr/share/zoneinfo/Asia/Shanghai  /etc/localtime\n然后再次 date 查看日期：\n\n\t2019年 06月 05日 星期三 11:10:58 CST\n时间就变成北京时间了.\n\n","categories":["linux"]},{"title":"按字节/字寻址，字长，地址总线，数据总线关系","url":"/2021/03/13/linux/按字节-字寻址_字长_地址总线_数据总线关系/","content":"设一个1MB容量的存储器，字长为32位，问：\n\n1）按字节编址，地址寄存器和数据寄存器个几位？\n\n2）按字编址，地址寄存器和数据寄存器个几位？\n\n\n答案是：\n1）1MB=2^20×8 位，地址寄存器为20位，数据寄存器为8位\n\n2）1MB=2^18×32 位，地址寄存器为18位，数据寄存器为32位\n\n\n\n我的理解：\n字长是CPU一次处理数据也就是读取和写数据的长度(等于数据总线)，与按字编址(每个存储单元长度bit)是不同的概念\n按字节编址(寻址)每个存储单元8bit\n按字编址(32bit)每个存储单元32bit\n\n\t1. 按字节编址，字长需要出题人根据心情随意给出比如16位，则CPU一次读写32bit的数据，存储容量1MB\n\t\t存储容量1MB = 2^20*8bit = 2^19*16bit, 则地址总线19根，数据总线16根\n\t\t也就是地址寄存器19位，数据寄存器16位\n\t2. 按字节编址(8bit)，字长32bit\n\t\tCPU一次读写4个内存地址空间数据\n\t3. 按字编址(32bit)，字长32bit\n\t\tCPU一次读写1个内存地址空间数据\n\nCPU的地址总线是由其通用寄存器的地址总线决定\n我们生活中的笔记本电脑一般CPU通用寄存器的地址总线和数据总线都为32bit\n因为我们笔记本电脑内存4GB，字长32bit，一般(如果不指明默认情况下)都是按字节编址(寻址)\n\t1. 字长32bit因此确定数据总线为32根\n\t2. 4GB = 2^32*8bit, 因此地址总线32根\n\n\n如下图:\n![](address.png)\n\t\n地址00h直接跳到04h, 后缀h,前缀0x代表16进制，每一行32bit，说明：\n\t1. 字长32bit\n\t2. 按字节寻址\n\t因此CPU一次处理读写4个地址空间数据如地址00h~03h\n\n\n32位的处理器(CPU)强调的是数据总线的宽度，32位操作系统强调的是地址总线的宽度, 然后默认按字节寻址(8bit)则只能支持4GB大小内存, 地址总线确定最大可寻址内存范围.\n\n32位cpu，应该说清楚是cpu的地址总线位宽为32位的时候最大只能支持4G的寻址，所以只能支持4G。 如果这里的32位CPU指的是数据总线位宽就不能说明最大可以使用4G内存。 另外，还受制于操作系统的位数，如果操作系统是32位就意味着逻辑地址寻址范围在4G以内，这样就算是64位地址总线也无法使用4G以上的内存空间。\n\n百度百科里32位处理器和32位操作系统都和4G内存扯上了关系。说明32位处理器的地址总线宽度是32位，32位操作系统的地址总线宽度也是32位。\n\n\nCPU在执行指令时需要先将指令的逻辑地址变换为物理地址才能执行。\n\t地址总线数目\t最大支持内存\n32位系统\t32\t2的32次方 = 4G\n64位系统\t36或40\t2的36次方 = 64G 或 2的40次方= 1024G = 1T\n\n\n问题：\n• 自己装的是4G内存条，可是操作系统显示的内存却是3.75G\n• 自己装的是8G内存条，可是操作系统显示的内存也是3.75G\n在使用计算机时，其支持的最大内存是由操作系统和硬件两方面决定的。\n实际上用户在使用计算机时，进程访问到的地址都是逻辑地址，并不是真实的物理地址，逻辑地址是由操作系统提供的，并维护了逻辑地址和物理地址的映射\n对于32位的windows操作系统，提供的逻辑地址寻址范围是4G，但是对于这4G的逻辑地址，又要划分出来一份给CPU寄存器、ROM的这些物理地址进行映射，那么剩下和内存条的物理地址进行映射的空间肯定没有4G了,如下图所示:\n![](logical_physical.png)\n其实操作系统显示的内存3.75G，是逻辑地址。\n","categories":["linux"]},{"title":"sda hda","url":"/2021/03/13/linux/sda_hda/","content":"\n## sda,hda\n\nhda一般是指IDE接口的硬盘，hda一般指第一块硬盘，类似的有hdb,hdc等\nsda一般是指SATA接口的硬盘，sda一般指第一块硬盘，类似的有sdb,sdc等\n\n现在的内核都会把硬盘，移动硬盘，U盘之类的识别为sdX的形式。\n\nSATA接口是串行数据接口，俗称串口。在硬盘外部传输速度目前最快可达250M/S 。\nIDE接口是并行数据接口，俗称并口，在硬盘外部传输速度最快可达133M/S。 \n![sda接口](sda_interface.png)\n\n![IDE接口](IDE_interface.png) ![](power_data_interface.png)\n\nIDE转SATA线如下，先把IDE一头接入到硬盘上，然后电源跟电脑电源线相连，然后把SATA则接入主板上面\n\n![ide接口](IDE_power_sata.png)\nIDE是英文IntegratedDriveElectronics的缩写，翻译成中文叫做“集成驱动器电子”，\nIDE接口即指并行IDE接口（PATA接口），使用并行接口的硬盘既称为并口硬盘，IDE是一种较老的技术，在几年前很常见，目前在在硬盘方面基本不再使用此接口，其接口连接比较简单，只需用一根40线电缆将它们与主板或接口卡连起来就可以了。\nsata接口\nSATA接口（SerialATA）又称为串行ATA，是一种接口技术，使用SATA接口的\n硬盘又叫串口硬盘，其最终将取代使用IDE接口的并口硬盘！SATA接口的历史：2001年，由Intel、APT、Dell、IBM、希捷、迈拓这几大厂商\n\nide接口与sata接口的区别是什么\n```\n1、SATA硬盘比IDE硬盘传输速度高。目前SATA可以提供150MB/s的高峰传输速率。今后将达到300 MB/s和600 MB/s。到时我们将得到比IDE硬盘快近10倍的传输速率。\n2、 相对于IDE硬盘的PATA40针的数据线，SATA的线缆少而细，传输距离远，可延伸至1米，使得安装设备和机内布线更加容易。连接器的体积小，这种线缆有效的改进了计算机内部的空气流动，也改善了机箱内的散热。\n3、相对于IDE硬盘系统功耗有所减少。SATA硬盘使用500毫伏的电压就可以工作。\n4、SATA可以通过使用多用途的芯片组或串行——并行转换器来向后兼容PATA设备。由于SATA和PATA可使用同样的驱动器，不需要对操作系统进行升级或其他改变。\n5、SATA不需要设置主从盘跳线。BIOS会为它按照1、2、3顺序编号。这取决于驱动器接在哪个SATA连接器上（安装方便）。而IDE硬盘需要设置通过跳线来设置主从盘。\n6、SATA还支持热插拔，可以象U盘一样使用。而IDE硬盘不支持热插拔.\n```\n","categories":["linux"]},{"title":"ln软连接, head, tail, more&less","url":"/2021/03/13/linux/ln软连接_head_tail_more_less/","content":"\n## **ln链接**\nln（link）创建链接，`ln 原文件 链接文件` 表示生成一个硬链接文件，`-s` 表示生成软链接文件。\n\n|         | 软链接 | 硬链接 |\n| :------:| :-----: | :------: |\n| 修改源文件 | 可以访问到 | 同步修改 |\n| 删除源文件 | 无法访问 | 可以访问 |\n| 文件类型 | 文件类型为l | 需要查看inode，和源文件用同一inode |\n| 大小 | 很小 | 和源文件一样大 |\n| 修改时间 | 新建时间 | 与源文件相同 |\n| 访问权限 | 全开，但能否访问取决于源文件 | 和源文件相同 |\n| 限制 |   | 不能跨分区建立、不能针对目录使用 |\n| 箭头标识 | 不能跨分区建立、不能针对目录使用 |   |\n\n\n所谓`硬链接`，其实就是多个文件公用一个inode，此时两个文件除了文件名以外所有信息都一样，读取文件时无论通过哪个文件名都能链接到唯一的inode，进而找到唯一的datablock，因此修改哪个文件最终都会影响两个文件，而如果删除任意一个文件，也不会导致数据不能访问，因为inode和block依然存在，所以硬链接比较安全。\n\n`硬链接`实质上就是多了一个目录的block而已，不会对磁盘和inode数产生很大的影响。\n\n因为要链接到同一个inode，所以硬链接不能跨文件系统（分区），至于为何硬链接不能针对目录，这主要是因为目录下的文件是经常变动的，这会导致这种链接关系难以维护。\n\n`软链接`仅仅是建立一个到源文件的快捷方式，它是一个独立的文件，故和源文件的inode不同，这种文件的大小就是源文件的文件名占用的空间，删除源文件会导致无法打开软链接。\n\n用cp命令也可以建立硬链接和软链接。\n\n## **head, tail**\n\nhead可以指定看前几行，`head -n 20` 文件表示看前20行，如果不加选项代表看前10行。\n\ntail和head类似，是看文件末尾行的，也可以使用-n来指定行数，-f代表动态查看，文件出现变化时tail命令的显示也会变化，常用来查看日志。按ctrl+c来退出查看状态。`tail -n +100` 文件表示查看文件100行后的数据。\n\n显示文件的第11到第20行：`head -n 20 文件 | tail -n 10`\n\n显示文件的第11到第20行且有行号：`cat -n 文件 | head -n 20 | tail -n 10`\n\n## **more和less命令**\n\nmore也可以查看文件内容，它以百分比的形式显示查看文件内容多少，按f或空格可以翻页，回车可以看下一行，q或Q表示看完退出，但more不能向上翻页。还有搜索功能，在浏览状态下输入/要查找内容回车，就可以跳到那个位置，但是只能向下搜寻。\n\nless和more类似，但加了向上翻的功能，pageup可以向上翻一页，上箭头可以向上看一行，less还可以查找内容，在浏览状态下输入/要查找内容回车，就可以跳到那个位置，按n跳到下一个搜索词，它可以向上搜寻，用？代替/即可。\n","categories":["linux"]},{"title":"emon","url":"/2021/03/13/linux/emon/","content":"Sampling Enabling Product (SEP) and Event Monitor (EMON) are software tools used for performance analysis & tuning of Intel h/w platforms, and the software running on them. This is the site where you can download latest releases of the tools for PMU based analysis on various public/NDA/private Intel® processors/platforms, and also request technical support for any issues you may encounter. Check out the SEP user guide and EMON user guide.\n\nFrom <https://soco.intel.com/groups/perf-tools> \n\n\n\nEMON is a low-level command-line tool that provides the ability to profile application and system performance. The tool leverages counters from hardware Performance Monitoring Units (PMUs) to collect performance monitoring events.\n--------------------- \n","categories":["linux"]},{"title":"intel_pstate/acpi-cpufreq驱动研究","url":"/2021/03/13/linux/intel_pstate-acpi-cpufreq驱动研究/","content":"查看intel_pstate & cpu-freq驱动\nTag: v5.0\n1.\nlinux/drivers/cpufreq/intel_pstate.c\n\tupdate_turbo_state(void)\n\t\n\tif (hwp_active)\n\t\tintel_pstate_hwp_enable(struct cpudata *cpudata)  \n\t\t\t0x773->0, 0x770->1\n\n\tPn: core_get_min_pstate(void)  \t\tCPU min MHz   // msr: 0x771 P0,P1,Pn HWP Performance Range Enumeration\n\t\t(0xce >> 40) & 0xFF = 0x0a = 10 -> 1000MHz\n\n\tP1: core_get_max_pstate_physical(void) \tCPU max MHz\n\t\t(0xce >> 8) & 0xFF  = 0x17 = 23 -> 2.3GHz\n\n\tP1: core_get_tdp_ratio(u64 plat_info) //1332\n\t\ttdp_msr = 0x64b(0x80000000) & 0x03 + 0x648 = 0x648\n\t\trdmsr 0x648 = 0x17 = 23 -> 2.3GHz\n\n\tP1: core_get_max_pstate(void) //1365\n\t\tif(hwp_active) return tdp_ratio = 2.3GHz\n\t\ttar_levels = 0x64c(80000000) & 0xff = 0\n\t\tif (tdp_ratio - 1 == tar_levels) max_pstate = tar_levels\n\n\tP0: core_get_turbo_pstate(void)  //1400\n\t\t0x1ad & 255 = 0x27 = 39 -> 3.9GHz   msr寄存器0x1ad前7bit值\n\n\tintel_pstate_set_pstate(struct cpudata *cpu, int pstate) // 1453\n\t\t0x199\n\t\t0x198 Core Voltage (R/O) P-state core voltage can be computed by MSR_PERF_STATUS[37:32] * (float) 1/(2^13).\n\n\tintel_pstate_hwp_boost_up(struct cpudata *cpu) // 1516\n\n\tintel_pstate_hwp_boost_down(struct cpudata *cpu) // 1562\n\n\tintel_pstate_calc_avg_perf(struct cpudata *cpu) //1618\n\t\tcore_avg_perf = (aperf << 14) / mperf\n\n\tget_avg_frequency(struct cpudata *cpu) // 1667\n\t\t(aperf << 14) / mperf * cpu_khz >> 14\t  cpu_khz /* TSC clocks / usec, not used here */\t\n\n\tget_avg_pstate(struct cpudata *cpu) //1672\n\t\t(aperf << 14) / mperf * max_pstate_physical >> 14\t\n\n\tstatic struct cpufreq_driver *default_driver = &intel_pstate; // 2327\n\t\t默认是intel_pstate驱动\n\t\t\n\tearly_param(\"intel_pstate\", intel_pstate_setup); //2682\n\t\t读取 /etc/default/grub 引导文件中的参数\n\n2.\nlinux/drivers/cpufreq/acpi-cpufreq.c\n\n3.\nlinux/drivers/cpufreq/cppc_cpufreq.c\n\n4.\nlinux/drivers/idle/intel_idle.c\n","categories":["linux"]},{"title":"VirutalBos安装Centos7","url":"/2021/03/13/linux/VirtualBox安装Centos7/","content":"\nReference Link: \n(外网): https://www.avoiderrors.com/install-centos-7-virtual-box/\n(csdn): https://blog.csdn.net/qq_23033339/article/details/80867195\n(csdn): https://www.cnblogs.com/hihtml5/p/8217062.html\n\n## 1. [Download and install VirtualBox](https://www.virtualbox.org/wiki/Downloads) from its official website, and make sure that you had downloaded the latest version.\n\n## 2. Also [download the official CentOS ISO](https://www.centos.org/download/) from the official website, the latest CentOS build is 7.\n\n## 3. Run your VirtualBox after you had installed it on your computer and located its icon on the desktop and click on “New“.\n\n![](01.png)\n\n## 4. Give your new OS name and set your RAM memory, and also select the version to be “Red Hat (64-bit).\n\n![](02.png)\n\n## 5. On the Hard Disk step, select “Create a virtual hard drive now” and then click Create.\n\n![](03.png)\n\n## 6. Select VDI “VirtualBox Disk Image” and click Next, and then select “Dynamically allocated” and click Next then Create.\n\n![](04.png)\n默认选项即可，默认选择的是VirtualBox虚拟机软件专用的磁盘映像格式，其他虚拟机软件可能无法读取。\n\n点击下一步，进行设置如何分配虚拟硬盘\n\n![](05.png)\n默认选项即可，两者有何不同界面上已经有很详细的说明了。\n\n点击下一步，指定虚拟硬盘文件的存放位置和虚拟硬盘的大小。\n\n![](06.png)\n\n## 7. (添加ISO和网络)From the Setting click on Storage, and then add the ISO file to the optical drive to install the operating system.\n**NOTE:**如下图System选项下Proessor默认处理器数量是1, 可以根据个人需要设置成3+\n\n![](07.jpg)\n\nVirtualBox的四种网络连接方式\n\n![](09.png)\nvirtualbox默认的网络连接方式如下\n\n![](10.png)\n可以看到桥接模式是最佳选项，它支持所有情况的访问, 修改虚拟机连接方式为桥接网卡, 如下所示：\n\n![](11.png)\n\n按照下图配置虚拟机的网络\n\n\n**Linux系统上通过执行`ip addr`查看并选择系统网卡**\n\n## 8. You had successfully configured your CentOS well, power on your virtual machine by clicking on Start.\n\n![](08.png)\n\n## 9. From the boot menu select “Install CentOS Linux 7” and press Enter.\n\n![](12.png)\n\n## 10. Select your language and press on Continue.\n\n![](13.png)\n\n## 11. Setup your time settings, location, network, and then click “Begin Installation”.\n\n![](14.png)\n![](14.1.jpg)\n**NOTE:** 以太网默认enp0s3打开之后会显示**`已连接`**并且自动生成**`IP地址`**\n\n## 12. During the installation, you set the root and the user account.\n\n![](15.png)\n\n## 13. After the installation is completed, press on Reboot.\n\n![](16.png)\n\n\n\n\n","tags":["docker"],"categories":["linux"]},{"title":"Windows10 mstsc远程登录Centos7.0","url":"/2021/03/13/linux/Windows10mstsc远程登录Centos7.0/","content":"配置iptables防火墙\n    在xrdp使用是3389端口，所以在iptables中也要开放相应的端口，否则无法访问\n 　　iptables -A INPUT -p tcp --dport 3389 -j ACCEPT\n    　　service iptables save\nFrom <https://www.cnblogs.com/Skyar/p/5260368.html> \n\ncentos 安装xrdp远程连接桌面\n1. 安装epel库，否则无法安装xrdp\n    \nyum install epel-release\n2.安装 xrdp\nyum install xrdp\n3. 安装tigervnc-server\nyum install tigervnc-server\n4. 配置xrdp.ini文件\nvim /etc/xrdp/xrdp.ini\n把max_bpp=32 改成24\n5.配置selinux\nchcon -t bin_t /usr/sbin/xrdp\nchcon -t bin_t /usr/sbin/xrdp-sesman\n6.设置xrdp服务，开机自动启动\nsystemctl start xrdp\nsystemctl enable xrdp\n7.打开防火墙\nfirewall-cmd  --permanent --zone=public --add-port=3389/tcp\nfirewall-cmd --reload\n8.查看xrdp是否启动\nsystemctl status xrdp.service\nss -antup|grep xrdp\n9.启动window rdp连接\nFrom <https://www.cnblogs.com/Jesse-Li/p/10284221.html> \n\n\n2 关闭防火墙\nsystemctl stop firewalld.service\n• 1\n设置开机不启动防火墙\nsystemctl disable firewalld.servie\n\n3、关闭SElinux\n1）查看selinux状态\nsestatus \n• 1\n2）临时关闭selinux\nsetenforce 0\n• 1\n永久关闭selinux\nvim /etc/selinux/config\nSELINUX=disabled\n\n查看服务列表状态:\n 1. systemctl list-units --type=service \n 2. systemctl   list-unit-files       列出所有已经安装的  服务  及  状态      （可为人所读,  内容简略、清晰）：\n\n 3. systemctl 可以列出正在运行的服务状态\n\n启动一个服务：\nsystemctl start postfix.service\n\n关闭一个服务：\nsystemctl stop postfix.service\n\n重启一个服务：\nsystemctl restart postfix.service\n\n显示一个服务的状态：\nsystemctl status postfix.service\n\n在开机时启用一个服务：systemctl enable postfix.service\n在开机时禁用一个服务：systemctl disable postfix.service\n\n查看服务是否开机启动：   systemctl is-enabled postfix.service\n\n查看已启动的服务列表：   systemctl list-unit-files | grep enabled\n\n查看启动失败的服务列表：   systemctl --failed\n\nPS：使用命令 systemctl is-enabled postfix.service 得到的值可以是enable、disable或static，这里的 static 它是指对应的 Unit 文件中没有定义[Install]区域，因此无法配置为开机启动服务。\n\n 说明：启用服务就是在当前“runlevel”的配置文件目录   /etc/systemd/system/multi-user.target.wants  里，建立  /usr/lib/systemd/system   里面对应服务配置文件的软链接；\n禁用服务就是删除此软链接，添加服务就是添加软连接。\n\nFrom <https://www.cnblogs.com/devilmaycry812839668/p/8481760.html> \n","categories":["linux"]},{"title":"centos升级gcc到9.1.0","url":"/2021/03/13/linux/centos升级gcc到9.1.0/","content":"centos 升级GCC到9.1.0\n下载gcc-9.1.0源代码\ncd gcc-9.1.0\nvim contrib/download_prerequisites   #查看需要安装的依赖库\n 30 gmp='gmp-6.1.0.tar.bz2'\n 31 mpfr='mpfr-3.1.4.tar.bz2'\n 32 mpc='mpc-1.0.3.tar.gz'\n 33 isl='isl-0.18.tar.bz2'\n直接搜索相应库下载对应版本或下面链接\n1.gmp http://ftp.gnu.org/gnu/gmp/\n2.mpfr http://ftp.gnu.org/gnu/mpfr/\n3.mpc http://ftp.gnu.org/gnu/mpc/\n4.isl http://isl.gforge.inria.fr/\n\ncd gmp-6.1.0\n./configure --prefix=/usr/local/gmp-6.1.0 && make\nmake install\n\ncd mpfr-3.1.4\n./configure --prefix=/usr/local/mpfr-3.1.4 --with-gmp=/usr/local/gmp-6.1.0 && make\nmake install\n\ncd mpc-1.0.3\n./configure --prefix=/usr/local/mpc-1.0.3 --with-gmp=/usr/local/gmp-6.1.0 --with-mpfr=/usr/local/mpfr-3.1.4 && make\nmake install\n\ncd isl-0.18\n$./configure --prefix=/usr/local/isl-0.18 --with-gmp-prefix=/usr/local/gmp-6.1.0\n$make\n$make check\n$make install\n\n$export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/isl-0.18/lib\n或设置到所有tty\n#vi ~/.bashrc\n添加export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/local/isl-0.18/lib\n再source bashrc\n\n如果报检测不到gmp,mpfr,mpc相关文件，则把下面对应的也加入进来\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/mpc-1.0.3/lib:/usr/local/gmp-6.1.0/lib:/usr/local/mpfr-3.1.4/lib:/usr/local/isl-0.18/lib\n\ncd gcc-9.1.0\n$./configure --prefix=/usr/local/gcc-9.1.0 --enable-threads=posix --disable-checking --disable-multilib --enable-languages=c,c++ --with-gmp=/usr/local/gmp-6.1.0 --with-mpfr=/usr/local/mpfr-3.1.4 --with-mpc=/usr/local/mpc-1.0.3 --with-isl=/usr/local/isl-0.18\n$make -j4 #启用4个job，需要大约25分钟时间\n$make install\n查看安装路径：\n$whereis gcc\n如果有旧的Gcc,替换版本：\n$cd /usr/bin\n$mv gcc gcc.bak\n$mv g++ g++.bak\n$ln -s /usr/local/gcc-9.1.0/bin/gcc /usr/bin/gcc\n$ln -s /usr/local/gcc-9.1.0/bin/g++ /usr/bin/g++\n查看版本：\n$gcc -v\n$g++ -v\n\n运行以下命令检查动态库：\nstrings /usr/lib64/libstdc++.so.6 | grep GLIBC\n\ncp /usr/local/gcc-9.1.0/lib64/libstdc++.so.6.0.26 /usr/lib64/\nmv libstdc++.so.6 libstdc++.so.6.bak\nln libstdc++.so.6.0.26 libstdc++.so.6\n","categories":["linux"]},{"title":"Intel_pstate/acpi-cpufreq驱动切换","url":"/2021/03/13/linux/Intel_pstate-acpi-cpufreq驱动切换/","content":"\nBy default it is intel_pstate driver\n               Change intel_pstate driver to acpi-cpufreq as follow:\n\n    /etc/default/grub, \nchange\nGRUB_CMDLINE_LINUX=\"***quiet\"\nto\nGRUB_CMDLINE_LINUX=\"***quiet intel_pstate=disable\"\nthen\nUEFI 系统上的指令是 grub2-mkconfig -o /boot/efi/EFI/centos/grub.cfg\nafter reboot, then acpi-cpufreq driver willbe used.\n","categories":["linux"]},{"title":"Linux kernel userspace体系结构","url":"/2021/03/13/linux/Linux_kernel_userspace体系结构/","content":"linux 系统体系结构：\n![](linux_gun.jpg)\n\nlinux kernel体系结构：\narm有7种工作模式，x86也实现了4个不同级别RING0-RING3,RING0级别最高，\n这样linux用户代码运行在RING3下，内核运行在RING0,这样系统本身就得到了\n充分的保护\n用户空间(用户模式)转到内核空间(系统模式)方法：\n·系统调用\n·硬件中断\nlinux kernel 体系结构：\n![](linux_kernel.jpg)\n\n虚拟文件系统VFS:\nVFS(虚拟文件系统)隐藏各种文件系统的具体细节，为文件操作提供统一的接口\n\n二.Linux内核源代码\nlinux内核下载www.kernel.org\n目录结构:\n解压linux kernel tar后目录\n·arch:根据cpu体系结构不同而分的代码\n·block:部分块设备驱动程序\n·crypto:加密，压缩，CRC校验算法\n·documentation:内核文档\n·drivers:设备驱动程序\n·fs(虚拟文件系统vfs):文件系统\n·include:内核所需的头文件，(与平台无关的头文件在include/linux中)\n·lib:库文件代码(与平台相关的)\n·mm:实现内存管理，与硬件体系结构无关的(与硬件体系结构相关的在arch中)\n·net:网络协议的代码\n·samples:一些内核编程的范例\n·scripts:配置内核的脚本\n·security:SElinux的模块\n·sound:音频设备的驱动程序\n·usr:cpio命令实现，用于制作根文件系统的命令(文件系统与内核放到一块的命令)\n·virt:内核虚拟机\nlinux DOC 编译生成:\nlinux源根目录/Documentation/00-INDEX:目录索引\nlinux源根目录/Documentation/HOWTO:指南\n·生成linux内核帮助文档:在linux源根目录(Documentation) 执行make htmldocs\nubuntu16下需要执行sudo apt-get install xmlto安装插件才可生成doc文档\n\n三.Linux内核配置与编译\n清理文件(在linux源码根目录):\n·make clean:只清理所有产生的文件\n·make mrproper:清理所有产生的文件与config配置文件\n·make distclean:清理所有产生的文件与config配置文件，并且编辑过的与补丁文件\n↓\n配置(收集硬件信息如cpu型号，网卡等...):\n·make config:基于文本模式的交互配置\n·make menuconfig:基于文本模式的菜单模式(推荐使用)\n·make oldconfig:使用已有的.config,但会询问新增的配置项\n·make xconfig:图形化的配置(需要安装图形化系统)\n配置方法：\n1)使用make menuconfig操作方法：\n1>按y:编译>连接>镜像文件\n2>按m:编译\n3>按n:什么都不做\n4>按\"空格键\":y,n轮换\n配置完并保存后会在linux源码根目录下生成一个.config文件\n注意：在ubuntu11上要执行apt-get install libncurses5-dev来安装支持包\n\n四.linux内核模块开发\n描述：\nlinux内核组件非常庞大，内核ximage并不包含某组件，而是在该组件需要被使用的时候，动态的添加到正在运行的内核中(也可以卸载)，这种机制叫做“内核模块”的机制。内核模块通常通过使用makefile文件对模块进行编译\n模块安装与卸载:\n1)加载：insmod hello.ko\n2)卸载：rmmod hello\n3)查看：lsmod\n4)加载(自动寻找模块依赖)：modprobe hello\nmodprobe会根据文件/lib/modules/version/modules.dep来查看要加载的模块，看它是否还依赖于其他模块，如果是,会先找到这些模块，把它们先加载到内核\n","categories":["linux"]},{"title":"PTU(Power Thermal Utility)","url":"/2021/03/13/linux/PTU(Power Thermal Utility)/","content":"PTU (Power Thermal Utility)\nIntel PTU (Power Thermal Utility) 可以用來測試 CPU 與 Memory 但是這工具需要有 Intel 帳號.\n\nFrom <https://blog.csdn.net/guyan1101/article/details/80591831> \n\n","categories":["linux"]},{"title":"TSC","url":"/2021/03/13/linux/TSC/","content":"linux/Documentation/virtual/kvm/timekeeping.txt\n\nFrom <https://github.com/torvalds/linux/blob/6f0d349d922ba44e4348a17a78ea51b7135965b1/Documentation/virtual/kvm/timekeeping.txt> \n\n1) Overview\n2) Timing Devices\n3) TSC Hardware\n4) Virtualization Problems\n","categories":["linux"]},{"title":"U盘RAW类型修复","url":"/2021/03/13/linux/U盘RAW类型修复/","content":"\n## U盘RAW类型修复\n由于不正常断电等因素，U盘插在windows上后OS读取不到，打开磁盘管理器查看发现U盘TYPE为RAW.\n解决办法: \n\n1. 打开系统磁盘管理器，鼠标右击U盘所在的卷，选择删除卷\n\n2. 下载DiskGenius压缩包，免安装，双击\"DiskGenius.exe\"打开\nhttps://dl.pconline.com.cn/download/356592-1.html\n\n3. 查看左边一列，右击U盘所在的选项，选择\"清除扇区数据\"，然后格式化，选择卷标等，之后就可以了.\n\n","categories":["linux"]},{"title":"U盘制作启动盘分区后恢复","url":"/2021/03/13/linux/U盘分区之后如何恢复/","content":"\n## **第一种操作步骤**\n1. 插入U盘。\n2. 按windows键，右键点击运行，再左键点击以管理员身份运行。\n3. 输入diskpart,按enter。\n![](diskpart.png)\n4. 输入list disk,按enter。\n![](list_disk.png)\n5. 之后会看到\ndisk 0\ndisk 1\n如果你给你电脑磁盘分过区的话可能还有disk 2  、  disk 3等\n![](list_disk.png)\n6. 输入select disk X(X代表磁盘后面的数字0、1，可磁盘的大小来判断数字是多少，一般是1),按enter\n![](select_disk.png)\n7. 输入clean，按enter\n![](clean.png)\n8. \n以上完成之后，到“磁盘管理”选择U盘，新建简单卷，并重命名你的U盘\n\n打开“磁盘管理”方法:\n任务栏\"Search\"输入\"disk management\", 选择运行\"Create and format hard disk partitions\"\n\"新建简单卷\"方法:\n打开磁盘管理后，选中U盘所在Disk 如Disk1， 右击鼠标，选中第一个新建卷，默认下一步，期间\"Volume lable\"输入盘volume名字就可以了.\n\n## **第二种使用DiskGenius软件**\n![](1.PNG)\n![](2.PNG)\n\n","categories":["linux"]},{"title":"CPU角度理解PCIE","url":"/2021/03/13/linux/CPU角度理解PCIE/","content":"概述\n为什么需要写这篇文章，当我阅读《深入浅出SSD》这篇书籍中PCIe章节时发现，本书籍的侧重点是放在PCIe控制器和PCIe协议上，从CPU角度理解PCIe知识偏少，本文对下面几个知识点做出一些补充。\n\nCPU访问外设寄存器与内存编址方式；\nCPU如何访问PCIe配置空间；\nCPU能够通过寄存器访问配置空间，为什么还需要映射PCIe配置空间；\n如何扫描PCIe树并且为PCIe分配ID；\n如何将pcie域地址映射到存储器域地址空间。\n通过本篇文章将对问题1、2、3做出解答。\n\n统一编址于独立编址\nCPU编址是程序指令与物理地址线建立链接的方式，在CPU内部有专门的地址集合，编址过程是由CPU体系架构所决定的，参考示意图如图 1所示（仅仅代表示意图，讲解一种逻辑结构，不代表实际电路）。CPU编址时就已经指定了0x8000_0000~0xFFFF_FFFF这个地址空间为连接到图中内存的地址线，内存如何连接到CPU需要当参考CPU的datasheet，当CPU程序指令对0x8000_0000这个物理地址地址发起访问时，等价于是在访问图中2G内存的首地址。\n![](picture1.jpg)\n    图 1\n\n内存通过CPU地址总线来寻址定位，然后通过CPU数据总线读写数据。CPU的地址总线位数是CPU设计时确定，因此一款CPU所能寻址的地址范围是一定的，而内存是需要占用CPU的寻址空间的，内存与CPU采用总线直接连接。\n\nIO指的是与CPU连接的各种外设，CPU访问各种外设有两种方式：一种是类似于访问内存的方式，即把外设的寄存器当成内存地址读写，从文可以以访问内存方式操作外设寄存器。这时，IO与内存统一编址，IO地址与内存地址在同一个地址空间下，这种编址方式叫做IO与内存统一编址。另外一种编址方式是IO地址与内存地址分开独立编址，这种编址方式叫做独立编址，此时，CPU访问外设寄存器需要通过CPU特定的指令去访问外设寄存器，而不能通过地址直接访问外设寄存器。常见的ARM、PowerPc、MIPS架构都是采用统一编址，X86架构采用独立编址。\n\n访问PCIe配置空间256bytes\nPCI总线规定访问配置空间总线事务，使用ID号进行寻址。PCI设备ID号由总线号（Bus Number）、设备号（Device Number）和功能号（Function Number）。其中总线号在HOST主桥遍历PCI总线树时确定，在一颗PCI总线树上，总线号由系统软件决定，通常与HOST主桥直接相连接的PCI总线编号为0，系统软件使用DFS（Depth-First Search）算法扫描PCI总线树上的所有PCI总线，并依次编号。一条PCI总线的设备号由PCI设备的IDSEL信号与PCI总线地址线的连接关系确定，功能号与PCI设备的具体设计有关。一个PCIe系统最多有256条Bus，每条Bus上最多可以挂在32个设备，每个PCIe设备最多有8个功能设备。\n\n在XX处理器中的HOST主桥中，与PCIE设备配置相关的寄存器由CFG_ADDR、CFG_DATA等组成。系统软件使用CFG_ADDR（CFG_ADDR寄存器结构如图 2所示）和CFG_DATA寄存器访问PCIe设备的配置空间，这些寄存器都是采取同一编址（所有内存寄存器都使用存储器映射方式进行寻址）。当处理器访问PCIe配置空间时，首先需要在CFG_ADD寄存器中设置这个PCIe设备对应的总线号、设备号、功能号和寄存器偏移，然后使能Enable位，之后当处理器对CFG_DATA读写访问时，HOST主桥将这个存储器读写访问转换成PCIe配置读写请求，并且发送到PCIe总线上。如果Enable位没有使能，那么CPU对寄存器的访问也就是一个普通IO的访问，而不能让HOST转换成总线请求访问，访问PCIe配置空间时按照PCIe总线标准配置TLP请求，CFG_DATA是读取的数据或者待写入的数据。\n![](picture2.jpg)\n图 2\n\n31位：Enable位，为1时，对CFG_DATA读写才能转换成PCIe总线配置请求。\n30~24位：保留。\n23~16位：总线号，最多=256个。\n15~11位：设备号，最多=32个。\n10~8位：功能号，最多=8个。\n7~2位：寄存器偏移，最多访问寄存器=64个地址，这里一个地址是DW，那么能干访问的PCIe配置空间大小为64*4=256Byte，所以访问PCIe配置空间都是以4字节对齐访问的。\n走到这里很多读者可能就会有这样的疑问，既然CPU能够直接通过寄存器访问配置空间，为啥还会出现配置空间在存储域地址的映射这一说法呢？下面给出详细解答。\n\n访问PCIe配置空间寄存器的方法需要追溯到原始的PCI规范。为了发起PCI总线配置周期，Intel（Intel是PCIe龙头老大，最新的PCIe的规范总是它最先尝试的）实现的PCI规范使用IO空间的CF8h和CFCh来分别作为索引和数据寄存器，这种方法可以访问所有PCI设备的255 bytes配置寄存器。Intel Chipsets目前仍然支持这种访PCI配置空间的方法。PCIe规范在PCI规范的基础上，将配置空间扩展到4K bytes，至于为什么扩展到4K，具体可以参考PCIe规范，这些配置CFG_ADDR和CFG_DATA寄存器方法仍然可以访问所有PCIe设备配置空间的头255 bytes，但是该方法访问不了剩下的（255B~4K）配置空间。怎么办呢？Intel外一种PCIe配置空间访问方法。Intel Chipset通过将配置空间映射到内存地址空间，PCIe配置空间可以像对映射范围内的内存进行read/write来访问了。这种映射是由北桥芯片来完成的，但是不同芯片的映射方式也是不同的。目前我查看了ARM芯片的datasheet，确实是这样的方式。\n\nPCIe规范为每个PCIe设备添加了更多的配置寄存器，空间为4K，尽管CFG_ADDR和CFG_DATA寄存器方法仍然能够访问lower 255 bytes，但是必须提供另外一种方法来访问剩下的（255B~4K）range寄存器。Intel的解决方案是使用了预留256MB内存地址空间，对这段内存的任何访问都会发起PCIe 配置cycle。由于4K的配置空间是directly mapped to memory的，那么PCIe规范必须保证所有的PCIe设备的配置空间占用不同的内存地址，按照PCIe规范，支持最多256个bus，每个Bus支持最多32个PCIe devices，每个device支持最多8个function,也就是说：占用内存的最大值为：256 * 32 * 8 * 4K = 256MB。图 3是ARM Cortex-A9 datasheet内存地址分配局部图。被PCIe配置空间占用的256M内存空间会屏蔽掉DRAM使用该段内存区，这些地址都由CPU出厂时已经固化好了。\n![](picture3.jpg)\n图 3\n\nPCIe配置空间的内存映射对32bit系统的影响\n由于PCIe配置空间占用了256M内存空间，而且该被占用空间对DRAM来说是不可用的，这意味着256M空间消失于系统内存，这在32bit系统中更为明显。比如，在32 bit winxp中（作者目前电脑还是用的XP系统，电脑用了七八年了），理论上可以访问到的内存是4G，如果4G空间都被DRAM给占用，由于PCIe的存在，被PCIe占用的那部分内存空间对OS来说是不可用的，莫名的消失了最多256M内存，其实还有其他外设寄存器需要映射到内存，如果是独立编址就不存在寄存器占用内存。所以在XP系统中实际能够访问DRAM空间最大值为3.2G。64位CPU寻址不存在这个情况，个地址目前来说应该用不完，这里读者需要注意的是CPU有32和64位寻址方式，同样操作系统也有32和64位之分，在Linux系统中主要体现在库文件上。\n\n有些CPU没有直接指定PCIe配置空间的地址范围，需要读取某个寄存器的值BaseAddr，这个值就说PCIe配置寄存器在内存区域映射的基地址。访问PCIe设备配置空间时候需要手动计算访问PCIe配置空间的地址。计算发放如下:\n\nSIZE_PER_FUNC = 4K = 1000h\n\nSIZE_PER_DEVICE = 4K * 8 = 8000h\n\nSIZE_PER_BUS = 4K *8* 32 = 100000h\n\n访问总线号为busNo，设备号为DevNo，功能号为funcNo的offset寄存器的计算公式是：\n\nMemory Address = BaseAddr+ busNo * SIZE_PER_BUS+ devNo * SIZE_PER_DEVICE+ funcNo * SIZE_PER_FUNC+ offset\n\n访问PCIe配置空间就需要通过总线号、设备号、功能号、寄存器偏移进行转换成内存地址。转换函数如图 2所示。\n![](picture4.jpg)\n图 4\n\n问题4和5在下面文章中讲解，介于作者实力有限。如有错误，望读者给出宝贵的意见。\n\n剩下两个问题，上电扫描PCIe树和存储地址到PCIe地址的映射，本篇文章将对这两个问题做出解答。本文可能会针对某一款芯片做出详细流程解答，读者可以只关注整个流程，具体映射机制和寄存器参考芯片datasheet。上篇文章已经了解到如何访问配置空间，前256Bytes可以通过寄存器方式访问，后面的256B~4k必须通过映射才能访问，映射无非就是把配置空间映射到存储地址空间，或者把PCIe设备空间映射到存储地址空间。下面开始讨论映射关系。\n\n地址映射关系\nPCIe在存储域地址空间分为三部分，PCIe控制器本身的寄存器、PCIe设备的配置空间、PCIe设备空间。寄存器和配置空间由处理器本身决定存储地址范围，本款处理器地址范围如图 1所示，配置空间地址、寄存器地址、内存地址都已经确定。PCIe设备空间需要编程人员去配置Outbound和Inbound寄存器组，确定映射关系\n![](picture5.jpg)\n图 5\n\nOutbound在PCIe控制器中扮演的角色是将存储地址翻译到PCIe域的PCIe地址，Inbound是将PCIe地址翻译成存储地址，图 2是一个完整的RC和EP模型地址翻译模型，图中的地址数字仅仅代表一种形态，具体地址应该是什么在后文中讲解。当cpu需要访问EP的内存空间时，首先应该将存储地址转换成PCIe地址，在根据TLP到达指定的EP，进而将PCIe地址转换成EP端的存储地址。\n![](picture6.jpg)\n图 6\n\nPCIe地址到存储地址之间的映射关系由三个寄存器决定（有两个寄存器组应该是32个寄存器）OB_SIZE、OB_OFFSET_INDEXn、OB_OFFSETn_HI，n的范围是0~31。在PCIe控制器中是把PCIe地址等分成32块regions (Regions 0 to 31)，每个regions的大小是可以通过编程设置OB_SIZE寄存器确定大小，大小有1, 2, 4, or 8 MB，那么通过Outbound能够翻译的地址最大为8M*32=256M。存储域地址中有5位作为识别32个regions的index，OB_SIZE的大小决定这5位在32位地址上的位置。当OB_SIZE等于0，1，2，3时，index在存储地址中对应的位置是Bits[24:20], bits[25:21], bits[26:22], and bits[27:23]，每个regions翻倍，是不是对应的地址应该按翻倍对齐呢，翻倍就是左移一位数据。OB_SIZE寄存器如图 3所示。\n![](picture7.jpg)                                                                                      图 7\n\nOB_OFFSET_INDEXn寄存器结构如图 4所示，n是上一段落提到的index的值。该寄存器第0位是地址翻译使能位，第31~20位是第n个regions的基地址的31~20位，这里的取值取决regions的大小，当OB_SIZE 等于0，1，2，3时，bits[31:20], bits[31:21], bits[31:22], and bits[31:23]位相应被使用。OB_OFFSETn_HI寄存器的值是64位PCIe地址中第n个regions的基地址的63~32位，在32位PCIe地址中，该寄存器的值等于0。\n![](picture8.jpg)\n图 8\n\n配置OutBound翻译的几个寄存器也做了详解，下面根据举例说明。图 5中配置空间存储地址由CPU本身架构所决定，这部分的地址映射才芯片内部完成，不需要由编程人员配置。PCIe设备空间被分成了32等分。假设region大小是2M，PCIe地址是64位，程序中需要对0x9D3A_1234存储地址做映射， 64位PCIe地址被使用在region 9上，初始化OBOFFSET9_HI值为0x3344 5566, OB_OFFSET9值56Ex xxxx（x的值这里不关心，看该寄存器结构就很清楚，第0位在地址翻译时候应该使能位1，这里仅仅用来讲解怎么做映射，不需要关心后面的Bits） ，下面分析怎么翻译到PCIe地址：\n\n由于是regions大小2M，那么index应该取地址的bits [25:21]，提取0x9D3A_1234存储地址的bits [25:21得到01001b，该值等于9，那么该地址应该启用regions 9 翻译。存储地址的bits[20:00]是用做翻译到PCIe地址的bits[20:00]位，该值也可以理解成reginos 9内的偏移值，值是0x001A 1234。\n生成regions 块PCIe的基地址，该地址应pcie_base=OBOFFSET9_HI <<32 + OB_OFFSET9的bits[31:21] = 0x 3344 5566 56E0 0000。\n计算PCIe地址，pcie_addr = pcie_base + 存储地址bits[20:0] =0x3344 5566 56FA 1234。\n![](picture9.jpg)\n图 9\n\n从上面存储地址到PCIe地址映射可以看到，通过cpu寻址可以直接访问到PCIe设备空间，最多可以访问PCIe设备空间大小为256M，具体Outbound能够访问的大小根据芯片而定，当CPU与FPGA之间有大量数据交互时候也可以采用Inbound方式（Inbound地址翻译流程如图 6所示，这里就不在翻译），将CPU的内存映射到FPGA的寻址空间（这里是站在CPU角度看的，从图2可以理解具体映射大小还由EP决定），FPGA可以采用DMA方式访问cpu的内存，并且速度很快。有些芯片厂商干脆采用同核异构方式将CPU于FPGA集成在一起（有的将cpu与dsp集成在一起），两者之间采用AXI高速总线通讯。\n![](picture10.jpg)                                                                                    \\图 10\n\n扫描PCIe树\n扫描树的流程如下：\n\n建立存储地址到PCIe地址映射 （映射方式上面段落已经讲解了，固定的PCIe配置空间映射）\n分配PCIe总线号\n分配设备号\n访问配置空间 （这里有一个原则读者需要注意，对PCIe设备配置空间访问时，一定要确定总线号、设备号、功能号、寄存器，不然无法找到设备）\n读写BAR0确定PCIe设备1空间大小\n分配PCIe地址\n分配总线号\n扫描PCIe总线树时，需要对这些PCIe总线进行编号，即初始化PCIe桥（在本文一律指透明桥）的Primary、Secondary和Subordinate Bus寄存器。在Linux内核中采用DFS算法对PCIe总线树进行遍历，DFS算法是按照深度优先的原则遍历PCIe树，局部代码如图 7所示，这里可以跟踪pci_scan_bridge函数，函数采用DFS算法对总线进行编号（后期会讲解搜索树，比较常见hash表、红黑树）。\n![](picture11.jpg)\n图 11\n\n分配设备号\nPCIe设备的IDSEL信号与PCIe总线的AD[31:0]信号的连接关系决定了该设备在这条总线上的设备号。在配置读写总线事务的地址周期中，AD[10:0]已经被信号已经被功能号和寄存器号使用，因此PCIe设备的IDSEL只能与AD[31:11]信号连接。上一篇文章中谈到CONFIG_ADDR寄存器中的Device Number字段一共有5位，最大能够表示32个设备，这里只有21位，显然在两者之间不能建立一一映射关系。一个PCIe总线号下最多可以挂在21个PCIe设备，那么多个PCIe总线不就可以挂载32个设备了么。\n\n访问配置空间\n在32位PCIe地址空间中，PCIe设备通常将PCIe配置存放在E2PROM中，PCIe设备进行上电初始化时，将E2PROM中的信息读到PCIe设备的配置空间作为初始值，由硬件自动完成。BAR0空间存储了PCIe设备空间的大小，某些位被设置成不可预读，当BAR0全部写入1时，然后在读取BAR0值，从数据低位看有多少连续位没有改变。没有改变的数据位数记录的该PCIe设备空间的大小，假如有n位没有改变，那么设备空间大小应该是2的n次方。第0位代表IO/Memory、第2，3位代表32/64位地址、第4位代表是否可预取，具体位定义格式可以直接参考内核PCIe总线代码，解析BAR函数如图 8所示。\n![](picture12.jpg)\n图 12\n\n分配PCIe地址\n系统软件根据根据设备空间大小建立存储地址PCIe设备地址空间的映射，给PCIe设备分配的PCIe基地址写入到BAR0，如果是64位PCIe地址，那么BAR1是高32位地址。\n\n结语\n 写《从cpu角度理解PCIe》文章我参考了部分芯片的datasheet，并结合linux代码分析，本文仅仅起到分析流程的作用，具体映射机制和寄存器芯片参考相应芯片datasheet。如有错误，还望指正。\n","categories":["linux"]},{"title":"BIOS","url":"/2021/03/13/linux/BIOS/","content":"\n英特尔公司从2000年开始，发明了可扩展固件接口（Extensible Firmware Interface），用以规范BIOS的开发。而支持EFI规范的BIOS也被称为EFI BIOS。之后为了推广EFI，业界多家著名公司共同成立了统一可扩展固件接口论坛（UEFI Forum），英特尔公司将EFI 1.1规范贡献给业界，用以制订新的国际标准UEFI规范。目前UEFI规范的最新版本是2.3.1，英特尔公司曾经预测，2010年，全世界或有有60%以上的个人电脑使用支持UEFI规范的BIOS产品。\n![](bios.gif)\n抠出纽扣电池可使BIOS恢复到出厂默认值\n\nBIOS设置程序是储存在BIOS芯片中的，BIOS芯片是主板上一块长方形或正方形芯片，只有在开机时才可以进行设置。（一般在计算机启动时按F2或者Delete进入BIOS进行设置，一些特殊机型按F1、Esc、F12等进行设置）。BIOS设置程序主要对计算机的基本输入输出系统进行管理和设置，使系统运行在最好状态下，使用BIOS设置程序还可以排除系统故障或者诊断系统问题。有人认为既然BIOS是\"程序\"，那它就应该是属于软件，感觉就像自己常用的Word或Excel。但也有很多人不这么认为，因为它与一般的软件还是有一些区别，而且它与硬件的联系也是相当地紧密。形象地说，BIOS应该是连接软件程序与硬件设备的一座\"桥梁\"，负责解决硬件的即时要求。主板上的BIOS芯片或许是主板上唯一贴有标签的芯片，一般它是一块32针的双列直插式的集成电路，上面印有\"BIOS\"字样。\nROM\n在微机（微型计算机Microcomputer System）的发展初期，BIOS都存放在ROM（Read Only Memory，只读存储器）中。ROM内部的资料是在ROM的制造工序中，在工厂里用特殊的方法被烧录进去的，其中的内容只能读不能改，一旦烧录进去，用户只能验证写入的资料是否正确，不能再作任何修改。如果发现资料有任何错误，则只有舍弃不用。\nEPROM\nEPROM（Erasable Programmable ROM，可擦除可编程ROM）芯片可重复擦除和写入，解决了ROM芯片只能写入一次的弊端。EPROM芯片有一个很明显的特征，在其正面的陶瓷封装上，开有一个玻璃窗口，透过该窗口，可以看到其内部的集成电路，紫外线透过该孔照射内部芯片就可以擦除其内的数据，完成芯片擦除的操作要用到EPROM擦除器。EPROM内资料的写入要用专用的编程器，并且往芯片中写内容时必须要加一定的编程电压（VPP=12—24V，随不同的芯片型号而定）。EPROM的型号是以27开头的，如27C020(8*256K）是一片2M Bits容量的EPROM芯片。EPROM芯片在写入资料后，还要以不透光的贴纸或胶布把窗口封住，以免受到周围的紫外线照射而使资料受损。\nEEPROM\n由于EPROM操作的不便，586以后的主板上BIOS ROM芯片大部分都采用EEPROM（Electrically Erasable Programmable ROM，电可擦除可编程ROM）。 [7]  通过跳线开关和系统配带的驱动程序盘，可以对EEPROM进行重写，方便地实现BIOS升级。\nBIOS芯片中主要存放：\n●自诊断程序：通过读取CMOSRAM中的内容识别硬件配置，并对其进行自检和初始化；\n● CMOS设置程序：引导过程中，用特殊热键启动，进行设置后，存入CMOS RAM中；\n● 系统自举装载程序：在自检成功后将磁盘相对0道0扇区上的引导程序装入内存，让其运行以装入DOS系统；\n● 主要I/O设备的驱动程序和中断服务：由于BIOS直接和系统硬件资源打交道，因此总是针对某一类型的硬件系统，而各种硬件系统又各有不同，所以存在各种不同种类的BIOS，随着硬件技术的发展，同一种BIOS也先后出现了不同的版本，新版本的BIOS比起老版本来说，功能更强。\nNORFlash\n从奔腾时代开始，现代的电脑主板都使用NORFlash来作为BIOS的存储芯片。除了容量比EEPROM更大外，主要是NORFlash具有写入功能 [1]  ，运行电脑通过软件的方式进行BIOS的更新，而无需额外的硬件支持（通常EEPROM的擦写需要不同的电压和条件），且写入速度快。\n\n\n\n\n","categories":["linux"]},{"title":"ACPI是什么,BIOS中ACPI要怎么设置","url":"/2021/03/13/linux/ACPI是什么&BIOS中ACPI要怎么设置/","content":"有很多朋友都不知道ACPI是什么，在设备管理器或是BIOS里我们可以看到ACPI选项，比较常见的就是我们查看设备管理器中的计算机，有ACPI X64-based PC的标识。那么ACPI是什么？BIOS中开启ACPI又有什么作用呢？下面一起来学习下相关知识。 \n![](acpi.jpg)\n\n一、ACPI是什么？\n\nACPI其实是一种电源管理标准，ACPI是Advanced Configuration and Power Interface的首字母缩写，一般翻译成高级电源管理，是Intel、Microsoft和东芝共同开发的一种电源管理标准。\n二、ACPI有什么用？\n  ACPI是Windows的一部分(Win98开始)，它帮助操作系统合理控制和分配计算机硬件设备的电量，有了ACPI，操作系统可以根据设备实际情况，根据需要把不同的硬件设备关闭。如Win7或者Win8系统，系统睡眠时，系统把当前信息储存在内存中，只保留内存等几个关键部件硬件的通电，使计算机处在高度节电状态。\n  ACPI功能强大，它还能够实现设备和处理器性能管理、配置/即插即用设备管理、系统事件、温度管理、嵌入式控制器以及SMBus控制器等。\n![](acpi_bios.jpg)\n\n开启acpi有什么好处\n\nACPI表示高级配置和电源管理接口ACPI可实现以下功能:\n\n1、用户可以使外设在指定时间开关；\n2、使用笔记本电脑的用户可以指定计算机在低电压的情况下进入低功耗状态，以保证重要的应用程序运行；\n3、操作系统可以在应用程序对时间要求不高的情况下降低时钟频率；\n4、操作系统可以根据外设和主板的具体需求为它分配能源；\n5、在无人使用计算机时可以使计算机进入休眠状态，但保证一些通信设备打开；\n6、即插即用设备在插入时能够由ACPI来控制。\n\n目前的主流电脑都是支持ACPI电源标准的。\n","categories":["linux"]},{"title":"10 输入输出流","url":"/2021/03/13/language/cpp/10_输入输出流/","content":"\n## 输入输出流\n\n![](1.jpg)\n * istream：常用于接收从键盘输入的数据；\n * ostream：常用于将数据输出到屏幕上；\n * ifstream：用于读取文件中的数据；\n * ofstream：用于向文件中写入数据；\n * iostream：继承自 istream 和 ostream 类，因为该类的功能兼两者于一身，既能用于输入，也能用于输出；\n * fstream：兼 ifstream 和 ofstream 类功能于一身，既能读取文件中的数据，又能向文件中写入数据。\n\n\n\n\n\n\n\n","categories":["language","cpp"]},{"title":"c++ learn links","url":"/2021/03/13/language/cpp/c++_learn_links/","content":"\n\n## Learn C++ blogs\n\nhttps://www.cnblogs.com/bczd01/p/10112212.html\n","categories":["language","cpp"]},{"title":"Unit 1","url":"/2021/03/13/language/english/english/","content":"\n### abandon ~ abolish  \n\n2. abandon 放弃  \na = at; bandon = ban 禁令。 处于禁令之中->放弃自身权利 \n392. ban 取缔， 查禁    \n394. band 捆绑，乐队(乐队队员都绑着同样的袋子) n.波段，一群，疑惑，v.束缚，绑扎  \n489. bind 捆， 绑， 包括， 束缚  过去分词->bound  与下面词是异源同形词  \n555. bound adj.被束缚的, 一定的 n. 界限 v.&跳(跃)  \n554. bounce n.&vi. 弹起, 弹回，跳起； n. 弹力  \n556. boundary n.分界线, 边界  ary -> 形容词or名词后缀  \n395. bandage n.绷带 v. 用绷带扎缚  \n2396. husband n.丈夫  （hus==house：与房子绑定的人 ==hut） \n464. bend v.弯曲; 屈从，屈服；n. 弯曲(处)  \n536. bond n.结合，粘结； 公债，债券； 契约  \n\n","categories":["language","english"]},{"title":"1.webstorm自动提示设置","url":"/2021/03/13/language/nodejs/1.webstorm自动提示设置/","content":"\n### 1. 设置 Webstorm js 语法支持到 es6（或根据需要选择）\n![](1.png)\n\n### 2. 下载 node 语法库\n![](2.png)\n![](3.png)\n![](4.png)\n\n### 3. 不要过滤node_modules文件夹！\n![](5.png)\n\n## 效果\n设置后，不仅没有语法波浪线，在输入的时候已经有代码候选补全，并且按ctrl点击还能跳转查看源码\n![](6.png)\n标签组件名也能补全了\n![](7.png)\n标签也不会出现语法背景黄色，并且还支持自定义属性参数的补全\n![](8.png)","categories":["language","nodejs"]},{"title":"STL 01","url":"/2021/03/13/language/cpp/STL/01.STL/","content":"\n通常认为，STL 是由容器、算法、迭代器、函数对象、适配器、内存分配器这 6 部分构成，其中后面 4 部分是为前 2 部分服务的，它们各自的含义如表 1 所示。\n\n表 1 STL 组成结构\n\n| STL的组成 | 含义 |\n| :-----: | :------: |\n| 容器 | 一些封装数据结构的模板类，例如 vector 向量容器、list 列表容器等。 |\n| 算法 | STL 提供了非常多（大约 100 个）的数据结构算法，它们都被设计成一个个的模板函数，这些算法在 std 命名空间中定义，其中大部分算法都包含在头文件 <algorithm> 中，少部分位于头文件 <numeric> 中。 |\n| 迭代器 | 在 C++ STL 中，对容器中数据的读和写，是通过迭代器完成的，扮演着容器和算法之间的胶合剂。 |\n| 函数对象 | 如果一个类将 () 运算符重载为成员函数，这个类就称为函数对象类，这个类的对象就是函数对象（又称仿函数）。 |\n| 适配器 | 可以使一个类的接口（模板的参数）适配成用户指定的形式，从而让原本不能在一起工作的两个类工作在一起。值得一提的是，容器、迭代器和函数都有适配器。 |\n| 内存分配器 | 为容器类模板提供自定义的内存申请和释放功能，由于往往只有高级用户才有改变内存分配策略的需求，因此内存分配器对于一般用户来说，并不常用。 |\n\n> 关于表 1 中罗列的 STL 的构成，初学者简单了解即可，后续章节将专门对它们做系统的深入讲解。\n\n另外，在惠普实验室最初发行的版本中，STL 被组织成 48 个头文件；但在 C++ 标准中，它们被重新组织为 13 个头文件，如表 2 所示。\n\n表 2 C++ STL头文件\n\n| head | head | head | head |\n| :-----: | :------: |:-----: | :------: |\n| `<iterator>` | `<functional>` | `<vector>` | `<deque>` |\n| `<list>` | `<queue>` | `<stack>` | `<set>` |\n| `<map>` | `<algorithm>` | `<numeric>` | `<memory>` |\n| `<utility>` |  |  |  |\n\n> 关于这些头文件的作用和用法，本节不做过多赘述，后续章节会做详细介绍。\n\n\n按照 C++ 标准库的规定，所有标准头文件都不再有扩展名。以 <vector> 为例，此为无扩展名的形式，而 <vector.h> 为有扩展名的形式。\n\n但是，或许是为了向下兼容，或许是为了内部组织规划，某些 STL 版本同时存储具备扩展名和无扩展名的两份文件（例如 Visual C++ 支持的 Dinkumware 版本同时具备 <vector.h> 和 <vector>）；甚至有些 STL 版本同时拥有 3 种形式的头文件（例如 SGI 版本同时拥有 <vector>、<vector.h> 和 <stl_vector.h>）；但也有个别的 STL 版本只存在包含扩展名的头文件（例如 C++ Builder 的 RaugeWare 版本只有 <vector.h>）。\n\n> 建议读者养成良好的习惯，遵照 C++ 规范，使用无扩展名的头文件。\n\n\n\n","categories":["language","cpp"]},{"title":"Hello World","url":"/2021/03/13/hello-world/","content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n"}]