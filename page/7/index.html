<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="format-detection" content="telephone=no"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="icon" href="/images/icons/favicon-16x16.png?v=2.6.1" type="image/png" sizes="16x16"><link rel="icon" href="/images/icons/favicon-32x32.png?v=2.6.1" type="image/png" sizes="32x32"><meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://kung-fu-master.github.io/page/7/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Kung-Fu-Master">
<meta name="twitter:card" content="summary"><title>Hexo</title><link ref="canonical" href="https://kung-fu-master.github.io/page/7/index.html"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.12.1/css/all.min.css" type="text/css"><link rel="stylesheet" href="/css/index.css?v=2.6.1"><script>var Stun = window.Stun || {};
var CONFIG = {
  root: '/',
  algolia: undefined,
  assistSearch: undefined,
  fontIcon: {"prompt":{"success":"fas fa-check-circle","info":"fas fa-arrow-circle-right","warning":"fas fa-exclamation-circle","error":"fas fa-times-circle"},"copyBtn":"fas fa-copy"},
  sidebar: {"offsetTop":"20px","tocMaxDepth":6},
  header: {"enable":true,"showOnPost":false,"scrollDownIcon":true},
  postWidget: {"endText":true},
  nightMode: {"enable":true},
  back2top: {"enable":true},
  codeblock: {"style":"default","highlight":"light","wordWrap":false},
  reward: false,
  fancybox: false,
  zoomImage: {"gapAside":"20px"},
  galleryWaterfall: undefined,
  lazyload: true,
  pjax: undefined,
  externalLink: {"icon":{"enable":true,"name":"fas fa-external-link-alt"}},
  shortcuts: undefined,
  prompt: {"copyButton":"复制","copySuccess":"复制成功","copyError":"复制失败"},
  sourcePath: {"js":"js","css":"css","images":"images"},
};

window.CONFIG = CONFIG;</script><meta name="generator" content="Hexo 4.2.0"></head><body><div class="container" id="container"><header class="header" id="header"><div class="header-inner"><nav class="header-nav header-nav--fixed"><div class="header-nav-inner"><div class="header-nav-menubtn"><i class="fas fa-bars"></i></div><div class="header-nav-menu"><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-home"></i></span><span class="header-nav-menu-item__text">首页</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="javascript:;" onclick="return false;"><span class="header-nav-menu-item__icon"><i class="fas fa-feather-alt"></i></span><span class="header-nav-menu-item__text">文章</span></a><div class="header-nav-submenu"><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/categories/"><span class="header-nav-submenu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-submenu-item__text">分类</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/archives/"><span class="header-nav-submenu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-submenu-item__text">归档</span></a></div><div class="header-nav-submenu-item"><a class="header-nav-submenu-item__link" href="/tags/"><span class="header-nav-submenu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-submenu-item__text">标签</span></a></div></div></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/categories/"><span class="header-nav-menu-item__icon"><i class="fas fa-layer-group"></i></span><span class="header-nav-menu-item__text">分类</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/archives/"><span class="header-nav-menu-item__icon"><i class="fas fa-folder-open"></i></span><span class="header-nav-menu-item__text">归档</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/tags/"><span class="header-nav-menu-item__icon"><i class="fas fa-tags"></i></span><span class="header-nav-menu-item__text">标签</span></a></div><div class="header-nav-menu-item"><a class="header-nav-menu-item__link" href="/"><span class="header-nav-menu-item__icon"><i class="fas fa-fingerprint"></i></span><span class="header-nav-menu-item__text">关于</span></a></div></div><div class="header-nav-search"><span class="header-nav-search__icon"><i class="fas fa-search"></i></span><span class="header-nav-search__text">搜索</span></div><div class="header-nav-mode"><div class="mode"><div class="mode-track"><span class="mode-track-moon"></span><span class="mode-track-sun"></span></div><div class="mode-thumb"></div></div></div></div></nav><div class="header-banner"><div class="header-banner-info"><div class="header-banner-info__title">Hexo</div><div class="header-banner-info__subtitle"></div></div><div class="header-banner-arrow"><div class="header-banner-arrow__icon"><i class="fas fa-angle-down"></i></div></div></div></div></header><main class="main" id="main"><div class="main-inner"><div class="content-wrap" id="content-wrap"><div class="content content-home" id="content"><section class="postlist"><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/01_etcd_high_availablity/">01 部署external etcd集群</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">2k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="部署etcd集群"   >
          <a href="#部署etcd集群" class="heading-link"><i class="fas fa-link"></i></a>部署etcd集群</h2>
      <div class="table-container"><table>
<thead>
<tr>
<th align="center">组件</th>
<th align="center">使用的证书</th>
</tr>
</thead>
<tbody><tr>
<td align="center">etcd</td>
<td align="center">ca.pem, server.pem, server-key.pem</td>
</tr>
<tr>
<td align="center">kube-apiserver</td>
<td align="center">ca.pem, server.pem, server-key.pem</td>
</tr>
<tr>
<td align="center">kubelet</td>
<td align="center">ca.pem, ca-key.pem</td>
</tr>
<tr>
<td align="center">kube-proxy</td>
<td align="center">ca.pem, kube-proxy.pem, kube-proxy-key.pem</td>
</tr>
<tr>
<td align="center">kubectl</td>
<td align="center">ca.pem, admin.pem, admin-key.pem</td>
</tr>
</tbody></table></div>
<p>所有k8s证书,配置,安装包都放到/opt/kubernetes/目录下</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/kubernetes/&#123;ssl,cfg,bin&#125;</span><br><span class="line">ls /opt/kubernetes/</span><br><span class="line">bin/  cfg/  ssl/</span><br></pre></td></tr></table></div></figure>


        <h3 id="1-生成ca-key-pem私匙和ca-pem证书"   >
          <a href="#1-生成ca-key-pem私匙和ca-pem证书" class="heading-link"><i class="fas fa-link"></i></a>1. 生成ca-key.pem私匙和ca.pem证书</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; ca-config.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    "signing": &#123;</span><br><span class="line">        "default": &#123;</span><br><span class="line">            "expiry": "87600h"</span><br><span class="line">        &#125;,</span><br><span class="line">        "profiles": &#123;</span><br><span class="line">            "kubernetes": &#123;</span><br><span class="line">                "expiry": "87600h",</span><br><span class="line">                "usages": [</span><br><span class="line">                    "signing",</span><br><span class="line">                    "key encipherment",</span><br><span class="line">                    "server auth"</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">cat &gt; ca-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    "CN": "kubernetes",</span><br><span class="line">    "key": &#123;</span><br><span class="line">        "algo": "rsa",			// 注意是algo而不是also</span><br><span class="line">        "size": 3072</span><br><span class="line">    &#125;,</span><br><span class="line">    "names": [</span><br><span class="line">        &#123;</span><br><span class="line">            "C": "CN",			// 哪个国家的可以随便写</span><br><span class="line">            "L": "Beijing",		// 地点随便写</span><br><span class="line">            "ST": "Beijing",	// 地点随便写</span><br><span class="line">            "O": "k8s",			// 用户组, 固定的， 不能随便写</span><br><span class="line">            "OU": "System"		// 用户, 固定的，不能随便写</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>

<p>生成ca.pem和ca-key.pem</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// -bare ca 表示生成以ca开头的证书和key</span><br><span class="line">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</span><br><span class="line">ls ca*</span><br><span class="line">ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem</span><br></pre></td></tr></table></div></figure>

        <h3 id="2-生成server端证书和key-用于k8s的etcd和kube-apiserver"   >
          <a href="#2-生成server端证书和key-用于k8s的etcd和kube-apiserver" class="heading-link"><i class="fas fa-link"></i></a>2. 生成server端证书和key, 用于k8s的etcd和kube-apiserver</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; server-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    "CN": "kubernetes",</span><br><span class="line">    "hosts": [			// 包含了哪些机器IP和域名需要此server端证书</span><br><span class="line">        "127.0.0.1",</span><br><span class="line">        "10.239.140.133",		// 要使用改证书的etcd或其他服务部署所在节点IP地址或域名</span><br><span class="line">        "10.239.131.206",</span><br><span class="line">        "10.239.141.145",</span><br><span class="line">        "10.239.141.194",</span><br><span class="line">        "kubernetes.default",</span><br><span class="line">        "kubernetes.default.svc",</span><br><span class="line">        "kubernetes.default.svc.cluster",</span><br><span class="line">        "kubernetes.default.svc.cluster.local"</span><br><span class="line">    ],</span><br><span class="line">    "key": &#123;</span><br><span class="line">        "algo": "rsa",</span><br><span class="line">        "size": 3072</span><br><span class="line">    &#125;,</span><br><span class="line">    "names": [</span><br><span class="line">        &#123;</span><br><span class="line">            "C": "CN",</span><br><span class="line">            "L": "Beijing",</span><br><span class="line">            "ST": "Beijing",</span><br><span class="line">            "O": "k8s",			//和下面一起代表了用户和组去请求集群</span><br><span class="line">            "OU": "System"</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>
<p>生成server端证书</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// -bare server 表示生成以server开头的证书和key</span><br><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</span><br><span class="line">ls server*</span><br><span class="line">server.csr  server-csr.json  server-key.pem  server.pem</span><br></pre></td></tr></table></div></figure>


        <h3 id="3-生成admin证书-集群管理员通过证书访问集群"   >
          <a href="#3-生成admin证书-集群管理员通过证书访问集群" class="heading-link"><i class="fas fa-link"></i></a>3. 生成admin证书, 集群管理员通过证书访问集群</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; admin-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    "CN": "admin",</span><br><span class="line">    "hosts": [],</span><br><span class="line">    "key": &#123;</span><br><span class="line">        "algo": "rsa",</span><br><span class="line">        "size": 3072</span><br><span class="line">    &#125;,</span><br><span class="line">    "names": [</span><br><span class="line">        &#123;</span><br><span class="line">            "C": "CN",</span><br><span class="line">            "L": "Beijing",</span><br><span class="line">            "ST": "Beijing",</span><br><span class="line">            "O": "system:masters",	// 用户组, 不要改动, 否则会认证失败</span><br><span class="line">            "OU": "System"			// 用户</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>

<p>生成管理员证书和key</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">// -bare admin 表示生成以admin开头的证书和key</span><br><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin</span><br><span class="line">ls admin*</span><br><span class="line">admin.csr  admin-csr.json  admin-key.pem  admin.pem</span><br></pre></td></tr></table></div></figure>


        <h3 id="4-生成kube-proxy证书"   >
          <a href="#4-生成kube-proxy证书" class="heading-link"><i class="fas fa-link"></i></a>4. 生成kube-proxy证书</h3>
      <p>工作节点通过kube-proxy组件访问api-server生成一些网络策略, 必须得有权限, 生成一个证书, 让kube-proxy携带证书去访问集群.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; kube-proxy-csr.json &lt;&lt; EOF</span><br><span class="line">&#123;</span><br><span class="line">    "CN": "system:kube-proxy",		//固定,不能改变</span><br><span class="line">    "hosts": [],</span><br><span class="line">    "key": &#123;</span><br><span class="line">        "algo": "rsa",</span><br><span class="line">        "size": 3072</span><br><span class="line">    &#125;,</span><br><span class="line">    "names": [</span><br><span class="line">        &#123;</span><br><span class="line">            "C": "CN",</span><br><span class="line">            "L": "Beijing",</span><br><span class="line">            "ST": "Beijing",</span><br><span class="line">            "O": "k8s",</span><br><span class="line">            "OU": "System"</span><br><span class="line">        &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>
<p>生成kube-proxy证书和key</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy</span><br><span class="line">ls kube-proxy*</span><br><span class="line">kube-proxy.csr  kube-proxy-csr.json  kube-proxy-key.pem  kube-proxy.pem</span><br></pre></td></tr></table></div></figure>


        <h3 id="5-只保留-pem-删除其它文件"   >
          <a href="#5-只保留-pem-删除其它文件" class="heading-link"><i class="fas fa-link"></i></a>5. 只保留*.pem, 删除其它文件</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ls | grep -v pem | xargs -i rm &#123;&#125;</span><br><span class="line">ls</span><br><span class="line">admin-key.pem  admin.pem  ca-key.pem  ca.pem  kube-proxy-key.pem  kube-proxy.pem  server-key.pem  server.pem</span><br></pre></td></tr></table></div></figure>


        <h2 id="关闭防火墙或开发端口"   >
          <a href="#关闭防火墙或开发端口" class="heading-link"><i class="fas fa-link"></i></a>关闭防火墙或开发端口</h2>
      
        <h3 id="关闭防火墙"   >
          <a href="#关闭防火墙" class="heading-link"><i class="fas fa-link"></i></a>关闭防火墙</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">setenforce 0</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables=1</span><br></pre></td></tr></table></div></figure>


        <h3 id="如果使用firewalld作为防火墙，则需要开放端口"   >
          <a href="#如果使用firewalld作为防火墙，则需要开放端口" class="heading-link"><i class="fas fa-link"></i></a>如果使用firewalld作为防火墙，则需要开放端口</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --zone=public --add-port=2379/tcp --permanent</span><br><span class="line">firewall-cmd --zone=public --add-port=2380/tcp --permanent</span><br><span class="line">firewall-cmd --reload</span><br><span class="line">firewall-cmd --list-all</span><br></pre></td></tr></table></div></figure>


        <h2 id="etcd安装"   >
          <a href="#etcd安装" class="heading-link"><i class="fas fa-link"></i></a>etcd安装</h2>
      <p>etcd是由coreos公司开发在GitHub上开源的存储键值对的数据库</p>

        <h3 id="etcd-下载"   >
          <a href="#etcd-下载" class="heading-link"><i class="fas fa-link"></i></a>etcd 下载</h3>
      <p>本次测试安装etcd的3.2.12版本<br><span class="exturl"><a class="exturl__link"   href="https://github.com/etcd-io/etcd/releases/tag/v3.2.12"  target="_blank" rel="noopener">https://github.com/etcd-io/etcd/releases/tag/v3.2.12</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br>下载解压并移动到指定目录</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget -L https://github.com/etcd-io/etcd/releases/download/v3.2.12/etcd-v3.2.12-linux-amd64.tar.gz</span><br><span class="line">tar -zxvf etcd-v3.2.12-linux-amd64.tar.gz</span><br><span class="line">ls etcd-v3.2.12-linux-amd64</span><br><span class="line">Documentation/  etcd  etcdctl  README-etcdctl.md  README.md  READMEv2-etcdctl.md</span><br><span class="line">// 将解压后文件中的可执行文件etct和etcdctl移动到/opt/kubernetes/bin/目录下</span><br><span class="line">mv etcd-v3.2.12-linux-amd64/etcd /opt/kubernetes/bin/</span><br><span class="line">mv etcd-v3.2.12-linux-amd64/etcdctl /opt/kubernetes/bin/</span><br></pre></td></tr></table></div></figure>


        <h3 id="创建ETCD的配置文件"   >
          <a href="#创建ETCD的配置文件" class="heading-link"><i class="fas fa-link"></i></a>创建ETCD的配置文件</h3>
      <figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /opt/kubernetes/cfg/etcd &lt;&lt; EOF</span><br><span class="line"><span class="comment">#[Member]</span></span><br><span class="line">ETCD_NAME=<span class="string">"etcd01"</span></span><br><span class="line">ETCD_DATA_DIR=<span class="string">"/var/lib/etcd/default.etcd"</span></span><br><span class="line">ETCD_LISTEN_PEER_URLS=<span class="string">"https://10.239.140.133:2380"</span></span><br><span class="line">ETCD_LISTEN_CLIENT_URLS=<span class="string">"https://10.239.133:2379"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#[Clustering]</span></span><br><span class="line">ETCD_INITIAL_ADVERTISE_PEER_URLS=<span class="string">"https://10.239.140.133:2380"</span></span><br><span class="line">ETCD_ADVERTISE_CLIENT_URLS=<span class="string">"https://10.239.140.133:2379"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER=<span class="string">"etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_TOKEN=<span class="string">"etcd-cluster"</span></span><br><span class="line">ETCD_INITIAL_CLUSTER_STATE=<span class="string">"new"</span></span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>

        <h3 id="使用systemd来管理etcd服务"   >
          <a href="#使用systemd来管理etcd服务" class="heading-link"><i class="fas fa-link"></i></a>使用systemd来管理etcd服务</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt; EOF</span><br><span class="line">[Unit]					//systemd依赖的一些服务, 网络服务启动之后再启动此服务</span><br><span class="line">Description=Etcd Server</span><br><span class="line">After=network.target</span><br><span class="line">After=network-online.target</span><br><span class="line">Wants=network-online.target</span><br><span class="line"></span><br><span class="line">[Service]</span><br><span class="line">Type=notify</span><br><span class="line">WorkingDirectory=/var/lib/etcd/					//看网上配置有这个参数, 自己配置过程中没有加入</span><br><span class="line">EnvironmentFile=-/opt/kubernetes/cfg/etcd		//指定服务启动配置文件</span><br><span class="line">ExecStart=/opt/kubernetes/bin/etcd \			//服务启动选项</span><br><span class="line">--name=$&#123;ETCD_NAME&#125; \</span><br><span class="line">--data-dir=$&#123;ETCD_DATA_DIR&#125; \</span><br><span class="line">--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \</span><br><span class="line">--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \</span><br><span class="line">--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \</span><br><span class="line">--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \</span><br><span class="line">--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \</span><br><span class="line">--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \</span><br><span class="line">--initial-cluster-state=new \</span><br><span class="line">--cert-file=/opt/kubernetes/ssl/server.pem \		// 指定数字证书</span><br><span class="line">--key-file=/opt/kubernetes/ssl/server-key.pem \</span><br><span class="line">--peer-cert-file=/opt/kubernetes/ssl/server.pem \</span><br><span class="line">--peer-key-file=/opt/kubernetes/ssl/server-key.pem \</span><br><span class="line">--trusted-ca-file=/opt/kubernetes/ssl/ca.pem \</span><br><span class="line">--peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem</span><br><span class="line">Restart=on-failure</span><br><span class="line">LimitNOFILE=65536</span><br><span class="line"></span><br><span class="line">[Install]</span><br><span class="line">WantedBy=multi-user.target</span><br><span class="line">EOF</span><br></pre></td></tr></table></div></figure>

<p>启动etcd服务</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">// 启动etcd服务发现卡住, 且过一会会报个错误, 原因是另外request另外两台etcd服务2380得不到响应</span><br><span class="line">// 待另外至少一台etcd服务启动后再回来查看etcd服务状态就正常了</span><br><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl enable etcd</span><br><span class="line">// 完整的启动etcd服务命令其实就是运行以下命令:</span><br><span class="line">/opt/kubernetes/bin/etcd --name="etcd01" --data-dir="/var/lib/etcd/default.etcd" --listen-peer-urls="https://10.239.140.133:2380" --listen-client-urls="https://10.239.140.133:2379,http://127.0.0.1:2379" --advertise-client-urls="https://10.239.140.133:2379" --initial-advertise-peer-urls="https://10.239.140.133:2380" --initial-cluster="etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380" --initial-cluster-token="etcd-cluster" --initial-cluster-state="new" --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem</span><br></pre></td></tr></table></div></figure>

<p>查看服务</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep etcd </span><br><span class="line">root     23726     1  5 21:41 ?        00:00:00 /opt/kubernetes/bin/etcd --name=etcd01 --data-dir=/var/lib/etcd/default.etcd --listen-peer-urls=https://10.239.140.133:2380 --listen-client-urls=https://10.239.140.133:2379,http://127.0.0.1:2379 --advertise-client-urls=https://10.239.140.133:2379 --initial-advertise-peer-urls=https://10.239.140.133:2380 --initial-cluster=etcd01=https://10.239.140.133:2380,etcd02=https://10.239.131.206:2380,etcd03=https://10.239.141.145:2380 --initial-cluster-token=etcd-cluster --initial-cluster-state=new --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --peer-cert-file=/opt/kubernetes/ssl/server.pem --peer-key-file=/opt/kubernetes/ssl/server-key.pem --trusted-ca-file=/opt/kubernetes/ssl/ca.pem --peer-trusted-ca-file=/opt/kubernetes/ssl/ca.pem</span><br><span class="line">root     23744 14957  0 21:41 pts/1    00:00:00 grep --color=auto etcd</span><br><span class="line">// 查看etcd服务状态和日志发现etcd一直尝试request其它两台机器, 状态不正常, 在其它两台也部署etcd后就可以了</span><br><span class="line">systemctl status etcd</span><br><span class="line">tail /var/log/messages</span><br></pre></td></tr></table></div></figure>


        <h3 id="etcd拷贝到其它机器"   >
          <a href="#etcd拷贝到其它机器" class="heading-link"><i class="fas fa-link"></i></a>etcd拷贝到其它机器</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/kubernetes root@10.239.131.206:/opt</span><br><span class="line">scp /usr/lib/systemd/system/etcd.service  root@10.239.131.206:/usr/lib/systemd/system/etcd.service</span><br><span class="line">// 只需要修改其它机器上/opt/kubernetes/cfg/etcd文件里的ETCD_NAME和其它参数IP地址然后就可以直接运行命令启动etcd服务.</span><br><span class="line">// 并且启动后不会卡住</span><br><span class="line">systemctl start etcd</span><br><span class="line">systemctl enable etcd</span><br></pre></td></tr></table></div></figure>


        <h3 id="测试etcd集群状态是否正常"   >
          <a href="#测试etcd集群状态是否正常" class="heading-link"><i class="fas fa-link"></i></a>测试etcd集群状态是否正常</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/opt/kubernetes/bin/etcdctl --ca-file=/opt/kubernetes/ssl/ca.pem --cert-file=/opt/kubernetes/ssl/server.pem --key-file=/opt/kubernetes/ssl/server-key.pem --endpoints="https://10.239.140.133:2379,https://10.239.131.206:2379,https://10.239.141.145:2379" cluster-health</span><br><span class="line">member 723f8ab932b4c3f6 is healthy: got healthy result from https://10.239.141.145:2379</span><br><span class="line">member 8af8f5fa5f0b0b39 is healthy: got healthy result from https://10.239.131.206:2379</span><br><span class="line">member 91dd39fb14e3de97 is healthy: got healthy result from https://10.239.140.133:2379</span><br><span class="line">cluster is healthy</span><br></pre></td></tr></table></div></figure>


        <h2 id="移除etcd集群"   >
          <a href="#移除etcd集群" class="heading-link"><i class="fas fa-link"></i></a>移除etcd集群</h2>
      <p>分别登陆各台部署etcd的机器上执行如下命令:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop etcd</span><br><span class="line">systemctl disable etcd</span><br><span class="line">rm -rf /usr/lib/systemd/system/etcd.service</span><br></pre></td></tr></table></div></figure>


        <h2 id="遇到的问题"   >
          <a href="#遇到的问题" class="heading-link"><i class="fas fa-link"></i></a>遇到的问题</h2>
      <p>查看Centos7系统日子</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /var/log/messages</span><br><span class="line">systemctl status etcd.service</span><br></pre></td></tr></table></div></figure>

<p>出现以下错误:</p>
<p>etcd.service: main process exited, code=exited, status=2/INVALIDARGUMENT</p>
<p>很明显是运行etcd命令时候的参数错误, 改对参数就可以了.</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/01_kubernetes_high_availablity_build/">01 Kubernetes build high availability</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">3.1k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="准备机器"   >
          <a href="#准备机器" class="heading-link"><i class="fas fa-link"></i></a>准备机器</h2>
      <div class="table-container"><table>
<thead>
<tr>
<th align="center">IP地址</th>
<th align="center">主机名</th>
<th align="center">角色</th>
</tr>
</thead>
<tbody><tr>
<td align="center">10.239.140.133</td>
<td align="center">master-node</td>
<td align="center">master</td>
</tr>
<tr>
<td align="center">10.239.131.156</td>
<td align="center">laboratory</td>
<td align="center">master</td>
</tr>
<tr>
<td align="center">10.239.141.123</td>
<td align="center">node-1</td>
<td align="center">master</td>
</tr>
<tr>
<td align="center">10.239.141.194</td>
<td align="center">node-2</td>
<td align="center">worker</td>
</tr>
<tr>
<td align="center">10.239.140.51</td>
<td align="center">k8s-vip</td>
<td align="center">Virtual IP</td>
</tr>
</tbody></table></div>
<p>总共四台机器，三台做master, 一台做work node, 部署好后可以把master上污点去掉, 照样可以部署k8s资源.<br>Virtual IP是部署过程中在机器网卡上添加的虚拟IP, 操作下方有涉及到.<br><strong><code>Note: k8s-vip地址需要独一无二, 不能与能ping同的所有其它机器的IP地址冲突, 因此, 设置k8s-vip ip地址之前线ping一下改地址看是否能ping通, 如果能平通就换成其它不能ping通, 也就是其它机器还没有占用的IP地址.</code></strong></p>
<p>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="finish_setup.PNG"  alt="">
      </p>

        <h2 id="环境配置"   >
          <a href="#环境配置" class="heading-link"><i class="fas fa-link"></i></a>环境配置</h2>
      <p>修改机器名字, 重开终端就可以看到机器名变了</p>
<figure class="highlight dsconfig"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">sudo </span><span class="string">hostnamectl </span><span class="built_in">set-hostname</span> <span class="string">master-node</span></span><br></pre></td></tr></table></div></figure>

<p>每台机器(master和node)都要配置</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// 设置k8s相关系统内核参数</span><br><span class="line">cat &lt;&lt; EOF &gt; /etc/sysctl.d/k8s.conf</span><br><span class="line">net.bridge.bridege-nf-call-iptables = 1</span><br><span class="line">net.bridge.bridege-nf-call-ip6tables = 1</span><br><span class="line">EOF</span><br><span class="line">sysctl -p</span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables</span><br><span class="line">echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables</span><br><span class="line">swapoff -a</span><br><span class="line">sed -i '/swap/d' /etc/fstab</span><br><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">setenforce 0</span><br><span class="line">sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config</span><br><span class="line">systemctl stop kubelet</span><br><span class="line">systemctl stop docker</span><br><span class="line">systemctl restart kubelet</span><br><span class="line">systemctl restart docker</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables=1</span><br><span class="line">sysctl net.bridge.bridge-nf-call-ip6tables=1</span><br><span class="line">iptables -F</span><br></pre></td></tr></table></div></figure>

<p><strong><code>Note:</code></strong></p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 有时候在公司开发机上部署不成功, 需要在~/.bashrc添加NO_PROXY</span><br><span class="line">// 不要忘了添加 127.0.0.1 和 虚拟出来的 Virtual IP</span><br><span class="line">cat &lt;&lt; EOF &gt;&gt; ~/.bashrc</span><br><span class="line">export NO_PROXY=127.0.0.1,&lt;master-node-IP&gt;,&lt;laboratory-IP&gt;,&lt;Node01-IP&gt;,&lt;Node02-IP&gt;,&lt;k8s-vip-IP&gt;, master-node,laboratory,Node01,Node02,k8s-vip</span><br><span class="line">EOF</span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></div></figure>

<p>如果在系统的<strong><code>/etc/environment</code></strong>中添加proxy, 则k8s安装过程api-server等组件会先读取/etc/environment文件中的proxy信息.</p>
<figure class="highlight makefile"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/environment</span><br><span class="line">http_proxy=<span class="string">"http://child-prc.intel.com:913"</span></span><br><span class="line">https_proxy=<span class="string">"http://child-prc.intel.com:913"</span></span><br><span class="line">no_proxy=<span class="string">"127.0.0.1,&lt;master-node-IP&gt;,&lt;laboratory-IP&gt;,&lt;Node01-IP&gt;,&lt;Node02-IP&gt;,&lt;k8s-vip-IP&gt;, master-node,laboratory,Node01,Node02,k8s-vip"</span></span><br></pre></td></tr></table></div></figure>


<p>修改 /etc/hosts文件内容</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line">......</span><br><span class="line">10.239.140.133 master-node</span><br><span class="line">10.239.131.156 laboratory</span><br><span class="line">10.239.141.123 node-1</span><br><span class="line">10.239.141.194 node-2</span><br><span class="line">10.239.140.51 k8s-vip</span><br></pre></td></tr></table></div></figure>
<p><strong><code>Note:</code></strong> 下面部署0.1.5版本的kube-vip时候 上面的/etc/hosts文件里的k8s-vip修改成如下内容</p>
<figure class="highlight accesslog"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">10.239.140.201</span> k8s-vip</span><br></pre></td></tr></table></div></figure>


        <h2 id="kube-vip方式部署高可用k8s集群"   >
          <a href="#kube-vip方式部署高可用k8s集群" class="heading-link"><i class="fas fa-link"></i></a>kube-vip方式部署高可用k8s集群</h2>
      <p>official website:<br><span class="exturl"><a class="exturl__link"   href="https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip"  target="_blank" rel="noopener">https://github.com/kubernetes/kubeadm/blob/master/docs/ha-considerations.md#kube-vip</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br><span class="exturl"><a class="exturl__link"   href="https://github.com/plunder-app/kube-vip/blob/master/kubernetes-control-plane.md"  target="_blank" rel="noopener">https://github.com/plunder-app/kube-vip/blob/master/kubernetes-control-plane.md</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span>  </p>

        <h2 id="在三台master机器上添加kube-vip配置文件"   >
          <a href="#在三台master机器上添加kube-vip配置文件" class="heading-link"><i class="fas fa-link"></i></a>在三台master机器上添加kube-vip配置文件</h2>
      
        <h3 id="kube-vip-0-1-1本地安装config-yaml版本"   >
          <a href="#kube-vip-0-1-1本地安装config-yaml版本" class="heading-link"><i class="fas fa-link"></i></a>kube-vip 0.1.1本地安装config.yaml版本</h3>
      <p><strong>master-node机器上</strong></p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /etc/kube-vip</span><br><span class="line">$ touch /etc/kube-vip/config.yaml</span><br><span class="line">localPeer:</span><br><span class="line">  id: master-node			// 机器hostname, 通过$ hostnamectl set-hostname <span class="tag">&lt;<span class="name">HostName</span>&gt;</span>修改</span><br><span class="line">  address: 10.239.140.133	// 指定本地IP地址</span><br><span class="line">  port: 10000</span><br><span class="line">remotePeers:</span><br><span class="line">- id: laboratory</span><br><span class="line">  address: 10.239.131.156	// 另外一台充当master机器IP地址</span><br><span class="line">  port: 10000</span><br><span class="line">- id: node-1</span><br><span class="line">  address: 10.239.141.123	// 另外一台充当master机器IP地址</span><br><span class="line">  port: 10000</span><br><span class="line"># [...]</span><br><span class="line">vip: 10.239.140.51		// 手动写的IP, 但必须在集群子网内, 部署集群后 $ ip addr 查看p8p1网卡下多出了此IP</span><br><span class="line">gratuitousARP: true</span><br><span class="line">singleNode: false</span><br><span class="line">startAsLeader: true			// 设置作为三台master机器的leader</span><br><span class="line">interface: p8p1				// 用本机的网卡名字, $ ip addr可查看</span><br><span class="line">loadBalancers:</span><br><span class="line">- name: API Server Load Balancer</span><br><span class="line">  type: tcp</span><br><span class="line">  port: 6443				// configure the load balancer to sit on the standard API-Server port 6443</span><br><span class="line">  bindToVip: true</span><br><span class="line">  backends:</span><br><span class="line">  - port: 6443		// configure the backends to point to the API-servers that will be configured to run on port 6444</span><br><span class="line">    address: 10.239.140.133</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.131.156</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.141.123</span><br><span class="line">  # [...]</span><br></pre></td></tr></table></div></figure>
<p><strong>laboratory机器上</strong></p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ touch /etc/kube-vip/config.yaml</span><br><span class="line">localPeer:</span><br><span class="line">  id: laboratory			// 改成本机的</span><br><span class="line">  address: 10.239.131.156</span><br><span class="line">  port: 10000</span><br><span class="line">remotePeers:</span><br><span class="line">- id: master-node</span><br><span class="line">  address: 10.239.140.133</span><br><span class="line">  port: 10000</span><br><span class="line">- id: node-1</span><br><span class="line">  address: 10.239.141.123</span><br><span class="line">  port: 10000</span><br><span class="line"># [...]</span><br><span class="line">vip: 10.239.140.51</span><br><span class="line">gratuitousARP: true</span><br><span class="line">singleNode: false</span><br><span class="line">startAsLeader: false		// 不要设置成为leader</span><br><span class="line">interface: eno1				// 改成本机的IP地址网卡名</span><br><span class="line">loadBalancers:</span><br><span class="line">- name: API Server Load Balancer</span><br><span class="line">  type: tcp</span><br><span class="line">  port: 6443</span><br><span class="line">  bindToVip: true</span><br><span class="line">  backends:</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.140.133</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.131.156</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.141.123</span><br><span class="line">  # [...]</span><br></pre></td></tr></table></div></figure>

<p><strong>node-1机器上</strong></p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ touch /etc/kube-vip/config.yaml</span><br><span class="line">localPeer:</span><br><span class="line">  id: node-1				// 改成本机的</span><br><span class="line">  address: 10.239.141.123</span><br><span class="line">  port: 10000</span><br><span class="line">remotePeers:</span><br><span class="line">- id: master-node</span><br><span class="line">  address: 10.239.140.133</span><br><span class="line">  port: 10000</span><br><span class="line">- id: laboratory</span><br><span class="line">  address: 10.239.131.156</span><br><span class="line">  port: 10000</span><br><span class="line"># [...]</span><br><span class="line">vip: 10.239.140.51</span><br><span class="line">gratuitousARP: true</span><br><span class="line">singleNode: false</span><br><span class="line">startAsLeader: false		// 不要设置成为leader</span><br><span class="line">interface: enp0s3			// 改成本机的IP地址网卡名</span><br><span class="line">loadBalancers:</span><br><span class="line">- name: API Server Load Balancer</span><br><span class="line">  type: tcp</span><br><span class="line">  port: 6443</span><br><span class="line">  bindToVip: true</span><br><span class="line">  backends:</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.140.133</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.131.156</span><br><span class="line">  - port: 6443</span><br><span class="line">    address: 10.239.141.123</span><br><span class="line">  # [...]</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>Use 6443 for both the VIP and the API-Servers, in order to do this we need to specify that the api-server is bound to it’s local IP. To do this we use the –apiserver-advertise-address flag as part of the init, this means that we can then bind the same port to the VIP and we wont have a port conflict.</p>
</blockquote>

        <h3 id="kube-vip-0-1-5本地安装config-yaml版本"   >
          <a href="#kube-vip-0-1-5本地安装config-yaml版本" class="heading-link"><i class="fas fa-link"></i></a>kube-vip 0.1.5本地安装config.yaml版本</h3>
      <p><strong>master-node机器上</strong></p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ touch /etc/kube-vip/config.yaml</span><br><span class="line">localPeer:</span><br><span class="line">  id: master-node</span><br><span class="line">  address: 10.239.140.137</span><br><span class="line">  port: 10000</span><br><span class="line">remotePeers:</span><br><span class="line">- id: laboratory</span><br><span class="line">  address: 10.239.131.157</span><br><span class="line">  port: 10000</span><br><span class="line">- id: node01</span><br><span class="line">  address: 10.239.140.50</span><br><span class="line">  port: 10000</span><br><span class="line"># [...]</span><br><span class="line">vip: 10.239.140.201</span><br><span class="line">gratuitousARP: true</span><br><span class="line">singleNode: false</span><br><span class="line">startAsLeader: true</span><br><span class="line">interface: p8p1</span><br><span class="line">loadBalancers:</span><br><span class="line">- name: API Server Load Balancer</span><br><span class="line">  type: tcp</span><br><span class="line">  port: 6443</span><br><span class="line">  bindToVip: true</span><br><span class="line">  backends:</span><br><span class="line">  - port: 6444</span><br><span class="line">    address: 10.239.140.137</span><br><span class="line">  - port: 6444</span><br><span class="line">    address: 10.239.131.157</span><br><span class="line">  - port: 6444</span><br><span class="line">    address: 10.239.140.50</span><br><span class="line">  # [...]</span><br></pre></td></tr></table></div></figure>


        <h2 id="部署High-Availability-K8s集群"   >
          <a href="#部署High-Availability-K8s集群" class="heading-link"><i class="fas fa-link"></i></a>部署High Availability K8s集群</h2>
      
        <h3 id="1-master-node机器上"   >
          <a href="#1-master-node机器上" class="heading-link"><i class="fas fa-link"></i></a>1. master-node机器上</h3>
      <p>现在master-node机器上配置好K8s集群, 然后再把其它两个master加进来就可以了.  </p>

        <h4 id="0-1-1版本"   >
          <a href="#0-1-1版本" class="heading-link"><i class="fas fa-link"></i></a>0.1.1版本:</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --rm plndr/kube-vip:0.1.1 /kube-vip sample manifest \</span><br><span class="line">    | sed "s|plndr/kube-vip:'|plndr/kube-vip:0.1.1'|" \</span><br><span class="line">    | sudo tee /etc/kubernetes/manifests/kube-vip.yaml</span><br></pre></td></tr></table></div></figure>

        <h4 id="0-1-5版本"   >
          <a href="#0-1-5版本" class="heading-link"><i class="fas fa-link"></i></a>0.1.5版本:</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it --rm plndr/kube-vip:0.1.5 sample manifest | sudo tee /etc/kubernetes/manifests/kube-vip.yaml</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>Ensure that image: plndr/kube-vip:<x> is modified to point to a specific version (0.1.5 at the time of writing), refer to docker hub for details.<br>Also ensure that the hostPath points to the correct kube-vip configuration, if it isn’t the above path.  </p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/kubernetes/manifests/kube-vip.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: null</span><br><span class="line">  name: kube-vip</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - command:</span><br><span class="line">    - /kube-vip</span><br><span class="line">    - start</span><br><span class="line">    - -c</span><br><span class="line">    - /vip.yaml</span><br><span class="line">    image: 'plndr/kube-vip:0.1.1'</span><br><span class="line">    name: kube-vip</span><br><span class="line">    resources: &#123;&#125;</span><br><span class="line">    securityContext:</span><br><span class="line">      capabilities:</span><br><span class="line">        add:</span><br><span class="line">        - NET_ADMIN</span><br><span class="line">        - SYS_TIME</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /vip.yaml</span><br><span class="line">      name: config</span><br><span class="line">  hostNetwork: true</span><br><span class="line">  volumes:</span><br><span class="line">  - hostPath:</span><br><span class="line">      path: /etc/kube-vip/config.yaml	// 跟上面的conf.yaml文件路径对应</span><br><span class="line">    name: config</span><br><span class="line">status: &#123;&#125;</span><br></pre></td></tr></table></div></figure>

        <h4 id="执行部署K8s集群命令"   >
          <a href="#执行部署K8s集群命令" class="heading-link"><i class="fas fa-link"></i></a>执行部署K8s集群命令</h4>
      
        <h5 id="0-1-1版本-1"   >
          <a href="#0-1-1版本-1" class="heading-link"><i class="fas fa-link"></i></a>(0.1.1版本)</h5>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --control-plane-endpoint "10.239.140.51:6443" --apiserver-advertise-address 10.239.140.133 --apiserver-bind-port 6443 --upload-certs --kubernetes-version "v1.19.0"</span><br><span class="line">* --control-plane-endpoin: 指定设置的Virtual IP和端口.</span><br><span class="line">* --apiserver-advertise-address: 指定第一台宿主机IP, 当Virtual IP所用port端口与apiserver port端口设置成相同时需要此参数.</span><br><span class="line">* --apiserver-bind-port: 指定apiserver运行所在的port, 此处与Virutal IP(做load balancing)所运行的port相同都是6443</span><br><span class="line">* --upload-certs: kubeadm部署方式下能够让证书自动上传.</span><br></pre></td></tr></table></div></figure>


        <h5 id="0-1-5版本-1"   >
          <a href="#0-1-5版本-1" class="heading-link"><i class="fas fa-link"></i></a>(0.1.5版本)</h5>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --control-plane-endpoint "10.239.140.201:6443" --apiserver-bind-port 6444 --upload-certs --kubernetes-version "v1.19.0"</span><br></pre></td></tr></table></div></figure>
<ul>
<li>–control-plane-endpoint: 指定Virtual IP地址和port为6443</li>
<li>–apiserver-bind-port: 指定apiserver运行所在的port为6444<br>这个 –upload-certs 标志用来将在所有控制平面实例之间的共享证书上传到集群.<br>当 –upload-certs 与 kubeadm init 一起使用时，主控制平面的证书被加密并上传到 kubeadm-certs 密钥中.  </li>
</ul>
<p>查看部署情况</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -A</span></span><br><span class="line">  NAMESPACE     NAME                                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">  &lt;...&gt;</span><br><span class="line">  kube-system   kube-vip-controlplane01                  1/1     Running   0          10m</span><br></pre></td></tr></table></div></figure>

<p><strong>查看网卡地址上是否多出了一个虚拟IP为:10.239.140.51</strong></p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip addr</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="2-laboratory和node-1机器上"   >
          <a href="#2-laboratory和node-1机器上" class="heading-link"><i class="fas fa-link"></i></a>2. laboratory和node-1机器上</h3>
      <blockquote>
<p>先不要在路径/etc/kubernetes/manifests/添加 kube-vip.yaml 文件, this is due to some bizarre kubeadm/kubelet behaviour.<br>等laboratory和node-1机器都添加进master集群后再添加kube-vip.yaml, kubeadm会自动检测/etc/kubernetes/manifests/文件变化并部署pod.<br>直接运行如下命令添加进master集群.</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.239.140.133:6443 --token &lt;tkn&gt; \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:&lt;hash&gt; \</span><br><span class="line">    --control-plane --certificate-key &lt;key&gt;</span><br></pre></td></tr></table></div></figure>
<p><strong>配置k8s访问环境变量</strong><br>这样就能在laboratory和node-1机器上执行kubectl命令了.<br>第一种:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/.kube</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp -i /etc/kubernetes/admin.conf <span class="variable">$HOME</span>/.kube/config</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo chown $(id -u):$(id -g) <span class="variable">$HOME</span>/.kube/config</span></span><br></pre></td></tr></table></div></figure>

<p>第二种:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> KUBECONFIG=/etc/kubernetes/admin.conf</span></span><br></pre></td></tr></table></div></figure>
<p>在master-node机器上查看laboratory和node-1机器已经加入master控制层面后, 再在laboratory和node-1机器上添加/etc/kubernetes/manifests/kube-vip.yaml文件.<br><strong>修改api-server访问地址为本机</strong><br>这样某一台master机器挂了其它机器照样可以正常访问api-server.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// laboratory机器上</span><br><span class="line">vim /etc/kubernetes/admin.conf</span><br><span class="line">  server: https://10.239.131.156:6443</span><br><span class="line">// node-1机器上</span><br><span class="line">vim /etc/kubernetes/admin.conf</span><br><span class="line">  server: https://10.239.141.123:6443</span><br></pre></td></tr></table></div></figure>


        <h4 id="配置0-1-5-或者0-1-1-版本kube-vip-yaml"   >
          <a href="#配置0-1-5-或者0-1-1-版本kube-vip-yaml" class="heading-link"><i class="fas fa-link"></i></a>配置0.1.5(或者0.1.1)版本kube-vip.yaml</h4>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -it --rm plndr/kube-vip:0.1.5 sample manifest | sudo tee /etc/kubernetes/manifests/kube-vip.yaml</span><br></pre></td></tr></table></div></figure>


        <h3 id="3-master-node机器上"   >
          <a href="#3-master-node机器上" class="heading-link"><i class="fas fa-link"></i></a>3. master-node机器上</h3>
      <p>在master-node机器上运行查看pod运行情况.  </p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -A | grep vip</span></span><br><span class="line">kube-system   kube-vip-controlplane01                  1/1     Running             1          16m</span><br><span class="line">kube-system   kube-vip-controlplane02                  1/1     Running             0          18m</span><br><span class="line">kube-system   kube-vip-controlplane03                  1/1     Running             0          20m</span><br></pre></td></tr></table></div></figure>

<p>查看 pod/kube-vip-master-node 运行日志</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs po/kube-vip-master-node -n kube-system</span></span><br><span class="line">  time=“2020-08-28T15:33:09Z” level=info msg=“The Node [10.239.140.133:10000] is leading”</span><br><span class="line">  time=“2020-08-28T15:33:09Z” level=info msg=“The Node [10.239.140.133:10000] is leading”</span><br></pre></td></tr></table></div></figure>
<p><strong>部署CNI网络</strong></p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"</span><br></pre></td></tr></table></div></figure>


        <h3 id="4-node-2机器上"   >
          <a href="#4-node-2机器上" class="heading-link"><i class="fas fa-link"></i></a>4. node-2机器上</h3>
      <p>添加work node节点</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo kubeadm join 10.239.140.133:6443 --token 9vr73a.a8uxyaju799qwdjv --discovery-token-ca-cert-hash sha256:7c2e69131a36ae2a042a339b33381c6d0d43887e2de83720eff5359e26aec866</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="查看kube-vip-api-server服务进程和监听端口"   >
          <a href="#查看kube-vip-api-server服务进程和监听端口" class="heading-link"><i class="fas fa-link"></i></a>查看kube-vip, api-server服务进程和监听端口</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> netstat -nltp | grep 10000	// 列出监听端口10000的进程</span></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> netstat -nltp | grep 6443		// 列出监听端口6443的进程</span></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> netstat -antp		// 列出所有tcp进程, State不仅包括Listen的, 还包括已建立链接状态为Established的进程.</span></span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      15700/kube-apiserve</span><br><span class="line">tcp        0      0 10.239.140.133:10000    0.0.0.0:*               LISTEN      21353/kube-vip</span><br><span class="line">tcp        0      0 10.239.140.133:10000    10.239.141.145:48554    ESTABLISHED 21353/kube-vip</span><br></pre></td></tr></table></div></figure>

<p><code>Local Address</code>可以看作是服务端IP和提供服务的监听端口, <code>Foreign Address</code>可以看作是客户端IP和发起链接请求的IP地址和请求端口.<br><code>ESTABLISHED</code>表示客户端与服务端已经建立tcp长链接.<br><code>LISTEN</code>表示服务端提供服务的端口仍处于监听状态, 等待客户端发起请求.<br>TCP才能在Foreign Address看到链接的客户端IP和端口, 而UDP无状态是没有的.<br>由以上输出可看到:</p>
<ul>
<li>kube-vip服务进程编号为21353, 监听端口为10000, 所在本机IP为10.239.140.133</li>
<li>api-server服务进程编号为15700, 监听端口为6443</li>
</ul>
<p>查看所有链接本机6443服务端口的客户端IP地址, 地址一致的合并, 然后连接数从高到底排序.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> netstat -antp | grep :6443 | awk <span class="string">'&#123;print $5&#125;'</span> | awk -F <span class="string">":"</span> <span class="string">'&#123;print $1&#125;'</span> | sort | uniq -c | sort -r -n</span></span><br><span class="line">      4 10.239.4.100	// 表示从IP地址为10.239.4.100的客户端请求访问本机6443服务端口的进程数为4</span><br><span class="line">      3 10.239.4.80</span><br><span class="line">      3 10.239.141.194</span><br><span class="line">      3 10.239.141.145</span><br><span class="line">      3</span><br><span class="line">      2 10.40.0.6</span><br><span class="line">      2 10.239.140.53</span><br><span class="line">      2 10.239.140.133</span><br><span class="line">      2 10.109.19.69</span><br><span class="line">      1 10.40.0.9</span><br><span class="line">      1 10.40.0.2</span><br><span class="line">      1 10.40.0.1</span><br><span class="line">      1 10.109.19.68</span><br></pre></td></tr></table></div></figure>


        <h2 id="去掉apiserver配置的proxy"   >
          <a href="#去掉apiserver配置的proxy" class="heading-link"><i class="fas fa-link"></i></a>去掉apiserver配置的proxy</h2>
      <p>部署完集群后在公司环境一定要去掉apiserver的proxy配置, 否则会遇到如下问题</p>
<p>问题1: The connection to the server 10.239.140.200:6443 was refused - did you specify the right host or port?<br>问题2: 执行systemctl status kubelet发现 类似如下错误</p>
<figure class="highlight routeros"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed <span class="keyword">to</span> <span class="builtin-name">get</span> status <span class="keyword">for</span> pod <span class="string">"kube-controller-manager-master-node_kube-system(185ec5bf52273f72fe5c4a72e3fbab62)"</span>: <span class="builtin-name">Get</span> <span class="string">"https://10.239.140.200:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-master-node"</span>: dial tcp 10.239.140.200:6443: connect:<span class="built_in"> connection </span>refused</span><br></pre></td></tr></table></div></figure>
<p>问题2： 执行kubectl get po -n kube-system 发现 controller-manager 和 scheduler 组件运行不正常<br>解决方案如下就是登陆每台master注释掉如下内容</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">登陆每台master注释如下内容</span><br><span class="line">vim /etc/kubernetes/manifests/kube-apiserver.yaml</span><br><span class="line">......</span><br><span class="line">    #env:</span><br><span class="line">    #- name: NO_PROXY</span><br><span class="line">    #  value: node-1,laboratory,node-2,k8s-vip,127.0.0.1,10.239.140.200</span><br><span class="line">    #- name: http_proxy</span><br><span class="line">    #  value: http://child-prc.intel.com:913</span><br><span class="line">    #- name: HTTPS_PROXY</span><br><span class="line">    #  value: http://child-prc.intel.com:913</span><br><span class="line">    #- name: https_proxy</span><br><span class="line">    #  value: http://child-prc.intel.com:913</span><br><span class="line">    #- name: HTTP_PROXY</span><br><span class="line">    #  value: http://child-prc.intel.com:913</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<p><strong>Note:</strong>添加注释保存退出后apiserver, controller manager, scheduler组件会重启, 如果没有重启可以执行 <code>kubectl delete po/&lt;组件名&gt; -n kube-system</code> 删掉然后就发现重启了.</p>

        <h2 id="查看并去掉node污点-taint"   >
          <a href="#查看并去掉node污点-taint" class="heading-link"><i class="fas fa-link"></i></a>查看并去掉node污点(taint)</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// 查看node机器污点</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe node/&lt;Node-Name&gt; | grep Taint</span></span><br><span class="line">  Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line"></span><br><span class="line">// 去掉污点</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl taint nodes &lt;Node-Name&gt; node-role.kubernetes.io/master:NoSchedule-</span></span><br><span class="line">// 去掉所有控制平面host污点</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl taint nodes --all node-role.kubernetes.io/master-</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="网卡上添加删除虚拟网址"   >
          <a href="#网卡上添加删除虚拟网址" class="heading-link"><i class="fas fa-link"></i></a>网卡上添加删除虚拟网址</h2>
      <p>网卡上增加一个IP</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ifconfig eth0:1 192.168.0.1 netmask 255.255.255.0</span></span><br></pre></td></tr></table></div></figure>

<p>删除网卡的第二个IP地址</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ip addr del 192.168.0.1/32 dev eth0</span></span><br></pre></td></tr></table></div></figure>
<p>上面IP后面加上 <code>/32</code> 否则会报 Warning: Executing wildcard deletion to stay compatible with old scripts.</p>
</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/01_kubernetes_build/">01 Kubernetes build with kubeadm</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">6.9k</span></span></div></header><div class="post-body"><div class="post-excerpt"><h2 id="Kubernetes-简介"   >
          <a href="#Kubernetes-简介" class="heading-link"><i class="fas fa-link"></i></a>Kubernetes 简介</h2></div><div class="post-readmore"><a class="post-readmore__link" href="/2021/03/13/micro_service/kubernetes/01_kubernetes_build/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/01_kubernetes%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%E5%92%8C%E9%85%8D%E7%BD%AE%E6%9D%A1%E4%BB%B6/">01 Kubernetes安装方式和配置条件</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">2k</span></span></div></header><div class="post-body"><div class="post-excerpt"><h2 id="安装方法"   >
          <a href="#安装方法" class="heading-link"><i class="fas fa-link"></i></a>安装方法</h2>
      <p>reference: <span class="exturl"><a class="exturl__link"   href="https://kubernetes.io/docs/setup/production-environment/tools/"  target="_blank" rel="noopener">https://kubernetes.io/docs/setup/production-environment/tools/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p></div><div class="post-readmore"><a class="post-readmore__link" href="/2021/03/13/micro_service/kubernetes/01_kubernetes%E5%AE%89%E8%A3%85%E6%96%B9%E5%BC%8F%E5%92%8C%E9%85%8D%E7%BD%AE%E6%9D%A1%E4%BB%B6/"><span class="post-readmore__text">阅读全文</span><span class="post-readmore__icon"><i class="fas fa-long-arrow-alt-right"></i></span></a></div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/05_kubernetes_volume/">05 Kubernetes volumes</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">2.3k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="volumes"   >
          <a href="#volumes" class="heading-link"><i class="fas fa-link"></i></a>volumes</h2>
      <blockquote>
<p>我们可能希望新的容器可以在之前容器结束的位置继续运行，比如在物理机上重启进程。 可能不需要（或者不想要）整个文件系统被持久化， 但又希望能保存实际数据的目录<br>在 pod 启动时创建卷， 并在删除 pod 时销毁卷. 在容器重新启动期间， 卷的内容将保持不变， 在重新启动容器之后， 新容器可以识别前一个容器写入卷的所有文件。 另外，如果一个 pod 包含多个容器， 那这个卷可以同时被所有的容器使用</p>
</blockquote>
<p>Kubernetes 的卷是 pod 的一个组成部分， 因此像容器一样在 pod 的规范中就定义了。<br>它们不是独立的 Kubernetes 对象， 也不能单独创建或删除。<br>pod 中的所有容器都可以使用卷， 但必须先将它挂载在每个需要访问它的容器中。 在每个容器中， 都可以在其文件系统的任意位置挂载卷<br>卷类型</p>
<ul>
<li>emptyDir 用于存储临时数据的简单空目录</li>
<li>gitRepo 通过检出Git仓库的内容来初始化的卷</li>
<li>hostPath 用于将目录从工作节点的文件系统挂载到pod中</li>
<li>configMap、secret、downwardAPI 用于将 Kubemetes 部分资源和集群信息公开给 pod 的特殊类型的卷<br>……</li>
</ul>
<p>fortune-pod.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: fortune</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: luksa/fortune</span><br><span class="line">    name: html-generator</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /var/htdocs</span><br><span class="line">  - image: nginx:alpine</span><br><span class="line">    name: web-server</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: html</span><br><span class="line">      mountPath: /usr/share/nginx/html</span><br><span class="line">      readOnly: true</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line">      protocol: TCP</span><br><span class="line">  volumes:</span><br><span class="line">  - name: html</span><br><span class="line">    emptyDir: &#123;&#125;</span><br></pre></td></tr></table></div></figure>

<p>作为卷来使用的 emptyDir 是在承载 pod 的工作节点的实际磁盘上创建的，因此其性能取决于节点的磁盘类型。<br>也可以通知 Kubemetes 在 tmfs 文件系统(存在内存而非硬盘)上创建 emptyDir. 因此，将 emptyDir 的 medium 设置为Memory</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">volumes:</span><br><span class="line">- name: html</span><br><span class="line">  emptyDir</span><br><span class="line">    medium: Memory</span><br></pre></td></tr></table></div></figure>
<p>创建pod</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f fortune-pod.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kuberctl port-forward fortune 8080:80</span></span><br><span class="line">Forwarding from 127.0.0.1:8080 -&gt; 80</span><br><span class="line">Forwarding from [::1]:8080 -&gt; 80</span><br><span class="line">Handling connection for 8080</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<p>查看 pod 描述</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe po/fortune</span></span><br><span class="line">......</span><br><span class="line">Containers:</span><br><span class="line">html-generator:</span><br><span class="line">......</span><br><span class="line">  Mounts:</span><br><span class="line">    /var/htdocs from html (rw)</span><br><span class="line">    /var/run/secrets/kubernetes.io/serviceaccount from default-token-b79jt (ro)</span><br><span class="line">......</span><br><span class="line">Volumes:</span><br><span class="line">  html:</span><br><span class="line">    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)</span><br><span class="line">    Medium:     Memory</span><br><span class="line">    SizeLimit:  &lt;unset&gt;</span><br><span class="line">  default-token-b79jt:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-b79jt</span><br><span class="line">    Optional:    false</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>
<p>测试访问两种方式:</p>
<ul>
<li><p>第一种浏览器访问:<br>浏览器输入 <span class="exturl"><a class="exturl__link"   href="http://127.0.0.1:8080"  target="_blank" rel="noopener">http://127.0.0.1:8080</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 或 <span class="exturl"><a class="exturl__link"   href="http://localhost:8080"  target="_blank" rel="noopener">http://localhost:8080</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span><br>即可看到每过10s按F5刷新一次HTML页面内容都不一样</p>
</li>
<li><p>第二种 curl 或 wget 访问<br>不去掉公司的proxy访问命令里加上 noproxy</p>
</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl http://127.0.0.1:8080 --noproxy <span class="string">"*"</span>	// <span class="string">"*"</span> 对所有路径的访问都不经过配置的proxy</span></span><br></pre></td></tr></table></div></figure>
<p>也可以去掉公司的proxy, 需要先执行 $ export http_proxy= 把公司的proxy去掉<br>再执行如下命令即可访问</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://127.0.0.1:8080</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://localhost:8080</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> wget http://127.0.0.1:8080</span></span><br></pre></td></tr></table></div></figure>

<p>开启两个终端进入这两个container，每过10s分别运行如下各自container里的cat html命令，发现HTML内容不断变化但两个容器的html内容一样<br>如果打开浏览器，发现浏览器输出跟这两个container里相应路径下html的内容都是一样同步变化的</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> po/fortune -c web-server -it -- sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /usr/share/nginx/html/index.html</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> po/fortune -c html-generator -it -- sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /var/htdocs/index.html</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="gitRepo-卷"   >
          <a href="#gitRepo-卷" class="heading-link"><i class="fas fa-link"></i></a>gitRepo 卷</h2>
      <blockquote>
<p>gitRepo 容器就像 emptyDir 卷一样， 基本上是一个专用目录， 专门用于包含卷的容器并单独使用。 当 pod 被删除时， 卷及其内容被删除。 然而， 其他类型的卷并不创建新目录， 而是将现有的外部目录挂载到 pod 的容器文件系统中.<br>使用对应私有 Git repo 的 gitRepo 卷， 其实不可行。 Kubemetes 开发入员的共识是保待 gitRepo 卷的简单性， 而不添加任何通过 SSH 协议克隆私有存储库的支待， 因为这需要向 gitRepo 卷添加额外的配置选项。如果想要将私有的 Git repo 克隆到容器中， 则应该使用 gitsync sidecar 或类似的方法， 而不是使用 gitRepo 卷.</p>
</blockquote>

        <h2 id="hostPath-卷"   >
          <a href="#hostPath-卷" class="heading-link"><i class="fas fa-link"></i></a>hostPath 卷</h2>
      <blockquote>
<p>hostPath 卷指向节点文件系统上的特定文件或目录,在同一个节点上运行并在其 hostPath 卷中使用相同路径的 pod 可以看到相同的文件.<br>持久性存储, 因为gitRepo 和 emptyDir 卷的内容都会在 pod 被删除时被删除， 而 hostPath 卷的内容则 不会被删除.<br>如果删除了一个pod, 并且下一个 pod 使用了指向主机上相同路径的hostPath 卷， 则新 pod 将会发现上一个 pod 留下的数据， 但前提是必须将其调度到与第一个 pod 相同的节点上.<br>不应该使用hostPath 卷作为存储数据库数据的目录. 因为卷的内容存储在特定节点的文件系统中, 当数据库 pod 被重新安排在另一个节点时， 会找不到数据, 这会使 pod 对预定规划的节点很敏感<br>请记住仅当需要在节点上读取或写入系统文件时才使用 hos七Path, 切勿使用它们来持久化跨 pod的数据.</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -n kube-system</span></span><br><span class="line">NAME                                  READY   STATUS    RESTARTS   AGE</span><br><span class="line">......</span><br><span class="line">kube-apiserver-master-node            1/1     Running   5          2d20h</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe po/kube-apiserver-master-node -n kube-system</span></span><br><span class="line">......</span><br><span class="line">Containers:</span><br><span class="line">  kube-apiserver:</span><br><span class="line">  ......</span><br><span class="line">  Mounts:</span><br><span class="line">    /etc/kubernetes/pki from k8s-certs (ro)</span><br><span class="line">    /etc/pki from etc-pki (ro)</span><br><span class="line">    /etc/ssl/certs from ca-certs (ro)</span><br><span class="line">......</span><br><span class="line">Volumes:</span><br><span class="line">  ca-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          /etc/ssl/certs</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  etc-pki:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          /etc/pki</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br><span class="line">  k8s-certs:</span><br><span class="line">    Type:          HostPath (bare host directory volume)</span><br><span class="line">    Path:          /etc/kubernetes/pki</span><br><span class="line">    HostPathType:  DirectoryOrCreate</span><br></pre></td></tr></table></div></figure>
<p>Pod使用三个HostPath卷来访问宿主主机的/etc/ssl/certs, /etc/pki, /etc/kubernetes/pki三个目录.</p>

        <h2 id="PV-amp-PVC-amp-storageclass-sc"   >
          <a href="#PV-amp-PVC-amp-storageclass-sc" class="heading-link"><i class="fas fa-link"></i></a>PV &amp; PVC &amp; storageclass(sc)</h2>
      <blockquote>
<p>在 Kubemetes 集群中为了使应用能够正常请求存储资源， 同时避免处理基础设施细节， 引入了两个新的资源， 分别是待久卷和持久卷声明. 这名字可能有点误导，因为正如在前面看到的， 甚至常规的 Kubemetes 卷 也可以用来存储持久性数据<br>在 pod 中使用 PersistentVolume (持久卷， 简称 PV) 要比使用常规的 pod 卷复杂一些<br>研发人员无须向他们的 pod 中添加特定技术的卷， 而是由集群管理员设置底层存储， 然后通过 Kubernetes API 服务器创建持久卷并注册。 在创建持久卷时， 管理员可以指定其大小和所支持的访问模式.<br>当集群用户需要在其 pod 中使用持久化存储时， 他们首先创建持久卷声明(PersistentVolumeClaim, 简称 PVC) 清单， 指定所需要的最低容量要求和访问模式，然后用户将待久卷声明清单提交给 Kubernetes API 服务器， Kubernetes 将找到可匹配的待久卷并将其绑定到持久卷声明<br>持久卷声明可以当作 pod 中的一个卷来使用， 其他用户不能使用相同的持久卷，除非先通过删除持久卷声明绑定来释放.<br>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="PV_PVC.PNG"  alt="">
      </p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pv</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pvc -n default</span></span><br><span class="line">NAME             STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS      AGE</span><br><span class="line">mysql-pv-claim   Bound    pvc-522c119b-a968-4a86-ae6c-521b64b775ee   20Gi       RWO            rook-ceph-block   5d2h</span><br><span class="line">wp-pv-claim      Bound    pvc-4b83da7a-4552-4806-9f68-af96d4a56d96   20Gi       RWO            rook-ceph-block   5d2h</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get sc	// storageclass的简写(sc)</span></span><br></pre></td></tr></table></div></figure>



        <h2 id="Additional"   >
          <a href="#Additional" class="heading-link"><i class="fas fa-link"></i></a>Additional:</h2>
      
        <h2 id="CephFS-example"   >
          <a href="#CephFS-example" class="heading-link"><i class="fas fa-link"></i></a>CephFS example</h2>
      <p>创建CephFS</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: ceph.rook.io/v1</span><br><span class="line">kind: CephFilesystem</span><br><span class="line">metadata:</span><br><span class="line">  name: myfs</span><br><span class="line">  namespace: rook-ceph</span><br><span class="line">spec:</span><br><span class="line">  metadataPool:</span><br><span class="line">    replicated:</span><br><span class="line">      size: 3</span><br><span class="line">  dataPools:</span><br><span class="line">    - replicated:</span><br><span class="line">        size: 3</span><br><span class="line">  preservePoolsOnDelete: true</span><br><span class="line">  metadataServer:</span><br><span class="line">    activeCount: 1</span><br><span class="line">    activeStandby: true</span><br></pre></td></tr></table></div></figure>

<p>创建sc(StorageClass), sc是不需要提前创建好PV, 而是根据PVC需求动态创建PV.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: rook-cephfs</span><br><span class="line"># Change "rook-ceph" provisioner prefix to match the operator namespace if needed</span><br><span class="line">provisioner: rook-ceph.cephfs.csi.ceph.com</span><br><span class="line">  parameters:</span><br><span class="line">  # clusterID is the namespace where operator is deployed.</span><br><span class="line">  clusterID: rook-ceph</span><br><span class="line">  </span><br><span class="line">  # CephFS filesystem name into which the volume shall be created</span><br><span class="line">  fsName: myfs</span><br><span class="line">  </span><br><span class="line">  # Ceph pool into which the volume shall be created</span><br><span class="line">  # Required for provisionVolume: "true"</span><br><span class="line">  pool: myfs-data0</span><br><span class="line">  </span><br><span class="line">  # Root path of an existing CephFS volume</span><br><span class="line">  # Required for provisionVolume: "false"</span><br><span class="line">  # rootPath: /absolute/path</span><br><span class="line">  </span><br><span class="line">  # The secrets contain Ceph admin credentials. These are generated automatically by the operator</span><br><span class="line">  # in the same namespace as the cluster.</span><br><span class="line">  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner</span><br><span class="line">  csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph</span><br><span class="line">  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node</span><br><span class="line">  csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph</span><br><span class="line">  </span><br><span class="line">reclaimPolicy: Delete</span><br></pre></td></tr></table></div></figure>

<p>创建PVC和deployment</p>
<blockquote>
<p>PV 是K8S全局资源<br>PVC 是指定在某个Namespace下的K8S资源, 如果PVC访问属性为ReadWriteMany， 多个不同Pod挂载此相同的PVC到容器指定目录, 该目录将共享文件<br>多个不同Pod挂载不同的PVC到容器指定目录，则文件不能共享</p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: my-pvc</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  accessModes:				// PV 具有的访问部署属性， PVC绑定后也应具有的访问部署属性(可读,可写,多机部署)</span><br><span class="line">  - ReadWriteMany</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 1Gi</span><br><span class="line">  storageClassName: rook-cephfs</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: my-test</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  labels:</span><br><span class="line">    app: my-test</span><br><span class="line">    kubernetes.io/cluster-service: "true"</span><br><span class="line">spec:</span><br><span class="line">  replicas: 2</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: my-test</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: my-test</span><br><span class="line">        kubernetes.io/cluster-service: "true"</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: my-test</span><br><span class="line">        image: registry:2</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br><span class="line">        resources:</span><br><span class="line">          limits:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: my-volume</span><br><span class="line">          mountPath: /var/lib/myVolume</span><br><span class="line">      volumes:</span><br><span class="line">      - name: my-volume</span><br><span class="line">         persistentVolumeClaim:</span><br><span class="line">           claimName: my-pvc			// 绑定上面的PVC</span><br><span class="line">           readOnly: false</span><br><span class="line">       affinity:</span><br><span class="line">         podAffinity:</span><br><span class="line">           requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">               - labelSelector:</span><br><span class="line">                   matchExpressions:</span><br><span class="line">                   - key: app</span><br><span class="line">                     operator: In</span><br><span class="line">                     values:</span><br><span class="line">                     - helm</span><br><span class="line">                 topologyKey: kubernetes.io/hostname</span><br></pre></td></tr></table></div></figure>




</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/04_kubernetes_service/">04 Kubernetes service</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">6k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="Kubernetes-Service"   >
          <a href="#Kubernetes-Service" class="heading-link"><i class="fas fa-link"></i></a>Kubernetes Service</h2>
      <blockquote>
<p>Kubemetes 服务是一种为一组功能相同的 pod 提供单一不变的接入点的资源.<br>当服务存在时，它的 IP 地址和端口不会改变。 客户端通过 IP 地址和端口号建立连接，这些连接会被路由到提供该服务的任意一个 pod上.<br>通过这种方式， 客户端不需要知道每个单独的提供服务的 pod 的地址， 这样这些 pod 就可以在集群中随时被创建或移除.</p>
</blockquote>
<p>通过为前端 pod 创建服务， 并且将其配置成可以在集群外部访问，可以暴露一个单一不变的 IP 地址让外部的客户端连接 pod。<br>同理，可以为后台数据库 pod 创建服务，并为其分配一个固定的 IP 地址。尽管 pod 的 IP 地址会改变，但是服务的 IP 地址固定不变。<br>另外，通过创建服务，能够让前端的 pod 通过环境变量或 DNS 以及服务名来访问后端服务<br>Pod 控制器中使用标签选择器来指定哪些 pod 属于同一 Service。</p>

        <h2 id="service"   >
          <a href="#service" class="heading-link"><i class="fas fa-link"></i></a>service</h2>
      <blockquote>
<p>如果 pod 的标签与服务的 pod 选择器相匹配，那么 pod 就将作为服务的后端.只要创建了具有适当标签的新 pod ，它就成为服务的一部分，并且请求开始被重定向到 pod.<br>如下所示Service和POD都采用命名端口的方式, 最大的好处就是即使更换spec pod中的端口号也无须更改服务 spec.</p>
</blockquote>
<ul>
<li>第一步，创建service<br>创建service yaml文件 kubia-svc.yaml</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">sepc:</span><br><span class="line">// sessionAffinity: ClientIP	// 默认此值是None, 若改为ClientIP，则SVC接受到的请求连接只会固定转发给同一个pod</span><br><span class="line">  ports:</span><br><span class="line">  - name: http			// 端口别名，可以当作端口号用</span><br><span class="line">    port: 80			// 该服务可用的端口</span><br><span class="line">    targetPort: http	// 服务将连接转发到的POD端口, pod需要将http映射pod本身的8080或其它端口，否则这里只能填写端口号</span><br><span class="line">  - name: https</span><br><span class="line">    port: 443</span><br><span class="line">    targetPort: https	// 含有label:app=kubia的pod需要将https映射pod本身8443或其它端口，否则这里只能填写端口号</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia			// 具有app=kubia标签的pod都属于该服务</span><br></pre></td></tr></table></div></figure>

<p>创建了 一个名叫kubia的服务，它将在端口80接收请求并将连接路由到具有标签选择器是app=kubia的pod的8080端口上.<br>在发布完YAML文件后， 可以在命名空间下列出来所有的服务资源, 新的服务已经被分配了一个内部集群IP, 只能在集群内部可以被访问.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   23h</span><br><span class="line">kubia        ClusterIP   10.98.229.76   &lt;none&gt;        80/TCP    20s</span><br></pre></td></tr></table></div></figure>

<ul>
<li>第二步，创建两个pod，一个添加标签app=kubia，另一个用来执行测试通过kubectl exec来访问第一个pod<br>kubia.yaml</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia	// name: kubia1; name: kubia2</span><br><span class="line">spec:</span><br><span class="line">  nodeSelector:		// pod被分配到含有标签gpu=true的node上，当然也可以注释掉这两行</span><br><span class="line">    gpu: "true"</span><br><span class="line">  containers:</span><br><span class="line">  - image: luksa/kubia</span><br><span class="line">    name: kubia</span><br></pre></td></tr></table></div></figure>

<p>kubia-label.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1					// api服务版本</span><br><span class="line">kind : Pod						// 资源类型</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-label			// pod 名字</span><br><span class="line">  labels:</span><br><span class="line">    app: kubia					// pod添加label</span><br><span class="line">spec :</span><br><span class="line">  nodeSelector:</span><br><span class="line">    gpu: "true"				// node 选择器</span><br><span class="line">  containers:</span><br><span class="line">  - image: luksa/kubia			// image 名字</span><br><span class="line">    name: kubia				// container 名字</span><br><span class="line">    ports:</span><br><span class="line">    - name: http				// pod端口映射，用http名字代替8080，名字随便取, 可以跟上面的service的targetPort对应起来</span><br><span class="line">      containerPort: 8080		// 用上面的名字定义这个端口号的别名</span><br><span class="line">    - name: https</span><br><span class="line">      containerPort: 8443</span><br></pre></td></tr></table></div></figure>

<p>查看POD并执行一个POD去通过上面创建的service(通过label)包含的pod提供的服务.<br>其中pod kubia-label中container运行的服务进程监听了8080端口, POD对外也暴露了8080端口</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --show-labels</span></span><br><span class="line">NAME           READY   STATUS    RESTARTS   AGE    LABELS</span><br><span class="line">kubia          1/1     Running   0          101m   &lt;none&gt;</span><br><span class="line">kubia-label    1/1     Running   0          98m    app=kubia</span><br><span class="line">kubia-label1   1/1     Running   0          99s    app=kubia</span><br><span class="line">kubia-label2   1/1     Running   0          79s    app=kubia</span><br></pre></td></tr></table></div></figure>

<ul>
<li>第三步: 执行一个pod用curl命令访问另一个pod提供的服务<br>双横杠(–)代表着kubectl命令项的结束.在两个横杠之后的内容是指在pod内部需要执行的命令.<br>k8s 服务代理接续curl请求连接，三个包含label为app=kubia的pod任意选择一个pod<br>访问服务三种方式,加不加端口都可以</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;p&gt;$ kubectl exec kubia -- curl -s http://10.98.229.76:http&lt;/p&gt;</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.98.229.76:80</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.98.229.76</span></span><br><span class="line">You've hit kubia-label</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.98.229.76</span></span><br><span class="line">You've hit kubia-label2</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.98.229.76</span></span><br><span class="line">You've hit kubia-label1</span><br></pre></td></tr></table></div></figure>


        <h3 id="Affinity-亲和性"   >
          <a href="#Affinity-亲和性" class="heading-link"><i class="fas fa-link"></i></a>Affinity 亲和性</h3>
      <p>Kubernetes 仅仅支持两种形式的会话亲和性服务： None 和 ClientIP<br>这种方式将会使服务代理将来自同 一个 client IP 的所有请求转发至同 一个 pod上.<br>Kubernetes 服务不是在 HTTP 层面上工作。服务处理 TCP 和 UDP 包，并不关心其中的载荷内容。<br>因为 cookie 是 HTTP 协议中的一部分，服务并不知道它们，这就解释了为什么会话亲和性不能基千 cookie。<br>如果希望特定客户端产生的所有请求每次都指向同 一个 pod, 可以设置服务的 sessionAffinity 属性为 ClientIP (而不是 None,None 是默认值）</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: vl</span><br><span class="line">kind: Service</span><br><span class="line">spec:</span><br><span class="line">  sessionAffinity: ClientIP</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>


        <h2 id="环境变量发现service"   >
          <a href="#环境变量发现service" class="heading-link"><i class="fas fa-link"></i></a>环境变量发现service</h2>
      
        <h3 id="创建replicaSet-管理-3-个-POD"   >
          <a href="#创建replicaSet-管理-3-个-POD" class="heading-link"><i class="fas fa-link"></i></a>创建replicaSet 管理 3 个 POD</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        gpu: "true"</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubia</span><br><span class="line">        image: luksa/kubia</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: https</span><br><span class="line">          containerPort: 8443</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc </span></span><br><span class="line">NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          29h</span><br><span class="line">kubia        ClusterIP   10.111.88.195   &lt;none&gt;        80/TCP,443/TCP   3h46m</span><br></pre></td></tr></table></div></figure>

<p>查看pod所在的service对应的IP和端口</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia-5rvfq env</span></span><br><span class="line">......</span><br><span class="line">KUBERNETES_SERVICE_PORT=443</span><br><span class="line">KUBIA_SERVICE_PORT=80				// 服务的集群IP</span><br><span class="line">......</span><br><span class="line">KUBERNETES_SERVICE_HOST=10.96.0.1</span><br><span class="line">KUBIA_SERVICE_HOST=10.111.88.195	// 服务所在的端口</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>
<p>pod 是否使用 内 部的 DNS 服务器是根据 pod 中 spec 的 dnsPolicy 属性来决定的</p>
<p>进入容器后执行如下命令</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia-5rvfq -it -- bash		// -- 表示kubectl 命令执行完了，开始执行pod容器里要运行的命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl http://kubia.default.svc.cluster.local</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl http://kubia.default</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> curl http://kubia</span></span><br><span class="line">You've hit kubia-5rvfq</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /etc/resolv.conf</span></span><br><span class="line">nameserver 10.96.0.10		// 对应kube-system 里的服务kube-dns服务IP</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local sh.intel.com</span><br><span class="line">options ndots:5</span><br><span class="line">root@kubia-5rvfq:/# curl http://svc.cluster.local</span><br><span class="line">curl: (6) Could not resolve host: svc.cluster.local</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> ping kubia</span></span><br><span class="line">PING kubia.default.svc.cluster.local (10.111.88.195): 56 data bytes</span><br><span class="line">^C--- kubia.default.svc.cluster.local ping statistics ---</span><br><span class="line">4 packets transmitted, 0 packets received, 100% packet loss</span><br></pre></td></tr></table></div></figure>

<p>上面的 curl 这个服务是工作的，但是却 ping 不通。这是 因为服务的集群 IP 是一个虚拟 IP，并且只有在与服务端口结合时才有意义。 </p>
<p>查看kube-system下面kube-dns信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc -n kube-system</span></span><br><span class="line">NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">kube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   2d3h</span><br></pre></td></tr></table></div></figure>


        <h3 id="删除service"   >
          <a href="#删除service" class="heading-link"><i class="fas fa-link"></i></a>删除service</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete svc kubia</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="Service-samples"   >
          <a href="#Service-samples" class="heading-link"><i class="fas fa-link"></i></a>Service samples</h2>
      
        <h3 id="查看service"   >
          <a href="#查看service" class="heading-link"><i class="fas fa-link"></i></a>查看service</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc -n kube-system</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc -n istio-system</span></span><br><span class="line">NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)                                                                                                                                      AGE</span><br><span class="line">grafana                     ClusterIP      10.104.1.236     &lt;none&gt;        3000/TCP                                                                                                                                     3d5h</span><br><span class="line">istio-egressgateway         ClusterIP      10.107.177.52    &lt;none&gt;        80/TCP,443/TCP,15443/TCP                                                                                                                     3d5h</span><br><span class="line">istio-ingressgateway        LoadBalancer   10.97.82.221     &lt;pending&gt;     15020:31237/TCP,80:31556/TCP,443:30614/TCP,15029:32511/TCP,15030:32423/TCP,15031:30670/TCP,15032:30961/TCP,31400:30196/TCP,15443:31028/TCP   3d5h</span><br><span class="line">istio-pilot                 ClusterIP      10.97.192.70     &lt;none&gt;        15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP                                                                                     3d5h</span><br><span class="line">istiod                      ClusterIP      10.107.202.199   &lt;none&gt;        15012/TCP,443/TCP</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get services</span></span><br><span class="line">NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes                  ClusterIP      10.3.240.l       &lt;none&gt;        443/TCP          34m</span><br><span class="line">kubia-http                  LoadBalancer   10.3.246.185     &lt;pending&gt;     8080:31348/TCP   4s</span><br><span class="line">暂时忽略 kubernetes 服务，仔细查看创建的kubian-http 服务 。 它还没有外部 IP 地址 ，因为 Kubernetes 运行的云基础设施创建负载均衡需要一段时间</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get services</span></span><br><span class="line">NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)          AGE</span><br><span class="line">kubernetes                  ClusterIP      10.3.240.l       &lt;none&gt;            443/TCP          34m</span><br><span class="line">kubia-http                  LoadBalancer   103.246.185      104 155.74.57     8080:31348/TCP   4s</span><br><span class="line">现在有外部 IP 了，应用就可以从任何地方通过 http://104.155.74.57:8080 访问</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl 104.155.74.57:8080</span></span><br><span class="line">You’ve hit kubia-4jfyf</span><br></pre></td></tr></table></div></figure>


        <h3 id="查看service的CRD信息"   >
          <a href="#查看service的CRD信息" class="heading-link"><i class="fas fa-link"></i></a>查看service的CRD信息</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc istio-ingressgateway -n istio-system -oyaml</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="endpoint-服务"   >
          <a href="#endpoint-服务" class="heading-link"><i class="fas fa-link"></i></a>endpoint 服务</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe svc kubia</span></span><br><span class="line">Name:              kubia</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          app=kubia		// 用于创建endpoint列表的服务pod选择器</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.111.88.195</span><br><span class="line">Port:              http  80/TCP</span><br><span class="line">TargetPort:        8080/TCP</span><br><span class="line">Endpoints:         10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080		// 服务endpoint的pod的IP和端口列表</span><br><span class="line">Port:              https  443/TCP</span><br><span class="line">TargetPort:        8443/TCP</span><br><span class="line">Endpoints:         10.44.0.1:8443,10.44.0.2:8443,10.44.0.3:8443</span><br><span class="line">Session Affinity:  ClientIP</span><br><span class="line">Events:            &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -o wide</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kubia         1/1     Running   0          15h   10.44.0.4   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-5rvfq   1/1     Running   0          15h   10.44.0.2   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-8cgnm   1/1     Running   0          15h   10.44.0.1   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-8kv8d   1/1     Running   0          15h   10.44.0.3   server02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></div></figure>
<p>Endpoint 资源和其他Kubernetes 资源一样，所以可以使用 kubectl info 来获取它的基本信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get endpoints kubia</span></span><br><span class="line">NAME    ENDPOINTS                                                  AGE</span><br><span class="line">kubia   10.44.0.1:8443,10.44.0.2:8443,10.44.0.3:8443 + 3 more...   16h</span><br></pre></td></tr></table></div></figure>

<p>Endpoint是一个单独的资源并不 是服务的一个属性, 必须手动创建<br>external-service.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: external-service</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br></pre></td></tr></table></div></figure>

<p>external-service-endpoints.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Endpoints</span><br><span class="line">metadata:</span><br><span class="line">  name: external-service</span><br><span class="line">subsets:</span><br><span class="line">  - addresses:</span><br><span class="line">    - ip: 11.11.11.11</span><br><span class="line">    - ip: 22.22.22.22</span><br><span class="line">    ports:</span><br><span class="line">    - port: 80</span><br></pre></td></tr></table></div></figure>
<p>部署service和endpoint</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f external-service.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f external-service-endpoints.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe svc/external-service</span></span><br><span class="line">Name:              external-service</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       &lt;none&gt;</span><br><span class="line">Selector:          &lt;none&gt;</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.97.153.150</span><br><span class="line">Port:              &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         11.11.11.11:80,22.22.22.22:80</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></div></figure>


        <h2 id="暴露service"   >
          <a href="#暴露service" class="heading-link"><i class="fas fa-link"></i></a>暴露service</h2>
      <p> • 将服务的类型设置成NodePort – 每个集群节点都会在节点上打开一个端口， 对于NodePort服务， 每个集群节点在节点本身（因此得名叫NodePort)上打开一个端口，并将在该端口上接收到的流量重定向到基础服务。<br>   该服务仅在内部集群 IP 和端口上才可访间， 但也可通过所有节点上的专用端口访问.<br> • 将服务的类型设置成LoadBalance, NodePort类型的一种扩展 – 这使得服务可以通过一个专用的负载均衡器来访问， 这是由Kubernetes中正在运行的云基础设施提供的。 负载均衡器将流量重定向到跨所有节点的节点端口。<br>   客户端通过负载均衡器的 IP 连接到服务<br> • 创建一 个Ingress资源， 这是一 个完全不同的机制， 通过一 个IP地址公开多个服务——它运行在 HTTP 层（网络协议第 7 层）上， 因此可以提供比工作在第4层的服务更多的功能</p>

        <h3 id="NodePort-类型-service"   >
          <a href="#NodePort-类型-service" class="heading-link"><i class="fas fa-link"></i></a>NodePort 类型 service</h3>
      <p>指定端口不是强制性的。 如果忽略它，Kubemetes将选择一个随机端口.<br>客户端发送请求的节点并不重要, 整个互联网可以通过任何节点上的30123(用户自己定义的)端口访问到pod,如下所示<br>如在个人机器上生成如下service和pod，可以在以前做的项目的任何机器上通过如下访问</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -s http://10.239.140.186:30123 (master节点的NodeIP:port)访问服务</span><br><span class="line">curl -s http://10.239.140.200:30123 (worker02节点的NodeIP:port)访问服务</span><br></pre></td></tr></table></div></figure>

<p>kubia-svc-nodeport.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-nodeport</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort			// 设置服务类型</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80				// 服务集群IP端口号</span><br><span class="line">    targetPort: 8080</span><br><span class="line">    nodePort: 30123			// 通过集群节点(master或worker)的NodeIP，加上30123端口可以访问服务</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-svc-nodeport.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes           ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP          43h</span><br><span class="line">kubia-nodeport       NodePort       10.103.7.50     &lt;none&gt;        80:30123/TCP     99m</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe svc kubia-nodeport</span></span><br><span class="line">Name:                     kubia-nodeport</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   &lt;none&gt;</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app=kubia</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.103.7.50</span><br><span class="line">Port:                     &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:               8080/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  30123/TCP</span><br><span class="line">Endpoints:                10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -o wide</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kubia         1/1     Running   0          17h   10.44.0.4   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-5rvfq   1/1     Running   0          17h   10.44.0.2   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-8cgnm   1/1     Running   0          17h   10.44.0.1   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-8kv8d   1/1     Running   0          17h   10.44.0.3   server02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></div></figure>
<p>查看server02机器IP</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl get node -o wide</span></span><br><span class="line">NAME       STATUS   ROLES    AGE     VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME</span><br><span class="line">alpha      Ready    master   2d18h   v1.18.2   10.239.140.186   &lt;none&gt;        Ubuntu 18.04.4 LTS   5.3.0-28-generic    docker://19.3.6</span><br><span class="line">server02   Ready    &lt;none&gt;   2d17h   v1.18.2   10.239.140.200   &lt;none&gt;        Ubuntu 18.04.4 LTS   4.15.0-76-generic   docker://19.3.6</span><br></pre></td></tr></table></div></figure>
<p>两种访问方式</p>
<ul>
<li>第一种: 通过NodeIP:Port 访问:</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://10.239.140.200:30123</span></span><br><span class="line">You've hit kubia-8kv8d</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -s http://10.239.140.186:30123</span></span><br><span class="line">You've hit kubia-5rvfq</span><br></pre></td></tr></table></div></figure>
<ul>
<li>第二种: 通过 service的CLUSTER-IP：port 进入port进行访问</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.103.7.50:80</span></span><br><span class="line">You've hit kubia-8kv8d</span><br></pre></td></tr></table></div></figure>


        <h3 id="LoadBalancer-方式访问"   >
          <a href="#LoadBalancer-方式访问" class="heading-link"><i class="fas fa-link"></i></a>LoadBalancer 方式访问</h3>
      <blockquote>
<p>如果Kubemetes在不支持Load Badancer服务的环境中运行， 则不会调配负载平衡器， 但该服务仍将表现得像 一 个NodePort服 务。 这是因为LoadBadancer服务是NodePo江服务的扩展<br>如果没有指定特定的节点端口， Kubernetes将会选择一个端口<br>创建服务后， 云基础架构需要一段时间才能创建负载均衡器并将其 IP 地址写入服务对象。 一旦这样做了， IP 地址将被列为服务的外部 IP 地址</p>
</blockquote>
<p>kubia-svc-loadbalancer.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-loadbalancer</span><br><span class="line">spec:</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-svc-loadbalancer.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe svc/kubia-loadbalancer</span></span><br><span class="line">Name:                     kubia-loadbalancer</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   &lt;none&gt;</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app=kubia</span><br><span class="line">Type:                     LoadBalancer</span><br><span class="line">IP:                       10.99.184.62</span><br><span class="line">Port:                     &lt;unset&gt;  80/TCP</span><br><span class="line">TargetPort:               8080/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  30994/TCP		// yaml资源文件里没有指定, Kubemetes将会选择一个端口</span><br><span class="line">Endpoints:                10.44.0.1:8080,10.44.0.2:8080,10.44.0.3:8080</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubernetes           ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP          43h</span><br><span class="line">kubia-loadbalancer   LoadBalancer   10.99.184.62    &lt;pending&gt;     80:30994/TCP     4m11s</span><br></pre></td></tr></table></div></figure>

<p>可以看到Kubemetes在不支持Load Badancer服务的环境中运行 EXTERNAL-IP显示为 <pending>状态，但仍然可以像NodePort方式一样访问服务</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia -- curl -s http://10.99.184.62:80</span></span><br><span class="line">You've hit kubia-8cgnm</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -s 10.239.140.186:30994		// masterIP：svcPort</span></span><br><span class="line">You've hit kubia-5rvfq</span><br><span class="line"><span class="meta">$</span><span class="bash"> curl -s 10.239.140.200:30994		// worker01：svcPort</span></span><br><span class="line">You've hit kubia-8kv8d</span><br></pre></td></tr></table></div></figure>
<p>如果支持LoadBalancer且获得EXTERNAL-IP为 130.211.53.173<br>可以通过 $ curl <span class="exturl"><a class="exturl__link"   href="http://130.211.53.173"  target="_blank" rel="noopener">http://130.211.53.173</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span> 进行访问</p>

        <h3 id="Ingress-暴露服务"   >
          <a href="#Ingress-暴露服务" class="heading-link"><i class="fas fa-link"></i></a>Ingress 暴露服务</h3>
      <blockquote>
<p>需要 Ingress一个重要的原因是每个 LoadBalancer 服务都需要自己的负载均衡器， 以及独有的公有 IP 地址， 而 Ingress 只需要一个公网 IP 就能为许多服务提供访问<br>Ingress 在网络栈 (HTTP) 的应用层操作， 并且可以提供一 些服务不能实现的功能， 诸如基于 cookie 的会话亲和性 (session affinity) 等功能<br>Ingress 对象提供的功能之前，必须强调只有 Ingress控制器在集群中运行，Ingress 资源才能正常工作。 不同的 Kubernetes 环境使用不同的控制器实现， 但有些并不提供默认控制器<br>Ingress通常向外暴露 Service.Type=NodePort 或者 Service.Type=LoadBalancer 类型的服务，因此先创建一个NodePort类型svc.</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubia-nodeport        NodePort       10.103.7.50     &lt;none&gt;        80:30123/TCP     144m</span><br></pre></td></tr></table></div></figure>
<p>创建kubia-ingress.yaml资源文件</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: kubia.example.com				// Ingress 将域名kubia.example.com映射到您的服务</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kubia-nodeport	// 将所有请求发送到kubia-nodeport服务的80端口</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>kubectl创建ingress.yaml资源文件遇到webhook …错误时修改master节点上/etc/kubernetes/manifests/kube-apiserver.yaml，将K8s默认用系统配置的proxy注释掉，稍后再运行kubectl create …就可以了</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-ingress.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get ingress		// 自己机器上没有获得ADDRESS这列IP</span></span><br><span class="line">NAME    CLASS    HOSTS               ADDRESS            PORTS   AGE</span><br><span class="line">kubia   &lt;none&gt;   kubia.example.com   192.168.99.100     80      92s</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>一旦知道 IP 地址，通过配置 DNS 服务器将 kubia.example.com 解析为此 IP地址，或者在/ect/hosts(Windows系统为C:\windows\system32\drivers\etc\hosts ）文件中添加下面一行内容：</p>
</blockquote>
<figure class="highlight basic"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">192 </span><span class="number">168.99.100</span> kubia.example.<span class="keyword">com</span></span><br></pre></td></tr></table></div></figure>
<p>通过Ingress访问pod, 环境都己经建立完毕，可以通过 http ：此ubia.example.com 地址访 问服务 （使用浏览器或者 curl 命令）</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl http://kubia.example.com</span></span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>客户端如何通过 Ingress 控制器连接到 其 中 一个 pod。客户端首先对 kubia.example.com 执行 DNS 查 找， DNS 服务器（或本地操作系统）返回了In gress 控制器的 IP。<br>客户端然后 向 Ingress 控制器发送 HTTP 请求，并在 Host 头中指定 kubia . example.com。<br>控制器从该头部确定客户端尝试访 问哪个服务，通过与该服务关联 的 Endpo int 对象查看 pod IP ， 并将客户端的请求转发给其中一个pod。<br>Ingress 控制器不会将请求转发给该服务，只用它来选择一个pod。大多数（即使不是全部）控制器都是这样工作的.</p>
</blockquote>
<p>Ingress规范的 rules 和 paths 都是数组，因此它们可以包含多个条目<br>一个 Ingress 可以将 多个主机和路径映射到多个服务</p>
<ul>
<li>客户端可以通过一个 IP 地址（ Ingress 控制器的 IP 地址 〉访问两种不同的服务</li>
<li>同样，可以使用 Ingress 根据 HTTP 请求中的主机而不是（仅）路径映射到不同的服务<br>kubia-ingress.yaml</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: networking.k8s.io/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: kubia.example.com				// 对 kubia.example.com 的请求将会转发至kubia服务</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kubia-nodeport</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /kubia					// 对 kubia.example.com/kubia 的请求将会转发至kubia服务</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: kubia</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /foo						// 对 kubia.example.com/foo 的请求将会转发至bar服务</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: bar</span><br><span class="line">          servicePort: 80</span><br><span class="line">  - host: bar.example.com				// 对 bar.example.com 的请求将会转发至bar服务</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: bar</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get ingress</span></span><br><span class="line">NAME    CLASS    HOSTS                               ADDRESS   PORTS   AGE</span><br><span class="line">kubia   &lt;none&gt;   kubia.example.com,bar.example.com             80      3s</span><br></pre></td></tr></table></div></figure>
<p>DNS 需要将 foo .example.com 和 bar.example.com 域名都指向 Ingress 控制器的 IP 地址.<br>然后像上面一样配置/ect/hosts文件内容,就可以通过域名访问了</p>

        <h3 id="Ingress处理TLS传输"   >
          <a href="#Ingress处理TLS传输" class="heading-link"><i class="fas fa-link"></i></a>Ingress处理TLS传输</h3>
      <p>配置 Ingress 以支持 TLS, Ingress 转发 HTTP 流量.</p>

        <h2 id="readiness-Probe-就绪探针"   >
          <a href="#readiness-Probe-就绪探针" class="heading-link"><i class="fas fa-link"></i></a>readiness Probe 就绪探针</h2>
      <blockquote>
<p>了解了存活探针，以及它们如何通过确保异常容器自动重启来保持应用程序的正常运行 。 与存活探针类似， Kubernetes 还允许为容器定义准备就绪探针<br>就绪探测器会定期调用，并确定特定的 pod 是否接收客户端请求 。 当容器的准备就绪探测返回成功时，表示容器己准备好接收请求<br>就绪探针有三种类型:</p>
</blockquote>
<ul>
<li>Exec 探针，执行进程的地方。容器的状态由进程的退出状态代码确定 。</li>
<li>HTTP GET 探针，向容器发送 HTTP GET 请求，通过响应的 HTTP 状态代码判断容器是否准备好 。</li>
<li>TCP socket 探针，它打开一个 TCP 连接到容器的指定端口。如果连接己建立，则认为容器己准备就绪 </li>
</ul>
<p>启动容器时，可以为 Kubernetes 配置一个等待时间，经过等待时间后才可以执行第一次准备就绪检查。<br>之后，它会周期性地调用探针，并根据就绪探针的结果采取行动。<br>如果某个 pod 报告它尚未准备就绪，则会从该服务中删除该 pod。如果 pod再次准备就绪，则重新添加 pod.<br>存活探针通过杀死异常的容器并用新的正常容器替代它们来保持 pod 正常工作，<br>就绪探针确保只有准备好处理请求的 pod 才可以接收它们（请求）</p>
<blockquote>
<p>设想一组pod (例如， 运行应用程序服务器的pod)取决于另 一 个pod (例如，后端数据库）提供的服务。 如果任何一个前端连接点出现连接间题并且无法再访问数据库， 那么就绪探针可能会告知Kubemet es该pod没有准备好处理任何请求。 如果其他pod实例没有遇到类似的连接问题， 则它们可以正常处理请求。 就绪探针确保客户端只与正常的pod交互， 并且永远不会知道系统存在问题.<br>通过kubectl ed江命令来向已存在的ReplicationController中的pod模板添加探针<br>kubia-replicaset-renameport.yaml</p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:</span><br><span class="line">        gpu: "true"</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubia</span><br><span class="line">        image: luksa/kubia</span><br><span class="line">        readinessProbe:			// pod中的每个容器都会有一个就绪探针</span><br><span class="line">          exec:</span><br><span class="line">            command:</span><br><span class="line">            - ls</span><br><span class="line">            - /var/ready</span><br><span class="line">        ports:</span><br><span class="line">        - name: http</span><br><span class="line">          containerPort: 8080</span><br><span class="line">        - name: https</span><br><span class="line">          containerPort: 8443</span><br></pre></td></tr></table></div></figure>

<p>创建RS资源并查看READY状态</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-replicaset-renameport.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-2xk54   0/1     Running   0          6m20s</span><br><span class="line">kubia-bj4tz   0/1     Running   0          6m20s</span><br><span class="line">kubia-hr9bx   0/1     Running   0          6m21s</span><br></pre></td></tr></table></div></figure>

<p>通过创建/var/ready文件使其中一个文件的就绪探针返回成功，该文件的存在可以模拟就绪探针成功<br>准备就绪探针会定期检查 默认情况下每 10 秒检查一次, 最晚 10 秒钟内， 该 pod 应该已经准备就绪.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> po/kubia-2xk54 -- touch /var/ready</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-2xk54   1/1     Running   0          6m26s</span><br><span class="line">kubia-bj4tz   0/1     Running   0          6m26s</span><br><span class="line">kubia-hr9bx   0/1     Running   0          6m27s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe po/kubia-2xk54</span></span><br><span class="line">......</span><br><span class="line">Readiness:      exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<p>修改创建过的RS的资源文件里的readiness命令是不生效的, 除非删了重建, 查看readiness Probe如下</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl edit rc kubia</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="再次测试-readiness"   >
          <a href="#再次测试-readiness" class="heading-link"><i class="fas fa-link"></i></a>再次测试 readiness</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-2xk54   1/1     Running   0          6m20s</span><br><span class="line">kubia-bj4tz   0/1     Running   0          6m20s</span><br><span class="line">kubia-hr9bx   0/1     Running   0          6m21s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">exec</span> kubia-2xk54 -- rm -rf /var/ready		// 过大概10s后</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-2xk54   0/1     Running   0          6m30s</span><br><span class="line">kubia-bj4tz   0/1     Running   0          6m30s</span><br><span class="line">kubia-hr9bx   0/1     Running   0          6m31s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> kubia-bj4tz -- touch /var/ready	// 过大概10s后</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kubia-2xk54   0/1     Running   0          6m40s   10.44.0.2   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-bj4tz   1/1     Running   0          6m40s   10.44.0.3   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-hr9bx   0/1     Running   0          6m40s   10.44.0.1   server02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></div></figure>
<p>查看SVC</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc	// 也可以通过kubectl <span class="built_in">exec</span> kubia-bj4tz env 来查看POD所支持的所有SVC的IP等信息</span></span><br><span class="line">NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubia                 ClusterIP      10.111.88.195   &lt;none&gt;        80/TCP,443/TCP   22h</span><br><span class="line">kubia-loadbalancer    LoadBalancer   10.102.224.77   &lt;pending&gt;     80:32671/TCP     3h8m</span><br></pre></td></tr></table></div></figure>

<p>两种访问方式，POD和Node<br>第一种POD访问SvcIP:SvcPort方式</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl <span class="built_in">exec</span> kubia -- curl -s  http://10.111.88.195:80		// SvcIP:SvcPort, SvcPort映射到PodIP, 可以通过describe svc查看</span></span><br><span class="line">You've hit kubia-bj4tz</span><br></pre></td></tr></table></div></figure>
<p>第二种NodeIP:NodePOrt方式</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s 10.239.140.186:32671		// NodeIP:NodePort</span></span><br><span class="line">You've hit kubia-bj4tz</span><br></pre></td></tr></table></div></figure>

<p>应该通过删除 pod 或更改 pod 标签而不是手动更改探针来从服务中手动移除pod.<br>如果想要从某个服务中手动添加或删除 pod, 请将 enabled=true 作为标签添加到 pod, 以及服务的标签选择器中。 当想要从服务中移除 pod 时，删除标签<br>应该始终定义一 个就绪探针， 即使它只是向基准 URL 发送 HTTP 请求一样简单。</p>

        <h2 id="headless-服务"   >
          <a href="#headless-服务" class="heading-link"><i class="fas fa-link"></i></a>headless 服务</h2>
      <p>让客户端连接到所有 pod, 需要找出每个 pod 的 IP.Kubemetes 允许客户通过 DNS 查找发现 pod IP.<br>但是对千 headless 服务， 由于 DNS 返回了 pod 的 IP,客户端直接连接到该 pod, 而不是通过服务代理.<br>headless 服务仍然提供跨 pod 的负载平衡， 但是通过 DNS 轮询机制不是通过服务代理.</p>
<blockquote>
<p>如果告诉Kubemetes, 不需要为服务提供集群 IP (通过在服务 spec 中将 clusterIP 字段设置为 None 来完成此操作）， 则 DNS 服务器将返回 pod IP 而不是单个服务 IP<br>将服务 spec中的clusterIP字段设置为None 会使服务成 为headless 服务，因为Kubemetes 不会 为其分配集群IP, 客户端可通过该IP将其连接到支持它的pod<br>kubia-svc-headless.yaml</p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiersion: v1</span><br><span class="line">kin: Service</span><br><span class="line">metdata:</span><br><span class="line">  nme: kubia-headless</span><br><span class="line">spe:</span><br><span class="line">  custerIP: None		// clusterIP字段设置为None 会使服务成 为headless服务</span><br><span class="line">  prts:</span><br><span class="line">  -port: 80</span><br><span class="line">   targetPort: 8080</span><br><span class="line">  slector:</span><br><span class="line">    app: kubia</span><br><span class="line"></span><br><span class="line">$ kubectl create -f kubia-svc-headless.yaml</span><br><span class="line">$ kubectl get svc</span><br><span class="line">NAME                  TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">kubia-headless        ClusterIP      None            <span class="tag">&lt;<span class="name">none</span>&gt;</span>        80/TCP           103s</span><br></pre></td></tr></table></div></figure>




</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/06_kubernetes_deployment/">06 Kubernetes deployment</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">3.1k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="deployment"   >
          <a href="#deployment" class="heading-link"><i class="fas fa-link"></i></a>deployment</h2>
      <p>Deployment 是一种更高阶资源， 用千部署应用程序并以声明的方式升级应用, 而不是通过 ReplicationController 或 ReplicaSet 进行部署， 它们都被认为是更底层的概念.<br>当创建一个 Deployment 时， ReplicaSet 资源也会随之创建.<br>在使用 Deployment 时， 实际的 pod是由 Deployment 的 Replicaset 创建和管理的， 而不是由 Deployment 直接创建和管理的.</p>
<blockquote>
<p>创建Deployment与创建ReplicationController并没有任何区别。Deployment也是由标签选择器、期望副数和pod模板组成的。此外，它还包含另 一 个字段，指定一 个部署策略，该策略定义在修改Deployment资源时应该如何执行更新<br>Deployment可以同时管理多个版本的 pod, 所以在命名时不需要指定应用的版本号<br>kubia-svc-loadbalancer.yaml</p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-loadbalancer</span><br><span class="line">spec:</span><br><span class="line">  type: LoadBalancer</span><br><span class="line">  ports:</span><br><span class="line">  - port: 80</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: kubia</span><br></pre></td></tr></table></div></figure>

<p>kubia-deployment-v1.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: kubia</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: luksa/kubia:v1</span><br><span class="line">        name: nodejs</span><br></pre></td></tr></table></div></figure>

<p>创建deployment</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-deployment-v1.yaml --record</span></span><br></pre></td></tr></table></div></figure>
<p>查看deployment过程</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout status deployment kubia</span></span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 1 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 2 out of 3 new replicas have been updated...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...</span><br><span class="line">Waiting for deployment "kubia" rollout to finish: 1 old replicas are pending termination...</span><br><span class="line">deployment "kubia" successfully rolled out</span><br></pre></td></tr></table></div></figure>
<p>当使用 ReplicationController 创建 pod 时， 它们的名称是由 Controller 的名称加上一个运行时生成的随机字符串.<br>由 Deployment 创建的三个 pod 名称中均包含一个额外的数字, 这个数字实际上对应 Deployment 和 ReplicaSet 中的 pod 模板的哈希值</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">kubia-59d857b444-2nrr7   1/1     Running   0          5m15s</span><br><span class="line">kubia-59d857b444-f9pnx   1/1     Running   0          5m15s</span><br><span class="line">kubia-59d857b444-wzgnj   1/1     Running   0          5m15s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get replicasets</span></span><br><span class="line">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-59d857b444   3         3         3       8m48s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get svc</span></span><br><span class="line">NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE</span><br><span class="line">kubernetes           ClusterIP      10.96.0.1       &lt;none&gt;        443/TCP        4d22h</span><br><span class="line">kubia-loadbalancer   LoadBalancer   10.100.58.157   &lt;pending&gt;     80:31170/TCP   3s</span><br></pre></td></tr></table></div></figure>
<p>查看master node 的IP为10.239.140.186</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl 10.239.140.186:31170</span></span><br><span class="line">This is v1 running in pod kubia-59d857b444-wzgnj</span><br></pre></td></tr></table></div></figure>

<p>查看deployment详细信息</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl describe deployment kubia</span><br><span class="line">Name:                   kubia</span><br><span class="line">Namespace:              default</span><br><span class="line">CreationTimestamp:      Mon, 18 May 2020 15:04:12 +0800</span><br><span class="line">Labels:                 <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">Annotations:            deployment.kubernetes.io/revision: 1</span><br><span class="line">                        kubernetes.io/change-cause: kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br><span class="line">Selector:               app=kubia</span><br><span class="line">Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable</span><br><span class="line">StrategyType:           RollingUpdate</span><br><span class="line">MinReadySeconds:        0</span><br><span class="line">RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  app=kubia</span><br><span class="line">  Containers:</span><br><span class="line">   nodejs:</span><br><span class="line">    Image:        luksa/kubia:v1</span><br><span class="line">    Port:         <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">    Host Port:    <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">    Environment:  <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">    Mounts:       <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">  Volumes:        <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason</span><br><span class="line">  ----           ------  ------</span><br><span class="line">  Available      True    MinimumReplicasAvailable</span><br><span class="line">  Progressing    True    NewReplicaSetAvailable</span><br><span class="line">OldReplicaSets:  <span class="tag">&lt;<span class="name">none</span>&gt;</span></span><br><span class="line">NewReplicaSet:   kubia-59d857b444 (3/3 replicas created)</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason             Age    From                   Message</span><br><span class="line">  ----    ------             ----   ----                   -------</span><br><span class="line">  Normal  ScalingReplicaSet  7m12s  deployment-controller  Scaled up replica set kubia-59d857b444 to 3</span><br></pre></td></tr></table></div></figure>

<p>略微减慢滚动升级的速度， 以便观察升级过程确实是以滚动的方式执行的。 可以通过在Deployment上设置minReadySeconds属性来实现<br>minReadySeconds的主要功能是避免部署出错版本的应用， 而不只是单纯地减慢部署的速度.<br>minReadySeconds属性指定新创建的pod至少要成功运行多久之后，才能将其视为可用.<br>通常情况下需要 将minReadySeconds设置为更高的值， 以确保pod在它们真正开始接收实际流量之后可以持续保持就绪状态.</p>
<p>如kubectl patch命令将其设置为10秒</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl patch deployment kubia -p <span class="string">'&#123;"spec": &#123;"minreadyseconds": 10&#125;&#125;'</span></span></span><br></pre></td></tr></table></div></figure>

        <h2 id="重新部署几种方式"   >
          <a href="#重新部署几种方式" class="heading-link"><i class="fas fa-link"></i></a>重新部署几种方式</h2>
      <p>重建(recreate)，滚动(rollingUpdate)，蓝绿，金丝雀</p>

        <h3 id="recreate"   >
          <a href="#recreate" class="heading-link"><i class="fas fa-link"></i></a>recreate</h3>
      <p>停止Pod并重新创建Pod</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-recreate</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate		// 停止Pod并重新创建Pod</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-recreate</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">......</span><br><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: web-recreate</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  rules:</span><br><span class="line">  - host: web-recreate.mooc.com  //配置下/etc/hosts 文件 "IP web-recreate.mooc.com"</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: web-recreate</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></div></figure>


        <h3 id="滚动更新deployment"   >
          <a href="#滚动更新deployment" class="heading-link"><i class="fas fa-link"></i></a>滚动更新deployment</h3>
      <p>滚动更新过程svc的IP等不会变，只是改变pod版本，修改pod的镜像就可以了</p>
<blockquote>
<p>实际上， 如何达到新的系统状态的过程是由 Deployment 的升级策略决定的，默认策略是执行滚动更新（策略名为 RollingUpdate)。 另 一种策略为 Recreate, 它会一次性删除所有旧版本的 pod, 然后创建新的 pod, 整个行为类似千修改ReplicationController 的 pod 模板， 然后删除所有的 pod<br>使用kubectl set image命令来更改任何包含容器资源的镜像(ReplicationController、ReplicaSet、 Deployment等）</p>
</blockquote>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-rollingupdate</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:			// 滚动更新, 不设置的话都会设成跟下面一样的默认值, $kubectl get deployment Name -o yaml查看</span><br><span class="line">      maxSurge: 25%			// 可以最大超出实力数的百分比，4个replicas，就是每次最多多启动1个实例</span><br><span class="line">      maxUnavailable: 25%	// 不可用实力百分比，4个replicas，就是每次必须有3个实例可用</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-rollingupdate</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-bluegreen</span><br><span class="line">        version: v1.0</span><br><span class="line">    spec:</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">set</span> image deployment kubia nodejs=luksa/kubia:v2</span></span><br><span class="line">deployment.apps/kubia image updated</span><br></pre></td></tr></table></div></figure>
<p>创建rs，然后新的rs会创建一个pod，然后原来的rs删除一个pod，新的rs再创建一个pod，依次完成设定的3个pod都完成更新.<br>通过命令定时查看输出:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">while</span> sleep 0.2; <span class="keyword">do</span> curl <span class="string">"http://web-rollingupdate.mooc.com/hello?name=michael"</span>; <span class="built_in">echo</span> <span class="string">""</span>; <span class="keyword">done</span></span></span><br></pre></td></tr></table></div></figure>

<p>查看Pod:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME                     READY   STATUS              RESTARTS   AGE</span><br><span class="line">fortune                  2/2     Running             0          3d</span><br><span class="line">kubia-59d857b444-6nq74   1/1     Running             0          72s</span><br><span class="line">kubia-59d857b444-jbln7   1/1     Terminating         0          72s</span><br><span class="line">kubia-59d857b444-lr2zw   1/1     Running             0          72s</span><br><span class="line">kubia-7d5c456ffc-69gg5   0/1     ContainerCreating   0          2s</span><br><span class="line">kubia-7d5c456ffc-vmz74   1/1     Running             0          14s</span><br></pre></td></tr></table></div></figure>

<p>滚动更新后旧的 ReplicaSet 仍然会被保留</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rs</span></span><br><span class="line">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-59d857b444   3         3         3       9m27s</span><br><span class="line">kubia-7d5c456ffc   1         1         0       3s</span><br></pre></td></tr></table></div></figure>
<p>再等待一会再查看rs</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rs</span></span><br><span class="line">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-59d857b444   0         0         0       10m</span><br><span class="line">kubia-7d5c456ffc   3         3         3       72s</span><br></pre></td></tr></table></div></figure>

<p>Deployment可以非常容易地回滚到先前部署的版本，它可以让Kubernetes 取消最后一次部署的 Deployment</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">set</span> image deployment kubia nodejs=luksa/kubia:v2</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rs</span></span><br><span class="line">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-59d857b444   0         0         0       3h3m</span><br><span class="line">kubia-79b84b44f4   3         3         3       96s</span><br><span class="line">kubia-7d5c456ffc   0         0         0       174m</span><br></pre></td></tr></table></div></figure>
<p>undo 命令也可以在滚动升级过程中运行，并直接停止滚动升级。 在升级过程中已创建的 pod 会被删除并被老版本的 pod 替代</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout undo deployment kubia</span></span><br><span class="line">deployment.apps/kubia rolled back</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rs</span></span><br><span class="line">NAME               DESIRED   CURRENT   READY   AGE</span><br><span class="line">kubia-59d857b444   0         0         0       3h7m</span><br><span class="line">kubia-79b84b44f4   0         0         0       5m19s</span><br><span class="line">kubia-7d5c456ffc   3         3         3       177m</span><br></pre></td></tr></table></div></figure>

<p>kubectl rollout history 来显示升级的版本</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout <span class="built_in">history</span> deployment kubia</span></span><br><span class="line">deployment.apps/kubia</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br><span class="line">2         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br><span class="line">3         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br></pre></td></tr></table></div></figure>


        <h3 id="回滚到一个特定的-Deployment-版本"   >
          <a href="#回滚到一个特定的-Deployment-版本" class="heading-link"><i class="fas fa-link"></i></a>回滚到一个特定的 Deployment 版本</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout undo deployment kubia --to-revision=l</span></span><br><span class="line">deployment.apps/kubia rolled back</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>旧版本的 ReplicaSet 过多会导致 ReplicaSet 列表过于混乱，可以通过指定Deployment 的 re visionHistoryLimit 属性来限制历史版本数量。默认值是 2</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout <span class="built_in">history</span> deployment kubia</span></span><br><span class="line">deployment.apps/kubia</span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">2         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br><span class="line">3         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br><span class="line">4         kubectl create --filename=kubia-deployment-v1.yaml --record=true</span><br></pre></td></tr></table></div></figure>


        <h3 id="滚动升级速率"   >
          <a href="#滚动升级速率" class="heading-link"><i class="fas fa-link"></i></a>滚动升级速率</h3>
      <p>在 Deployment 的滚动升级期间，有两个属性会决定一次替换多少个pod: maxSurge 和 maxUnavailable。可以通过 Deployment 的 strategy 字 段下rollingUpdate 的子属性来配置.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge· 1</span><br><span class="line">      maxUnavailable: 0</span><br><span class="line">    type: RollingUpdate</span><br></pre></td></tr></table></div></figure>

<blockquote>
<ul>
<li>maxSurge: 决定了 Deployment 配置中期望的副本数之外，最多允许超出的 pod 实例的数量。默认值为 25%，所以 pod 实例最多可以比期望数量多25%. 这个值也可以不是百分数而是绝对值(例如，可以允许最多多出一个成两个pod).</li>
<li>maxUnavailable: 决定了在滚动升级期间 ，相对于期望副本数能够允许有多少 pod 实例处于不可用状态。默认值也是25%, 所以可用 pod 实例的数量不能低于期望副本数的75%. 与 maxSurge 一样，也可以指定绝对值而不是百分比.</li>
</ul>
</blockquote>

        <h3 id="暂停滚动升级"   >
          <a href="#暂停滚动升级" class="heading-link"><i class="fas fa-link"></i></a>暂停滚动升级</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">set</span> image deployment kubia nodejs=luksa/kubia:v4</span></span><br><span class="line">deployment "kubia" image updated</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout pause deployment kubia</span></span><br><span class="line">deployment "kubia" paused</span><br></pre></td></tr></table></div></figure>


        <h3 id="恢复滚动升级"   >
          <a href="#恢复滚动升级" class="heading-link"><i class="fas fa-link"></i></a>恢复滚动升级</h3>
      <p>如果部署被暂停， 那么在恢复部署之前， 撤销命令不会撤销它</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout resume deployment kubia</span></span><br><span class="line">deployment "kubia" resumed</span><br></pre></td></tr></table></div></figure>


        <h3 id="readinessProbe"   >
          <a href="#readinessProbe" class="heading-link"><i class="fas fa-link"></i></a>readinessProbe</h3>
      <p>kubia-deployment-v3-with-readinesscheck.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3					// POD副本个数为3</span><br><span class="line">  minReadySeconds: 10			// 设置minReadySeconds 值为10s,指定新创建的pod至少要成功运行多久之后，才能将其视为可用.</span><br><span class="line">  progressDeadlineSeconds: 120	// 设置升级失败超时时间，超过则自动停止升级，如下describe deployment 可查看到"Progressing False..."信息，需要手动undo来取消升级</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:</span><br><span class="line">      maxSurge: 1</span><br><span class="line">      maxUnavailable: 0			// 设置maxUnavailable的 中值为0来确保升级过程 pod被挨个替换</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      name: kubia</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: luksa/kubia:v3	// 修改image</span><br><span class="line">        name: nodejs</span><br><span class="line">        readinessProbe:</span><br><span class="line">          periodSeconds: 1		// 定义一个就绪探针，并且每隔—秒钟执行一次</span><br><span class="line">          httpGet:				// 就绪探针会执行发送HTTP GET请求到容器</span><br><span class="line">            path: /</span><br><span class="line">            port: 8080</span><br></pre></td></tr></table></div></figure>

<p>直接使用kubectl apply来升级Deployment<br>apply命令可以用YAML 文件中声明的字段来更新Deployment。不仅更新镜像，而且还添加了就绪探针， 以及在 YAML 中添加或修改的其他声明。<br>如果新的 YAML也包含rep巨 cas字段， 当它与现有Deployment中的数量不一致时， 那么apply 操作也会对Deployment进行扩容.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f kubia-deployment-v3-with-readinesscheck.yaml</span></span><br><span class="line">deployment "kubia" configured</span><br></pre></td></tr></table></div></figure>


        <h3 id="取消滚动升级"   >
          <a href="#取消滚动升级" class="heading-link"><i class="fas fa-link"></i></a>取消滚动升级</h3>
      <p>默认情况下， 在10分钟内不能完成滚动升级的话， 将被视为失败。 如果运行kubectl describe deployment命令， 将会显示一条ProgressDeadlineExceeded的记录.<br>判定Deployment滚动升级失败的超时时间， 可以通过设定Deployment spec中的progressDeadlineSeconds来指定.<br>如果达到了progressDeadlineSeconds指定的时间， 则滚动升级过程会自动取消.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe deployment kubia</span></span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason</span><br><span class="line">  ----           ------  ------</span><br><span class="line">  Available      True    MinimumReplicasAvailable</span><br><span class="line">  Progressing    True    ReplicaSetUpdated</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>
<p>过了3，4分钟后再次执行</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe deployment kubia</span></span><br><span class="line">Name:                   kubia</span><br><span class="line">......</span><br><span class="line">Conditions:</span><br><span class="line">  Type           Status  Reason</span><br><span class="line">  ----           ------  ------</span><br><span class="line">  Available      True    MinimumReplicasAvailable</span><br><span class="line">  Progressing    False   ProgressDeadlineExceeded</span><br><span class="line">OldReplicaSets:  kubia-59d857b444 (3/3 replicas created)</span><br><span class="line">NewReplicaSet:   kubia-7d6c89d47b (1/1 replicas created)</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<p>因为滚动升级过程不再继续， 所以只能通过rollout undo命令来取消滚动升级, 其实就是回滚上一个版本.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl rollout undo deployment kubia</span></span><br><span class="line">deployment.apps/kubia rolled back</span><br></pre></td></tr></table></div></figure>


        <h3 id="蓝绿部署"   >
          <a href="#蓝绿部署" class="heading-link"><i class="fas fa-link"></i></a>蓝绿部署</h3>
      <p>可以通过修改Service下的不同version值，选择应用不同版本的Pod来提供服务.<br>新版本运行起来一段时间没问题后才可以删除旧版本.<br>一般保持有两个版本的应用也就是Pod保持运行状态, 只有下个版本部署一段时间没问题后，第一个版本的可以删除.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-rollingupdate</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  strategy:</span><br><span class="line">    rollingUpdate:			// 滚动更新, 不设置的话都会设成跟下面一样的默认值, $kubectl get deployment Name -o yaml查看</span><br><span class="line">      maxSurge: 25%			// 可以最大超出实力数的百分比，4个replicas，就是每次最多多启动1个实例</span><br><span class="line">      maxUnavailable: 25%	// 不可用实力百分比，4个replicas，就是每次必须有3个实例可用</span><br><span class="line">    type: RollingUpdate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-rollingupdate</span><br><span class="line">  replicas: 2</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-bluegreen</span><br><span class="line">        version: v2.0		// 蓝绿部署，与下面的version一致，部署不同版本的应用</span><br><span class="line">    spec:</span><br><span class="line">......</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: web-bluegreen</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - ports: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 8080</span><br><span class="line">  selector:</span><br><span class="line">    app: web-bluegreen</span><br><span class="line">    version: v2.0			// 与上面的version一致，部署不同版本应用, 下次部署单独抽离Service，选择不同版本version就可以了</span><br><span class="line">  type: ClusterIP</span><br><span class="line">---</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>


        <h3 id="金丝雀发布"   >
          <a href="#金丝雀发布" class="heading-link"><i class="fas fa-link"></i></a>金丝雀发布</h3>
      <blockquote>
<p>在蓝绿部署的基础之上，去掉version, 修改selector就是金丝雀部署.<br>一个新的 pod 会被创建， 与此同时所有旧的 pod 还在运行。 一旦新的 pod 成功运行， 服务的一部分请求将被切换到新的 pod。 这样相当于 运行了一个金丝雀版本。金丝雀发布是一种可以将应用程序的出错版本和其影响到的用户的风险化为最小的技术.<br>与其直接向每个用户发布新版本， 不如用新版本替换 一个或一小部分的 pod。通过 这种方式， 在升级的初期只有少数用户会访问新版本。 验证新版本是否正常工作之后， 可以将剩余的 pod 继续升级或者回滚到上一个的版本</p>
</blockquote>

        <h3 id="删除deployment"   >
          <a href="#删除deployment" class="heading-link"><i class="fas fa-link"></i></a>删除deployment</h3>
      <p>删除deployment会连带删除创建的replicaset和由replicaset创建的pod</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete deployment kubia</span></span><br><span class="line">deployment.apps "kubia" deleted</span><br></pre></td></tr></table></div></figure>













</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/03_kubernetes_ns_pod/">03 Kubernetes nodes, namespace, pod</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">9.4k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="nodes"   >
          <a href="#nodes" class="heading-link"><i class="fas fa-link"></i></a>nodes</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get nodes -o wide</span></span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION           CONTAINER-RUNTIME</span><br><span class="line">hci-node01   Ready    master   5d    v1.18.1   10.67.108.211   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8</span><br><span class="line">hci-node02   Ready    &lt;none&gt;   5d    v1.18.1   10.67.109.142   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8</span><br><span class="line">hci-node03   Ready    &lt;none&gt;   5d    v1.18.1   10.67.109.147   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8</span><br><span class="line">hci-node04   Ready    &lt;none&gt;   5d    v1.18.1   10.67.109.144   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-1062.el7.x86_64   docker://19.3.8</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe node hci-node02</span></span><br><span class="line">输出显示了节点的状态、 CPU 和内存数据、系统信息、运行容器的节点等</span><br><span class="line"></span><br><span class="line">查看某台机器的资源</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe node hci-node01</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="创建别名和补全"   >
          <a href="#创建别名和补全" class="heading-link"><i class="fas fa-link"></i></a>创建别名和补全</h3>
      <p>kubectl 会被经常使用。很快你就会发现每次不得不打全命令是非常痛苦的。<br>将下面的代码添加到 ~/.bashrc 或类似的文件中 ：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">alias</span> k=kubectl</span></span><br></pre></td></tr></table></div></figure>
<p>为kuebctl配置 tab 补全<br>需要先安装一个叫作 bashcompletio口的包来启用 bash 中的 tab 命令补全， 然后可以运行接下来的命令（也需要加到 ~/.bashrc 或类似的文件中）</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> &lt;&#123;kubectl completion bash)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl desc&lt;TAB&gt; nod&lt;TAB&gt; hci&lt;TAB&gt;</span></span><br></pre></td></tr></table></div></figure>
<p>但是需要注意的是， tab 命令行补全只在使用完整的 kubectl 命令时会起作用,(当使用别名 k 时不会起作用). 需要改变 kubectl completion 的输出来修复：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> &lt;(kubectl completion bash | sed s/kubectl/k/g)</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="node-标签"   >
          <a href="#node-标签" class="heading-link"><i class="fas fa-link"></i></a>node 标签</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get nodes</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl label node server02 gpu=<span class="literal">false</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl label node server02 gpu=<span class="literal">true</span> --overwrite	//修改node标签</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -L gpu		// 列出所有node，并添加GPU一列进行展示</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -l gpu		// 只列出含标签的key为gpu的node</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get node -l gpu=<span class="literal">false</span>	// 只列出含gpu=<span class="literal">false</span>的node</span></span><br></pre></td></tr></table></div></figure>

<p>将POD调度到指定的node上: kubia-gpu.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">name: kubia-gpu		// 指定生成的POD名字</span><br><span class="line">spec:</span><br><span class="line">nodeSelector:		// node选择器,选择含标签gpu=true的node机器</span><br><span class="line">	gpu: "true"		</span><br><span class="line">containers:</span><br><span class="line">- image: luksa/kubia	// 要拉取的 image 名字</span><br><span class="line">	name: kubia			// 生成的 container 名字</span><br></pre></td></tr></table></div></figure>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-gpu.yaml</span></span><br></pre></td></tr></table></div></figure>
<p>如果没有标签为gpu=true的合适node， 通过 $ kubectl describe pod/kubia-nogpu 查看Message， 会报 0/2 nodes are available: 2 node(s) didn’t match node selector.信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod/kubia-gpu</span></span><br><span class="line">......</span><br><span class="line">Node-Selectors:  gpu=true</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>


        <h3 id="taint污点"   >
          <a href="#taint污点" class="heading-link"><i class="fas fa-link"></i></a>taint污点</h3>
      <p>给Node添加污点可以让配置tolerations的Pod部署上来，而不让平常的Pod部署.<br>配置tolerations的Pod可以部署到添加污点的机器也可以部署到其它平常机器</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">// 查看node机器污点</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe node/&lt;Node-Name&gt; | grep Taint</span></span><br><span class="line">  Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">// 去掉污点</span><br><span class="line"><span class="meta">$</span><span class="bash">  kubectl taint nodes &lt;Node-Name&gt; node-role.kubernetes.io/master:NoSchedule-</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl taint nodes NodeName gpu=<span class="literal">true</span>:NoSchedule</span></span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-demo</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-demo</span><br><span class="line">  replicas: 3			// 副本数3来测试能部署到哪些机器</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-demo</span><br><span class="line">    spec:</span><br><span class="line">      selector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: web-demo</span><br><span class="line">      replicas: 1</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: web-demo</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: web-demo</span><br><span class="line">            image: hub.mooc.com/kubernetes/web:v1</span><br><span class="line">            ports:</span><br><span class="line">            - containerPort: 8080</span><br><span class="line">          tolerations:		// 可以部署到有设置taint的机器，也可以部署到其它机器</span><br><span class="line">          - key: "gpu"</span><br><span class="line">            operator: "Equal"</span><br><span class="line">            value: "true"</span><br><span class="line">            effect: "NoSchedule"</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>典型的使用kubeadm部署和初始化的Kubernetes集群，master节点被设置了一个node-role.kubernetes.io/master:NoSchedule的污点，可以使用kubectl describe node <node-name>命令查看<br>这个污点表示默认情况下master节点将不会调度运行Pod，即不运行工作负载, 对于使用二进制手动部署的集群设置和移除这个污点的命令如下:</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl taint nodes &lt;node-name&gt; node-role.kubernetes.io/master=:NoSchedule</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl taint nodes &lt;node-name&gt; node-role.kubernetes.io/master:NoSchedule-</span></span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>kubeadm初始化的Kubernetes集群，master节点也被打上了一个node-role.kubernetes.io/master=的label，标识这个节点的角色为master。给Node设置Label和设置污点是两个不同的操作。设置Label和移除Label的操作命令如下</p>
</blockquote>
<p>设置Label</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label node node1 node-role.kubernetes.io/master=</span></span><br></pre></td></tr></table></div></figure>

<p>移除Label</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label node node1 node-role.kubernetes.io/master-</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="Namespace"   >
          <a href="#Namespace" class="heading-link"><i class="fas fa-link"></i></a>Namespace</h2>
      <blockquote>
<p>大多数对象的名称必须符合 RFC 1035 （域名）中规定的命名规范 ，这意味着它们可能只包含字母、数字、横杠（－）和点号，但命名空间（和另外几个）不允许包含点号</p>
</blockquote>

        <h3 id="隔离性"   >
          <a href="#隔离性" class="heading-link"><i class="fas fa-link"></i></a>隔离性</h3>
      <blockquote>
<p>名字的隔离只是 通过svc名称(DNS) 访问的隔离，通过svc的IP和Pod的IP再加上端口号(Port) 照样可以访问不同命名空间下的服务.</p>
</blockquote>

        <h3 id="设置默认命名空间"   >
          <a href="#设置默认命名空间" class="heading-link"><i class="fas fa-link"></i></a>设置默认命名空间</h3>
      <blockquote>
<p>默认Kubeclt获取default命名空间下的资源，可以通过设置K8s上下文配置文件如kube.config 使得某个命名空间变为默认namespace，获取pod时候不需要在加上 -n 参数</p>
</blockquote>

        <h3 id="创建命名空间"   >
          <a href="#创建命名空间" class="heading-link"><i class="fas fa-link"></i></a>创建命名空间</h3>
      <blockquote>
<p>namespace不提供网络隔离, 如果命名空间 foo 中的某个 pod 知道命名空间 bar 中 pod 的 IP 地址，那它就可以将流量（例如 HTTP 请求）发送到另一个 pod<br>第一种： commands方式</p>
</blockquote>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create namespace custom-namespace</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create ns custom-namespace</span></span><br></pre></td></tr></table></div></figure>

<p>第二种： Yaml方式， 之所以选择使用 YAML 文件，只是为了强化Kubemetes中的所有内容都是一 个 API 对象这一概念</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> touch custom-namespace.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: custom-namespace</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f custom-namespace.yaml</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="划分方式"   >
          <a href="#划分方式" class="heading-link"><i class="fas fa-link"></i></a>划分方式</h3>
      <pre><code>* 按环境划分: dev(开发), test(测试)
* 按团队划分
* 自定义多级划分</code></pre>
        <h3 id="标记命名空间"   >
          <a href="#标记命名空间" class="heading-link"><i class="fas fa-link"></i></a>标记命名空间</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label namespace default istio-injection=enabled --overwrite         // enabled</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl label namespace default istio-injection=disabled --overwrite        // disabled</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl label namespace default istio-injection= --overwrite                // cancel <span class="built_in">set</span></span></span><br><span class="line">namespace/default labeled</span><br></pre></td></tr></table></div></figure>


        <h3 id="查看标记-istio-injection-enabled-标签的命名空间"   >
          <a href="#查看标记-istio-injection-enabled-标签的命名空间" class="heading-link"><i class="fas fa-link"></i></a>查看标记 istio-injection=enabled 标签的命名空间</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get namespace -L istio-injection</span></span><br><span class="line">NAME              STATUS   AGE   ISTIO-INJECTION</span><br><span class="line">default           Active   85m   enabled</span><br><span class="line">istio-system      Active   25m   disabled</span><br><span class="line">kube-node-lease   Active   85m</span><br><span class="line">kube-public       Active   85m</span><br><span class="line">kube-system       Active   85m</span><br><span class="line">[root@hci-node01 istio-1.5.2]#</span><br></pre></td></tr></table></div></figure>


        <h3 id="删除namespace"   >
          <a href="#删除namespace" class="heading-link"><i class="fas fa-link"></i></a>删除namespace</h3>
      <p>删除当前命名空间中的所有资源，可以删除ReplicationCcontroller和pod,以及我们创建的所有service<br>第一个 all 指定正在删除所有资源类型, –all 选项指定将删除所有资源实例, 而不是按名称指定它们<br>使用 all 关键字删除所有内容并不是真的完全删除所有内容。 一些资源比如Secret会被保留下来， 并且需要被明确指定删除</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete all --all		// 命令也会删除名为 kubernetes 的Service, 但它应该会在几分钟后自动重新创建</span></span><br></pre></td></tr></table></div></figure>
<p>可以简单地删除整个命名空间（ pod 将会伴随命名空间 自动删除〉</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete ns custom-namespace</span></span><br></pre></td></tr></table></div></figure>
<p>强制删除NAMESPACE</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete namespace NAMESPACENAME --force --grace-period=0</span></span><br></pre></td></tr></table></div></figure>
<p>进入kube-system下得etcd pod 删除需要删除的NAMESPACE</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -n kube-system</span></span><br><span class="line">NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">etcd-hci-node01                      1/1     Running   5          16d</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> -it etcd-hci-node01 sh -n kube-system</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> etcdctl del /registry/namespaces/NAMESPACENAME</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="POD"   >
          <a href="#POD" class="heading-link"><i class="fas fa-link"></i></a>POD</h2>
      
        <h3 id="查看pod解释"   >
          <a href="#查看pod解释" class="heading-link"><i class="fas fa-link"></i></a>查看pod解释</h3>
      <figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl explain pod</span><br><span class="line">KIND:     Pod</span><br><span class="line">VERSION:  v1</span><br><span class="line"></span><br><span class="line">DESCRIPTION:</span><br><span class="line">	Pod is a collection of containers that can run on a host. This resource is</span><br><span class="line">	created by clients and scheduled onto hosts.</span><br><span class="line"></span><br><span class="line">FIELDS:</span><br><span class="line">apiVersion   <span class="tag">&lt;<span class="name">string</span>&gt;</span></span><br><span class="line">	APIVersion defines the versioned schema of this representation of an</span><br><span class="line">	object. Servers should convert recognized schemas to the latest internal</span><br><span class="line">	value, and may reject unrecognized values. More info:</span><br><span class="line">	https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources</span><br><span class="line"></span><br><span class="line">kind <span class="tag">&lt;<span class="name">string</span>&gt;</span></span><br><span class="line">	Kind is a string value representing the REST resource this object</span><br><span class="line">	represents. Servers may infer this from the endpoint the client submits</span><br><span class="line">	requests to. Cannot be updated. In CamelCase. More info:</span><br><span class="line">	https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds</span><br><span class="line"></span><br><span class="line">metadata     <span class="tag">&lt;<span class="name">Object</span>&gt;</span></span><br><span class="line">	Standard object's metadata. More info:</span><br><span class="line">	https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata</span><br><span class="line"></span><br><span class="line">spec <span class="tag">&lt;<span class="name">Object</span>&gt;</span></span><br><span class="line">	Specification of the desired behavior of the pod. More info:</span><br><span class="line">	https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br><span class="line"></span><br><span class="line">status       <span class="tag">&lt;<span class="name">Object</span>&gt;</span></span><br><span class="line">	Most recently observed status of the pod. This data may not be up to date.</span><br><span class="line">	Populated by the system. Read-only. More info:</span><br><span class="line">	https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#spec-and-status</span><br></pre></td></tr></table></div></figure>

<p>深入理解POD属性</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl explain pod.apiVersion</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl explain pod.kind</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl explain pod.spec</span></span><br></pre></td></tr></table></div></figure>
<p>pods 的缩写是 po, service 的缩写是 SVC, replicationcontroller 的缩写 rc</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -n kube-system</span></span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>我们提到过每个 pod 都有自己的 IP 地址，但是这个地址是集群 内部的，不能从集群外部访问。<br>要让 pod 能够从外部访问 ， 需要通过服务对象公开它， 要创建一个特殊的 LoadBalancer 类型的服务。<br>因为如果你创建一个常规服务（ 一个 Cluster IP 服务）， 比如 pod ，它也 只能从集群内部访问。<br>通过创建 LoadBalanc er 类型 的服务，将创建一个外部的负载均衡 ，可以通过 负载均衡的公共 IP 访问 pod </p>
</blockquote>

        <h3 id="创建POD"   >
          <a href="#创建POD" class="heading-link"><i class="fas fa-link"></i></a>创建POD</h3>
      <p>通过上传 JSON 或 YAML 描述文件到 Kubemetes API 服务器来创建 pod.<br>kubectl create -f 命令用于从YAML或JSON文件创建任何资源（不只是 pod).</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-manual.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-gpu.yaml -n custom-namespace	//创建pod到custom-namespace命名空间下</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod/kubia</span></span><br><span class="line">......</span><br><span class="line">Events:</span><br><span class="line">Type    Reason     Age    From               Message</span><br><span class="line">----    ------     ----   ----               -------</span><br><span class="line">Normal  Scheduled  5m12s  default-scheduler  Successfully assigned default/kubia-liveness to server02</span><br><span class="line">Normal  Pulling    5m8s   kubelet, server02  Pulling image "luksa/kubia-unhealthy"</span><br></pre></td></tr></table></div></figure>
<p>编写好yaml文件在本地某个目录后, cd到此目录, 用一条command全部创建或删除资源</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f .	// 添加所有资源</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete -f .	// 删除所有资源</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="一直查看pod状态"   >
          <a href="#一直查看pod状态" class="heading-link"><i class="fas fa-link"></i></a>一直查看pod状态</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -w</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="pod标签labels"   >
          <a href="#pod标签labels" class="heading-link"><i class="fas fa-link"></i></a>pod标签labels</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po --show-labels</span></span><br></pre></td></tr></table></div></figure>
<p>查看pod标签的key值为creation_method 和 env 的信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -L creation_method,env</span></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV</span><br><span class="line">kubia                          1/1     Running   0          16h</span><br><span class="line">kubia-manual-v2                1/1     Running   0          34m   manual            pod</span><br></pre></td></tr></table></div></figure>
<p>POD添加标签</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label po kubia  creation_method=manual</span></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE   CREATION_METHOD   ENV</span><br><span class="line">kubia                          1/1     Running   0          16h   manual</span><br><span class="line">kubia-manual-v2                1/1     Running   0          43m   manual            pod</span><br></pre></td></tr></table></div></figure>
<p>更改现有标签, 在更改现有标签时， 需要使用–overwrite选项</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label po kubia-manual-v2 env=debug --overwrite</span></span><br></pre></td></tr></table></div></figure>
<p>使用标签列出POD</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -1 creation_method=manual</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -l env</span></span><br></pre></td></tr></table></div></figure>
<p>同样列出没有env标签的pod<br>确保使用单引号来圈引 !env, 这样bash shell才不会解释感叹号（译者注：感叹号在bash中有特殊含义， 表示事件指示器)</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -l <span class="string">'!env'</span></span></span><br><span class="line">creation_method!=manual 选择带有creation_method标签， 并且值不等于manual的pod</span><br><span class="line">env in (prod, devel)选择带有env标签且值为prod或devel的pod</span><br><span class="line">env notin (prod, devel)选择带有env标签， 但其 值不是prod或devel的pod</span><br><span class="line">app=pc,rel=beta 选择pc微服务的beta版本pod</span><br></pre></td></tr></table></div></figure>


        <h3 id="pod-注解"   >
          <a href="#pod-注解" class="heading-link"><i class="fas fa-link"></i></a>pod 注解</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl annotate pod kubia-gpu mycompany.com/someannotion=<span class="string">"foo bar"</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod/kubia-gpu</span></span><br><span class="line">......</span><br><span class="line">Annotations:  mycompany.com/someannotion: foo bar</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

        <h3 id="查看该-pod-的完整描述文件："   >
          <a href="#查看该-pod-的完整描述文件：" class="heading-link"><i class="fas fa-link"></i></a>查看该 pod 的完整描述文件：</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po kubia-manual -o yaml	// 获取yaml格式信息</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubect1 get po kubia-manual -o json	// 获取json格式信息</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="执行pod容器"   >
          <a href="#执行pod容器" class="heading-link"><i class="fas fa-link"></i></a>执行pod容器</h3>
      <p>直接执行:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> fortioclient-f8d65c6bb-5k4td -c captured date -n twopods</span></span><br></pre></td></tr></table></div></figure>
<p>进入容器执行, 当pod中只有一个容器时可以不加-c参数指定某个容器</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> fortioclient-f8d65c6bb-5k4td -c captured -i -t /bin/sh -n twopods</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="容器进程-网络等"   >
          <a href="#容器进程-网络等" class="heading-link"><i class="fas fa-link"></i></a>容器进程, 网络等</h3>
      <p>查看进程command完整信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps auxwww</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pa -ef</span></span><br></pre></td></tr></table></div></figure>
<p>查看网络</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> netstat -ntlp</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="查看Pod-svc日志"   >
          <a href="#查看Pod-svc日志" class="heading-link"><i class="fas fa-link"></i></a>查看Pod, svc日志</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs pod/istiod-774777b79-ddfk4 -n istio-system</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs -f pod/&lt;pod_name&gt; <span class="comment">#类似tail -f的方式查看(tail -f 实时查看日志文件 tail -f 日志文件log)</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs svc/istiod -n istio-system</span></span><br><span class="line">如果该pod中有其他容器， 可以通过如下命令获取其日志：</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs kubia-manual -c kubia</span></span><br></pre></td></tr></table></div></figure>

<p>查看容器重启后前一个容器为什么重启的日志信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs mypod --previous</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="部署应用程序"   >
          <a href="#部署应用程序" class="heading-link"><i class="fas fa-link"></i></a>部署应用程序</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="重启Pod"   >
          <a href="#重启Pod" class="heading-link"><i class="fas fa-link"></i></a>重启Pod</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod &#123;podname&#125; -n &#123;namespace&#125; -o yaml | kubectl replace --force -f -</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="删除Pod"   >
          <a href="#删除Pod" class="heading-link"><i class="fas fa-link"></i></a>删除Pod</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pod PODNAME -n custom-namespace		// 删除指定命名空间下的POD</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete po -l creation_method=manual			// 通过标签选择器来删除</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete po --all -n custom-namespace			// 删除当前命名空间中的所有 pod</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete all --all -n custom-namespace			// 删除所有pod和svc，系统带的kubernetes服务会过一会重启</span></span><br></pre></td></tr></table></div></figure>
<p>可使用kubectl中的强制删除命令删除POD</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete pod PODNAME --force --grace-period=0</span></span><br></pre></td></tr></table></div></figure>
<p>直接从ETCD中删除源数据<br>删除default namespace下的pod名为pod-to-be-deleted-0</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ETCDCTL_API=3 etcdctl del /registry/pods/default/pod-to-be-deleted-0</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="livenessProbe-存活探针-readinessProbe"   >
          <a href="#livenessProbe-存活探针-readinessProbe" class="heading-link"><i class="fas fa-link"></i></a>livenessProbe 存活探针, readinessProbe</h2>
      <p>Kubemetes 可以通过存活探针 (liveness probe) 检查容器是否还在运行.<br>Kubemetes 可以通过readinessProbe探针 检查容器是否准备完毕可以挂到负载均衡上供外部访问.<br>livenessProbe与readinessProbe探针用法完全一样, 都有三种，下面介绍这三种健康检查方式.</p>
<p>可以为 pod 中的每个容器单独指定存活探针。 如果探测失败， Kubemetes 将定期执行探针并重新启动容器</p>
<ul>
<li>第一种健康检查方式: 执行命令检查存活探针是否存活</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-demo</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-demo</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-demo</span><br><span class="line">    spec:</span><br><span class="line">      selector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: web-demo</span><br><span class="line">      replicas: 1</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: web-demo</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: web-demo</span><br><span class="line">            image: hub.mooc.com/kubernetes/web:v1</span><br><span class="line">            ports:</span><br><span class="line">            - containerPort: 8080</span><br><span class="line">            livenessProbe:				// 检查应用是否存活的探针, 和容器一个级别</span><br><span class="line">              exec:						// 第一种健康检查方式, 通过执行命令</span><br><span class="line">                command:</span><br><span class="line">                - /bin/sh</span><br><span class="line">                - -c</span><br><span class="line">                - ps -ef|grep java|grep -v grep</span><br><span class="line">              initialDelaySeconds: 10		// 容器起来后过10s开始检查</span><br><span class="line">              periodSeconds: 10				// 每隔10s检查一次</span><br><span class="line">              failureThreshold: 2			// 连续健康检查失败2次放弃检查, 重启容器</span><br><span class="line">              successThreshold: 1			// 检查一次满足条件就认为健康检查通过</span><br><span class="line">              timeoutSeconds: 5				// 每次健康检查delay时间是5s, 超时也认为健康检查失败, 重启容器</span><br><span class="line">            readinessProbe:				// readinessProbe与livenessProbe用法完全一样.</span><br><span class="line">              exec:						// 第一种检查方式, 通过执行命令</span><br><span class="line">                command:</span><br><span class="line">                - /bin/sh</span><br><span class="line">                - -c</span><br><span class="line">                - ps -ef|grep java|grep -v grep</span><br><span class="line">              initialDelaySeconds: 10		// 容器起来后过10s开始检查</span><br><span class="line">              periodSeconds: 10				// 每隔10s检查一次</span><br><span class="line">              failureThreshold: 2			// 连续健康检查失败2次放弃检查, 重启容器</span><br><span class="line">              successThreshold: 1			// 检查一次满足条件就认为健康检查通过</span><br><span class="line">              timeoutSeconds: 5				// 每次健康检查delay时间是5s, 超时也认为健康检查失败, 重启容器</span><br></pre></td></tr></table></div></figure>

<ul>
<li>第二种健康检查方式: 执行网络请求检查存活探针是否存活</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ touch kubia-liveness-probe.yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia-liveness</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: luksa/kubia-unhealthy</span><br><span class="line">    name: kubia</span><br><span class="line">    livenessProbe:		// 一个 HTTP GET 存活探针</span><br><span class="line">      httpGet:			// 第二种健康检查方式, 通过httpGet</span><br><span class="line">        path: /			// 应用要访问的路径</span><br><span class="line">        port: 8080		// 容器本身启动的端口</span><br><span class="line">        scheme: HTTP</span><br><span class="line">      initialDelaySeconds: 15		// 容器起来后过10s开始检查</span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></div></figure>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods</span></span><br><span class="line">NAME                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">kubia-liveness                 1/1     Running   1          13m</span><br></pre></td></tr></table></div></figure>
<p>查看该pod描述</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod/kubia-liveness</span></span><br><span class="line">......</span><br><span class="line">Last State:     Terminated</span><br><span class="line">  Reason:       Error</span><br><span class="line">  Exit Code:    137</span><br><span class="line">  Started:      Wed, 13 May 2020 15:49:36 +0800</span><br><span class="line">  Finished:     Wed, 13 May 2020 15:51:25 +0800</span><br><span class="line">Ready:          True</span><br><span class="line">Restart Count:  1</span><br><span class="line">Liveness:       http-get http://:8080/ delay=0s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>
<p>数字137是两个 数字的总和：128+x, 其中x是终止进程的信号编号.<br>在这个例子中，x等于9, 这是SIGKILL的信号编号，意味着这个进程被强行终止.<br>当容器被强行终止时，会创建一个全新的容器—-而不是重启原来的容器.<br>delay=Os部分显示在容器启动后立即开始探测.<br>timeout仅设置为1秒，因此容器必须在1秒内进行响应， 不然这次探测记作失败.<br>每10秒探测一次容器(period=lOs), 并在探测连续三次失败(#failure=3)后重启容器.<br>定义探 针时可以自定义这些附加参数。例如，要设 置初始延迟，请将initialDelaySeconds属性添加到存活探针的配置中.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">livenessProbe:		// 一个 HTTP GET 存活探针</span><br><span class="line">  httpGet:</span><br><span class="line">    path: /</span><br><span class="line">    port: 8080</span><br><span class="line">  initialDelaySeconds: 15	// Kubernetes会在第—次探测前等待15秒</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod/kubia-liveness</span></span><br><span class="line">......</span><br><span class="line">Liveness:       http-get http://:8080/ delay=15s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line"></span><br><span class="line">* 第三种健康检查方式: 通过TCP检查端口是否处于监听状态</span><br><span class="line">    livenessProbe:		// 一个 HTTP GET 存活探针</span><br><span class="line">      tcpSocket:</span><br><span class="line">        port: 8080</span><br><span class="line">      initialDelaySeconds: 20	// Kubernetes会在第—次探测前等待15秒 </span><br><span class="line">      periodSeconds: 5</span><br></pre></td></tr></table></div></figure>
<blockquote>
<p>如果没有设置初始延迟，探针将在启动时立即开始探测容器， 这通常会导致探测失败， 因为应用程序还没准备好开始接收请求.<br>务必记得设置一个初始延迟未说明应用程序的启动时间.<br>对于在生产中运行的pod, 一定要定义一个存活探针。没有探针的话，Kubemetes无法知道你的应用是否还活着。只要进程还在运行， Kubemetes会认为容器是健康的<br>Kubernetes会在你的容器崩溃或其存活探针失败时， 通过重启容器来保持运行。 这项任务由承载pod的节点上的Kubelet 执行 一— 在主服务器上运行的Kubernetes Control Plane组件不会参与此过程.<br>但如果节点本身崩溃， 那么Control Plane 必须为所有随节点停止运行的pod创建替代品。 它不 会为你直接创建的pod执行此操作 。 这些pod只被Kubelet 管理.</p>
</blockquote>

        <h3 id="查看-readinessProbe-healthProbe"   >
          <a href="#查看-readinessProbe-healthProbe" class="heading-link"><i class="fas fa-link"></i></a>查看 readinessProbe, healthProbe</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl edit po -n istio-system istio-ingressgateway-6489d9556d-wjr58</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl edit deployment -n istio-system istio-ingressgateway</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs po/istio-ingressgateway-6489d9556d-wjr58 -n istio-system</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -A</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="affinity"   >
          <a href="#affinity" class="heading-link"><i class="fas fa-link"></i></a>affinity</h3>
      <p>匹配Node标签, Pod部署到哪台机器上.</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-demo</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-demo</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-demo</span><br><span class="line">    spec:</span><br><span class="line">      selector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: web-demo</span><br><span class="line">      replicas: 1</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: web-demo</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: web-demo</span><br><span class="line">            image: hub.mooc.com/kubernetes/web:v1</span><br><span class="line">            ports:</span><br><span class="line">            - containerPort: 8080</span><br><span class="line">          affinity:</span><br><span class="line">            nodeAffinity:		// node亲和性, 要部署到哪台机器，不要部署到哪台机器</span><br><span class="line">              requiredDuringSchedulingIgnoredDuringExecution:	// 必须满足下面条件才会执行调度</span><br><span class="line">                nodeSelectorTerms:		// 数组形式, 下面可以定义多个Terms, 它们之间是或的关系</span><br><span class="line">                - matchExpressions:		// 数组形式, 如果定义多个matchExpressions它们之间是与的关系</span><br><span class="line">                  - key: beta.kubernetes.io/arch	// 节点的label含有的key名字, 这里的是由K8s根据机器自动生成的</span><br><span class="line">                    operator: In</span><br><span class="line">                    values:				// 前提是Node机器有amd64标签K8s才会把容器部署到次Node机器</span><br><span class="line">                    - amd64				// 通过kubectl get nodes NodeName -o yaml进行查看</span><br><span class="line">              preferredDuringSchedulingIgnoredDuringExecution:	// 最好是怎样调度</span><br><span class="line">              - weight: 1				// 权重</span><br><span class="line">                perference:</span><br><span class="line">                  matchExpressions:</span><br><span class="line">                  - key: disktype		// 通过kubectl get nodes --show-labels查看</span><br><span class="line">                    operator: NotIn</span><br><span class="line">                    values:</span><br><span class="line">                    - ssd</span><br><span class="line">            podAffinity:		// Pod亲和性, 想和哪些Pod部署到一台机器, 不想和哪些Pod部署在一台机器</span><br><span class="line">              requiredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">              - labelSelector:</span><br><span class="line">                  matchExpressions:</span><br><span class="line">                  - key: app</span><br><span class="line">                    operator: In		// 要跟app=web-demo的Pod运行在同一个节点上</span><br><span class="line">                    values:</span><br><span class="line">                    - web-demo-node</span><br><span class="line">                topologyKey: kubernetes.io/hostname		// 节点的label名字</span><br><span class="line">              preferredDuringSchedulingIgnoredDuringExecution:</span><br><span class="line">              - weight: 100</span><br><span class="line">                podAffinityTerm:</span><br><span class="line">                  labelSelector:</span><br><span class="line">                    matchExpressions:</span><br><span class="line">                    - key: app</span><br><span class="line">                      operator: In</span><br><span class="line">                      values:</span><br><span class="line">                      - web-demo-node</span><br><span class="line">                  topologyKey: kubernetes.io/hostname</span><br><span class="line">            podAntiAffinity:		// Pod反亲和性, 不想和哪些Pod部署到一台机器, 用法和podAffinity用法完全一样</span><br><span class="line">            pod反亲和性用的很多的是上面的replicas: 的值 &gt;=2 时候会把容器副本分别部署到不同机器</span><br></pre></td></tr></table></div></figure>


        <h3 id="Pod启动停止控制"   >
          <a href="#Pod启动停止控制" class="heading-link"><i class="fas fa-link"></i></a>Pod启动停止控制</h3>
      <p>Pod容器启动时候和停止前所做的事</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: web-demo</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: web-demo</span><br><span class="line">  replicas: 1</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: web-demo</span><br><span class="line">    spec:</span><br><span class="line">      selector:</span><br><span class="line">        matchLabels:</span><br><span class="line">          app: web-demo</span><br><span class="line">      replicas: 1</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: web-demo</span><br><span class="line">        spec:</span><br><span class="line">          containers:</span><br><span class="line">          - name: web-demo</span><br><span class="line">            image: hub.mooc.com/kubernetes/web:v1</span><br><span class="line">            ports:</span><br><span class="line">            - containerPort: 8080</span><br><span class="line">            volumeMounts:</span><br><span class="line">            - name: shared-volume</span><br><span class="line">              mounthPath: /shared-web</span><br><span class="line">            lifecycle:				Pod里容器启动前和停止前要做的事</span><br><span class="line">              postStart:</span><br><span class="line">                exec:</span><br><span class="line">                  command: ["/bin/sh", "-c", "echo web starting ... &gt;&gt; /var/log/messages"]</span><br><span class="line">              preStop:</span><br><span class="line">                exec:</span><br><span class="line">                  command: ["/bin/sh", "-c", "echo web stopping ... &gt;&gt; /var/log/messages &amp;&amp; sleep 3"]</span><br></pre></td></tr></table></div></figure>


        <h2 id="ReplicationController"   >
          <a href="#ReplicationController" class="heading-link"><i class="fas fa-link"></i></a>ReplicationController</h2>
      <p>一个ReplicationController有三个主要部分<br> • label selector ( 标签选择器）， 用于确定ReplicationController作用域中有哪些pod<br> • replica count (副本个数）， 指定应运行的pod 数量<br> • pod template (pod模板）， 用于创建新的pod 副本<br>使用 ReplicationController 的好处<br> •确保一 个 pod (或多个 pod副本）持续运行， 方法是在现有pod 丢失时启动一个新 pod。<br> • 集群节点发生故障时， 它将为故障节 点 上运 行的所有 pod (即受ReplicationController 控制的节点上的那些 pod) 创建替代副本。<br> • 它能轻松实现 pod的水平伸缩 手动和自动都可以</p>

        <h3 id="由RC创建POD"   >
          <a href="#由RC创建POD" class="heading-link"><i class="fas fa-link"></i></a>由RC创建POD</h3>
      <p>kubia-rc.yaml, 内容如下:</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ReplicationController		// 这里的配置定义了ReplicationController(RC)</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia		// ReplicationController 的名字</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3		// pod 实例的目标数目</span><br><span class="line">  selector:			// selector也可以不写，replica 直接根据下面的template模板里的lables标签选择创建POD</span><br><span class="line">    app: kubia		// pod 选择器决定了 RC 的操作对象</span><br><span class="line">  template:			// 从此以下都是创建新 pod 所用的 pod 模板, 与单独创建的pod定义yaml文件内容几乎相同</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubia</span><br><span class="line">        image: luksa/kubia</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br></pre></td></tr></table></div></figure>

<p>创建ReplicationController并由其创建pod</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f kubia-rc.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -o wide</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE   IP          NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kubia-6wnj5   1/1     Running   0          56s   10.44.0.3   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-788p8   1/1     Running   0          56s   10.44.0.2   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-c9kn6   1/1     Running   0          56s   10.44.0.1   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete po kubia-6wnj5</span></span><br><span class="line">NAME          READY   STATUS              RESTARTS   AGE</span><br><span class="line">kubia-6ntgt   0/1     ContainerCreating   0          12s</span><br><span class="line">kubia-6wnj5   1/1     Terminating         0          3m3s</span><br><span class="line">kubia-788p8   1/1     Running             0          3m3s</span><br><span class="line">kubia-c9kn6   1/1     Running             0          3m3s</span><br></pre></td></tr></table></div></figure>

<p>上面重新列出pod会显示四个， 因为你删除的pod己终止， 并且己创建一个新的pod<br>虽然ReplicationController会立即收到删除pod的通知 (API 服务器允许客户端监听资源和资源列表的更改），但这不是它创建替代pod的原因。<br>该通知会触发控制器检查实际的pod数量并采取适当的措施.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po -o wide</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE     IP          NODE       NOMINATED NODE   READINESS GATES</span><br><span class="line">kubia-6ntgt   1/1     Running   0          53s     10.44.0.4   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-788p8   1/1     Running   0          3m44s   10.44.0.2   server02   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubia-c9kn6   1/1     Running   0          3m44s   10.44.0.1   server02   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></div></figure>


        <h3 id="获取有关-ReplicationController-的信息"   >
          <a href="#获取有关-ReplicationController-的信息" class="heading-link"><i class="fas fa-link"></i></a>获取有关 ReplicationController 的信息</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rc -o wide</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rc -o wide -n default		// RC是针对某个namespace下做的副本pod控制</span></span><br><span class="line">NAME    DESIRED   CURRENT   READY   AGE   CONTAINERS   IMAGES        SELECTOR</span><br><span class="line">kubia   3         3         3       17m   kubia        luksa/kubia   app=kubia</span><br><span class="line">获取RC的详细信息</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe rc kubia</span></span><br></pre></td></tr></table></div></figure>

<p>如果你更改了 一个 pod 的标签，使它不再 与 ReplicationController 的标签选择器相匹配 ， 那么该 pod 就变得和其他手动创建的 pod 一样了<br>更改 pod 的标签时， ReplicationController 发现一个 pod 丢失了 ， 并启动一个新的pod替换它.</p>
<p>给其中一个 pod 添加了 type=special 标签，再次列出所有 pod 会显示和以前一样的三个 pod 。 因为从 ReplicationCon位oiler 角度而言， 没发生任何更改.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label pod/kubia-6ntgt <span class="built_in">type</span>=special</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --show-labels</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE   LABELS</span><br><span class="line">kubia-6ntgt   1/1     Running   0          27m   app=kubia,type=special</span><br><span class="line">kubia-788p8   1/1     Running   0          30m   app=kubia</span><br><span class="line">kubia-c9kn6   1/1     Running   0          30m   app=kubia</span><br></pre></td></tr></table></div></figure>

<p>更改app标签该 pod 不再与 RC 的标签选择器相匹配</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label pod/kubia-6ntgt app=foo --overwrite</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod --show-labels</span></span><br><span class="line">NAME          READY   STATUS              RESTARTS   AGE   LABELS</span><br><span class="line">kubia-6ntgt   1/1     Running             0          30m   app=foo,type=special</span><br><span class="line">kubia-788p8   1/1     Running             0          33m   app=kubia</span><br><span class="line">kubia-c9kn6   1/1     Running             0          33m   app=kubia</span><br><span class="line">kubia-dqshz   0/1     ContainerCreating   0          4s    app=kubia</span><br></pre></td></tr></table></div></figure>
<p>使用 -L app 选项在列 中显示 app 标签</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod -L app</span></span><br><span class="line">NAME          READY   STATUS    RESTARTS   AGE     APP</span><br><span class="line">kubia-6ntgt   1/1     Running   0          32m     foo</span><br><span class="line">kubia-788p8   1/1     Running   0          35m     kubia</span><br><span class="line">kubia-c9kn6   1/1     Running   0          35m     kubia</span><br><span class="line">kubia-dqshz   1/1     Running   0          2m17s   kubia</span><br></pre></td></tr></table></div></figure>
<p>可能有一个 bug 导致你的 pod 在特定时间或特定事件后开始出问题。<br>如果你知道某个 pod 发生了故障， 就可以将它从 Replication-Controller 的管理范围中移除， 让控制器将它替换为新 pod, 接着这个 pod 就任你处置了。 完成后删除该pod 即可。</p>

        <h3 id="编辑RC的YAML配置"   >
          <a href="#编辑RC的YAML配置" class="heading-link"><i class="fas fa-link"></i></a>编辑RC的YAML配置</h3>
      <p>用默认文本编辑器中打开ReplicationController的YAML配置，会在/tmp目录生成一个临时yaml文件，退出后/tmp目录下的yaml文件也会删掉<br>如果你想使用nano编辑Kubernetes资源，请执行以下命令（或将其放入 ~/.bashrc或等效文件中）<br>export KUBE_EDITOR=”/usr/bin/nano”</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl edit rc kubia</span><br><span class="line">......</span><br><span class="line"> spec:</span><br><span class="line">   replicas: 3</span><br><span class="line">   selector:</span><br><span class="line">     app: kubia1				 RC selector 修改，需要配合下面的label一起修改</span><br><span class="line">   template:</span><br><span class="line">     metadata:</span><br><span class="line">       creationTimestamp: null</span><br><span class="line">       labels:</span><br><span class="line">         app: kubia1			 Pod label 修改，需要配合上面的 RC selector 一起修改</span><br><span class="line">     spec:</span><br><span class="line">       containers:</span><br><span class="line">       - image: luksa/kubia</span><br><span class="line">         imagePullPolicy: Always</span><br><span class="line">         name: kubia</span><br><span class="line">         ports:</span><br><span class="line">         - containerPort: 8080</span><br><span class="line">           protocol: TCP</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod</span></span><br><span class="line">NAME          READY   STATUS              RESTARTS   AGE   APP</span><br><span class="line">kubia-279wl   0/1     ContainerCreating   0          2s    kubia1</span><br><span class="line">kubia-6ntgt   1/1     Running             0          44m   foo</span><br><span class="line">kubia-788p8   1/1     Running             0          47m   kubia</span><br><span class="line">kubia-c9kn6   1/1     Running             0          47m   kubia</span><br><span class="line">kubia-dqshz   1/1     Running             0          14m   kubia</span><br><span class="line">kubia-m6vml   0/1     Pending             0          2s    kubia1</span><br><span class="line">kubia-xxjqr   0/1     ContainerCreating   0          2s    kubia1</span><br></pre></td></tr></table></div></figure>


        <h3 id="RC-扩容"   >
          <a href="#RC-扩容" class="heading-link"><i class="fas fa-link"></i></a>RC 扩容</h3>
      <p>扩展/缩容 RC管理的pod为5个<br>第一种，commands方式:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl scale rc kubia --replicas=5</span></span><br></pre></td></tr></table></div></figure>
<p>第二种， edit rc yaml文件</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl edit rc kubia</span></span><br><span class="line">......</span><br><span class="line">spec:</span><br><span class="line">  replicas: 5</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>


        <h3 id="删除RC"   >
          <a href="#删除RC" class="heading-link"><i class="fas fa-link"></i></a>删除RC</h3>
      <p>当使用 kubectl delete 删除 ReplicationController 时， 可以通过给命令增加 –cascade= false 选项来保持 pod 的运行.</p>
<pre><code>$ kubectl delete rc kubia --cascade=false</code></pre><p>已经删除了 ReplicationController, 所以这些 pod 独立了， 它们不再被管理。但是你始终可以使用适当的标签选择器创建新的 ReplicationController, 并再次将它们管理起来</p>

        <h2 id="ReplicaSet"   >
          <a href="#ReplicaSet" class="heading-link"><i class="fas fa-link"></i></a>ReplicaSet</h2>
      <blockquote>
<p>最 初， ReplicationController 是用于复制和在异常时重新调度节点的唯 一Kubemetes 组件， 后来又引入了 一个名为 ReplicaSet 的类似资源 。 它是新一代的ReplicationController, 并且将其完全替换掉 (ReplicationController 最终将被弃用）。<br>也就是说从现在起， 你应该始终创建 ReplicaSet 而不是 ReplicationController。 它们几乎完全相同， 所以你不会碰到任何麻烦<br>ReplicaSet 的行为与ReplicationController 完全相同， 但pod 选择器的表达能力更强<br>ReplicationController 都无法仅基千标签名的存在来匹配 pod, 而ReplicaSet 则可以。 例如， ReplicaSet 可匹配所有包含名为 env 的标签的 pod, 无论ReplicaSet 的实际值是什么（可以理解为 env=*)</p>
</blockquote>
<p>kubia-replicaset.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: ReplicaSet</span><br><span class="line">metadata:</span><br><span class="line">  name: kubia</span><br><span class="line">spec:</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: kubia</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: kubia</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: kubia</span><br><span class="line">        image: luksa/kubia</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8080</span><br></pre></td></tr></table></div></figure>
<p>检查replicaset:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get rs</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="matchExpressions选择器"   >
          <a href="#matchExpressions选择器" class="heading-link"><i class="fas fa-link"></i></a>matchExpressions选择器</h3>
      <p>创建个yaml文件<br>kubia-replicaset-matchexpressions.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">selector:</span><br><span class="line">  matchExpressions:</span><br><span class="line">    - key: app</span><br><span class="line">      operator: In</span><br><span class="line">      values:</span><br><span class="line">        - kubia</span><br></pre></td></tr></table></div></figure>
<p>每个表达式都必须 包含一个key, 一个operator (运算符），并且可能还有一个values的列表（取决于 运算符）.<br>• In : Label的值 必须与其中 一个指定的values 匹配。<br>• Notln : Label的值与任何指定的values 不匹配。<br>• Exists : pod 必须包含一个指定名称的标签（值不重要）。使用此运算符时，<br>不应指定 values字段。<br>• DoesNotExist : pod不得包含有指定名称的标签。values属性不得指定<br>如果同时指定matchLabels和matchExpressions, 则所有标签都必须匹配，并且所有表达式必须计算为true以使该pod与选择器匹配.</p>

        <h3 id="查看-replicaset-和-deployment-的详细信息"   >
          <a href="#查看-replicaset-和-deployment-的详细信息" class="heading-link"><i class="fas fa-link"></i></a>查看 replicaset 和 deployment 的详细信息</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe deployment details-v1</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe rs details-v1-6fc55d65c9</span></span><br></pre></td></tr></table></div></figure>


        <h3 id="删除ReplicaSet"   >
          <a href="#删除ReplicaSet" class="heading-link"><i class="fas fa-link"></i></a>删除ReplicaSet</h3>
      <p>删除ReplicaSet会删除所有的pod,这种情况下是需要列出pod来确认.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete rs kubia</span></span><br></pre></td></tr></table></div></figure>

        <h2 id="DaemonSet"   >
          <a href="#DaemonSet" class="heading-link"><i class="fas fa-link"></i></a>DaemonSet</h2>
      <p>如果节点下线， DaemonSet不会在其他地方重新创建pod。 但是， 当将一个新节点添加到集群中时， DaemonSet会立刻部署一个新的pod实例。<br>如果有人无意中删除了 一个 pod ， 那么它也会重新创建 一个新的 pod。<br>与 ReplicaSet一样，DaemonSet 从配置的 pod 模板创建 pod.</p>
<blockquote>
<p>如果节点可以被设置为不可调度的 ， 防止 pod 被部署到节点上. DaemonSet 甚至会将 pod 部署到这些节点上，因为无法调度的属性只会被调度器使用，而 DaemonSet 管理的 pod 则完全绕过调度器. 这是预期的，因为DaemonSet的目的是运行系统服务，即使是在不可调度的节点上，系统服务通常也需要运行.</p>
</blockquote>
<p>给node节点打上label</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl label node server02 disk=ssd</span></span><br></pre></td></tr></table></div></figure>

<p>ssd-monitor-daemonset.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1			// DaemooSet在apps的API组 中，版本是v1</span><br><span class="line">kind: DaemonSet</span><br><span class="line">metadata:</span><br><span class="line">  name: ssd-monitor</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: ssd-monitor</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: ssd-monitor</span><br><span class="line">    spec:</span><br><span class="line">      nodeSelector:			// pod模板包含 会选择有disk=ssd标签的节点 一个节点选择器</span><br><span class="line">        disk: ssd</span><br><span class="line">      containers:</span><br><span class="line">      - name: main</span><br><span class="line">        image: luksa/ssd-monitor</span><br></pre></td></tr></table></div></figure>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f ssd-monitor-daemonset.yaml</span></span><br></pre></td></tr></table></div></figure>

        <h3 id="查看DaemonSet"   >
          <a href="#查看DaemonSet" class="heading-link"><i class="fas fa-link"></i></a>查看DaemonSet</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get ds</span></span><br><span class="line">NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE     CONTAINERS   IMAGES              SELECTOR</span><br><span class="line">ssd-monitor   1         1         1       1            1           disk=ssd        3m29s   main         luksa/ssd-monitor   app=ssd-monitor</span><br></pre></td></tr></table></div></figure>
<p>如果你有多个节点并且其他的节点也加上了同样的标签，将会看到 DaemonSet 在每个节点上都启动 pod.<br>给其中一个节点修改标签disk=hdd, 假设它的硬盘换成磁盘而不是SSD, 那个节点上的pod会如预期中被终止.<br>如果还有其他的 pod在运行， 删除 DaemonSet 也会一起删除这些 pod。</p>

        <h3 id="删除ds"   >
          <a href="#删除ds" class="heading-link"><i class="fas fa-link"></i></a>删除ds</h3>
      <p>删除ds会删除由ds控制schedule到每个节点的pod</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete ds ssd-monitor</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="Job资源"   >
          <a href="#Job资源" class="heading-link"><i class="fas fa-link"></i></a>Job资源</h2>
      <p>Kubemetes 通过 Job 资源提供了对此的支持，它允许你运行一种 pod, 该 pod 在内部进程成功结束时， 不重启容器。<br>一旦任务完成， pod 就被认为处于完成状态.<br>由Job管理的pod会一直被重新安排，直到它们成功完成任务.</p>
<p>exporter.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: batch-job</span><br><span class="line">spec:</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: batch-job</span><br><span class="line">    spec:</span><br><span class="line">      restartPolicy: OnFailure		// 默认为Always,Job pod不能使用默认策略， 因为它们不是要无限期地运行</span><br><span class="line">      containers:</span><br><span class="line">      - name: main</span><br><span class="line">        image: luksa/batch-job		// 运行luksa/batch-job镜像，该镜像调用 一个运行120秒的进程，然后退出</span><br></pre></td></tr></table></div></figure>
<p>需要明确地将重启策略 restartPolicy 设置为 OnFailure 或 Never。 此设置防止容器在完成任务时重新启动</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f exporter.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod</span></span><br><span class="line">NAME                READY   STATUS    RESTARTS   AGE</span><br><span class="line">batch-job-lhnfg     1/1     Running   0          113s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get <span class="built_in">jobs</span></span></span><br><span class="line">NAME        COMPLETIONS   DURATION   AGE</span><br><span class="line">batch-job   0/1           111s       111s</span><br></pre></td></tr></table></div></figure>

<p>等待两三分钟后</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pod</span></span><br><span class="line">NAME                READY   STATUS      RESTARTS   AGE</span><br><span class="line">batch-job-lhnfg     0/1     Completed   0          3m21s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get job</span></span><br><span class="line">NAME        COMPLETIONS   DURATION   AGE</span><br><span class="line">batch-job   1/1           2m41s      3m27s</span><br></pre></td></tr></table></div></figure>
<p>完成后pod未被删除的原因是允许你查阅其日志</p>
<figure class="highlight angelscript"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl logs po/batch-job-lhnfg</span><br><span class="line">Thu May <span class="number">14</span> <span class="number">05</span>:<span class="number">04</span>:<span class="number">38</span> UTC <span class="number">2020</span> Batch job starting</span><br><span class="line">Thu May <span class="number">14</span> <span class="number">05</span>:<span class="number">06</span>:<span class="number">38</span> UTC <span class="number">2020</span> Finished succesfully</span><br></pre></td></tr></table></div></figure>
<p>pod 可以被直接删除， 或者在删除创建它的Job时被删除.<br>作业可以配置为创建多个pod实例，并以并行或串行方式运行它们.<br>在Job配置中设置 completions和parallelism属性来完成的.<br>如果你需要 一个Job运行多次，则可以将comple巨ons设为你希望作业的pod运行多少次.  </p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1</span><br><span class="line">kind: Job</span><br><span class="line">metadata:</span><br><span class="line">  name: multi-completion-batch-job</span><br><span class="line">spec:</span><br><span class="line">  completions: 5	// job将一个接一个地运行五个pod</span><br><span class="line">  parallelism: 2	// 最多两个pod可以并行运行</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: batch-job</span><br><span class="line">    spec:</span><br><span class="line">      restartPolicy: OnFailure</span><br><span class="line">      containers:</span><br><span class="line">      - name: main</span><br><span class="line">        image: luksa/batch-job</span><br></pre></td></tr></table></div></figure>

<p>它最初创建一个pod, 当pod的容器运行完成时，它创建第二个pod, 以此类推，直到五个pod成功完成。<br>如果其中 一个pod发生故障，工作会创建一个新的pod, 所以Job总共可以创建五个以上的pod.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f multi-completion-batch-job.yaml</span></span><br><span class="line">NAME                               READY   STATUS              RESTARTS   AGE</span><br><span class="line">multi-completion-batch-job-8kzd5   0/1     ContainerCreating   0          4s</span><br><span class="line">multi-completion-batch-job-9rnxs   0/1     ContainerCreating   0          4s</span><br></pre></td></tr></table></div></figure>
<p>只要其中 一个pod完成任务，工作将运行下 一个pod, 直到五个pod都成功完成任务.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl get po</span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE</span><br><span class="line">multi-completion-batch-job-8kzd5   1/1     Running   0          2m8s</span><br><span class="line">multi-completion-batch-job-9rnxs   1/1     Running   0          2m8s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get job</span></span><br><span class="line">NAME                         COMPLETIONS   DURATION   AGE</span><br><span class="line">multi-completion-batch-job   0/5           2m13s      2m13s</span><br></pre></td></tr></table></div></figure>
<p>POD虽然创建，但是POD里的进程任务还没有完成，因此job显示任然是0/5没有一个pod任务完成<br>再等待一会时间</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME                               READY   STATUS      RESTARTS   AGE</span><br><span class="line">multi-completion-batch-job-8kzd5   0/1     Completed   0          4m18s</span><br><span class="line">multi-completion-batch-job-9rnxs   0/1     Completed   0          4m18s</span><br><span class="line">multi-completion-batch-job-blntb   1/1     Running     0          107s</span><br><span class="line">multi-completion-batch-job-qhsr5   1/1     Running     0          92s</span><br><span class="line">ssd-monitor-jbhpd                  1/1     Running     0          176m</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get job</span></span><br><span class="line">NAME                         COMPLETIONS   DURATION   AGE</span><br><span class="line">multi-completion-batch-job   2/5           4m16s      4m16s</span><br></pre></td></tr></table></div></figure>
<p>如上显示已经有2个POD任务完成，POD退出.<br>甚至可以在 Job 运行时更改 Job 的 parallelism 属性, command如下，实验环境没有成功使用</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl scale job multi-completion-batch-job --replicas 3</span></span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>通过在 pod 配置中设置 activeDeadlineSeconds 属性，可以限制 pod的时间。如果 pod 运行时间超过此时间， 系统将尝试终止 pod, 并将 Job 标记为失败。<br>通过指定 Job manifest 中的 spec.backoff巨m辽字段， 可以配置 Job在被标记为失败之前可以重试的次数。 如果你没有明确指定它， 则默认为6</p>
</blockquote>

        <h3 id="删除job"   >
          <a href="#删除job" class="heading-link"><i class="fas fa-link"></i></a>删除job</h3>
      <p>删除job时，由job创建的pod也被直接删除</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl delete job multi-completion-batch-job</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="CornJob-资源"   >
          <a href="#CornJob-资源" class="heading-link"><i class="fas fa-link"></i></a>CornJob 资源</h2>
      <blockquote>
<p>批处理任务需要在特定的时间运行，或者在指定的时间间隔内重复运行,在 Linux 和类 UNIX 操作系统中， 这些任务通常被称为 cron 任务。 Kubemetes 也支持这种任务<br>Kubemetes 中的 cron 任务通过创建 CronJob 资源进行配置, 运行任务的时间表以知名的 cron 格式指定<br>时间表从左到右包含以下五个条目<br>• 分钟<br>• 小时<br>• 每月中的第几天<br>• 月<br>• 星期几</p>
</blockquote>
<p>创建资源文件(kube API 对象文件)cronjob.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: batch/v1beta1</span><br><span class="line">kind: CronJob</span><br><span class="line">metadata:</span><br><span class="line">  name: batch-job-every-fifteen-minutes</span><br><span class="line">spec:</span><br><span class="line">  schedule: "0,15,30,55 * * * *"</span><br><span class="line">  startingDeadlineSeconds: 15	// pod最迟必须在预定时间后15秒开始运行， 如果因为任何原因到该启动时间15s后仍不启动，任务将不会运行，并将显示为Failed</span><br><span class="line">  jobTemplate:</span><br><span class="line">    spec:</span><br><span class="line">      template:</span><br><span class="line">        metadata:</span><br><span class="line">          labels:</span><br><span class="line">            app: periodic-batch-job</span><br><span class="line">        spec:</span><br><span class="line">          restartPolicy: OnFailure</span><br><span class="line">          containers:</span><br><span class="line">          - name: main</span><br><span class="line">            image: luksa/batch-job</span><br></pre></td></tr></table></div></figure>

<blockquote>
<p>希望每 15 分钟运行一 次任务因此 schedule 字段的值应该是”0, 15, 30, 45****” 这意味着每小时的 0 、 15 、 30和 45 分钟（第一个星号），每月的每一天（第二个星号），每月（第三个星号）和每周的每一天（第四个星号）。<br>相反，如果你希望每隔 30 分钟运行一 次，但仅在每月的第一天运行，则应将计划设置为 “0,30 * 1 * *”, 并且如果你希望它每个星期天的 3AM 运行，将它设置为 “0 3 * * 0” (最后一个零代表星期天）。</p>
</blockquote>

        <h3 id="查看cronjob"   >
          <a href="#查看cronjob" class="heading-link"><i class="fas fa-link"></i></a>查看cronjob</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get cronjob -o wide</span></span><br><span class="line">NAME                              SCHEDULE             SUSPEND   ACTIVE   LAST SCHEDULE   AGE    CONTAINERS   IMAGES            SELECTOR</span><br><span class="line">batch-job-every-fifteen-minutes   0,15,30,55 * * * *   False     0        18m             112m   main         luksa/batch-job   &lt;none&gt;</span><br></pre></td></tr></table></div></figure>


        <h3 id="cronjob运行状态"   >
          <a href="#cronjob运行状态" class="heading-link"><i class="fas fa-link"></i></a>cronjob运行状态</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME                                               READY   STATUS      RESTARTS   AGE</span><br><span class="line">batch-job-every-fifteen-minutes-1589439300-4v8wd   0/1     Completed   0          36m</span><br><span class="line">batch-job-every-fifteen-minutes-1589439600-99pns   0/1     Completed   0          31m</span><br><span class="line">batch-job-every-fifteen-minutes-1589440500-vs5vm   0/1     Completed   0          16m</span><br><span class="line">batch-job-every-fifteen-minutes-1589441400-52rzb   1/1     Running     0          112s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get job</span></span><br><span class="line">NAME                                         COMPLETIONS   DURATION   AGE</span><br><span class="line">batch-job-every-fifteen-minutes-1589439300   1/1           2m24s      36m</span><br><span class="line">batch-job-every-fifteen-minutes-1589439600   1/1           2m24s      31m</span><br><span class="line">batch-job-every-fifteen-minutes-1589440500   1/1           2m22s      16m</span><br><span class="line">batch-job-every-fifteen-minutes-1589441400   0/1           116s       116s</span><br></pre></td></tr></table></div></figure>
<p>再过一点时间查看</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po</span></span><br><span class="line">NAME                                               READY   STATUS      RESTARTS   AGE</span><br><span class="line">batch-job-every-fifteen-minutes-1589439600-99pns   0/1     Completed   0          32m</span><br><span class="line">batch-job-every-fifteen-minutes-1589440500-vs5vm   0/1     Completed   0          17m</span><br><span class="line">batch-job-every-fifteen-minutes-1589441400-52rzb   0/1     Completed   0          2m34s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get job</span></span><br><span class="line">NAME                                         COMPLETIONS   DURATION   AGE</span><br><span class="line">batch-job-every-fifteen-minutes-1589439600   1/1           2m24s      32m</span><br><span class="line">batch-job-every-fifteen-minutes-1589440500   1/1           2m22s      17m</span><br><span class="line">batch-job-every-fifteen-minutes-1589441400   1/1           2m20s      2m37s</span><br></pre></td></tr></table></div></figure>

<p>总结: CornJob过指定的时间执行一次POD，执行完退出，会保留三个POD和Job记录.</p>

        <h3 id="删除cronjob"   >
          <a href="#删除cronjob" class="heading-link"><i class="fas fa-link"></i></a>删除cronjob</h3>
      <p>运行中的 Job 将不会被终止，不会删除 Job 或 它们的 Pod。为了清理那些 Job 和 Pod，需要列出该 Cron Job 创建的Job，然后删除它们.</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> batch-job-every-fifteen-minutes</span></span><br><span class="line">cronjob.batch "batch-job-every-fifteen-minutes" deleted</span><br></pre></td></tr></table></div></figure>

        <h2 id="secret"   >
          <a href="#secret" class="heading-link"><i class="fas fa-link"></i></a>secret</h2>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get secret -n default</span></span><br><span class="line">NAME                  TYPE                                  DATA   AGE</span><br><span class="line">default-token-gtcjx   kubernetes.io/service-account-token   3      32d</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -o wide</span></span><br><span class="line">NAME                               READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">wordpress-7bfc545758-vtfvm         1/1     Running   5          10d     10.36.0.10   hci-node04   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">wordpress-mysql-764fc64f97-sjnjk   1/1     Running   0          10d     10.36.0.8    hci-node04   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get po/wordpress-7bfc545758-vtfvm -o yaml</span></span><br><span class="line">......</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - env:</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - mountPath: /var/www/html</span><br><span class="line">      name: wordpress-persistent-storage</span><br><span class="line">    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount</span><br><span class="line">      name: default-token-gtcjx</span><br><span class="line">      readOnly: true</span><br><span class="line">......</span><br><span class="line">  volumes:</span><br><span class="line">  - name: wordpress-persistent-storage</span><br><span class="line">    persistentVolumeClaim:</span><br><span class="line">      claimName: wp-pv-claim</span><br><span class="line">  - name: default-token-gtcjx</span><br><span class="line">    secret:</span><br><span class="line">      defaultMode: 420		// 访问权限</span><br><span class="line">      secretName: default-token-gtcjx</span><br><span class="line">......</span><br></pre></td></tr></table></div></figure>

<p>进入wordpress-7bfc545758-vtfvm所在机器的容器里查看/var/run/secrets/kubernetes.io/serviceaccount路径文件</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it daec0458a397 /bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ls /var/run/secrets/kubernetes.io/serviceaccount</span></span><br><span class="line">  ca.crt  namespace  token</span><br></pre></td></tr></table></div></figure>


        <h3 id="创建自己的Secret"   >
          <a href="#创建自己的Secret" class="heading-link"><i class="fas fa-link"></i></a>创建自己的Secret</h3>
      <p>serviceAccount 用来跟Apiserver通信，用来授权, 可以创建自己的Secret<br>编写Secret配置文件 secret.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: dbpass</span><br><span class="line">type: Opaque		// 不透明，浑浊的.</span><br><span class="line">data:</span><br><span class="line">  username: aW1vb2M=		// base64加密的用户名</span><br><span class="line">  passwd: aW1vb2MxMjM=		// base64加密的密码</span><br></pre></td></tr></table></div></figure>

<p>把字符串生成base64很简单，命令如下</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -n imooc | base64	// -n 表示换行</span></span><br><span class="line">aW1vb2M=</span><br></pre></td></tr></table></div></figure>
<p>编写Pod资源配置文件 pod-secret.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-secret</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: springbook-web</span><br><span class="line">    image: hub.mooc.com/kubernetes/springboot-web:v1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: db-secret</span><br><span class="line">      mountPath: /db-secret</span><br><span class="line">      readOnly: true</span><br><span class="line">  volumes:</span><br><span class="line">  - name: db-secret</span><br><span class="line">    projected:</span><br><span class="line">      sources:			// secret 来源</span><br><span class="line">      - secret:</span><br><span class="line">        name: dbpass	// secret 名字</span><br></pre></td></tr></table></div></figure>
<p>生成Pod并进入查看</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> / <span class="comment"># cd /db-secret/</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">  passwd username</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat -n username		// 查看容器里存放的是base64解码过的数据</span></span><br><span class="line">  immoc</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat -n passwd</span></span><br><span class="line">  imooc123</span><br></pre></td></tr></table></div></figure>
<p>可以通过修改secret.yaml文件修改secret账号密码等再$ kubectl apply -f secret.yaml来更改密码.</p>

        <h2 id="Configmap"   >
          <a href="#Configmap" class="heading-link"><i class="fas fa-link"></i></a>Configmap</h2>
      <blockquote>
<p>configmap常用来存储不需要加密的数据, 比如应用的启动参数，一些参数的配置等</p>
</blockquote>
<ul>
<li>第一种向k8s添加很多key value的键值对属性值，就可以用configmap</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> touch game.properties</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> vim game.properties</span></span><br><span class="line">  enemies=aliens</span><br><span class="line">  lives=3</span><br><span class="line">  enemies.cheat=true</span><br><span class="line">  secret.code.allowed=true</span><br><span class="line">  ......</span><br></pre></td></tr></table></div></figure>
<p>配置到K8S里</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create configmap web-game --from-file game.properties</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get cm</span></span><br></pre></td></tr></table></div></figure>

<p>使用configmap, Pod-game.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-game</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: hub.mooc.com/kubernetes/springboot-web:v1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: game</span><br><span class="line">      mountPath: /etc/config/game</span><br><span class="line">      readOnly: true</span><br><span class="line">  volumes:</span><br><span class="line">  - name: game</span><br><span class="line">    configMap:</span><br><span class="line">      name: web-game</span><br></pre></td></tr></table></div></figure>
<p>生成Pod并进入查看</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /etc/config/game</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls</span></span><br><span class="line">  game.properties</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat game.properties</span></span><br><span class="line">  enemies=aliens</span><br><span class="line">  lives=3</span><br><span class="line">  enemies.cheat=true</span><br><span class="line">  secret.code.allowed=true</span><br><span class="line">  ......</span><br></pre></td></tr></table></div></figure>

<p>可以通过kubectl edit 修改configMap账号密码等</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl edit cm web-game -o yaml</span></span><br><span class="line">  enemies.cheat=false	//等等操作</span><br></pre></td></tr></table></div></figure>

<ul>
<li>第二种配置文件方式创建configMap<br>configmap.yaml</li>
</ul>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apeVersion: v1</span><br><span class="line">kind: Configmap</span><br><span class="line">metadata:</span><br><span class="line">  name: configs</span><br><span class="line">data:</span><br><span class="line">  Java_OPTS: -Xms1024m</span><br><span class="line">  LOG_LEVEL: DEBUG</span><br></pre></td></tr></table></div></figure>

<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f configmap.yaml</span></span><br></pre></td></tr></table></div></figure>

<p>编写资源配置文件pod-env.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-env</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: hub.mooc.com/kubernetes/springboot-web:v1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    env:</span><br><span class="line">      - name: LOG_LEVEL_CONFIG</span><br><span class="line">        valueFrom:</span><br><span class="line">          configMapKeyRef:</span><br><span class="line">            name: configs		// 指定configMap名字</span><br><span class="line">            key: LOG_LEVEL		// configs下面的LOG_LEVEL</span><br></pre></td></tr></table></div></figure>
<p>进入容器查看环境变量</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> env | grep LOG</span></span><br><span class="line">  LOG_LEVEL_CONFIG=DEBUG</span><br></pre></td></tr></table></div></figure>
<p>之后次容器就可以通过环境变量获取值</p>
<ul>
<li>第三种 通过命令行方式传进参数<br>也是先跟第二种一样创建configMap资源</li>
</ul>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f configmap.yaml</span></span><br></pre></td></tr></table></div></figure>
<p>编写资源配置文件pod-cmd.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-cmd</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: hub.mooc.com/kubernetes/springboot-web:v1</span><br><span class="line">    command: ["/bin/sh", "-c", "java -jar /springboot-web.jar -DJAVA_OPTS=$(JAVA_OPTS)"]</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    env:</span><br><span class="line">      - name: Java_OPTS</span><br><span class="line">        valueFrom:</span><br><span class="line">          configMapKeyRef:</span><br><span class="line">            name: configs		// 指定configMap名字</span><br><span class="line">            key: Java_OPTS		// configs下面的LOG_LEVEL</span><br></pre></td></tr></table></div></figure>
<p>进入容器查看进程</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ps -ef</span></span><br><span class="line">  java -jar /springboot-web.jar -DJAVA_OPTS=-Xms1024m</span><br></pre></td></tr></table></div></figure>


        <h2 id="downwardAPI"   >
          <a href="#downwardAPI" class="heading-link"><i class="fas fa-link"></i></a>downwardAPI</h2>
      <p>downwardAPI主要作用是在程序中取得Pod对象本身的一些相关信息<br>pod-downwardapi.yaml</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: pod-downwardapi</span><br><span class="line">  labels:</span><br><span class="line">    app： downwardapi</span><br><span class="line">    type: webapp</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: web</span><br><span class="line">    image: hub.mooc.com/kubernetes/springboot-web:v1</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 8080</span><br><span class="line">    volumeMounts:</span><br><span class="line">      - name: podinfo</span><br><span class="line">        mountPath: /etc/podinfo</span><br><span class="line">  volumes:</span><br><span class="line">    - name: podinfo</span><br><span class="line">      projected:</span><br><span class="line">        sources:</span><br><span class="line">        - downwardAPI:</span><br><span class="line">          items:</span><br><span class="line">            - path: "labels"</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.labels</span><br><span class="line">            - path: "name"</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.name</span><br><span class="line">            - path: "namespace"</span><br><span class="line">              fieldRef:</span><br><span class="line">                fieldPath: metadata.namespace</span><br><span class="line">            - path: "mem-request"</span><br><span class="line">              resourceFieldRef:</span><br><span class="line">                containerName: web</span><br><span class="line">                resource: limits.memory</span><br></pre></td></tr></table></div></figure>

<p>进入容器查看文件信息</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> /etc/podinfo</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls -l</span></span><br><span class="line">  labels mem-request name namespace</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat -n labels</span></span><br><span class="line">  app="downwardapi"</span><br><span class="line">  type="webapp"</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat -n namespace</span></span><br><span class="line">  default</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat -n name</span></span><br><span class="line">  pod-downwardapi</span><br></pre></td></tr></table></div></figure>

</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/07_kubernetes_Etcd/">07 Kubernetes Etcd</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">1.2k</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="Etcd"   >
          <a href="#Etcd" class="heading-link"><i class="fas fa-link"></i></a>Etcd</h2>
      <p><span class="exturl"><a class="exturl__link"   href="https://github.com/coreos/etcd"  target="_blank" rel="noopener">https://github.com/coreos/etcd</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<blockquote>
<p>Etcd 是 CoreOS 团队（同时发起了 CoreoS 、 Rocket 等热门项目）发起的一个开源分布式键值仓库项目，可以用于分布式系统中的配置信息管理和服务发现（ service discovery），目前已经被广泛应用到大量开源项目中，包括 Kubernetes 、 CloudFoundry 、 CoreOS Fleet 和Salesforce 等<br>Etcd 是 CoreOS 团队于 2013 年 6 月发起的开源项目，它的目标是构建一个高可用的分布式键值（ key-value ）仓库，遵循 Apache v2许可，基于 Go 语言实现<br>分布式系统中最基本的问题之一就是实现信息的共识，在此基础上才能实现对服务配置信息的管理、服务的发现、更新、同步,Etcd 专门为集群环境设计，采用了更为简洁的 Raft 共识算法＠，同样可以实现数据强一.致性，并支持集群节点状态管理和服务自动发现等.</p>
</blockquote>
<ul>
<li>简单：支持 RESTfulAPI 和 gRPCAPI;</li>
<li>安全： 基于 TLS 方式实现安全连接访问 ；</li>
<li>快速： 支持每秒一万次的并发写操作，超时控制在毫秒量级 ；</li>
<li>可靠： 支持分布式结构 ， 基于 Raft 算法实现一致性<br>
        <img   class="lazyload lazyload-gif"
          src="/images/loading.svg" data-src="etcd_3.PNG"  alt="">
      <br>默认etcd在k8s集群中的 3 or 5 or 7 台node上部署.</li>
</ul>

        <h2 id="下载安装"   >
          <a href="#下载安装" class="heading-link"><i class="fas fa-link"></i></a>下载安装</h2>
      
        <h3 id="二进制文件方式下载"   >
          <a href="#二进制文件方式下载" class="heading-link"><i class="fas fa-link"></i></a>二进制文件方式下载</h3>
      <figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -L https://github.com/coreos/etcd/releases/download/v3.3.1/etcd-v3.3.1-linux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tar xzvf etcd-v3.3.llinux-amd64.tar.gz</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> etcd-v3.3.llinux-amd64.tar.gz | ls</span></span><br><span class="line">其中 etcd 是服务主文件， etcdctl 是提供给用户的命令客户端, 其他都是文档文件.</span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo cp etcd* /usr/<span class="built_in">local</span>/bin/</span></span><br></pre></td></tr></table></div></figure>
<p>Etcd 安装到此完成</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcd --version</span></span><br></pre></td></tr></table></div></figure>
<p>直接执行 Etcd 命令，将启动一个服务节点，监昕在本地的 2379 （客户端请求端口）和 2380 （其他节点连接端口 ） </p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcd</span></span><br></pre></td></tr></table></div></figure>
<p>可以通过 REST API 直接查看集群健康状态：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -L http://127.0.0.1:2379/health</span></span><br><span class="line">&#123;”health”: ”true”&#125;</span><br></pre></td></tr></table></div></figure>
<p>也可以使用自带的 etcdctl 命令进行查看（实际上是封装了阻STAPI 调用）：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcdctl cluster-health</span></span><br></pre></td></tr></table></div></figure>
<p>通过 etcdctl 设置和获取键值, 设置键值对 testkey：”hello world”</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcdctl put testkey <span class="string">"hello world111"</span></span></span><br></pre></td></tr></table></div></figure>
<p>也可以直接通过 HTTP 访问本地 2379 端口的方式来进行操作，例如查看 test key 的值：</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -L -X PUT http://localhost:2379/v2/keys/testkey -d value=<span class="string">"hello world"</span></span></span><br></pre></td></tr></table></div></figure>


        <h3 id="Docker-镜像方式下载"   >
          <a href="#Docker-镜像方式下载" class="heading-link"><i class="fas fa-link"></i></a>Docker 镜像方式下载</h3>
      <p>镜像名称为 quay.io/coreos/etcd:v3.3.1，可以通过下面的命令启动 etcd 服务监听到本地的 2379 和 2380 端口</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker run -p 2379:2379 -p 2380:2380 -v /etc/ssl/certs/:/etc/ssl/certs/ quay.io/coreos/etcd:v3.3.1</span></span><br></pre></td></tr></table></div></figure>


        <h2 id="Etcd-集群管理"   >
          <a href="#Etcd-集群管理" class="heading-link"><i class="fas fa-link"></i></a>Etcd 集群管理</h2>
      <p>启动各个节点上的 etcd 服务, 指向主节点etcd存储</p>

        <h3 id="时钟同步"   >
          <a href="#时钟同步" class="heading-link"><i class="fas fa-link"></i></a>时钟同步</h3>
      <p>对于分布式集群来说，各个节点上的同步时钟十分重要， Etcd 集群需要各个节点时钟差异不超过 ls ，否则可能会导致 Raft 协议的异常.</p>

        <h3 id="节点恢复"   >
          <a href="#节点恢复" class="heading-link"><i class="fas fa-link"></i></a>节点恢复</h3>
      <blockquote>
<p>Etcd 集群中的节点会通过数据目录来存放修改信息和集群配置 。<br>一般来说，当某个节点出现故障时候，本地数据已经过期甚至格式破坏. 如果只是简单地重启进程，容易造成数据的不一致。 这个时候，保险的做法是先通过命令（例如 etcdctlmember rm [member ］）来删除该节点，然后清空数据目录，再重新作为空节点加入.<br>Etcd 提供了－ strict-reconf ig-check 选项，确保当集群状态不稳定时候（例如启动节点数还不够达到 quorum ）拒绝对配置状态的修改</p>
</blockquote>

        <h3 id="重启集群"   >
          <a href="#重启集群" class="heading-link"><i class="fas fa-link"></i></a>重启集群</h3>
      <blockquote>
<p>极端情况下，集群中大部分节点都出现问题，需要重启整个集群.<br>这个时候，最保险的办法是找到一个数据记录完整且比较新的节点，先以它为唯一节点创建新的集群，然后将其他节点一个一个地添加进来，添加过程中注意保证集群的稳定性.</p>
</blockquote>

        <h3 id="K8s证书访问etcd-server"   >
          <a href="#K8s证书访问etcd-server" class="heading-link"><i class="fas fa-link"></i></a>K8s证书访问etcd server</h3>
      <p>Enter etcd pod:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> etcd-hci-node01 -n kube-system -i -t – sh</span></span><br></pre></td></tr></table></div></figure>

<p>Get all key:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get / --prefix --keys-only</span></span><br></pre></td></tr></table></div></figure>

<p>Check the value of the key:</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get /registry/statefulsets/kafka/kafka -w=json</span></span><br></pre></td></tr></table></div></figure>

<p>向etcd数据库添加key,value</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key put testkey <span class="string">"hello world111"</span></span></span><br></pre></td></tr></table></div></figure>

<p>获取添加的key, value值</p>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ETCDCTL_API=3 etcdctl --endpoints 127.0.0.1:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key /etc/kubernetes/pki/etcd/server.key get testkey</span></span><br></pre></td></tr></table></div></figure>








</div></div></article><article class="postlist-item post"><header class="post-header"><h1 class="post-title"><a class="post-title__link" href="/2021/03/13/micro_service/kubernetes/08_kubernetes_samples_problems/">08 Kubernetes samples and problems</a></h1><div class="post-meta"><span class="post-meta-item post-meta-item--createtime"><span class="post-meta-item__icon"><i class="far fa-calendar-plus"></i></span><span class="post-meta-item__info">发表于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--updatetime"><span class="post-meta-item__icon"><i class="far fa-calendar-check"></i></span><span class="post-meta-item__info">更新于</span><span class="post-meta-item__value">2021-03-13</span></span><span class="post-meta-item post-meta-item--wordcount"><span class="post-meta-item__icon"><i class="far fa-file-word"></i></span><span class="post-meta-item__info">字数统计</span><span class="post-meta-item__value">349</span></span></div></header><div class="post-body"><div class="post-excerpt">
        <h2 id="01samples"   >
          <a href="#01samples" class="heading-link"><i class="fas fa-link"></i></a>01samples</h2>
      <p>Reference link:<br><span class="exturl"><a class="exturl__link"   href="https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/"  target="_blank" rel="noopener">https://kubernetes.io/docs/tutorials/stateless-application/expose-external-ip-address/</a><span class="exturl__icon"><i class="fas fa-external-link-alt"></i></span></span></p>
<ol>
<li>填写yaml文件</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> touch test.yaml</span></span><br></pre></td></tr></table></div></figure>

<p>添加如下内容</p>
<figure class="highlight xml"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">labels:</span><br><span class="line">	app.kubernetes.io/name: load-balancer-example</span><br><span class="line">name: hello-world</span><br><span class="line">spec:</span><br><span class="line">replicas: 5</span><br><span class="line">selector:</span><br><span class="line">	matchLabels:</span><br><span class="line">	app.kubernetes.io/name: load-balancer-example</span><br><span class="line">template:</span><br><span class="line">	metadata:</span><br><span class="line">	labels:</span><br><span class="line">		app.kubernetes.io/name: load-balancer-example</span><br><span class="line">	spec:</span><br><span class="line">	containers:</span><br><span class="line">	- image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">		name: hello-world</span><br><span class="line">		ports:</span><br><span class="line">		- containerPort: 8080</span><br></pre></td></tr></table></div></figure>

<ol start="2">
<li>执行yaml生成POD</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f test.yaml</span></span><br></pre></td></tr></table></div></figure>

<ol start="3">
<li>Display information about the Deployment:</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get deployments hello-world</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe deployments hello-world</span></span><br></pre></td></tr></table></div></figure>

<ol start="4">
<li>Display information about your ReplicaSet objects:</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get replicasets</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe replicasets</span></span><br></pre></td></tr></table></div></figure>

<ol start="5">
<li>Create a Service object that exposes the deployment:</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl expose deployment hello-world --<span class="built_in">type</span>=LoadBalancer --name=my-service</span></span><br></pre></td></tr></table></div></figure>

<ol start="6">
<li>Display information about the Service:</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get services my-service</span></span><br><span class="line">NAME         TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">my-service   LoadBalancer   10.103.40.210   &lt;pending&gt;     8080:30972/TCP   16m</span><br></pre></td></tr></table></div></figure>

<ol start="7">
<li>Display detailed information about the Service:</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe services my-service</span></span><br><span class="line">Name:                     my-service</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   app.kubernetes.io/name=load-balancer-example</span><br><span class="line">Annotations:              &lt;none&gt;</span><br><span class="line">Selector:                 app.kubernetes.io/name=load-balancer-example</span><br><span class="line">Type:                     LoadBalancer</span><br><span class="line">IP:                       10.103.40.210</span><br><span class="line">Port:                     &lt;unset&gt;  8080/TCP</span><br><span class="line">TargetPort:               8080/TCP</span><br><span class="line">NodePort:                 &lt;unset&gt;  30972/TCP</span><br><span class="line">Endpoints:                10.44.0.2:8080,10.44.0.3:8080,10.44.0.4:8080 + 2 more...</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></div></figure>
<ol start="8">
<li>通过 NODE-IP + NodePort 访问<br>如自己cubic中的开发机IP是10.239.140.186<ul>
<li>第一种访问方式:</li>
</ul>
</li>
</ol>
<figure class="highlight shell"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl http://10.239.140.186:30972</span></span><br><span class="line">Hello Kubernetes!</span><br></pre></td></tr></table></div></figure>

<ul>
<li>第二种访问方式:</li>
</ul>
<figure class="highlight groovy"><div class="table-container"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">浏览器输入<span class="string">http:</span><span class="comment">//10.239.140.186:30972 也能正常显示 Hello Kubernetes!</span></span><br></pre></td></tr></table></div></figure>
<p>但是公司lab实验室的开发机同样用上面方式部署之后，无法通过 NODE-IP + NodePort 访问， 问题估计跟网络有关</p>
</div></div></article></section><nav class="paginator"><div class="paginator-inner"><a class="extend prev" rel="prev" href="/page/6/"><i class="fas fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><span class="space">&hellip;</span><a class="page-number" href="/page/29/">29</a><a class="extend next" rel="next" href="/page/8/"><i class="fas fa-angle-right"></i></a></div></nav></div></div><div class="sidebar-wrap" id="sidebar-wrap"><aside class="sidebar" id="sidebar"><section class="sidebar-toc hide"></section><!-- ov = overview--><section class="sidebar-ov"><div class="sidebar-ov-author"><div class="sidebar-ov-author__avatar"><img class="sidebar-ov-author__avatar_img" src="/images/icons/stun-logo.svg" alt="avatar"></div><p class="sidebar-ov-author__text">Kung-Fu-Master</p></div><div class="sidebar-ov-social"><a class="sidebar-ov-social-item" href="https://github.com/" target="_blank" rel="noopener" data-popover="Github" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-github"></i></span></a><a class="sidebar-ov-social-item" href="https://plus.google.com/" target="_blank" rel="noopener" data-popover="Google" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-google"></i></span></a><a class="sidebar-ov-social-item" href="https://twitter.com/" target="_blank" rel="noopener" data-popover="Twitter" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-twitter"></i></span></a><a class="sidebar-ov-social-item" href="https://youtube.com/" target="_blank" rel="noopener" data-popover="Youtube" data-popover-pos="up"><span class="sidebar-ov-social-item__icon"><i class="fab fa-youtube"></i></span></a></div><div class="sidebar-ov-state"><a class="sidebar-ov-state-item sidebar-ov-state-item--posts" href="/archives/"><div class="sidebar-ov-state-item__count">290</div><div class="sidebar-ov-state-item__name">归档</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--categories" href="/categories/"><div class="sidebar-ov-state-item__count">40</div><div class="sidebar-ov-state-item__name">分类</div></a><a class="sidebar-ov-state-item sidebar-ov-state-item--tags" href="/tags/"><div class="sidebar-ov-state-item__count">12</div><div class="sidebar-ov-state-item__name">标签</div></a></div><div class="sidebar-ov-cc"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en" target="_blank" rel="noopener" data-popover="知识共享许可协议" data-popover-pos="up"><img src="/images/cc-by-nc-sa.svg"></a></div></section></aside></div><div class="clearfix"></div></div></main><footer class="footer" id="footer"><div class="footer-inner"><div><span>Copyright © 2021</span><span class="footer__icon"><i class="fas fa-heart"></i></span><span>Kung-Fu-Master</span></div><div><span>由 <a href="http://hexo.io/" title="Hexo" target="_blank" rel="noopener">Hexo</a> 强力驱动</span><span> v4.2.0</span><span class="footer__devider">|</span><span>主题 - <a href="https://github.com/liuyib/hexo-theme-stun/" title="Stun" target="_blank" rel="noopener">Stun</a></span><span> v2.6.1</span></div></div></footer><div class="loading-bar" id="loading-bar"><div class="loading-bar__progress"></div></div><div class="back2top" id="back2top"><span class="back2top__icon"><i class="fas fa-rocket"></i></span></div></div><div class="search-mask"></div><div class="search-popup"><span class="search-close"></span><div class="search-input"><input placeholder="搜索文章（支持多关键词，请用空格分隔）"></div><div class="search-results"></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@v3.4.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@1.5.2/velocity.ui.min.js"></script><script src="https://cdn.jsdelivr.net/npm/lazyload@2.0.0-rc.2/lazyload.min.js"></script><script>function initSearch() {
  var isXML = true;
  var search_path = 'search.json';

  if (!search_path) {
    search_path = 'search.xml';
  } else if (/json$/i.test(search_path)) {
    isXML = false;
  }

  var path = '/' + search_path;
  $.ajax({
    url: path,
    dataType: isXML ? 'xml' : 'json',
    async: true,
    success: function (res) {
      var datas = isXML ? $('entry', res).map(function () {
        // 将 XML 转为 JSON
        return {
          title: $('title', this).text(),
          content: $('content', this).text(),
          url: $('url', this).text()
        };
      }).get() : res;
      var $input = $('.search-input input');
      var $result = $('.search-results');
      // 搜索对象（标题、内容）的权重，影响显示顺序
      var WEIGHT = { title: 100, content: 1 };
      var searchPost = function () {
        var searchText = $input.val().toLowerCase().trim();
        // 根据空白字符分隔关键字
        var keywords = searchText.split(/[\s]+/);
        // 搜索结果
        var matchPosts = [];

        // 有多个关键字时，将原文字整个保存下来
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        // 防止未输入字符时搜索
        if (searchText.length > 0) {
          datas.forEach(function (data) {
            var isMatch  = false;
            // 没有标题的文章使用预设的 i18n 变量代替
            var title = (data.title && data.title.trim()) || '[ 文章无标题 ]';
            var titleLower = title && title.toLowerCase();
            // 删除 HTML 标签 和 所有空白字符
            var content = data.content && data.content.replace(/<[^>]+>/g, '');
            var contentLower = content && content.toLowerCase();
            // 删除重复的 /
            var postURL = data.url && decodeURI(data.url).replace(/\/{2,}/g, '/');
            // 标题中匹配到的关键词
            var titleHitSlice = [];
            // 内容中匹配到的关键词
            var contentHitSlice = [];

            keywords.forEach(function (keyword) {
              /**
              * 获取匹配的关键词的索引
              * @param {String} keyword 要匹配的关键字
              * @param {String} text 原文字
              * @param {Boolean} caseSensitive 是否区分大小写
              * @param {Number} weight 匹配对象的权重。权重大的优先显示
              * @return {Array}
              */
              function getIndexByword (word, text, caseSensitive, weight) {
                if (!word || !text) {
                  return [];
                };

                var startIndex = 0; // 每次匹配的开始索引
                var index = -1;     // 匹配到的索引值
                var result = [];    // 匹配结果

                if (!caseSensitive) {
                  word = word.toLowerCase();
                  text = text.toLowerCase();
                }

                while((index = text.indexOf(word, startIndex)) !== -1) {
                  var hasMatch = false;
                  // 索引位置相同的关键词，保留长度较长的
                  titleHitSlice.forEach(function (hit) {
                    if (hit.index === index && hit.word.length < word.length) {
                      hit.word = word;
                      hasMatch = true;
                    }
                  });
                  startIndex = index + word.length;
                  !hasMatch && result.push({ index: index, word: word, weight: weight });
                }
                return result;
              }
              titleHitSlice = titleHitSlice.concat(getIndexByword(keyword, titleLower, false, WEIGHT.title));
              contentHitSlice = contentHitSlice.concat(getIndexByword(keyword, contentLower, false, WEIGHT.content));
            });

            var hitTitle = titleHitSlice.length;
            var hitContent = contentHitSlice.length;

            if (hitTitle > 0 || hitContent > 0) {
              isMatch = true;
            }
            if (isMatch) {
              ;[titleHitSlice, contentHitSlice].forEach(function (hit) {
                // 按照匹配文字的索引的递增顺序排序
                hit.sort(function (left, right) {
                  return left.index - right.index;
                });
              });
              /**
              * 给文本中匹配到的关键词添加标记，从而进行高亮显示
              * @param {String} text 原文本
              * @param {Array} hitSlice 匹配项的索引信息
              * @param {Number} start 开始索引
              * @param {Number} end 结束索引
              * @return {String}
              */
              function highlightKeyword (text, hitSlice, start, end) {
                if (!text || !hitSlice || !hitSlice.length) {
                  return;
                }

                var result = '';
                var startIndex = start;
                var endIndex = end;
                hitSlice.forEach(function (hit) {
                  if (hit.index < startIndex) {
                    return;
                  }

                  var hitWordEnd = hit.index + hit.word.length;
                  result += text.slice(startIndex, hit.index);
                  result += '<b>' + text.slice(hit.index, hitWordEnd) + '</b>';
                  startIndex = hitWordEnd;
                });
                result += text.slice(startIndex, endIndex);
                return result;
              }

              var postData = {};
              // 文章总的搜索权重
              var postWeight = titleHitSlice.length * WEIGHT.title + contentHitSlice.length * WEIGHT.content;
              // 标记匹配关键词后的标题
              var postTitle = highlightKeyword(title, titleHitSlice, 0, title.length) || title;
              // 标记匹配关键词后的内容
              var postContent;
              // 显示内容的长度
              var SHOW_WORD_LENGTH = 200;
              // 命中关键词前的字符显示长度
              var SHOW_WORD_FRONT_LENGTH = 20;
              var SHOW_WORD_END_LENGTH = SHOW_WORD_LENGTH - SHOW_WORD_FRONT_LENGTH;

              // 截取匹配的第一个字符，前后共 200 个字符来显示
              if (contentHitSlice.length > 0) {
                var firstIndex = contentHitSlice[0].index;
                var start = firstIndex > SHOW_WORD_FRONT_LENGTH ? firstIndex - SHOW_WORD_FRONT_LENGTH : 0;
                var end = firstIndex + SHOW_WORD_END_LENGTH;
                postContent = highlightKeyword(content, contentHitSlice, start, end);
              } else { // 未匹配到内容，直接截取前 200 个字符来显示
                postContent = content.slice(0, SHOW_WORD_LENGTH);
              }
              postData.title = postTitle;
              postData.content = postContent;
              postData.url = postURL;
              postData.weight = postWeight;
              matchPosts.push(postData);
            }
          });
        }

        var resultInnerHtml = '';
        if (matchPosts.length) {
          // 按权重递增的顺序排序，使权重大的优先显示
          matchPosts.sort(function (left, right) {
            return right.weight - left.weight;
          });
          resultInnerHtml += '<ul>';
          matchPosts.forEach(function (post) {
            resultInnerHtml += '<li><a class="search-results-title" href="' + post.url + '">';
            resultInnerHtml += post.title;
            resultInnerHtml += '</a><div class="search-results-content">';
            resultInnerHtml += post.content;
            resultInnerHtml += '</div></li>';
          });
          resultInnerHtml += '</ul>';
        } else {
          resultInnerHtml += '<div class="search-results-none"><i class="far fa-meh"></i></div>';
        }
        $result.html(resultInnerHtml);
      };
      $input.on('input', searchPost);
      $input.on('keyup', function (e) {
        if (e.keyCode === Stun.utils.codeToKeyCode('Enter')) {
          searchPost();
        }
      });
    }
  });
}

function closeSearch () {
  $('body').css({ overflow: 'auto' });
  $('.search-popup').css({ display: 'none' });
  $('.search-mask').css({ display: 'none' });
}

window.addEventListener('DOMContentLoaded', function () {
  Stun.utils.pjaxReloadLocalSearch = function () {
    $('.header-nav-search').on('click', function (e) {
      e.stopPropagation();
      $('body').css('overflow', 'hidden');
      $('.search-popup')
        .velocity('stop')
        .velocity('transition.expandIn', {
          duration: 300,
          complete: function () {
            $('.search-popup input').focus();
          }
        });
      $('.search-mask')
        .velocity('stop')
        .velocity('transition.fadeIn', {
          duration: 300
        });

      initSearch();
    });
    $('.search-mask, .search-close').on('click', function () {
      closeSearch();
    });
    $(document).on('keydown', function (e) {
      // Escape <=> 27
      if (e.keyCode === Stun.utils.codeToKeyCode('Escape')) {
        closeSearch();
      }
    });
  };

  Stun.utils.pjaxReloadLocalSearch();
}, false);

function safeOpenUrl(url) {
  var newTab = window.open();
  newTab.opener = null;
  newTab.location = url;
}

function extSearch(engine) {
  var engines = {
    google: 'https://www.google.com/search?q=',
    bing: 'https://cn.bing.com/search?q=',
    baidu: 'https://www.baidu.com/s?ie=UTF-8&wd=',
  };
  var host = window.location.host;
  var query = $('.search-input input').val().toLowerCase().trim();
  var uri = engines[engine] + query + ' site:' + host;

  if (query) {
    safeOpenUrl(uri);
  } else {
    Stun.utils.popAlert('warning', '请输入字符');
  }
}

var assistSearchList = window.CONFIG.assistSearch;

if (Array.isArray(assistSearchList)) {
  assistSearchList.forEach(function (name) {
    document.querySelector('.search-btns-item--' + name).addEventListener('click', function () {
      extSearch(name);
    }, false);
  });
}</script><script src="/js/utils.js?v=2.6.1"></script><script src="/js/stun-boot.js?v=2.6.1"></script><script src="/js/scroll.js?v=2.6.1"></script><script src="/js/header.js?v=2.6.1"></script><script src="/js/sidebar.js?v=2.6.1"></script><script type="application/json" src="/search.json"></script></body></html>